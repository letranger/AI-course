<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-07-04 Mon 15:21 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>自然語言處理(Natural Language Processing, NLP)</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/white.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">自然語言處理(Natural Language Processing, NLP)</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org741f8cc">1. NLP Introduction</a>
<ul>
<li><a href="#orgcb857ef">1.1. 自然語言處理有哪些技術</a></li>
<li><a href="#orgbd70b21">1.2. 自然語言處理核心問題是什麼</a></li>
<li><a href="#org0e03410">1.3. 自然語言處理有哪些應用方向</a></li>
</ul>
</li>
<li><a href="#org7b3edb4">2. NLP 資料預處理</a>
<ul>
<li><a href="#org1a68422">2.1. IMDB</a></li>
<li><a href="#orgd23f792">2.2. Tokenization</a></li>
<li><a href="#org3c1392c">2.3. Sequence padding</a></li>
<li><a href="#org5c906f8">2.4. one-hot encoding</a></li>
<li><a href="#orga3e7a7d">2.5. Multi-hot encoding</a></li>
<li><a href="#org683d375">2.6. 資料預處理</a></li>
<li><a href="#orgd12b3f2">2.7. 共同部分</a></li>
<li><a href="#org746e779">2.8. Define Tokenization, Stop-word and Punctuation Removal Functions</a></li>
<li><a href="#org8f8062b">2.9. Tokenization</a></li>
<li><a href="#orgcbb5dae">2.10. Stop-word removal</a></li>
</ul>
</li>
<li><a href="#org6b57324">3. 資料預處理實務</a>
<ul>
<li><a href="#org0f5bf4d">3.1. IMDB 介簡</a></li>
<li><a href="#orgc39dda4">3.2. 查看 IMDB 資料</a></li>
<li><a href="#orgdf54f2d">3.3. 資料預處理</a></li>
<li><a href="#org0f7a0d9">3.4. Define Tokenization, Stop-word and Punctuation Removal Functions</a></li>
<li><a href="#org4e27c9e">3.5. Stop-word removal</a></li>
<li><a href="#orgd1952a9">3.6. Bag-of-words model</a></li>
<li><a href="#org509b56a">3.7. Putting It All Together To Assemble Dataset</a></li>
</ul>
</li>
<li><a href="#org7644c94">4. 自然語言處理學習路線</a></li>
<li><a href="#orgcbbff97">5. 如何表示自然語言與字詞(數學化)</a>
<ul>
<li><a href="#org0286423">5.1. 詞庫</a></li>
<li><a href="#orgc74abfd">5.2. 計數</a></li>
<li><a href="#orgecae144">5.3. Python事前準備</a></li>
<li><a href="#orgba26ee3">5.4. 字詞的分散式表示</a></li>
<li><a href="#orgb16d079">5.5. 計算向量間的相似度</a></li>
<li><a href="#orgc050b8f">5.6. 相似詞排名</a></li>
<li><a href="#orgca529fa">5.7. Pointwise Mutual Information (PMI)</a></li>
<li><a href="#org5b87bc8">5.8. 降維</a></li>
<li><a href="#orge5b1450">5.9. 使用正式的語料庫試試: PTB</a></li>
<li><a href="#org1f0d6bb">5.10. 詞袋模型(Bag-of-words model)</a></li>
</ul>
</li>
<li><a href="#org019300c">6. Representing text as numbers</a>
<ul>
<li><a href="#org0c9b439">6.1. example</a></li>
<li><a href="#org9196c94">6.2. Word embedding</a></li>
</ul>
</li>
<li><a href="#orgd45f0d7">7. 推論: word2vec</a>
<ul>
<li><a href="#org937177d">7.1. 計數 v.s. 推論</a></li>
<li><a href="#org63fba5b">7.2. 推論</a></li>
<li><a href="#org258afa8">7.3. 在類神經網網路中表達字詞</a></li>
<li><a href="#orga1e437f">7.4. CBOW模型</a></li>
<li><a href="#org25a721b">7.5. CBOW模型的學習</a></li>
</ul>
</li>
<li><a href="#org3a61432">8. word2vec的學習</a>
<ul>
<li><a href="#org1e26c54">8.1. 先找出上下文與目標對象</a></li>
<li><a href="#orgafcf7fc">8.2. 將上下文與目標以one-hot encoding編碼</a></li>
<li><a href="#org80c6faf">8.3. 執行CBOW模型</a></li>
</ul>
</li>
<li><a href="#orgd6b6509">9. NLP實作: Fake News Classification</a>
<ul>
<li><a href="#org4bc1edd">9.1. 資料集</a></li>
<li><a href="#org7e74017">9.2. 思路</a></li>
<li><a href="#orge030566">9.3. 資料前處理：讓機器能夠處理文字</a></li>
<li><a href="#orgd9cf56b">9.4. RNN</a></li>
<li><a href="#org5ca09a8">9.5. 詞向量：將詞彙表達成有意義的向量¶</a></li>
<li><a href="#orge7a3e01">9.6. 一個神經網路，兩個新聞標題</a></li>
<li><a href="#org4e087a5">9.7. 建立模型</a></li>
<li><a href="#org7bc49b4">9.8. 進行預測並提交結果</a></li>
</ul>
</li>
<li><a href="#org74d739f">10. 3 門推薦的線上課程</a></li>
<li><a href="#org7390409">11. 工具</a>
<ul>
<li><a href="#org5469080">11.1. 中研院CkipTagger中文處理工具</a></li>
<li><a href="#org4bd992d">11.2. 結巴Jieba1</a></li>
<li><a href="#org30c8b8c">11.3. NLTK</a></li>
</ul>
</li>
<li><a href="#org23cdcb5">12. 語料集</a>
<ul>
<li><a href="#org1b2794b">12.1. NLP-Chinese-Corpus</a></li>
<li><a href="#orgb9f1bf6">12.2. PTT-Gossiping-Corpus</a></li>
<li><a href="#orgc241b58">12.3. Wiki-corpus</a></li>
<li><a href="#org8ca2f70">12.4. Tencent AI LAB</a></li>
<li><a href="#org5cea815">12.5. BERT</a></li>
</ul>
</li>
<li><a href="#org235fe07">13. NLP 應用</a>
<ul>
<li><a href="#org2e42a76">13.1. 領域</a></li>
<li><a href="#orgde68e67">13.2. 範例</a></li>
</ul>
</li>
<li><a href="#org2c43469">14. 文本分類:IMDB 電影評論</a></li>
<li><a href="#orgcae434e">15. 官方範例</a>
<ul>
<li><a href="#org6699b3c">15.1. 文本分類 Text classification::電影評論</a></li>
</ul>
</li>
<li><a href="#org8b3bcc6">16. 練習</a>
<ul>
<li><a href="#org20b1a65">16.1. SPAM email detection 垃圾郵件偵測</a></li>
<li><a href="#org20c263d">16.2. fake News detection 假新聞偵測</a></li>
</ul>
</li>
<li><a href="#org1442f78">17. 教學資源</a>
<ul>
<li><a href="#org1f3c942">17.1. paper</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-org741f8cc" class="outline-2">
<h2 id="org741f8cc"><span class="section-number-2">1.</span> NLP Introduction</h2>
<div class="outline-text-2" id="text-1">
<p>
NLP，全名 Natural Language Processing（自然語言處理），是一門集電腦科學，人工智慧，語言學三者於一身的交叉性學科。其終極研究目標是讓電腦能夠處理甚至是“理解”人類的自然語言，進而説明人類解決一些現實生活中遇到的實際問題。這裡的語言“理解”是一個很抽象也很哲學的概念<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。<br />
</p>

<p>
在 NLP 中，我們將對語言的“理解”定義為是學習一個能夠解決具體問題的複雜函數的過程。<br />
</p>

<p>
對 NLP 的研究通常在 5 個 Level 上進行<sup><a id="fnr.1.100" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>：<br />
</p>
<ol class="org-ol">
<li>語音／文本分析：包括語言識別技術、OCR 技術、分詞技術等<br /></li>
<li>詞形分析：例如分析一個 word 的前尾碼、詞根等<br /></li>
<li>語法分析：從語法結構上分析一個句子的構成<br /></li>
<li>語義分析：理解一個句子或單詞的語義<br /></li>
<li>篇章分析：理解一段篇章的含義<br /></li>
</ol>
<p>
注意，雖然這 5 個 Level 在研究的物件和難度上是一種遞進的關係，但對這 5 個 Level 的研究並不一定是連續的——例如：我們可以跳過對一個句子語法結構的分析而直接去理解句子的語義。<br />
</p>
</div>

<div id="outline-container-orgcb857ef" class="outline-3">
<h3 id="orgcb857ef"><span class="section-number-3">1.1.</span> 自然語言處理有哪些技術</h3>
<div class="outline-text-3" id="text-1-1">
<p>
大致包括如下技術<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>:<br />
</p>
<ol class="org-ol">
<li>分詞(Word Segmentation或Word Breaker，WB)<br />
在英文文本當中每個詞之間都有間隔好分，但在中文文本當中一句話之間每個詞是沒有間隔的，所以需要對一個句子當中每個字進行切分，句子的基本語義單元就變成了詞，這就是分詞任務。<br /></li>
<li>句法分析（Parsing）<br />
句法分析指的是將句子中每個部分的組塊(也就是每個詞、字的歸屬類)標註出來。<br />
<ul class="org-ul">
<li>組塊分析:標出句子的短語塊,如“This is a dog(NP)” 超級標籤分析:給每個句子加上超級標籤，超級標籤是一個樹形結構圖<br /></li>
<li>成分句法分析:分析句子成分，給出一顆由終結符和非終結符構成的成分句法樹<br /></li>
<li>依存句法分析:分析句中詞的依存關係，給出一顆由詞語依存關係構成的依存句法樹。<br /></li>
</ul></li>
<li>信息抽取（Information Extraction，IE）：命名實體識別和關係抽取（Named Entity Recognition &amp; Relation Extraction，NER):我們從一段文本中抽取關鍵信息即從無結構的文本中抽取結構化的信息，<br /></li>
<li>詞性標註（Part Of Speech Tagging，POS）:對詞語的詞性進行標註<br /></li>
<li>指代消解（Coreference Resolution）:消除一些對文本處理沒有意義的指代名詞，減輕程序對語言的處理。<br /></li>
<li>詞義消歧（Word Sense Disambiguation，WSD）:一個詞他可能會有歧義，該任務是用來消除歧義的。<br /></li>
<li>機器翻譯（Machine Translation，MT）:要實現文本的自動翻譯<br /></li>
<li>自動文摘(Automatic Summarization):摘要是一大段文字，我們需要將裏面的梗提取出來然後縮短方便閱讀或方便提取信息。<br /></li>
<li>問答系統（Question Answering）:你提出一個問題機器給予你準確的答案<br /></li>
<li>OCR:也屬於視覺模塊內容，將圖片當中的文字通過機器識別圖像翻譯成文本形式<br /></li>
<li>信息檢索(Information Retrieval，IR):用戶進行信息查詢和獲取的主要方式，是查找信息的方法和手段。<br /></li>
</ol>
</div>
</div>
<div id="outline-container-orgbd70b21" class="outline-3">
<h3 id="orgbd70b21"><span class="section-number-3">1.2.</span> 自然語言處理核心問題是什麼<sup><a id="fnr.2.100" class="footref" href="#fn.2" role="doc-backlink">2</a></sup></h3>
<div class="outline-text-3" id="text-1-2">
<ul class="org-ul">
<li>文本分類<br /></li>
<li>關鍵詞提取<br /></li>
<li>情感分析<br /></li>
<li>語義消歧<br /></li>
<li>主題模型<br /></li>
<li>機器翻譯<br /></li>
<li>問題問答<br /></li>
<li>漢語分詞<br /></li>
<li>垂直領域的對話機器人<br /></li>
</ul>
</div>
</div>
<div id="outline-container-org0e03410" class="outline-3">
<h3 id="org0e03410"><span class="section-number-3">1.3.</span> 自然語言處理有哪些應用方向<sup><a id="fnr.2.100" class="footref" href="#fn.2" role="doc-backlink">2</a></sup></h3>
<div class="outline-text-3" id="text-1-3">
<ul class="org-ul">
<li>搜索引擎<br /></li>
<li>文本主題/標籤分類<br /></li>
<li>文本創作與生成<br /></li>
<li>機器翻譯<br /></li>
<li>情感分析<br /></li>
<li>輿情監控<br /></li>
<li>語音識別系統<br /></li>
<li>對話機器人<br /></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org7b3edb4" class="outline-2">
<h2 id="org7b3edb4"><span class="section-number-2">2.</span> NLP 資料預處理</h2>
<div class="outline-text-2" id="text-2">
<p>
情緒分析 (sentiment analysis) 又稱為意見探勘 (opinion mining). 是使用 &ldquo;自然語言處理&rdquo;, 文字分析等方法, 找出作者某些話題上的態度, 情感, 評價或情緒.<br />
</p>

<p>
情緒分析的商業價值, 可以提早得知顧客對公司或產品觀感, 以調整銷售策略方向.<br />
</p>

<p>
IMDb 網路資料庫 (Internet Movie Database), 是一個電影相關的線上資料庫. IMDb 開始於 1990 年, 自 1998 年起成為亞馬遜旗下的網站, 至今已經累積大量的電影資訊. IMDb 共收錄了四百多萬作品資料.<br />
</p>

<p>
IMDb 資料集共有 50,000 筆 &ldquo;影評文字&rdquo;, 分為訓練資料與測試資料各 25,000 筆,<br />
每一筆 &ldquo;影評文字&rdquo; 都被標記成 &ldquo;正面評價&rdquo; 或 &ldquo;負面評價&rdquo;.<br />
</p>

<p>
我們希望能建立一個模型, 經過大量 &ldquo;影評文字&rdquo; 訓練後, 此模型能用於預測 &ldquo;影評文字&rdquo; 是 &ldquo;正面評價&rdquo; 或 &ldquo;負面評價&rdquo;.<br />
</p>

<div id="org579cd84" class="figure">
<p><img src="images/TrainingPredict.jpg" alt="TrainingPredict.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 1: </span>資料前處理</p>
</div>
</div>
<div id="outline-container-org1a68422" class="outline-3">
<h3 id="org1a68422"><span class="section-number-3">2.1.</span> IMDB</h3>
<div class="outline-text-3" id="text-2-1">
<ul class="org-ul">
<li>num_words=10000 意味著只保留訓練集中最常出現的前 10000 個詞，不經常出現的單詞被拋棄—最終所有評論的維度保持相同。<br /></li>
<li>變數 train_data,test_data 是電影評論的列表，每條評論由數字(對應單詞在詞典中出現的位置下標)列表組成。<br /></li>
<li>train_labels,test_labels 是 0,1 列表，0負面評論，1表示正面評論。<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr">2: </span>
<span class="linenr">3: </span>(train_data,train_labels),(<span style="color: #dcaeea;">test_data</span>,<span style="color: #dcaeea;">test_labels</span>) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd23f792" class="outline-3">
<h3 id="orgd23f792"><span class="section-number-3">2.2.</span> Tokenization</h3>
<div class="outline-text-3" id="text-2-2">
<p>
Token 法就是將所有會用到的 Token 做成一個詞彙對照表（或稱為詞彙字典)，並用它來記錄每一個詞彙和其對應的數值，這樣我們就能照著這個對照表來將詞轉成數值。<br />
</p>
<ul class="org-ul">
<li>fit_on_text(texts): 使用一系列文檔來生成 token 詞典，texts 為 list 類，每個元素為一個文檔。<br /></li>
<li>texts_to_sequences(texts): 將多個文檔轉換為 word 下標的向量形式,shape 為[len(texts)，len(text)] &#x2013; (文檔數，每條文檔的長度)<br /></li>
<li>Tokenizer 對象.word_counts: 獲取字典{單詞:單詞出現次數}，將單詞（字符串）映射為它們在訓練期間出現的次數。僅在調用 fit_on_texts 之後設置。<br /></li>
<li>Tokenizer 對象.word_docs: 獲取字典{單詞:單詞出現次數}，將單詞（字符串）映射為它們在訓練期間所出現的文檔或文本的數量。僅在調用 fit_on_texts 之後設置。<br /></li>
<li>Tokenizer 對象.word_index: 獲取字典{單詞:單詞在字典中的索引值}，將單詞（字符串）映射為它們的排名或者索引。僅在調用 fit_on_texts 之後設置。<br /></li>
<li>Tokenizer 對象.index_word[index]: 獲取單詞（字符串），根據傳入單詞所在字典中的索引值來獲取該單詞。<br /></li>
<li>Tokenizer 對象.word_index[word]: 獲取單詞在字典中的索引值，根據傳入單詞，獲取單詞所在字典中的索引值。<br /></li>
<li>Tokenizer 對象.document_count: 獲取整數。分詞器被訓練的文檔（文本或者序列）數量。僅在調用 fit_on_texts 或 fit_on_sequences 之後設置。<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">from</span> tensorflow.keras.preprocessing.text <span style="color: #51afef;">import</span> Tokenizer
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #dcaeea;">samples</span> = [<span style="color: #98be65;">'&#21507;&#20160;&#40636;?'</span>, <span style="color: #98be65;">'&#21654;&#21737;&#39151; &#36996;&#26159; &#29275;&#25490; &#36996;&#26159; &#40629;&#21253; &#36996;&#26159; &#37117; &#21507;'</span>]
<span class="linenr"> 4: </span><span style="color: #dcaeea;">tokenizer</span> = Tokenizer()
<span class="linenr"> 5: </span>tokenizer.fit_on_texts(samples)
<span class="linenr"> 6: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'word_counts:'</span>, tokenizer.word_counts)
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'index_word:'</span>, tokenizer.index_word)
<span class="linenr"> 8: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'word_index:'</span>, tokenizer.word_index)
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>seq = tokenizer.texts_to_sequences([<span style="color: #98be65;">'&#21507; &#21654;&#21737;&#39151;'</span>,<span style="color: #98be65;">'&#29275;&#25490;&#160;&#21644;&#160;&#40629;&#21253;'</span>])
<span class="linenr">11: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'texts_to_sequences: '</span>,seq)
<span class="linenr">12: </span>seq2 = tokenizer.texts_to_sequences([[<span style="color: #98be65;">'&#21507;'</span>,<span style="color: #98be65;">'&#21654;&#21737;&#39151;'</span>],[<span style="color: #98be65;">'&#29275;&#25490;'</span>, <span style="color: #98be65;">'&#21644;'</span>,<span style="color: #98be65;">'&#40629;&#21253;'</span>]])
<span class="linenr">13: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'texts_to_sequences: '</span>, seq2)
<span class="linenr">14: </span>text = tokenizer.sequences_to_texts(seq)
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(text)
</pre>
</div>

<pre class="example">
word_counts: OrderedDict([('吃什麼', 1), ('咖哩飯', 1), ('還是', 3), ('牛排', 1), ('麵包', 1), ('都', 1), ('吃', 1)])
index_word: {1: '還是', 2: '吃什麼', 3: '咖哩飯', 4: '牛排', 5: '麵包', 6: '都', 7: '吃'}
word_index: {'還是': 1, '吃什麼': 2, '咖哩飯': 3, '牛排': 4, '麵包': 5, '都': 6, '吃': 7}
texts_to_sequences:  [[7, 3], []]
texts_to_sequences:  [[7, 3], [4, 5]]
['吃 咖哩飯', '']
</pre>
</div>

<ol class="org-ol">
<li><a id="orgcc714ba"></a>num_words<br />
<div class="outline-text-4" id="text-2-2-1">
<p>
何時要設定？當字數太多時(例如，拿 wiki 的文章來分析&#x2026;)<br />
</p>
<ul class="org-ul">
<li>num_words: None 或整數,個人理解就是對統計單詞出現數量後選擇次數多的前 n 個單詞，後面的單詞都不做處理。<br /></li>
<li>tokenizer.texts_to_sequences(texts): 使用字典將對應詞轉成 index。shape 為 (文檔數，每條文檔的長度)<br /></li>
<li>tokenizer.texts_to_matrix(texts): 轉成 one-hot，與前面的不同。shape 為[len(texts),num_words]<br /></li>
<li>tokenizer.word_counts: 單詞在所有文檔中的總數量，如果 num_words=4，應該選擇 some thing to<br /></li>
<li>tokenizer.word_docs: 單詞出現在文檔中的數量<br /></li>
<li>tokenizer.word_index: 單詞對應的 index<br /></li>
<li>tokenizer.index_docs: index 對應單詞出現在文檔中的數量<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> tensorflow.keras.preprocessing.text <span style="color: #51afef;">import</span> Tokenizer
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">samples</span> =[<span style="color: #98be65;">'&#21507; &#20160;&#40636;&#65311;'</span>,<span style="color: #98be65;">'&#21654;&#21737;&#39151; &#36996;&#26159; &#29275;&#25490; &#36996;&#26159; &#40629;&#21253; &#36996;&#26159; &#37117; &#21507;'</span>]
<span class="linenr">4: </span><span style="color: #dcaeea;">tokenizer</span> = Tokenizer(num_words=<span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">5: </span>tokenizer.fit_on_texts(samples)
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(tokenizer.index_word)
<span class="linenr">7: </span>
<span class="linenr">8: </span>seq= tokenizer.texts_to_sequences([<span style="color: #98be65;">'&#21507; &#21654;&#21737;&#39151; &#21644; &#40629;&#21253;'</span>])
<span class="linenr">9: </span><span style="color: #c678dd;">print</span>(seq)
</pre>
</div>

<pre class="example">
{1: '還是', 2: '吃', 3: '什麼？', 4: '咖哩飯', 5: '牛排', 6: '麵包', 7: '都'}
[[2]]
</pre>
</div>
</li>
</ol>
</div>
<div id="outline-container-org3c1392c" class="outline-3">
<h3 id="org3c1392c"><span class="section-number-3">2.3.</span> Sequence padding</h3>
<div class="outline-text-3" id="text-2-3">
<p>
在 NLP 中，文本一般是不定長的，所以在進行 batch 訓練之前，要先進行長度的統一，過長的句子可以通過 truncating 截斷到固定的長度，過短的句子可以通過 padding 增加到固定的長度，但是 padding 對應的字符只是為了統一長度，並沒有實際的價值，因此希望在之後的計算中屏蔽它們，這時候就需要 Mask<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> tensorflow.keras.preprocessing.sequence <span style="color: #51afef;">import</span> pad_sequences
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">seq</span>= [[<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">7</span>, <span style="color: #da8548; font-weight: bold;">8</span>]]
<span class="linenr">4: </span>
<span class="linenr">5: </span><span style="color: #dcaeea;">pad_seq</span> = pad_sequences(seq, maxlen=<span style="color: #da8548; font-weight: bold;">4</span>, dtype= <span style="color: #98be65;">'int32'</span>, padding=<span style="color: #98be65;">'pre'</span>, truncating=<span style="color: #98be65;">'post'</span>, value=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'pad_sequences:\n'</span>,pad_seq)
<span class="linenr">7: </span>
<span class="linenr">8: </span>pad_seq = pad_sequences(seq, maxlen=<span style="color: #da8548; font-weight: bold;">4</span>, dtype= <span style="color: #98be65;">'int32'</span>, padding=<span style="color: #98be65;">'post'</span>, truncating=<span style="color: #98be65;">'post'</span>, value=-<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'pad_sequences:\n'</span>, pad_seq)
</pre>
</div>

<pre class="example">
pad_sequences:
 [[0 0 5 1]
 [0 0 0 2]
 [3 5 4 7]]
pad_sequences:
 [[ 5  1 -1 -1]
 [ 2 -1 -1 -1]
 [ 3  5  4  7]]
</pre>
</div>
</div>
<div id="outline-container-org5c906f8" class="outline-3">
<h3 id="org5c906f8"><span class="section-number-3">2.4.</span> one-hot encoding</h3>
<div class="outline-text-3" id="text-2-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">from</span> tensorflow.keras.preprocessing.text <span style="color: #51afef;">import</span> Tokenizer
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> tensorflow.keras.preprocessing.sequence <span style="color: #51afef;">import</span> pad_sequences
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> tensorflow.keras.utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">num_classes</span> = <span style="color: #da8548; font-weight: bold;">10</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">convsat</span> =[<span style="color: #98be65;">'&#20320;&#22909; &#21704;&#22217;'</span>, <span style="color: #98be65;">'&#20160;&#40636; &#24590;&#40636;'</span>, <span style="color: #98be65;">'&#20320;&#26159;&#35504; &#19981;&#21578;&#35380; &#20320;'</span>, <span style="color: #98be65;">'&#20320; &#26371;&#20570; &#20160;&#40636; &#37117;&#19981;&#26371; &#26371;&#20570;'</span>, <span style="color: #98be65;">'&#20160;&#40636;&#26159; &#38622;&#34507; &#19981;&#30693;&#36947; &#24590;&#40636;'</span>]
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">tokenizer</span> = Tokenizer(num_words = num_classes, oov_token=<span style="color: #98be65;">'N'</span>)
<span class="linenr"> 9: </span>tokenizer.fit_on_texts(convsat)
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'tokenizer: '</span>, tokenizer)
<span class="linenr">11: </span>
<span class="linenr">12: </span>convsat_seq = tokenizer.texts_to_sequences(convsat)
<span class="linenr">13: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'texts_to_sequences(&#21463;&#38480;num_classes&#65292;&#26410;&#21015;&#20837;class&#32773;sequence&#28858;1):\n'</span>, convsat_seq)
<span class="linenr">14: </span>convsat_seq = pad_sequences(convsat_seq)
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'pad_sequences:\n'</span>, convsat_seq)
<span class="linenr">16: </span>
<span class="linenr">17: </span>convsat_one_hot = to_categorical(convsat_seq, num_classes=num_classes)
<span class="linenr">18: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'to_categorical:\n'</span>, convsat_one_hot)
</pre>
</div>

<pre class="example" id="org2801f2a">
tokenizer:  &lt;keras_preprocessing.text.Tokenizer object at 0x103ba3160&gt;
texts_to_sequences(受限num_classes，未列入class者sequence為1):
 [[6, 7], [2, 3], [8, 9, 4], [4, 5, 2, 1, 5], [1, 1, 1, 3]]
pad_sequences:
 [[0 0 0 6 7]
 [0 0 0 2 3]
 [0 0 8 9 4]
 [4 5 2 1 5]
 [0 1 1 1 3]]
to_categorical:
 [[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]

 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]

 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]
  [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]
  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]

 [[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
  [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]

 [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]]
</pre>
</div>
</div>
<div id="outline-container-orga3e7a7d" class="outline-3">
<h3 id="orga3e7a7d"><span class="section-number-3">2.5.</span> Multi-hot encoding</h3>
<div class="outline-text-3" id="text-2-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">from</span> tensorflow.keras.preprocessing.text <span style="color: #51afef;">import</span> Tokenizer
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #dcaeea;">comment</span>= [<span style="color: #98be65;">'&#30495; &#22909;&#30475; &#25105; &#21916;&#27489;'</span>,  <span style="color: #98be65;">'&#36996;&#22909; &#32570;&#20047; &#19968;&#40670; &#21127;&#24773;'</span>, <span style="color: #98be65;">'&#26377; &#19968;&#40670; &#38647; &#20491;&#20154; &#19981; &#21916;&#27489;'</span>, <span style="color: #98be65;">'&#25105; &#21916;&#27489; &#20294; &#25105; &#26379;&#21451; &#37117; &#19981; &#24859;'</span>]
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">tokenizer</span> = Tokenizer(num_words=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>tokenizer.fit_on_texts(comment)
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>comment_multi_hot= tokenizer.texts_to_matrix(comment, mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'texts_to_matrix\n'</span>, comment_multi_hot)
<span class="linenr">11: </span>
<span class="linenr">12: </span>comment_multi_hot2= tokenizer.texts_to_matrix(comment, mode= <span style="color: #98be65;">'count'</span> )
<span class="linenr">13: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'texts_to_matrix\n'</span>, comment_multi_hot2)
</pre>
</div>

<pre class="example" id="orgd3beba1">
texts_to_matrix
 [[0. 1. 1. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 1. 1. 1.]
 [0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]
 [0. 1. 1. 0. 1. 0. 0. 0. 0. 0.]]
texts_to_matrix
 [[0. 1. 1. 0. 0. 1. 1. 0. 0. 0.]
 [0. 0. 0. 1. 0. 0. 0. 1. 1. 1.]
 [0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]
 [0. 2. 1. 0. 1. 0. 0. 0. 0. 0.]]
</pre>
</div>
</div>
<div id="outline-container-org683d375" class="outline-3">
<h3 id="org683d375"><span class="section-number-3">2.6.</span> 資料預處理</h3>
<div class="outline-text-3" id="text-2-6">
<p>
不能直接將 list 型別的資料送到神經網路中訓練，必須將 list 型別轉換為 tensor 張量型別。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org6b75ccb"></a>有兩種轉換方式：<br />
<div class="outline-text-4" id="text-2-6-1">
<ul class="org-ul">
<li>填充列表使每個列表長度都相同，然後轉換為整數型別的張量，形狀為(samples, word_indices),使用張量作為神經網路的第一層(Embedding 層，能處理這樣的整數型別張量)；<br /></li>
<li>將列表進行 one-hot 編碼，轉換成 0、 1 向量。然後用 Dense 網路層作為神經網路的第一層，處理浮點型別向量資料。<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 4: </span>    results = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension)) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#36039;&#26009;&#38598;&#38263;&#24230;&#65292;&#27599;&#20491;&#35413;&#35542;&#32173;&#24230;10000</span>
<span class="linenr"> 5: </span>    <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 6: </span>        results[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">one-hot</span>
<span class="linenr"> 7: </span>    <span style="color: #51afef;">return</span> results
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>x_train = vectorize_sequences(train_data)
<span class="linenr">10: </span>x_test = vectorize_sequences(test_data)
<span class="linenr">11: </span>y_train = np.asarray(train_labels).astype(<span style="color: #98be65;">'float32'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21521;&#37327;&#21270;&#27161;&#31844;&#36039;&#26009;</span>
<span class="linenr">12: </span>y_test = np.asarray(test_labels).astype(<span style="color: #98be65;">'float32'</span>)
</pre>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgd12b3f2" class="outline-3">
<h3 id="orgd12b3f2"><span class="section-number-3">2.7.</span> 共同部分</h3>
<div class="outline-text-3" id="text-2-7">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> random
<span class="linenr">2: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #5B6268;">## </span><span style="color: #5B6268;">Read-in the reviews and print some basic descriptions of them</span>
<span class="linenr">5: </span>
<span class="linenr">6: </span>!wget -q <span style="color: #98be65;">"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"</span>
<span class="linenr">7: </span>!tar xzf aclImdb_v1.tar.gz
</pre>
</div>
</div>
</div>
<div id="outline-container-org746e779" class="outline-3">
<h3 id="org746e779"><span class="section-number-3">2.8.</span> Define Tokenization, Stop-word and Punctuation Removal Functions</h3>
<div class="outline-text-3" id="text-2-8">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">Nsamp</span> = <span style="color: #da8548; font-weight: bold;">1000</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">number of samples to generate in each class -</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">maxtokens</span> = <span style="color: #da8548; font-weight: bold;">200</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">the maximum number of tokens per document</span>
<span class="linenr">3: </span><span style="color: #dcaeea;">maxtokenlen</span> = <span style="color: #da8548; font-weight: bold;">100</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">the maximum length of each token</span>
</pre>
</div>
</div>
</div>
<div id="outline-container-org8f8062b" class="outline-3">
<h3 id="org8f8062b"><span class="section-number-3">2.9.</span> Tokenization</h3>
<div class="outline-text-3" id="text-2-9">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">tokenize</span>(row):
<span class="linenr">2: </span>    <span style="color: #51afef;">if</span> row <span style="color: #51afef;">is</span> <span style="color: #a9a1e1;">None</span> <span style="color: #51afef;">or</span> row <span style="color: #51afef;">is</span> <span style="color: #98be65;">''</span>:
<span class="linenr">3: </span>        <span style="color: #dcaeea;">tokens</span> = <span style="color: #98be65;">""</span>
<span class="linenr">4: </span>    <span style="color: #51afef;">else</span>:
<span class="linenr">5: </span>        tokens = row.split(<span style="color: #98be65;">" "</span>)[:maxtokens]
<span class="linenr">6: </span>    <span style="color: #51afef;">return</span> tokens
</pre>
</div>
<p>
p** Use regular expressions to remove unnecessary characters<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> re
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">reg_expressions</span>(row):
<span class="linenr"> 4: </span>    <span style="color: #dcaeea;">tokens</span> = []
<span class="linenr"> 5: </span>    <span style="color: #51afef;">try</span>:
<span class="linenr"> 6: </span>        <span style="color: #51afef;">for</span> token <span style="color: #51afef;">in</span> row:
<span class="linenr"> 7: </span>            token = token.lower() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">make all characters lower case</span>
<span class="linenr"> 8: </span>            <span style="color: #dcaeea;">token</span> = re.sub(r<span style="color: #98be65;">'[\W\d]'</span>, <span style="color: #98be65;">""</span>, token)
<span class="linenr"> 9: </span>            <span style="color: #dcaeea;">token</span> = token[:maxtokenlen] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">truncate token</span>
<span class="linenr">10: </span>            tokens.append(token)
<span class="linenr">11: </span>    <span style="color: #51afef;">except</span>:
<span class="linenr">12: </span>        token = <span style="color: #98be65;">""</span>
<span class="linenr">13: </span>        tokens.append(token)
<span class="linenr">14: </span>    <span style="color: #51afef;">return</span> tokens
<span class="linenr">15: </span>
</pre>
</div>
</div>
</div>
<div id="outline-container-orgcbb5dae" class="outline-3">
<h3 id="orgcbb5dae"><span class="section-number-3">2.10.</span> Stop-word removal</h3>
<div class="outline-text-3" id="text-2-10">
<p>
Stop-words are words that are very common in text but offer no useful information that can be used to classify the text.<br />
Words such as is, and, the, are are examples of stop-words.<br />
The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings.<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> nltk
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>nltk.download(<span style="color: #98be65;">'stopwords'</span>)
<span class="linenr"> 4: </span><span style="color: #51afef;">from</span> nltk.corpus <span style="color: #51afef;">import</span> stopwords
<span class="linenr"> 5: </span><span style="color: #dcaeea;">stopwords</span> = stopwords.words(<span style="color: #98be65;">'english'</span>)
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">print(stopwords) # see default stopwords</span>
<span class="linenr"> 8: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">it may be beneficial to drop negation words from the removal list,</span>
<span class="linenr"> 9: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">as they can change the positive/negative meaning</span>
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">of a sentence</span>
<span class="linenr">11: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">stopwords.remove("no")</span>
<span class="linenr">12: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">stopwords.remove("nor")</span>
<span class="linenr">13: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">stopwords.remove("not")</span>
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">stop_word_removal</span>(row):
<span class="linenr">2: </span>    <span style="color: #dcaeea;">token</span> = [token <span style="color: #51afef;">for</span> token <span style="color: #51afef;">in</span> row <span style="color: #51afef;">if</span> token <span style="color: #51afef;">not</span> <span style="color: #51afef;">in</span> stopwords]
<span class="linenr">3: </span>    <span style="color: #dcaeea;">token</span> = <span style="color: #c678dd;">filter</span>(<span style="color: #a9a1e1;">None</span>, token)
<span class="linenr">4: </span>    <span style="color: #51afef;">return</span> token
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org6b57324" class="outline-2">
<h2 id="org6b57324"><span class="section-number-2">3.</span> 資料預處理實務</h2>
<div class="outline-text-2" id="text-3">
<ul class="org-ul">
<li>情緒分析 (sentiment analysis) 又稱為意見探勘 (opinion mining). 是使用 &ldquo;自然語言處理&rdquo;, 文字分析等方法, 找出作者某些話題上的態度, 情感, 評價或情緒.<br /></li>
<li>情緒分析的商業價值, 可以提早得知顧客對公司或產品觀感, 以調整銷售策略方向.<br /></li>
</ul>
</div>
<div id="outline-container-org0f5bf4d" class="outline-3">
<h3 id="org0f5bf4d"><span class="section-number-3">3.1.</span> IMDB 介簡</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>IMDB 網路資料庫 (Internet Movie Database), 是一個電影相關的線上資料庫. IMDB 開始於 1990 年, 自 1998 年起成為亞馬遜旗下的網站, 至今已經累積大量的電影資訊. IMDb 共收錄了四百多萬作品資料.<br /></li>
<li>IMDB 資料集共有 50,000 筆 &ldquo;影評文字&rdquo;, 分為訓練資料與測試資料各 25,000 筆,每一筆 &ldquo;影評文字&rdquo; 都被標記成 &ldquo;正面評價&rdquo; 或 &ldquo;負面評價&rdquo;.<br /></li>
<li>我們希望能建立一個模型, 經過大量 &ldquo;影評文字&rdquo; 訓練後, 此模型能用於預測 &ldquo;影評文字&rdquo; 是 &ldquo;正面評價&rdquo; 或 &ldquo;負面評價&rdquo;<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>。<br /></li>
</ul>

<div id="org316cfb0" class="figure">
<p><img src="images/TrainingPredict.jpg" alt="TrainingPredict.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 2: </span>資料前處理</p>
</div>
</div>
</div>
<div id="outline-container-orgc39dda4" class="outline-3">
<h3 id="orgc39dda4"><span class="section-number-3">3.2.</span> 查看 IMDB 資料</h3>
<div class="outline-text-3" id="text-3-2">
<ul class="org-ul">
<li>num_words=10000 意味著只保留訓練集中最常出現的前 10000 個詞，不經常出現的單詞被拋棄—最終所有評論的維度保持相同。<br /></li>
<li>變數 train_data,test_data 是電影評論的列表，每條評論由數字(對應單詞在詞典中出現的位置下標)列表組成。<br /></li>
<li>train_labels,test_labels 是 0,1 列表，0負面評論，1表示正面評論。<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr">2: </span>
<span class="linenr">3: </span>(train_data,train_labels),(<span style="color: #dcaeea;">test_data</span>,<span style="color: #dcaeea;">test_labels</span>) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">4: </span>
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>]) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#26597;&#30475;&#31532; 0 &#31558; "&#24433;&#35413;&#25991;&#23383;"</span>
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(train_labels[<span style="color: #da8548; font-weight: bold;">0</span>]) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#31532; 0 &#31558;&#30340;&#35413;&#20729; label &#28858; 1, &#20063;&#23601;&#26159; &#27491;&#38754;&#35413;&#20729;</span>
</pre>
</div>

<pre class="example">
[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
1
</pre>
</div>
</div>
<div id="outline-container-orgdf54f2d" class="outline-3">
<h3 id="orgdf54f2d"><span class="section-number-3">3.3.</span> 資料預處理</h3>
<div class="outline-text-3" id="text-3-3">
<p>
不能直接將 list 型別的資料送到神經網路中訓練，必須將 list 型別轉換為 tensor 張量型別。有兩種轉換方式：<br />
</p>
<ul class="org-ul">
<li>填充列表使每個列表長度都相同，然後轉換為整數型別的張量，形狀為(samples, word_indices),使用張量作為神經網路的第一層(Embedding 層，能處理這樣的整數型別張量)；<br /></li>
<li>將列表進行 one-hot 編碼，轉換成 0、 1 向量。然後用 Dense 網路層作為神經網路的第一層，處理浮點型別向量資料。<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>(train_data,train_labels),(<span style="color: #dcaeea;">test_data</span>,<span style="color: #dcaeea;">test_labels</span>) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 7: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36039;&#26009;&#38598;&#38263;&#24230;&#65292;&#27599;&#20491;&#35413;&#35542;&#32173;&#24230;10000</span>
<span class="linenr"> 8: </span>    results = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension)) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#36039;&#26009;&#38598;&#38263;&#24230;&#65292;&#27599;&#20491;&#35413;&#35542;&#32173;&#24230;10000</span>
<span class="linenr"> 9: </span>    <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr">10: </span>        results[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">one-hot</span>
<span class="linenr">11: </span>    <span style="color: #51afef;">return</span> results
<span class="linenr">12: </span>
<span class="linenr">13: </span>x_train = vectorize_sequences(train_data)
<span class="linenr">14: </span>x_test = vectorize_sequences(test_data)
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'vectorize_sequences: train_data'</span>)
<span class="linenr">16: </span><span style="color: #c678dd;">print</span>(x_train)
<span class="linenr">17: </span>
<span class="linenr">18: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21521;&#37327;&#21270;&#27161;&#31844;&#36039;&#26009;</span>
<span class="linenr">19: </span>y_train = np.asarray(train_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">20: </span>y_test = np.asarray(test_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">21: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'np.asarray: train_labels'</span>)
<span class="linenr">22: </span><span style="color: #c678dd;">print</span>(y_train)
</pre>
</div>

<pre class="example" id="org2cc06cc">
vectorize_sequences: train_data
[[0. 1. 1. ... 0. 0. 0.]
 [0. 1. 1. ... 0. 0. 0.]
 [0. 1. 1. ... 0. 0. 0.]
 ...
 [0. 1. 1. ... 0. 0. 0.]
 [0. 1. 1. ... 0. 0. 0.]
 [0. 1. 1. ... 0. 0. 0.]]
np.asarray: train_labels
[1. 0. 0. ... 0. 1. 0.]
</pre>
</div>
</div>
<div id="outline-container-org0f7a0d9" class="outline-3">
<h3 id="org0f7a0d9"><span class="section-number-3">3.4.</span> Define Tokenization, Stop-word and Punctuation Removal Functions</h3>
<div class="outline-text-3" id="text-3-4">
<p>
Before proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">Nsamp</span> = <span style="color: #da8548; font-weight: bold;">1000</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">number of samples to generate in each class - 'spam', 'not spam'</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">maxtokens</span> = <span style="color: #da8548; font-weight: bold;">200</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">the maximum number of tokens per document</span>
<span class="linenr">3: </span><span style="color: #dcaeea;">maxtokenlen</span> = <span style="color: #da8548; font-weight: bold;">100</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">the maximum length of each token</span>
</pre>
</div>
</div>
<ol class="org-ol">
<li><a id="org2a184ca"></a>Tokenization<br />
<div class="outline-text-4" id="text-3-4-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">tokenize</span>(row):
<span class="linenr">2: </span>    <span style="color: #51afef;">if</span> row <span style="color: #51afef;">is</span> <span style="color: #a9a1e1;">None</span> <span style="color: #51afef;">or</span> row <span style="color: #51afef;">is</span> <span style="color: #98be65;">''</span>:
<span class="linenr">3: </span>        <span style="color: #dcaeea;">tokens</span> = <span style="color: #98be65;">""</span>
<span class="linenr">4: </span>    <span style="color: #51afef;">else</span>:
<span class="linenr">5: </span>        tokens = row.split(<span style="color: #98be65;">" "</span>)[:maxtokens]
<span class="linenr">6: </span>    <span style="color: #51afef;">return</span> tokens
</pre>
</div>
<p>
#+end_src<br />
</p>
</div>
</li>
<li><a id="orgd730ac6"></a>Use regular expressions to remove unnecessary characters<br />
<div class="outline-text-4" id="text-3-4-2">
<p>
Next, we define a function to remove punctuation marks and other nonword characters (using regular expressions) from the emails with the help of the ubiquitous python regex library. In the same step, we truncate all tokens to hyperparameter maxtokenlen defined above.<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> re
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">reg_expressions</span>(row):
<span class="linenr"> 4: </span>    <span style="color: #dcaeea;">tokens</span> = []
<span class="linenr"> 5: </span>    <span style="color: #51afef;">try</span>:
<span class="linenr"> 6: </span>        <span style="color: #51afef;">for</span> token <span style="color: #51afef;">in</span> row:
<span class="linenr"> 7: </span>            token = token.lower() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">make all characters lower case</span>
<span class="linenr"> 8: </span>            <span style="color: #dcaeea;">token</span> = re.sub(r<span style="color: #98be65;">'[\W\d]'</span>, <span style="color: #98be65;">""</span>, token)
<span class="linenr"> 9: </span>            <span style="color: #dcaeea;">token</span> = token[:maxtokenlen] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">truncate token</span>
<span class="linenr">10: </span>            tokens.append(token)
<span class="linenr">11: </span>    <span style="color: #51afef;">except</span>:
<span class="linenr">12: </span>        token = <span style="color: #98be65;">""</span>
<span class="linenr">13: </span>        tokens.append(token)
<span class="linenr">14: </span>    <span style="color: #51afef;">return</span> tokens
</pre>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-org4e27c9e" class="outline-3">
<h3 id="org4e27c9e"><span class="section-number-3">3.5.</span> Stop-word removal</h3>
<div class="outline-text-3" id="text-3-5">
<p>
Stop-words are also removed. Stop-words are words that are very common in text but offer no useful information that can be used to classify the text. Words such as is, and, the, are are examples of stop-words. The NLTK library contains a list of 127 English stop-words and can be used to filter our tokenized strings.<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> nltk
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>nltk.download(<span style="color: #98be65;">'stopwords'</span>)
<span class="linenr"> 4: </span><span style="color: #51afef;">from</span> nltk.corpus <span style="color: #51afef;">import</span> stopwords
<span class="linenr"> 5: </span><span style="color: #dcaeea;">stopwords</span> = stopwords.words(<span style="color: #98be65;">'english'</span>)
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">print(stopwords) # see default stopwords</span>
<span class="linenr"> 8: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">it may be beneficial to drop negation words from the removal list,</span>
<span class="linenr"> 9: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">as they can change the positive/negative meaning</span>
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">of a sentence</span>
<span class="linenr">11: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">stopwords.remove("no")</span>
<span class="linenr">12: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">stopwords.remove("nor")</span>
<span class="linenr">13: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">stopwords.remove("not")</span>
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">stop_word_removal</span>(row):
<span class="linenr">2: </span>    <span style="color: #dcaeea;">token</span> = [token <span style="color: #51afef;">for</span> token <span style="color: #51afef;">in</span> row <span style="color: #51afef;">if</span> token <span style="color: #51afef;">not</span> <span style="color: #51afef;">in</span> stopwords]
<span class="linenr">3: </span>    <span style="color: #dcaeea;">token</span> = <span style="color: #c678dd;">filter</span>(<span style="color: #a9a1e1;">None</span>, token)
<span class="linenr">4: </span>    <span style="color: #51afef;">return</span> token
</pre>
</div>
</div>
</div>
<div id="outline-container-orgd1952a9" class="outline-3">
<h3 id="orgd1952a9"><span class="section-number-3">3.6.</span> Bag-of-words model</h3>
<div class="outline-text-3" id="text-3-6">
<ul class="org-ul">
<li>For the computer to make inferences of the e-mails, it has to be able to interpret the text by making a numerical representation of it. One way to do this is by using something called a &ldquo;bag-of-words&rdquo; model. This model simply counts the frequency of word tokens for each email and thereby represents it as a vector of these counts.<br /></li>
<li>詞袋模型（英語：Bag-of-words model）是個在自然語言處理和信息檢索(IR)下被簡化的表達模型。此模型下，一段文本（比如一個句子或是一個文檔）可以用一個裝著這些詞的袋子來表示，這種表示方式不考慮文法以及詞的順序。<br /></li>
<li>詞袋模型被廣泛應用在文件分類，詞出現的頻率可以用來當作訓練分類器的特徵<br /></li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org587594f"></a>Example implementation<br />
<div class="outline-text-4" id="text-3-6-1">
<p>
The following models a text document using bag-of-words. Here are two simple text documents:<br />
</p>

<pre class="example" id="orga94168c">
1) John likes to watch movies. Mary likes movies too.
2) Mary also likes to watch football games.
</pre>

<p>
Based on these two text documents, a list is constructed as follows for each document:<br />
</p>

<pre class="example" id="org02fbb73">
1) "John","likes","to","watch","movies","Mary","likes","movies","too"
2) "Mary","also","likes","to","watch","football","games"
</pre>

<p>
Representing each bag-of-words as a JSON object, and attributing to the respective JavaScript variable:<br />
</p>

<pre class="example" id="org8a7f6ec">
1) BoW1 = {"John":1,"likes":2,"to":1,"watch":1,"movies":2,"Mary":1,"too":1};
2) BoW2 = {"Mary":1,"also":1,"likes":1,"to":1,"watch":1,"football":1,"games":1};
</pre>

<p>
Each key is the word, and each value is the number of occurrences of that word in the given text document.<br />
</p>

<p>
The order of elements is free, so, for example<br />
</p>

<p>
{&ldquo;too&rdquo;:1,&ldquo;Mary&rdquo;:1,&ldquo;movies&rdquo;:2,&ldquo;John&rdquo;:1,&ldquo;watch&rdquo;:1,&ldquo;likes&rdquo;:2,&ldquo;to&rdquo;:1} is also equivalent to BoW1. It is also what we expect from a strict JSON object representation.<br />
</p>

<p>
Note: if another document is like a union of these two,<br />
</p>

<p>
(3) John likes to watch movies. Mary likes movies too. Mary also likes to watch football games.<br />
</p>

<p>
its JavaScript representation will be:<br />
</p>
<pre class="example" id="org5de23e5">
BoW3 = {"John":1,"likes":3,"to":2,"watch":2,"movies":2,"Mary":2,"too":1,"also":1,"football":1,"games":1};

若只有上述單字,且以BoW3 的順序為基準來表示BoW1和BoW2:

#+BEGIN_EXAMPLE
BoW1 = [1, 2, 1, 1, 2, 1, 1, 0, 0, 0]
BoW2 = [0, 1, 1, 1, 0, 1, 0, 1, 1, 1]
</pre>

<p>
So, as we see in the bag algebra, the &ldquo;union&rdquo; of two documents in the bags-of-words representation is, formally, the disjoint union, summing the multiplicities of each element.<br />
\(Bow3=Bow1 \uplus Bow2\)<br />
</p>
</div>
</li>

<li><a id="org1345ae0"></a>Assemble matrices function<br />
<div class="outline-text-4" id="text-3-6-2">
<p>
The assemble_bag() function assembles a new dataframe containing all the unique words found in the text documents. It counts the word frequency and then returns the new dataframe.<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">assemble_bag</span>(data):
<span class="linenr"> 2: </span>    <span style="color: #dcaeea;">used_tokens</span> = []
<span class="linenr"> 3: </span>    <span style="color: #dcaeea;">all_tokens</span> = []
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>    <span style="color: #51afef;">for</span> item <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">data</span>:
<span class="linenr"> 6: </span>        <span style="color: #51afef;">for</span> token <span style="color: #51afef;">in</span> item:
<span class="linenr"> 7: </span>            <span style="color: #51afef;">if</span> token <span style="color: #51afef;">in</span> all_tokens:
<span class="linenr"> 8: </span>                <span style="color: #51afef;">if</span> token <span style="color: #51afef;">not</span> <span style="color: #51afef;">in</span> used_tokens:
<span class="linenr"> 9: </span>                    used_tokens.append(token)
<span class="linenr">10: </span>            <span style="color: #51afef;">else</span>:
<span class="linenr">11: </span>                all_tokens.append(token)
<span class="linenr">12: </span>
<span class="linenr">13: </span>    df = pd.DataFrame(<span style="color: #da8548; font-weight: bold;">0</span>, index = np.arange(<span style="color: #c678dd;">len</span>(data)), columns = used_tokens)
<span class="linenr">14: </span>
<span class="linenr">15: </span>    <span style="color: #51afef;">for</span> i, item <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(data):
<span class="linenr">16: </span>        <span style="color: #51afef;">for</span> token <span style="color: #51afef;">in</span> item:
<span class="linenr">17: </span>            <span style="color: #51afef;">if</span> token <span style="color: #51afef;">in</span> used_tokens:
<span class="linenr">18: </span>                df.iloc[i][<span style="color: #dcaeea;">token</span>] += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">19: </span>    <span style="color: #51afef;">return</span> df
</pre>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-org509b56a" class="outline-3">
<h3 id="org509b56a"><span class="section-number-3">3.7.</span> Putting It All Together To Assemble Dataset</h3>
<div class="outline-text-3" id="text-3-7">
<p>
Now, putting all the preprocessing steps together we assemble our dataset&#x2026;<br />
</p>

<ol class="org-ol">
<li>shuffle raw data and load data<br /></li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> os
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">shuffle raw data first</span>
<span class="linenr"> 5: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">unison_shuffle_data</span>(data, header):
<span class="linenr"> 6: </span>    <span style="color: #dcaeea;">p</span> = np.random.permutation(<span style="color: #c678dd;">len</span>(header))
<span class="linenr"> 7: </span>    <span style="color: #dcaeea;">data</span> = data[p]
<span class="linenr"> 8: </span>    <span style="color: #dcaeea;">header</span> = np.asarray(header)[p]
<span class="linenr"> 9: </span>    <span style="color: #51afef;">return</span> data, header
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">load data in appropriate form</span>
<span class="linenr">12: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">load_data</span>(path):
<span class="linenr">13: </span>    <span style="color: #dcaeea;">data</span>, <span style="color: #dcaeea;">sentiments</span> = [], []
<span class="linenr">14: </span>    <span style="color: #51afef;">for</span> folder, sentiment <span style="color: #51afef;">in</span> ((<span style="color: #98be65;">'neg'</span>, <span style="color: #da8548; font-weight: bold;">0</span>), (<span style="color: #98be65;">'pos'</span>, <span style="color: #da8548; font-weight: bold;">1</span>)):
<span class="linenr">15: </span>        <span style="color: #dcaeea;">folder</span> = os.path.join(path, folder)
<span class="linenr">16: </span>        <span style="color: #51afef;">for</span> name <span style="color: #51afef;">in</span> os.listdir(folder):
<span class="linenr">17: </span>            <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(os.path.join(folder, name), <span style="color: #98be65;">'r'</span>) <span style="color: #51afef;">as</span> <span style="color: #dcaeea;">reader</span>:
<span class="linenr">18: </span>                  text = reader.read()
<span class="linenr">19: </span>            <span style="color: #dcaeea;">text</span> = tokenize(text)
<span class="linenr">20: </span>            <span style="color: #dcaeea;">text</span> = stop_word_removal(text)
<span class="linenr">21: </span>            <span style="color: #dcaeea;">text</span> = reg_expressions(text)
<span class="linenr">22: </span>            data.append(text)
<span class="linenr">23: </span>            sentiments.append(sentiment)
<span class="linenr">24: </span>    <span style="color: #dcaeea;">data_np</span> = np.array(data)
<span class="linenr">25: </span>    <span style="color: #dcaeea;">data</span>, <span style="color: #dcaeea;">sentiments</span> = unison_shuffle_data(data_np, sentiments)
<span class="linenr">26: </span>
<span class="linenr">27: </span>    <span style="color: #51afef;">return</span> data, sentiments
<span class="linenr">28: </span>
<span class="linenr">29: </span><span style="color: #dcaeea;">train_path</span> = os.path.join(<span style="color: #98be65;">'aclImdb'</span>, <span style="color: #98be65;">'train'</span>)
<span class="linenr">30: </span><span style="color: #dcaeea;">test_path</span> = os.path.join(<span style="color: #98be65;">'aclImdb'</span>, <span style="color: #98be65;">'test'</span>)
<span class="linenr">31: </span><span style="color: #dcaeea;">raw_data</span>, <span style="color: #dcaeea;">raw_header</span> = load_data(train_path)
<span class="linenr">32: </span>
<span class="linenr">33: </span><span style="color: #c678dd;">print</span>(raw_data.shape)
<span class="linenr">34: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(raw_header))
</pre>
</div>

<ol class="org-ol">
<li><p>
Subsample required number of samples<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Subsample required number of samples</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">random_indices</span> = np.random.choice(<span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(raw_header)),size=(Nsamp*<span style="color: #da8548; font-weight: bold;">2</span>,),replace=<span style="color: #a9a1e1;">False</span>)
<span class="linenr">3: </span>data_train = raw_data[random_indices]
<span class="linenr">4: </span>header = raw_header[random_indices]
<span class="linenr">5: </span>
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"DEBUG::data_train::"</span>)
<span class="linenr">7: </span><span style="color: #c678dd;">print</span>(data_train)
</pre>
</div></li>

<li>Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes<br /></li>
</ol>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">unique_elements</span>, <span style="color: #dcaeea;">counts_elements</span> = np.unique(header, return_counts=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Sentiments and their frequencies:"</span>)
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(unique_elements)
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(counts_elements)
</pre>
</div>

<ol class="org-ol">
<li>Featurize and Create Labels<br /></li>
</ol>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">MixedBagOfReviews</span> = assemble_bag(data_train)
<span class="linenr">2: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">this is the list of words in our bag-of-words model</span>
<span class="linenr">3: </span><span style="color: #dcaeea;">predictors</span> = [column <span style="color: #51afef;">for</span> column <span style="color: #51afef;">in</span> MixedBagOfReviews.columns]
<span class="linenr">4: </span>
<span class="linenr">5: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">expand default pandas display options to make emails more clearly visible when printed</span>
<span class="linenr">6: </span>pd.set_option(<span style="color: #98be65;">'display.max_colwidth'</span>, <span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">7: </span>
<span class="linenr">8: </span>MixedBagOfReviews <span style="color: #5B6268;"># </span><span style="color: #5B6268;">you could do print(MixedBagOfReviews), but Jupyter displays this nicer for pandas DataFrames</span>
</pre>
</div>

<ol class="org-ol">
<li><p>
split into independent 70% training and 30% testing sets<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">split into independent 70% training and 30% testing sets</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">data</span> = MixedBagOfReviews.values
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">idx</span> = <span style="color: #c678dd;">int</span>(<span style="color: #da8548; font-weight: bold;">0.7</span>*data.shape[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">70% of data for training</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">train_x</span> = data[:idx,:]
<span class="linenr"> 8: </span><span style="color: #dcaeea;">train_y</span> = header[:idx]
<span class="linenr"> 9: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">remaining 30% for testing</span>
<span class="linenr">10: </span><span style="color: #dcaeea;">test_x</span> = data[<span style="color: #dcaeea;">idx</span>:,:]
<span class="linenr">11: </span>test_y = header[idx:]
<span class="linenr">12: </span>
<span class="linenr">13: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"train_x/train_y list details, to make sure it is of the right form:"</span>)
<span class="linenr">14: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(train_x))
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(train_x)
<span class="linenr">16: </span><span style="color: #c678dd;">print</span>(train_y[:<span style="color: #da8548; font-weight: bold;">5</span>])
<span class="linenr">17: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(train_y))
</pre>
</div></li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org7644c94" class="outline-2">
<h2 id="org7644c94"><span class="section-number-2">4.</span> 自然語言處理學習路線</h2>
<div class="outline-text-2" id="text-4">
<p>
若要讓電腦理解人類的語言，以中文來說，分成兩步驟：第一步是斷詞、理解詞；第二步則是分析句子，包含語法及語義的自動解析。自然語言處理透過這兩個步驟，將複雜的語言轉化為電腦容易處理、計算的形式。早期是人工訂定規則，現在則是讓機器自己學習<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>。<br />
</p>

<blockquote>
<p>
早期的方式是基於一套詞彙資料庫，用程式語言寫好人工訂定的規則，讓電腦依指令做出反應。但這種人工方式不可能包含所有語言的歧異性，例如，當同樣的詞在不同上下文產生不同意思，就會和原本的人工規則相互牴觸。1980 年代末期，自然語言處理引進機器學習 (Machine Learning) 的演算法，不再用程式語言命令電腦所有規則，而是建立演算法模型，讓電腦學會從訓練的資料中，尋找資料所含的特定模式和趨勢<sup><a id="fnr.5.100" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>。<br />
</p>
</blockquote>

<p>
要掌握自然語言處理，學習順序大至如下<br />
</p>
<ol class="org-ol">
<li>熟悉基本知識、基本操作<br /></li>
<li>如文本操作、正則、掌握一些基本文本處理框架英文有NLTK、spaCy，中文有中研院CKIP Transformers、CKIP Tagger、Jeiba<br /></li>
<li>知道什麼是語言模型、利用語言模型來完成一些項目<br /></li>
<li>文本表示:將文本中的字符串轉化爲計算機當中的向量<br /></li>
<li>文本分類:分類模型傳統的一個解決方法就是標帶標註的語料，再特徵提取，然後訓分類器進行分類。這個分類器就會用比如說邏輯迴歸、貝葉斯、支持向量機、決策樹等等。<br /></li>
<li>主題模型:使用無監督學習的方式對文本中的隱含語義進行聚類的統計模型<br /></li>
<li>seq2seq模型:通過深度神經網絡將一個序列作爲映射爲另外一個輸出的序列。<br /></li>
<li>文本生成:GAN文本生成，也叫機器人寫作。<br /></li>
</ol>
</div>
</div>

<div id="outline-container-orgcbbff97" class="outline-2">
<h2 id="orgcbbff97"><span class="section-number-2">5.</span> 如何表示自然語言與字詞(數學化)</h2>
<div class="outline-text-2" id="text-5">
<p>
語言由文字構成，利用「字詞」組合成各種意思。電腦透過三種手段來理解「字詞」：<br />
</p>
<ol class="org-ol">
<li>詞庫<br /></li>
<li>計數<br /></li>
<li>推論(word2vec)<br /></li>
</ol>
</div>
<div id="outline-container-org0286423" class="outline-3">
<h3 id="org0286423"><span class="section-number-3">5.1.</span> 詞庫</h3>
<div class="outline-text-3" id="text-5-1">
<p>
表達詞意的方式之一是利用辭典，例如，以辭典查「車」的字詞會有對應的解釋。<br />
另一種方式是透過詞庫（thesaurus）型的辭典，將同義詞分在同一群中，如：<br />
</p>

<div id="orgb275d47" class="figure">
<p><img src="./images/thesaurus.png" alt="thesaurus.png" /><br />
</p>
</div>

<p>
透過這種方式定義字詞間的關係。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orga2efa70"></a>WordNet<br />
<div class="outline-text-4" id="text-5-1-1">
<p>
NLP中最有名的詞連: WordNet，由普林斯頓大學於1985年開發。<br />
詞庫的問題：<br />
</p>
<ol class="org-ol">
<li>難以因應時代變遷：舊詞與新詞（如，crowdfunding、比特幣）<br /></li>
<li>人工作業成本昂貴<br /></li>
<li>無法表現字詞的細微差異：vintage與retrospective<br /></li>
</ol>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgc74abfd" class="outline-3">
<h3 id="orgc74abfd"><span class="section-number-3">5.2.</span> 計數</h3>
<div class="outline-text-3" id="text-5-2">
<p>
語料庫(corpus):以研究自然語言處理或應用程式為目的所大量收集的文本資料。<br />
</p>
</div>
</div>
<div id="outline-container-orgecae144" class="outline-3">
<h3 id="orgecae144"><span class="section-number-3">5.3.</span> Python事前準備</h3>
<div class="outline-text-3" id="text-5-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'You say goodbye and I say hello.'</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">text</span> = text.replace(<span style="color: #98be65;">'.'</span>, <span style="color: #98be65;">' .'</span>)
<span class="linenr"> 3: </span><span style="color: #dcaeea;">words</span> = text.split(<span style="color: #98be65;">' '</span>)
<span class="linenr"> 4: </span><span style="color: #c678dd;">print</span>(words)
<span class="linenr"> 5: </span><span style="color: #dcaeea;">word_to_id</span> = {}
<span class="linenr"> 6: </span><span style="color: #dcaeea;">id_to_word</span> = {}
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #51afef;">for</span> word <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">words</span>:
<span class="linenr"> 9: </span>    <span style="color: #51afef;">if</span> word <span style="color: #51afef;">not</span> <span style="color: #51afef;">in</span> word_to_id:
<span class="linenr">10: </span>        new_id = <span style="color: #c678dd;">len</span>(word_to_id)
<span class="linenr">11: </span>        <span style="color: #dcaeea;">word_to_id</span>[<span style="color: #dcaeea;">word</span>] = new_id
<span class="linenr">12: </span>        <span style="color: #dcaeea;">id_to_word</span>[<span style="color: #dcaeea;">new_id</span>] = word
<span class="linenr">13: </span><span style="color: #c678dd;">print</span>(word_to_id)
<span class="linenr">14: </span><span style="color: #c678dd;">print</span>(id_to_word)
<span class="linenr">15: </span>
<span class="linenr">16: </span><span style="color: #dcaeea;">corpus</span> = [word_to_id[w] <span style="color: #51afef;">for</span> w <span style="color: #51afef;">in</span> words]
<span class="linenr">17: </span><span style="color: #c678dd;">print</span>(corpus) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#25991;&#23383;&#21477;&#23383;&#36681;&#25976;&#23383;&#34920;&#31034;</span>
</pre>
</div>

<pre class="example">
['You', 'say', 'goodbye', 'and', 'I', 'say', 'hello', '.']
{'You': 0, 'say': 1, 'goodbye': 2, 'and': 3, 'I': 4, 'hello': 5, '.': 6}
{0: 'You', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'I', 5: 'hello', 6: '.'}
[0, 1, 2, 3, 4, 1, 5, 6]
</pre>


<p>
corpus, word_to_id, id_to_word的產生可以寫成function<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">preprocess</span>(text):
<span class="linenr"> 2: </span>    <span style="color: #dcaeea;">text</span> = text.lower()
<span class="linenr"> 3: </span>    <span style="color: #dcaeea;">text</span> = text.replace(<span style="color: #98be65;">'.'</span>, <span style="color: #98be65;">' .'</span>)
<span class="linenr"> 4: </span>    <span style="color: #dcaeea;">words</span> = text.split(<span style="color: #98be65;">' '</span>)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>    <span style="color: #dcaeea;">word_to_id</span> = {}
<span class="linenr"> 7: </span>    <span style="color: #dcaeea;">id_to_word</span> = {}
<span class="linenr"> 8: </span>    <span style="color: #51afef;">for</span> word <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">words</span>:
<span class="linenr"> 9: </span>        <span style="color: #51afef;">if</span> word <span style="color: #51afef;">not</span> <span style="color: #51afef;">in</span> word_to_id:
<span class="linenr">10: </span>            new_id = <span style="color: #c678dd;">len</span>(word_to_id)
<span class="linenr">11: </span>            <span style="color: #dcaeea;">word_to_id</span>[<span style="color: #dcaeea;">word</span>] = new_id
<span class="linenr">12: </span>            <span style="color: #dcaeea;">id_to_word</span>[<span style="color: #dcaeea;">new_id</span>] = word
<span class="linenr">13: </span>
<span class="linenr">14: </span>    <span style="color: #dcaeea;">corpus</span> = np.array([word_to_id[w] <span style="color: #51afef;">for</span> w <span style="color: #51afef;">in</span> words])
<span class="linenr">15: </span>
<span class="linenr">16: </span>    <span style="color: #51afef;">return</span> corpus, word_to_id, id_to_word
</pre>
</div>

<pre class="example">
&gt;&gt;&gt;
</pre>
</div>
</div>
<div id="outline-container-orgba26ee3" class="outline-3">
<h3 id="orgba26ee3"><span class="section-number-3">5.4.</span> 字詞的分散式表示</h3>
<div class="outline-text-3" id="text-5-4">
<ul class="org-ul">
<li>RGB的向量表示可以設定各種顏色，可以從(R,G,B)=(201, 23, 30)大略猜出這是紅色系的顏色。<br /></li>
<li>字詞的分散式表示就是希望能以向量來表達詞意<br /></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org8feb464"></a>分布假說(distributional hypothesis)<br />
<div class="outline-text-4" id="text-5-4-1">
<p>
即，詞意是由周圍(前後)的字詞形成的。至於「周圍」的大小取決於window size，當window size=1，則取該字的前後一字詞為其上下文。<br />
</p>
</div>
</li>
<li><a id="orgc98198e"></a>共生矩陣<br />
<div class="outline-text-4" id="text-5-4-2">
<p>
計算一個字周圍的字詞<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> nlputil <span style="color: #51afef;">import</span> preprocess, create_co_matrix, ppmi
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'You say goodbye and I say hello.'</span>
<span class="linenr">4: </span><span style="color: #dcaeea;">corpus</span>, <span style="color: #dcaeea;">word_to_id</span>, <span style="color: #dcaeea;">id_to_word</span> = preprocess(text)
<span class="linenr">5: </span><span style="color: #dcaeea;">vocab_size</span> = <span style="color: #c678dd;">len</span>(id_to_word)
<span class="linenr">6: </span><span style="color: #dcaeea;">C</span> = create_co_matrix(corpus, vocab_size, window_size=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">7: </span><span style="color: #c678dd;">print</span>(text)
<span class="linenr">8: </span><span style="color: #c678dd;">print</span>(corpus)
<span class="linenr">9: </span><span style="color: #c678dd;">print</span>(C)
</pre>
</div>

<pre class="example">
You say goodbye and I say hello.
[0 1 2 3 4 1 5 6]
[[0 1 0 0 0 0 0]
 [1 0 1 0 1 1 0]
 [0 1 0 1 0 0 0]
 [0 0 1 0 1 0 0]
 [0 1 0 1 0 0 0]
 [0 1 0 0 0 0 1]
 [0 0 0 0 0 1 0]]
</pre>

<p>
其意思為<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">you</th>
<th scope="col" class="org-right">say</th>
<th scope="col" class="org-right">goodbye</th>
<th scope="col" class="org-right">and</th>
<th scope="col" class="org-right">I</th>
<th scope="col" class="org-right">hello</th>
<th scope="col" class="org-right">.</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">you</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">say</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">goodbye</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">and</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">I</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">hello</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">.</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>
</tbody>
</table>
<p>
如此解決了以向量表示字詞的目標。<br />
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgb16d079" class="outline-3">
<h3 id="orgb16d079"><span class="section-number-3">5.5.</span> 計算向量間的相似度</h3>
<div class="outline-text-3" id="text-5-5">
<p>
\[similarity(x,y)=\frac{x\cdot{y}}{\lvert\lvert{x}\rvert\rvert\lvert\lvert{y}\rvert\rvert}=\frac{x_1y_1+\cdots+x_ny_n}{\sqrt{x_1^2+\cdots+x_n^2}\sqrt{y_1^2+\cdots+y_n^2}}\]<br />
即，將向量正規化後取得內積，實作(假設x,y為numpy陣列):<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">cos_similarity</span>(x, y, eps=1e-<span style="color: #da8548; font-weight: bold;">8</span>):
<span class="linenr"> 3: </span>    <span style="color: #83898d;">'''&#35336;&#31639;&#39192;&#24358;&#30456;&#20284;&#24230;</span>
<span class="linenr"> 4: </span><span style="color: #83898d;">    :param x: &#21521;&#37327;</span>
<span class="linenr"> 5: </span><span style="color: #83898d;">    :param y: &#21521;&#37327;</span>
<span class="linenr"> 6: </span><span style="color: #83898d;">    :param eps: &#38450;&#27490;&#8221;&#38500;&#20197;0&#8221;&#30340;&#23567;&#25976;&#20540;</span>
<span class="linenr"> 7: </span><span style="color: #83898d;">    :return:</span>
<span class="linenr"> 8: </span><span style="color: #83898d;">    '''</span>
<span class="linenr"> 9: </span>    nx = x / (np.sqrt(np.<span style="color: #c678dd;">sum</span>(x ** <span style="color: #da8548; font-weight: bold;">2</span>)) + eps)
<span class="linenr">10: </span>    ny = y / (np.sqrt(np.<span style="color: #c678dd;">sum</span>(y ** <span style="color: #da8548; font-weight: bold;">2</span>)) + eps)
<span class="linenr">11: </span>    <span style="color: #51afef;">return</span> np.dot(nx, ny)
</pre>
</div>
<p>
上述程式在遇到零向量(向量元素皆為0)時會出現「除以0」的狀況，故加入一個epsilon解決問題。<br />
這樣就能計算字詞間的相似度，例如在句子&ldquo;You say goodbye and I say hello.&rdquo;中，You和I的相似度為:<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">from</span> nlputil <span style="color: #51afef;">import</span> preprocess, create_co_matrix, ppmi, cos_similarity
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'You say goodbye and I say hello.'</span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">corpus</span>, <span style="color: #dcaeea;">word_to_id</span>, <span style="color: #dcaeea;">id_to_word</span> = preprocess(text)
<span class="linenr"> 5: </span><span style="color: #dcaeea;">vocab_size</span> = <span style="color: #c678dd;">len</span>(id_to_word)
<span class="linenr"> 6: </span><span style="color: #dcaeea;">C</span> = create_co_matrix(corpus, vocab_size, window_size=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>c0 = C[word_to_id[<span style="color: #98be65;">'you'</span>]]
<span class="linenr"> 9: </span>c1 = C[word_to_id[<span style="color: #98be65;">'i'</span>]]
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>(cos_similarity(c0, c1))
</pre>
</div>

<pre class="example">
0.7071067691154799
</pre>
</div>
</div>
<div id="outline-container-orgc050b8f" class="outline-3">
<h3 id="orgc050b8f"><span class="section-number-3">5.6.</span> 相似詞排名</h3>
<div class="outline-text-3" id="text-5-6">
<p>
所有和 <i>you</i> 的字詞相似度排名<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">most_similar</span>(query, word_to_id, id_to_word, word_matrix, top=<span style="color: #da8548; font-weight: bold;">5</span>):
<span class="linenr"> 2: </span>    <span style="color: #83898d;">'''&#30456;&#20284;&#35422;&#25628;&#23563;</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #83898d;">    :param query: &#26597;&#35426;&#65288;&#25991;&#26412;&#65289;</span>
<span class="linenr"> 5: </span><span style="color: #83898d;">    :param word_to_id: &#23559;&#23383;&#35422;&#36681;&#25563;&#25104;&#23383;&#35422;ID&#30340;&#23383;&#20856;</span>
<span class="linenr"> 6: </span><span style="color: #83898d;">    :param id_to_word: &#23559;&#23383;&#35422;ID&#36681;&#25563;&#25104;&#23383;&#35422;&#30340;&#23383;&#20856;</span>
<span class="linenr"> 7: </span><span style="color: #83898d;">    :param word_matrix: &#25972;&#21512;&#35422;&#21521;&#37327;&#30340;&#30697;&#38499;&#12290;&#29992;&#20358;&#20786;&#23384;&#23565;&#25033;&#21508;&#21015;&#30340;&#35422;&#21521;&#37327;    :param top: &#35201;&#39023;&#31034;&#21040;&#31532;&#24190;&#21517;&#28858;&#27490;</span>
<span class="linenr"> 8: </span><span style="color: #83898d;">    '''</span>
<span class="linenr"> 9: </span>    <span style="color: #51afef;">if</span> query <span style="color: #51afef;">not</span> <span style="color: #51afef;">in</span> word_to_id:
<span class="linenr">10: </span>        <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'%s is not found'</span> % query)
<span class="linenr">11: </span>        <span style="color: #51afef;">return</span>
<span class="linenr">12: </span>
<span class="linenr">13: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'\n[query] '</span> + query)
<span class="linenr">14: </span>    query_id = word_to_id[query]
<span class="linenr">15: </span>    query_vec = word_matrix[query_id]
<span class="linenr">16: </span>
<span class="linenr">17: </span>    vocab_size = <span style="color: #c678dd;">len</span>(id_to_word)
<span class="linenr">18: </span>
<span class="linenr">19: </span>    similarity = np.zeros(vocab_size)
<span class="linenr">20: </span>    <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(vocab_size):
<span class="linenr">21: </span>        similarity[<span style="color: #dcaeea;">i</span>] = cos_similarity(word_matrix[i], query_vec)
<span class="linenr">22: </span>
<span class="linenr">23: </span>    count = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">24: </span>    <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> (-<span style="color: #da8548; font-weight: bold;">1</span> * similarity).argsort():
<span class="linenr">25: </span>        <span style="color: #51afef;">if</span> id_to_word[i] == query:
<span class="linenr">26: </span>            <span style="color: #51afef;">continue</span>
<span class="linenr">27: </span>        <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">' %s: %s'</span> % (id_to_word[i], similarity[i]))
<span class="linenr">28: </span>
<span class="linenr">29: </span>        count += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">30: </span>        <span style="color: #51afef;">if</span> count &gt;= top:
<span class="linenr">31: </span>            <span style="color: #51afef;">return</span>
</pre>
</div>
<p>
結果<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> nlputil <span style="color: #51afef;">import</span> preprocess, create_co_matrix, ppmi, most_similar
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'You say goodbye and I say hello.'</span>
<span class="linenr">4: </span><span style="color: #dcaeea;">corpus</span>, <span style="color: #dcaeea;">word_to_id</span>, <span style="color: #dcaeea;">id_to_word</span> = preprocess(text)
<span class="linenr">5: </span><span style="color: #dcaeea;">vocab_size</span> = <span style="color: #c678dd;">len</span>(id_to_word)
<span class="linenr">6: </span><span style="color: #dcaeea;">C</span> = create_co_matrix(corpus, vocab_size, window_size=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">7: </span>
<span class="linenr">8: </span>most_similar(<span style="color: #98be65;">'you'</span>, word_to_id, id_to_word, C, top=<span style="color: #da8548; font-weight: bold;">5</span>)
</pre>
</div>

<pre class="example">

[query] you
 goodbye: 0.7071067691154799
 i: 0.7071067691154799
 hello: 0.7071067691154799
 say: 0.0
 and: 0.0
</pre>
</div>
</div>
<div id="outline-container-orgca529fa" class="outline-3">
<h3 id="orgca529fa"><span class="section-number-3">5.7.</span> Pointwise Mutual Information (PMI)</h3>
<div class="outline-text-3" id="text-5-7">
<p>
從共生矩陣中看不出&ldquo;&#x2026;the car&#x2026;&rdquo;這種常見用法中the和car的強烈關聯性。<br />
\[PMI(x,y)=\log_2{\frac{P(x,y)}{P(x)P(y)}}=\log_2{\frac{\frac{C(x,y)}{N}}{\frac{C(x)}{N}\frac{C(y)}{N}}=\log_2{\frac{C(x,y)\cdot{N}}{C(x)C(y)}\]<br />
P(X)為字詞x在語料庫出現的機率、P(y)為字詞y在語料庫出現的機率、P(X,y)為字詞x與y同時在語料庫出現的機率。PMI越高、x,y關聯性越強。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">ppmi</span>(C, verbose=<span style="color: #a9a1e1;">False</span>, eps = 1e-<span style="color: #da8548; font-weight: bold;">8</span>):
<span class="linenr"> 2: </span>    <span style="color: #83898d;">'''&#24314;&#31435;PPMI&#65288;&#19979;&#19968;&#20491;&#27491;&#21521;&#40670;&#38291;&#20114;&#36039;&#35338;&#65289;</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #83898d;">    :param C: &#20849;&#29983;&#30697;&#38499;</span>
<span class="linenr"> 5: </span><span style="color: #83898d;">    :param verbose: &#26159;&#21542;&#36664;&#20986;&#22519;&#34892;&#29376;&#27841;</span>
<span class="linenr"> 6: </span><span style="color: #83898d;">    :return:</span>
<span class="linenr"> 7: </span><span style="color: #83898d;">    '''</span>
<span class="linenr"> 8: </span>    M = np.zeros_like(C, dtype=np.float32)
<span class="linenr"> 9: </span>    N = np.<span style="color: #c678dd;">sum</span>(C)
<span class="linenr">10: </span>    S = np.<span style="color: #c678dd;">sum</span>(C, axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">11: </span>    total = C.shape[<span style="color: #da8548; font-weight: bold;">0</span>] * C.shape[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">12: </span>    cnt = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">13: </span>
<span class="linenr">14: </span>    <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(C.shape[<span style="color: #da8548; font-weight: bold;">0</span>]):
<span class="linenr">15: </span>        <span style="color: #51afef;">for</span> j <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(C.shape[<span style="color: #da8548; font-weight: bold;">1</span>]):
<span class="linenr">16: </span>            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)
<span class="linenr">17: </span>            M[i, j] = <span style="color: #c678dd;">max</span>(<span style="color: #da8548; font-weight: bold;">0</span>, pmi)
<span class="linenr">18: </span>
<span class="linenr">19: </span>            <span style="color: #51afef;">if</span> verbose:
<span class="linenr">20: </span>                cnt += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">21: </span>                <span style="color: #51afef;">if</span> cnt % (total//<span style="color: #da8548; font-weight: bold;">100</span>) == <span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">22: </span>                    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'%.1f%% done'</span> % (<span style="color: #da8548; font-weight: bold;">100</span>*cnt/total))
<span class="linenr">23: </span>    <span style="color: #51afef;">return</span> M
</pre>
</div>
<p>
結果<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> nlputil <span style="color: #51afef;">import</span> preprocess, create_co_matrix, ppmi, most_similar
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'You say goodbye and I say hello.'</span>
<span class="linenr">4: </span><span style="color: #dcaeea;">corpus</span>, <span style="color: #dcaeea;">word_to_id</span>, <span style="color: #dcaeea;">id_to_word</span> = preprocess(text)
<span class="linenr">5: </span><span style="color: #dcaeea;">vocab_size</span> = <span style="color: #c678dd;">len</span>(id_to_word)
<span class="linenr">6: </span><span style="color: #dcaeea;">C</span> = create_co_matrix(corpus, vocab_size, window_size=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">7: </span>W = ppmi(C)
<span class="linenr">8: </span><span style="color: #c678dd;">print</span>(C)
<span class="linenr">9: </span><span style="color: #c678dd;">print</span>(W)
</pre>
</div>

<pre class="example" id="org1c35c4d">
[[0 1 0 0 0 0 0]
 [1 0 1 0 1 1 0]
 [0 1 0 1 0 0 0]
 [0 0 1 0 1 0 0]
 [0 1 0 1 0 0 0]
 [0 1 0 0 0 0 1]
 [0 0 0 0 0 1 0]]
[[0.        1.8073549 0.        0.        0.        0.        0.       ]
 [1.8073549 0.        0.8073549 0.        0.8073549 0.8073549 0.       ]
 [0.        0.8073549 0.        1.8073549 0.        0.        0.       ]
 [0.        0.        1.8073549 0.        1.8073549 0.        0.       ]
 [0.        0.8073549 0.        1.8073549 0.        0.        0.       ]
 [0.        0.8073549 0.        0.        0.        0.        2.807355 ]
 [0.        0.        0.        0.        0.        2.807355  0.       ]]
</pre>
</div>
</div>
<div id="outline-container-org5b87bc8" class="outline-3">
<h3 id="org5b87bc8"><span class="section-number-3">5.8.</span> 降維</h3>
<div class="outline-text-3" id="text-5-8">
<p>
dimensionality reduction: 刪減向量的維度並 <b>儘量保留</b> 「重要資訊」。<br />
如下圖:<br />
</p>

<div id="org7da6807" class="figure">
<p><img src="images/2022-05-10_13-36-40.png" alt="2022-05-10_13-36-40.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 3: </span>Caption</p>
</div>
</div>
<ol class="org-ol">
<li><a id="org8f40c51"></a>奇異值分解(Singular Value Decomposition, SVD)<br />
<div class="outline-text-4" id="text-5-8-1">
<p>
把任意矩陣分解成三個矩陣乘積：\(A=U\Sigma V^T\) ，U與V為正交矩陣(orthogonal matrix)，其行矩陣彼此正交，S為對角矩陣(對角線以外全部為0)<br />
&lt;#+CAPTION: Caption<br />
</p>
<p width="500">
<img src="images/20190622_22.jpg" alt="20190622_22.jpg" width="500" /><br />
我們可以將SVD視為變換矩陣A的三個分解步驟：<br />
</p>
<ol class="org-ol">
<li>旋轉\(V^T\)<br /></li>
<li>伸縮\(\Sigma\)<br /></li>
<li>再旋轉\(U\)<br /></li>
</ol>

<div id="org442ae4e" class="figure">
<p><img src="images/20190622_45.png" alt="20190622_45.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 4: </span>Caption</p>
</div>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> nlputil <span style="color: #51afef;">import</span> preprocess, create_co_matrix, ppmi
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'You say goodbye and I say hello.'</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">corpus</span>, <span style="color: #dcaeea;">word_to_id</span>, <span style="color: #dcaeea;">id_to_word</span> = preprocess(text)
<span class="linenr"> 7: </span><span style="color: #dcaeea;">vocab_size</span> = <span style="color: #c678dd;">len</span>(id_to_word)
<span class="linenr"> 8: </span><span style="color: #dcaeea;">C</span> = create_co_matrix(corpus, vocab_size, window_size=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 9: </span>W = ppmi(C)
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">SVD</span>
<span class="linenr">12: </span>U, S, V = np.linalg.svd(W)
<span class="linenr">13: </span>
<span class="linenr">14: </span>np.set_printoptions(precision=<span style="color: #da8548; font-weight: bold;">3</span>)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;3&#20301;&#26377;&#25928;&#25976;&#23383;</span>
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(C[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">16: </span><span style="color: #c678dd;">print</span>(W[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">17: </span><span style="color: #c678dd;">print</span>(U[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">18: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot</span>
<span class="linenr">19: </span><span style="color: #51afef;">for</span> word, word_id <span style="color: #51afef;">in</span> word_to_id.items():
<span class="linenr">20: </span>    plt.annotate(word, (U[word_id, <span style="color: #da8548; font-weight: bold;">0</span>], U[word_id, <span style="color: #da8548; font-weight: bold;">1</span>]))
<span class="linenr">21: </span>plt.scatter(U[:,<span style="color: #da8548; font-weight: bold;">0</span>], U[:,<span style="color: #da8548; font-weight: bold;">1</span>], alpha=<span style="color: #da8548; font-weight: bold;">0.5</span>)
<span class="linenr">22: </span>
<span class="linenr">23: </span>plt.savefig(<span style="color: #98be65;">'images/SVD-1.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<pre class="example">
[0 1 0 0 0 0 0]
[0.    1.807 0.    0.    0.    0.    0.   ]
[ 3.409e-01 -1.110e-16 -1.205e-01 -4.163e-16 -9.323e-01 -1.110e-16
 -2.426e-17]
[ 0.    -0.598  0.     0.18   0.    -0.781  0.   ]
[ 4.363e-01 -5.551e-17 -5.088e-01 -2.220e-16  2.253e-01 -1.388e-17
 -7.071e-01]
[ 1.665e-16 -4.978e-01  2.776e-17  6.804e-01 -1.110e-16  5.378e-01
  7.467e-17]
</pre>

<p>
利用SVD，可以把sparse vector W[0]轉變成dense vector U[0]，若要對U降維，則只要取U的前幾個元素，下圖為降成二維的結果。<br />
</p>
<p width="500">
<img src="images/SVD-1.png" alt="SVD-1.png" width="500" /><br />
若降維成三維，結果如下<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> nlputil <span style="color: #51afef;">import</span> preprocess, create_co_matrix, ppmi
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'You say goodbye and I say hello.'</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">corpus</span>, <span style="color: #dcaeea;">word_to_id</span>, <span style="color: #dcaeea;">id_to_word</span> = preprocess(text)
<span class="linenr"> 7: </span><span style="color: #dcaeea;">vocab_size</span> = <span style="color: #c678dd;">len</span>(id_to_word)
<span class="linenr"> 8: </span><span style="color: #dcaeea;">C</span> = create_co_matrix(corpus, vocab_size, window_size=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 9: </span>W = ppmi(C)
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">SVD</span>
<span class="linenr">12: </span>U, S, V = np.linalg.svd(W)
<span class="linenr">13: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">3D</span>
<span class="linenr">14: </span>fig = plt.figure()
<span class="linenr">15: </span>ax = fig.gca(projection=<span style="color: #98be65;">'3d'</span>)
<span class="linenr">16: </span>UX = <span style="color: #c678dd;">list</span>(word_to_id.keys())
<span class="linenr">17: </span><span style="color: #c678dd;">print</span>(UX)
<span class="linenr">18: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">ax.scatter(U[:,0],U[:,1],U[:,2], alpha=0.5)</span>
<span class="linenr">19: </span><span style="color: #51afef;">for</span> row, word <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(U, UX):
<span class="linenr">20: </span>    x = row[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">21: </span>    y = row[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">22: </span>    z = row[<span style="color: #da8548; font-weight: bold;">2</span>]
<span class="linenr">23: </span>    ax.scatter(x, y, z, alpha=<span style="color: #da8548; font-weight: bold;">0.5</span>)
<span class="linenr">24: </span>    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">print(x, y, z)</span>
<span class="linenr">25: </span>    ax.text(x, y, z,  <span style="color: #98be65;">'%s'</span> % (word), size=<span style="color: #da8548; font-weight: bold;">10</span>, zorder=<span style="color: #da8548; font-weight: bold;">1</span>,  color=<span style="color: #98be65;">'k'</span>)
<span class="linenr">26: </span>
<span class="linenr">27: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">28: </span>plt.savefig(<span style="color: #98be65;">'images/SVD-3D.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<pre class="example">
['you', 'say', 'goodbye', 'and', 'i', 'hello', '.']
</pre>


<div id="org208d6c5" class="figure">
<p><img src="images/SVD-3D.png" alt="SVD-3D.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 5: </span>Caption</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-orge5b1450" class="outline-3">
<h3 id="orge5b1450"><span class="section-number-3">5.9.</span> 使用正式的語料庫試試: PTB</h3>
<div class="outline-text-3" id="text-5-9">
</div>
<ol class="org-ol">
<li><a id="org76db5a1"></a>NLP中常用的語料庫(Penn Treebank)，目的是對語料進行標註(詞性及句法分析)，語料來源為1989年的華爾街日報，共1M words、2499篇文章。<br /></li>
<li><a id="orgc3bfd56"></a>下載PTB<br />
<div class="outline-text-4" id="text-5-9-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> sys
<span class="linenr"> 3: </span>sys.path.append(<span style="color: #98be65;">'..'</span>)
<span class="linenr"> 4: </span><span style="color: #51afef;">from</span> dataset <span style="color: #51afef;">import</span> ptb
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">corpus</span>, <span style="color: #dcaeea;">word_to_id</span>, <span style="color: #dcaeea;">id_to_word</span> = ptb.load_data(<span style="color: #98be65;">'train'</span>)
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'corpus size:'</span>, <span style="color: #c678dd;">len</span>(corpus))
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'corpus[:30]:'</span>, corpus[:<span style="color: #da8548; font-weight: bold;">30</span>])
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>()
<span class="linenr">11: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'id_to_word[0]:'</span>, id_to_word[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">12: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'id_to_word[1]:'</span>, id_to_word[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">13: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'id_to_word[2]:'</span>, id_to_word[<span style="color: #da8548; font-weight: bold;">2</span>])
<span class="linenr">14: </span><span style="color: #c678dd;">print</span>()
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"word_to_id['car']:"</span>, word_to_id[<span style="color: #98be65;">'car'</span>])
<span class="linenr">16: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"word_to_id['happy']:"</span>, word_to_id[<span style="color: #98be65;">'happy'</span>])
<span class="linenr">17: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"word_to_id['lexus']:"</span>, word_to_id[<span style="color: #98be65;">'lexus'</span>])
<span class="linenr">18: </span>
</pre>
</div>

<pre class="example" id="org3600bef">
corpus size: 929589
corpus[:30]: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29]

id_to_word[0]: aer
id_to_word[1]: banknote
id_to_word[2]: berlitz

word_to_id['car']: 3856
word_to_id['happy']: 4428
word_to_id['lexus']: 7426
</pre>
<p>
計算相似度<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> nlputil <span style="color: #51afef;">import</span> preprocess, create_co_matrix, ppmi, most_similar
<span class="linenr"> 4: </span><span style="color: #51afef;">from</span> dataset <span style="color: #51afef;">import</span> ptb
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">window_size</span> = <span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">wordvec_size</span> = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">corpus</span>, <span style="color: #dcaeea;">word_to_id</span>, <span style="color: #dcaeea;">id_to_word</span> = ptb.load_data(<span style="color: #98be65;">'train'</span>)
<span class="linenr">10: </span><span style="color: #dcaeea;">vocab_size</span> = <span style="color: #c678dd;">len</span>(word_to_id)
<span class="linenr">11: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'counting  co-occurrence ...'</span>)
<span class="linenr">12: </span><span style="color: #dcaeea;">C</span> = create_co_matrix(corpus, vocab_size, window_size)
<span class="linenr">13: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'calculating PPMI ...'</span>)
<span class="linenr">14: </span><span style="color: #dcaeea;">W</span> = ppmi(C, verbose=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">15: </span>
<span class="linenr">16: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'calculating SVD ...'</span>)
<span class="linenr">17: </span><span style="color: #51afef;">try</span>:
<span class="linenr">18: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">truncated SVD (fast!)</span>
<span class="linenr">19: </span>    <span style="color: #51afef;">from</span> sklearn.utils.extmath <span style="color: #51afef;">import</span> randomized_svd
<span class="linenr">20: </span>    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=<span style="color: #da8548; font-weight: bold;">5</span>,
<span class="linenr">21: </span>                             random_state=<span style="color: #a9a1e1;">None</span>)
<span class="linenr">22: </span><span style="color: #51afef;">except</span> <span style="color: #ECBE7B;">ImportError</span>:
<span class="linenr">23: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">SVD (slow)</span>
<span class="linenr">24: </span>    U, S, V = np.linalg.svd(W)
<span class="linenr">25: </span>
<span class="linenr">26: </span>word_vecs = U[:, :wordvec_size]
<span class="linenr">27: </span>
<span class="linenr">28: </span>querys = [<span style="color: #98be65;">'you'</span>, <span style="color: #98be65;">'year'</span>, <span style="color: #98be65;">'car'</span>, <span style="color: #98be65;">'toyota'</span>]
<span class="linenr">29: </span><span style="color: #51afef;">for</span> query <span style="color: #51afef;">in</span> querys:
<span class="linenr">30: </span>    most_similar(query, word_to_id, id_to_word, word_vecs, top=<span style="color: #da8548; font-weight: bold;">5</span>)
</pre>
</div>
<p>
計算結果(要跑很久&#x2026;)<br />
[query] you<br />
 i: 0.7198655605316162<br />
 we: 0.6279044151306152<br />
 do: 0.5669074654579163<br />
 &rsquo;ll: 0.5507640242576599<br />
 anybody: 0.5294491648674011<br />
</p>

<p>
[query] year<br />
 month: 0.722590446472168<br />
 earlier: 0.6536098122596741<br />
 quarter: 0.6418145895004272<br />
 last: 0.6153697371482849<br />
 fiscal: 0.581279993057251<br />
</p>

<p>
[query] car<br />
 auto: 0.5696402192115784<br />
 truck: 0.5232617259025574<br />
 luxury: 0.508746862411499<br />
 vehicle: 0.49371591210365295<br />
 domestic: 0.47702285647392273<br />
</p>

<p>
[query] toyota<br />
 motor: 0.7109977006912231<br />
 nissan: 0.661170482635498<br />
 motors: 0.6224923729896545<br />
 lexus: 0.6070802211761475<br />
 mazda: 0.6053721308708191<br />
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org1f0d6bb" class="outline-3">
<h3 id="org1f0d6bb"><span class="section-number-3">5.10.</span> 詞袋模型(Bag-of-words model)</h3>
<div class="outline-text-3" id="text-5-10">
<p>
詞袋模型（英語：Bag-of-words model）是個在自然語言處理和信息檢索(IR)下被簡化的表達模型。在此模型下，一段文本（比如一個句子或是一個文檔）可以用一個裝著這些詞的袋子來表示，這種表示方式不考慮文法以及詞的順序。<br />
</p>

<p>
詞袋模型被廣泛應用在文件分類，詞出現的頻率可以用來當作訓練分類器的特徵<br />
</p>
<ul class="org-ul">
<li><a href="https://en.wikipedia.org/wiki/Bag-of-words_model">https://en.wikipedia.org/wiki/Bag-of-words_model</a><br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">BoW1</span> = {<span style="color: #98be65;">"John"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"likes"</span>:<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #98be65;">"to"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"watch"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"movies"</span>:<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #98be65;">"Mary"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"too"</span>:<span style="color: #da8548; font-weight: bold;">1</span>};
<span class="linenr">2: </span><span style="color: #dcaeea;">BoW2</span> = {<span style="color: #98be65;">"Mary"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"also"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"likes"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"to"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"watch"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"football"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"games"</span>:<span style="color: #da8548; font-weight: bold;">1</span>};
<span class="linenr">3: </span><span style="color: #dcaeea;">BoW3</span> = {<span style="color: #98be65;">"John"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"likes"</span>:<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #98be65;">"to"</span>:<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #98be65;">"watch"</span>:<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #98be65;">"movies"</span>:<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #98be65;">"Mary"</span>:<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #98be65;">"too"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"also"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"football"</span>:<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #98be65;">"games"</span>:<span style="color: #da8548; font-weight: bold;">1</span>};
</pre>
</div>
<ul class="org-ul">
<li>只有上述單字,且以 BoW3 的順序為基準<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>(<span style="color: #da8548; font-weight: bold;">1</span>) <span style="color: #dcaeea;">BoW1</span> = [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">2: </span>(<span style="color: #da8548; font-weight: bold;">2</span>) <span style="color: #dcaeea;">BoW2</span> = [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>]
</pre>
</div>
</div>
<ol class="org-ol">
<li><a id="org757f85f"></a>Bag-of-words model<br />
<div class="outline-text-4" id="text-5-10-1">
<p>
To interpret the text by making a numerical representation of it.<br />
</p>

<p>
One way to do this is by using something called a &ldquo;bag-of-words&rdquo; model.<br />
This model simply counts the frequency of word tokens for each email and thereby represents it as a vector of these counts.<br />
</p>
</div>
</li>

<li><a id="orge8560e1"></a>Assemble matrices function**<br />
<div class="outline-text-4" id="text-5-10-2">
<p>
The assemble_bag() function assembles a new dataframe containing all the unique words found in the text documents.<br />
It counts the word frequency and then returns the new dataframe.<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">assemble_bag</span>(data):
<span class="linenr"> 2: </span>    <span style="color: #dcaeea;">used_tokens</span> = []
<span class="linenr"> 3: </span>    <span style="color: #dcaeea;">all_tokens</span> = []
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>    <span style="color: #51afef;">for</span> item <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">data</span>:
<span class="linenr"> 6: </span>        <span style="color: #51afef;">for</span> token <span style="color: #51afef;">in</span> item:
<span class="linenr"> 7: </span>            <span style="color: #51afef;">if</span> token <span style="color: #51afef;">in</span> all_tokens:
<span class="linenr"> 8: </span>                <span style="color: #51afef;">if</span> token <span style="color: #51afef;">not</span> <span style="color: #51afef;">in</span> used_tokens:
<span class="linenr"> 9: </span>                    used_tokens.append(token)
<span class="linenr">10: </span>            <span style="color: #51afef;">else</span>:
<span class="linenr">11: </span>                all_tokens.append(token)
<span class="linenr">12: </span>
<span class="linenr">13: </span>    df = pd.DataFrame(<span style="color: #da8548; font-weight: bold;">0</span>, index = np.arange(<span style="color: #c678dd;">len</span>(data)), columns = used_tokens)
<span class="linenr">14: </span>
<span class="linenr">15: </span>    <span style="color: #51afef;">for</span> i, item <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(data):
<span class="linenr">16: </span>        <span style="color: #51afef;">for</span> token <span style="color: #51afef;">in</span> item:
<span class="linenr">17: </span>            <span style="color: #51afef;">if</span> token <span style="color: #51afef;">in</span> used_tokens:
<span class="linenr">18: </span>                df.iloc[i][<span style="color: #dcaeea;">token</span>] += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">19: </span>    <span style="color: #51afef;">return</span> df
</pre>
</div>
</div>
</li>
<li><a id="org64cf054"></a>Putting It All Together To Assemble Dataset<br />
<div class="outline-text-4" id="text-5-10-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> os
<span class="linenr">2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">shuffle raw data first</span>
<span class="linenr">5: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">unison_shuffle_data</span>(data, header):
<span class="linenr">6: </span>    <span style="color: #dcaeea;">p</span> = np.random.permutation(<span style="color: #c678dd;">len</span>(header))
<span class="linenr">7: </span>    <span style="color: #dcaeea;">data</span> = data[p]
<span class="linenr">8: </span>    <span style="color: #dcaeea;">header</span> = np.asarray(header)[p]
<span class="linenr">9: </span>    <span style="color: #51afef;">return</span> data, header
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">load data in appropriate form</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">load_data</span>(path):
<span class="linenr"> 3: </span>    <span style="color: #dcaeea;">data</span>, <span style="color: #dcaeea;">sentiments</span> = [], []
<span class="linenr"> 4: </span>    <span style="color: #51afef;">for</span> folder, sentiment <span style="color: #51afef;">in</span> ((<span style="color: #98be65;">'neg'</span>, <span style="color: #da8548; font-weight: bold;">0</span>), (<span style="color: #98be65;">'pos'</span>, <span style="color: #da8548; font-weight: bold;">1</span>)):
<span class="linenr"> 5: </span>        <span style="color: #dcaeea;">folder</span> = os.path.join(path, folder)
<span class="linenr"> 6: </span>        <span style="color: #51afef;">for</span> name <span style="color: #51afef;">in</span> os.listdir(folder):
<span class="linenr"> 7: </span>            <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(os.path.join(folder, name), <span style="color: #98be65;">'r'</span>) <span style="color: #51afef;">as</span> <span style="color: #dcaeea;">reader</span>:
<span class="linenr"> 8: </span>                  text = reader.read()
<span class="linenr"> 9: </span>            <span style="color: #dcaeea;">text</span> = tokenize(text)
<span class="linenr">10: </span>            <span style="color: #dcaeea;">text</span> = stop_word_removal(text)
<span class="linenr">11: </span>            <span style="color: #dcaeea;">text</span> = reg_expressions(text)
<span class="linenr">12: </span>            data.append(text)
<span class="linenr">13: </span>            sentiments.append(sentiment)
<span class="linenr">14: </span>    <span style="color: #dcaeea;">data_np</span> = np.array(data)
<span class="linenr">15: </span>    <span style="color: #dcaeea;">data</span>, <span style="color: #dcaeea;">sentiments</span> = unison_shuffle_data(data_np, sentiments)
<span class="linenr">16: </span>
<span class="linenr">17: </span>    <span style="color: #51afef;">return</span> data, sentiments
<span class="linenr">18: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">train_path</span> = os.path.join(<span style="color: #98be65;">'aclImdb'</span>, <span style="color: #98be65;">'train'</span>)
<span class="linenr">2: </span><span style="color: #dcaeea;">test_path</span> = os.path.join(<span style="color: #98be65;">'aclImdb'</span>, <span style="color: #98be65;">'test'</span>)
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #dcaeea;">raw_data</span>, <span style="color: #dcaeea;">raw_header</span> = load_data(train_path)
<span class="linenr">5: </span>
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(raw_data.shape)
<span class="linenr">7: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(raw_header))
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Subsample required number of samples</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">random_indices</span> = np.random.choice(<span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(raw_header)),size=(Nsamp*<span style="color: #da8548; font-weight: bold;">2</span>,),replace=<span style="color: #a9a1e1;">False</span>)
<span class="linenr">3: </span>
<span class="linenr">4: </span>data_train = raw_data[random_indices]
<span class="linenr">5: </span>header = raw_header[random_indices]
<span class="linenr">6: </span>
<span class="linenr">7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"DEBUG::data_train::"</span>)
<span class="linenr">8: </span><span style="color: #c678dd;">print</span>(data_train)
</pre>
</div>
<ul class="org-ul">
<li>Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">unique_elements</span>, <span style="color: #dcaeea;">counts_elements</span> = np.unique(header, return_counts=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Sentiments and their frequencies:"</span>)
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(unique_elements)
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(counts_elements)
</pre>
</div>
</div>
</li>
<li><a id="org9ece658"></a>Featurize and Create Labels<br />
<div class="outline-text-4" id="text-5-10-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">MixedBagOfReviews</span> = assemble_bag(data_train)
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">this is the list of words in our bag-of-words model</span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">predictors</span> = [column <span style="color: #51afef;">for</span> column <span style="color: #51afef;">in</span> MixedBagOfReviews.columns]
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">expand default pandas display options to</span>
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">make emails more clearly visible when printed</span>
<span class="linenr"> 8: </span>pd.set_option(<span style="color: #98be65;">'display.max_colwidth'</span>, <span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>MixedBagOfReviews
<span class="linenr">11: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">you could do print(MixedBagOfReviews),</span>
<span class="linenr">12: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">but Jupyter displays this nicer for pandas DataFrames</span>
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">split into independent 70% training and 30% testing sets</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">data</span> = MixedBagOfReviews.values
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">idx</span> = <span style="color: #c678dd;">int</span>(<span style="color: #da8548; font-weight: bold;">0.7</span>*data.shape[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">70% of data for training</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">train_x</span> = data[:idx,:]
<span class="linenr"> 8: </span><span style="color: #dcaeea;">train_y</span> = header[:idx]
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">remaining 30% for testing</span>
<span class="linenr">11: </span><span style="color: #dcaeea;">test_x</span> = data[<span style="color: #dcaeea;">idx</span>:,:]
<span class="linenr">12: </span>test_y = header[idx:]
<span class="linenr">13: </span>
<span class="linenr">14: </span>
<span class="linenr">15: </span>
<span class="linenr">16: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"train_x/train_y list details, to make sure it is of the right form:"</span>)
<span class="linenr">17: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(train_x))
<span class="linenr">18: </span><span style="color: #c678dd;">print</span>(train_x)
<span class="linenr">19: </span><span style="color: #c678dd;">print</span>(train_y[:<span style="color: #da8548; font-weight: bold;">5</span>])
<span class="linenr">20: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(train_y))
</pre>
</div>
</div>
</li>
<li><a id="orgdea79d8"></a>fastText embedding vectors<br />
<ol class="org-ol">
<li><a id="orgd7a664a"></a>TL for NLP: section3.1_Movies_Word_Embeddin<br />
<div class="outline-text-5" id="text-5-10-5-1">
<p>
The following functions are used to extract fastText embedding vectors for each review<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> time
<span class="linenr">2: </span><span style="color: #51afef;">from</span> gensim.models <span style="color: #51afef;">import</span> FastText, KeyedVectors
<span class="linenr">3: </span>
<span class="linenr">4: </span>!ls
<span class="linenr">5: </span>
<span class="linenr">6: </span><span style="color: #dcaeea;">start</span>=time.time()
<span class="linenr">7: </span><span style="color: #dcaeea;">FastText_embedding</span> = KeyedVectors.load_word2vec_format(<span style="color: #98be65;">"../input/jigsaw/wiki.en.vec"</span>)
<span class="linenr">8: </span><span style="color: #dcaeea;">end</span> = time.time()
<span class="linenr">9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Loading the embedding took %d seconds"</span>%(end-start))
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">handle_out_of_vocab</span>(embedding,in_txt):
<span class="linenr"> 2: </span>    <span style="color: #dcaeea;">out</span> = <span style="color: #a9a1e1;">None</span>
<span class="linenr"> 3: </span>    <span style="color: #51afef;">for</span> word <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">in_txt</span>:
<span class="linenr"> 4: </span>        <span style="color: #51afef;">try</span>:
<span class="linenr"> 5: </span>            tmp = embedding[word]
<span class="linenr"> 6: </span>            <span style="color: #dcaeea;">tmp</span> = tmp.reshape(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #c678dd;">len</span>(tmp))
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>            <span style="color: #51afef;">if</span> out <span style="color: #51afef;">is</span> <span style="color: #a9a1e1;">None</span>:
<span class="linenr"> 9: </span>                out = tmp
<span class="linenr">10: </span>            <span style="color: #51afef;">else</span>:
<span class="linenr">11: </span>                out = np.concatenate((out,tmp),axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">12: </span>        <span style="color: #51afef;">except</span>:
<span class="linenr">13: </span>            <span style="color: #51afef;">pass</span>
<span class="linenr">14: </span>
<span class="linenr">15: </span>    <span style="color: #51afef;">return</span> out
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">assemble_embedding_vectors</span>(data):
<span class="linenr"> 2: </span>    <span style="color: #dcaeea;">out</span> = <span style="color: #a9a1e1;">None</span>
<span class="linenr"> 3: </span>    <span style="color: #51afef;">for</span> item <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">data</span>:
<span class="linenr"> 4: </span>        tmp = handle_out_of_vocab(FastText_embedding,item)
<span class="linenr"> 5: </span>        <span style="color: #51afef;">if</span> tmp <span style="color: #51afef;">is</span> <span style="color: #51afef;">not</span> <span style="color: #a9a1e1;">None</span>:
<span class="linenr"> 6: </span>            dim = tmp.shape[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 7: </span>            <span style="color: #51afef;">if</span> out <span style="color: #51afef;">is</span> <span style="color: #51afef;">not</span> <span style="color: #a9a1e1;">None</span>:
<span class="linenr"> 8: </span>                vec = np.mean(tmp,axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 9: </span>                vec = vec.reshape((<span style="color: #da8548; font-weight: bold;">1</span>,dim))
<span class="linenr">10: </span>                out = np.concatenate((out,vec),axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">11: </span>            <span style="color: #51afef;">else</span>:
<span class="linenr">12: </span>                out = np.mean(tmp,axis=<span style="color: #da8548; font-weight: bold;">0</span>).reshape((<span style="color: #da8548; font-weight: bold;">1</span>,dim))
<span class="linenr">13: </span>        <span style="color: #51afef;">else</span>:
<span class="linenr">14: </span>            <span style="color: #51afef;">pass</span>
<span class="linenr">15: </span>
<span class="linenr">16: </span>    <span style="color: #51afef;">return</span> out
<span class="linenr">17: </span>
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">shuffle raw data first</span>
<span class="linenr"> 5: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">unison_shuffle_data</span>(data, header):
<span class="linenr"> 6: </span>    <span style="color: #dcaeea;">p</span> = np.random.permutation(<span style="color: #c678dd;">len</span>(header))
<span class="linenr"> 7: </span>    <span style="color: #dcaeea;">data</span> = data[p]
<span class="linenr"> 8: </span>    <span style="color: #dcaeea;">header</span> = np.asarray(header)[p]
<span class="linenr"> 9: </span>    <span style="color: #51afef;">return</span> data, header
<span class="linenr">10: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">load data in appropriate form</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">load_data</span>(path):
<span class="linenr"> 3: </span>    <span style="color: #dcaeea;">data</span>, <span style="color: #dcaeea;">sentiments</span> = [], []
<span class="linenr"> 4: </span>    <span style="color: #51afef;">for</span> folder, sentiment <span style="color: #51afef;">in</span> ((<span style="color: #98be65;">'neg'</span>, <span style="color: #da8548; font-weight: bold;">0</span>), (<span style="color: #98be65;">'pos'</span>, <span style="color: #da8548; font-weight: bold;">1</span>)):
<span class="linenr"> 5: </span>        <span style="color: #dcaeea;">folder</span> = os.path.join(path, folder)
<span class="linenr"> 6: </span>        <span style="color: #51afef;">for</span> name <span style="color: #51afef;">in</span> os.listdir(folder):
<span class="linenr"> 7: </span>            <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(os.path.join(folder, name), <span style="color: #98be65;">'r'</span>) <span style="color: #51afef;">as</span> <span style="color: #dcaeea;">reader</span>:
<span class="linenr"> 8: </span>                  text = reader.read()
<span class="linenr"> 9: </span>            <span style="color: #dcaeea;">text</span> = tokenize(text)
<span class="linenr">10: </span>            <span style="color: #dcaeea;">text</span> = stop_word_removal(text)
<span class="linenr">11: </span>            <span style="color: #dcaeea;">text</span> = reg_expressions(text)
<span class="linenr">12: </span>            data.append(text)
<span class="linenr">13: </span>            sentiments.append(sentiment)
<span class="linenr">14: </span>    <span style="color: #dcaeea;">data_np</span> = np.array(data)
<span class="linenr">15: </span>    <span style="color: #dcaeea;">data</span>, <span style="color: #dcaeea;">sentiments</span> = unison_shuffle_data(data_np, sentiments)
<span class="linenr">16: </span>
<span class="linenr">17: </span>    <span style="color: #51afef;">return</span> data, sentiments
<span class="linenr">18: </span>
<span class="linenr">19: </span><span style="color: #dcaeea;">train_path</span> = os.path.join(<span style="color: #98be65;">'aclImdb'</span>, <span style="color: #98be65;">'train'</span>)
<span class="linenr">20: </span><span style="color: #dcaeea;">test_path</span> = os.path.join(<span style="color: #98be65;">'aclImdb'</span>, <span style="color: #98be65;">'test'</span>)
<span class="linenr">21: </span><span style="color: #dcaeea;">raw_data</span>, <span style="color: #dcaeea;">raw_header</span> = load_data(train_path)
<span class="linenr">22: </span>
<span class="linenr">23: </span><span style="color: #c678dd;">print</span>(raw_data.shape)
<span class="linenr">24: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(raw_header))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Subsample required number of samples</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">random_indices</span> = np.random.choice(<span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(raw_header)),size=(Nsamp*<span style="color: #da8548; font-weight: bold;">2</span>,),replace=<span style="color: #a9a1e1;">False</span>)
<span class="linenr">3: </span>data_train = raw_data[random_indices]
<span class="linenr">4: </span>header = raw_header[random_indices]
<span class="linenr">5: </span>
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"DEBUG::data_train::"</span>)
<span class="linenr">7: </span><span style="color: #c678dd;">print</span>(data_train)
</pre>
</div>
<ul class="org-ul">
<li>Display sentiments and their frequencies in the dataset, to ensure it is roughly balanced between classes<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">unique_elements</span>, <span style="color: #dcaeea;">counts_elements</span> = np.unique(header, return_counts=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Sentiments and their frequencies:"</span>)
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(unique_elements)
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(counts_elements)
</pre>
</div>
</div>
</li>
</ol>
</li>
</ol>
</div>
</div>

<div id="outline-container-org019300c" class="outline-2">
<h2 id="org019300c"><span class="section-number-2">6.</span> Representing text as numbers</h2>
<div class="outline-text-2" id="text-6">
<p>
Machine learning models take vectors (arrays of numbers) as input.<br />
</p>

<p>
When working with text, the first thing we must do come up with a strategy to convert strings to numbers (or to &ldquo;vectorize&rdquo; the text) before feeding it to the model.<br />
</p>
</div>
<div id="outline-container-org0c9b439" class="outline-3">
<h3 id="org0c9b439"><span class="section-number-3">6.1.</span> example</h3>
<div class="outline-text-3" id="text-6-1">
<p>
Consider the sentence<br />
</p>
<pre class="example" id="org306c0d5">
 The cat sat on the mat
</pre>
<p>
The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the).<br />
To represent each word, we will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word.<br />
</p>
<p width="300">
<img src="images/word-encoding.jpg" alt="word-encoding.jpg" width="300" /><br />
To create a vector that contains the encoding of the sentence, we could then concatenate the one-hot vectors for each word.<br />
</p>

<p>
This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine we have 10,000 words in the vocabulary. To one-hot encode each word, we would create a vector where 99.99% of the elements are zero.<br />
</p>

<p>
Continuing the example above, we could assign 1 to &ldquo;cat&rdquo;, 2 to &ldquo;mat&rdquo;, and so on.<br />
</p>

<p>
We could then encode the sentence &ldquo;The cat sat on the mat&rdquo; as a dense vector like [5, 1, 4, 3, 5, 2].<br />
</p>

<p>
This appoach is efficient. Instead of a sparse vector, we now have a dense one (where all elements are full).<br />
</p>

<p>
缺點有二:<br />
</p>
<ol class="org-ol">
<li>The integer-encoding is arbitrary (it does not capture any relationship between words).<br /></li>
<li>An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.<br /></li>
</ol>
</div>
</div>
<div id="outline-container-org9196c94" class="outline-3">
<h3 id="org9196c94"><span class="section-number-3">6.2.</span> Word embedding</h3>
<div class="outline-text-3" id="text-6-2">
<p>
Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding.<br />
</p>

<p>
Importantly, we do not have to specify this encoding by hand.<br />
</p>

<p>
An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets.<br />
A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.<br />
</p>

<div id="org2867d51" class="figure">
<p><img src="images/word-embedding.jpg" alt="word-embedding.jpg" width="300" /><br />
</p>
<p><span class="figure-number">Figure 6: </span>Word Embedding</p>
</div>

<p>
Each word is represented as a 4-dimensional vector of floating point values.<br />
Another way to think of an embedding is as &ldquo;lookup table&rdquo;.<br />
After these weights have been learned, we can encode each word by looking up the dense vector it corresponds to in the table<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orgb4934b3"></a>Learning embeddings from scratch<br />
<div class="outline-text-4" id="text-6-2-1">
<p>
<a href="https://www.tensorflow.org/tutorials/text/word_embeddings">https://www.tensorflow.org/tutorials/text/word_embeddings</a><br />
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgd45f0d7" class="outline-2">
<h2 id="orgd45f0d7"><span class="section-number-2">7.</span> 推論: word2vec</h2>
<div class="outline-text-2" id="text-7">
<p>
前章以「計數手法」取得字詞的分散式表示，本章要以「推論」取代「計數」，在推論中可以使用類神經網路，即word2vec。<br />
</p>
</div>
<div id="outline-container-org937177d" class="outline-3">
<h3 id="org937177d"><span class="section-number-3">7.1.</span> 計數 v.s. 推論</h3>
<div class="outline-text-3" id="text-7-1">
<ul class="org-ul">
<li>計數要建立共生矩陣，再套用到SVD獲得dense matrix，處理資料量為 \(n\times n\) 的矩陣，花費 \(O(n^2)\) 的時間成本。<br /></li>
<li>推論(以類神經網路為例)一般使用小批次進行學習。<br /></li>
</ul>

<div id="org7011cae" class="figure">
<p><img src="images/2022-05-10_11-39-22.png" alt="2022-05-10_11-39-22.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 7: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-org63fba5b" class="outline-3">
<h3 id="org63fba5b"><span class="section-number-3">7.2.</span> 推論</h3>
<div class="outline-text-3" id="text-7-2">
<p>
以下圖為例，推論的工作是由you與goodbye推論出?會出現哪個字詞。<br />
</p>
<p width="500">
<img src="images/2022-05-10_11-47-05.png" alt="2022-05-10_11-47-05.png" width="500" /><br />
在推論中，將上下文輸入某一模型，然後輸出各字詞可能出現的機率<br />
</p>

<div id="orgb223029" class="figure">
<p><img src="images/2022-05-10_11-49-39.png" alt="2022-05-10_11-49-39.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 8: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-org258afa8" class="outline-3">
<h3 id="org258afa8"><span class="section-number-3">7.3.</span> 在類神經網網路中表達字詞</h3>
<div class="outline-text-3" id="text-7-3">
<p>
類神經網路無法直接處理you、say等字詞，而是將這些字詞 <b>轉換</b> 為「固定長度的向量」，方法之一是使用one-hot-encoding。<br />
如:<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">字詞</th>
<th scope="col" class="org-right">字詞ID</th>
<th scope="col" class="org-left">one-hot encoding</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">you</td>
<td class="org-right">0</td>
<td class="org-left">(1,0,0,0,0,0,0)</td>
</tr>

<tr>
<td class="org-left">goodbye</td>
<td class="org-right">2</td>
<td class="org-left">(0,0,1,0,0,0,0)</td>
</tr>
</tbody>
</table>
</div>
<ol class="org-ol">
<li><a id="org9a0d7d4"></a>輸入層<br />
<div class="outline-text-4" id="text-7-3-1">
<p>
字詞以one-hot encoding表示後，輸入層就如下圖所示，每個字詞為一個神經元<br />
</p>
<p width="500">
<img src="images/2022-05-10_13-38-22.png" alt="2022-05-10_13-38-22.png" width="500" /><br />
每個神經元(字詞)以 <b>全連接(full-connected)</b> 方式連至中間層，每個箭頭都有權重（參數)，將權重與輸入層的神經元加權求平均就得到中間層。此處的權重為一矩陣，此處略過bias的概念。<br />
</p>

<div id="orga0d4805" class="figure">
<p><img src="images/2022-05-10_13-39-45.png" alt="2022-05-10_13-39-45.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 9: </span>Caption</p>
</div>
</div>
</li>
<li><a id="org94653dd"></a>輸入層至中間層的計算<br />
<div class="outline-text-4" id="text-7-3-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span><span style="color: #51afef;">from</span> nlplayers <span style="color: #51afef;">import</span> MatMul
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #dcaeea;">c</span> = np.array([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>]])
<span class="linenr">5: </span><span style="color: #dcaeea;">W</span> = np.random.randn(<span style="color: #da8548; font-weight: bold;">7</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">6: </span><span style="color: #dcaeea;">layer</span> = MatMul(W)
<span class="linenr">7: </span><span style="color: #dcaeea;">h</span> = layer.forward(c)
<span class="linenr">8: </span><span style="color: #c678dd;">print</span>(h)
</pre>
</div>

<pre class="example">
[[ 1.75309728  0.45025495 -1.25773857]]
</pre>

<p>
下圖為上述程式以字詞 <b>you</b> 和weight矩陣運算後得到的vector。這裡使用MatMul層，以forword()進行正向傳播，MatMul的內容大致如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">MatMul</span>:
<span class="linenr"> 2: </span>    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, W):
<span class="linenr"> 3: </span>        <span style="color: #51afef;">self</span>.params = [W]
<span class="linenr"> 4: </span>        <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">grads</span> = [np.zeros_like(W)]
<span class="linenr"> 5: </span>        <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">x</span> = <span style="color: #a9a1e1;">None</span>
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">forward</span>(<span style="color: #51afef;">self</span>, x):
<span class="linenr"> 8: </span>        <span style="color: #dcaeea;">W</span>, = <span style="color: #51afef;">self</span>.params
<span class="linenr"> 9: </span>        <span style="color: #dcaeea;">out</span> = np.dot(x, W)
<span class="linenr">10: </span>        <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">x</span> = x
<span class="linenr">11: </span>        <span style="color: #51afef;">return</span> out
<span class="linenr">12: </span>
<span class="linenr">13: </span>    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">backward</span>(<span style="color: #51afef;">self</span>, dout):
<span class="linenr">14: </span>        <span style="color: #dcaeea;">W</span>, = <span style="color: #51afef;">self</span>.params
<span class="linenr">15: </span>        <span style="color: #dcaeea;">dx</span> = np.dot(dout, W.T)
<span class="linenr">16: </span>        <span style="color: #dcaeea;">dW</span> = np.dot(<span style="color: #51afef;">self</span>.x.T, dout)
<span class="linenr">17: </span>        <span style="color: #51afef;">self</span>.grads[<span style="color: #da8548; font-weight: bold;">0</span>][...] = dW
<span class="linenr">18: </span>        <span style="color: #51afef;">return</span> dx
</pre>
</div>

<div id="orgf8151fd" class="figure">
<p><img src="images/2022-05-10_14-08-04.png" alt="2022-05-10_14-08-04.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 10: </span>Caption</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-orga1e437f" class="outline-3">
<h3 id="orga1e437f"><span class="section-number-3">7.4.</span> CBOW模型</h3>
<div class="outline-text-3" id="text-7-4">
<p>
CBOW有兩個輸入層（即前後字詞、各自有其weight matrix），中間層的神經元由兩個輸入層的全連接轉換後求其平均，即第一個輸入層變成 \(h_1\) 、第二個輸入層轉為 \(h_2\) 、中間層為 \(\frac{1}{2}(h_1+h_2)\) 。<br />
</p>

<div id="orgc078356" class="figure">
<p><img src="images/2022-05-10_14-15-25.png" alt="2022-05-10_14-15-25.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 11: </span>Caption</p>
</div>


<div id="org165fbcd" class="figure">
<p><img src="images/2022-05-10_14-20-05.png" alt="2022-05-10_14-20-05.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 12: </span>Caption</p>
</div>

<p>
以輸入you、goodbye兩個字詞為輸入層來看，其計算方式如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> common.layers <span style="color: #51afef;">import</span> MatMul
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27171;&#26412;&#30340;&#19978;&#19979;&#25991;&#36039;&#26009;</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">c0</span> = np.array([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>]])
<span class="linenr"> 8: </span><span style="color: #dcaeea;">c1</span> = np.array([[<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>]])
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21021;&#22987;&#21270;&#27402;&#37325;W_in = np.random.randn(7, 3)</span>
<span class="linenr">11: </span><span style="color: #dcaeea;">W_in</span> = np.random.randn(<span style="color: #da8548; font-weight: bold;">7</span>, <span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">12: </span><span style="color: #dcaeea;">W_out</span> = np.random.randn(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">7</span>)
<span class="linenr">13: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;&#21508;&#23652;</span>
<span class="linenr">14: </span><span style="color: #dcaeea;">in_layer0</span> = MatMul(W_in)
<span class="linenr">15: </span><span style="color: #dcaeea;">in_layer1</span> = MatMul(W_in)
<span class="linenr">16: </span><span style="color: #dcaeea;">out_layer</span> = MatMul(W_out)
<span class="linenr">17: </span>
<span class="linenr">18: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27491;&#21521;&#20659;&#25773;</span>
<span class="linenr">19: </span><span style="color: #dcaeea;">h0</span> = in_layer0.forward(c0)
<span class="linenr">20: </span><span style="color: #dcaeea;">h1</span> = in_layer1.forward(c1)
<span class="linenr">21: </span><span style="color: #dcaeea;">h</span> = <span style="color: #da8548; font-weight: bold;">0.5</span> * (h0 + h1)
<span class="linenr">22: </span><span style="color: #dcaeea;">s</span> = out_layer.forward(h)
<span class="linenr">23: </span><span style="color: #c678dd;">print</span>(s)
<span class="linenr">24: </span>
</pre>
</div>

<pre class="example">
[[-0.72028045 -0.02070112  0.71300738  0.57026871 -0.4409504  -3.70044407
  -0.27397258]]
</pre>
</div>
</div>
<div id="outline-container-org25a721b" class="outline-3">
<h3 id="org25a721b"><span class="section-number-3">7.5.</span> CBOW模型的學習</h3>
<div class="outline-text-3" id="text-7-5">
<p>
求出中間層的各神經元內容值後，再透過另一個輸出weight matrix加權計算即可得輸出層，最後經由Softmax函數將每一字詞的機率轉換為正解label，其流程如下所示：<br />
</p>
<p width="500">
<img src="images/2022-05-10_15-08-20.png" alt="2022-05-10_15-08-20.png" width="500" /><br />
如上例，若前後文為you、goodbye，則其label應為say；在一個擁有優質weight matrix的類神經網路中，我們可以期待對應到正解的神經元「機率」變高。<br />
這件事可以透過數學手段來解決:利用Softmax把分數轉成機率，從機率與label求出交叉熵誤差，把它當成損失函數(loss function)，學習目標就是找出能將中oss function值最小化的那組weight matrix。<br />
</p>

<div id="org89ca951" class="figure">
<p><img src="images/2022-05-10_15-23-46.png" alt="2022-05-10_15-23-46.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 13: </span>Caption</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org3a61432" class="outline-2">
<h2 id="org3a61432"><span class="section-number-2">8.</span> word2vec的學習</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org1e26c54" class="outline-3">
<h3 id="org1e26c54"><span class="section-number-3">8.1.</span> 先找出上下文與目標對象</h3>
<div class="outline-text-3" id="text-8-1">

<div id="orgbb84bbf" class="figure">
<p><img src="images/2022-05-10_15-30-52.png" alt="2022-05-10_15-30-52.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 14: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-orgafcf7fc" class="outline-3">
<h3 id="orgafcf7fc"><span class="section-number-3">8.2.</span> 將上下文與目標以one-hot encoding編碼</h3>
<div class="outline-text-3" id="text-8-2">

<div id="org34063f1" class="figure">
<p><img src="images/2022-05-10_15-33-23.png" alt="2022-05-10_15-33-23.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 15: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-org80c6faf" class="outline-3">
<h3 id="org80c6faf"><span class="section-number-3">8.3.</span> 執行CBOW模型</h3>
<div class="outline-text-3" id="text-8-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> sys
<span class="linenr"> 3: </span>sys.path.append(<span style="color: #98be65;">'..'</span>)
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span><span style="color: #51afef;">from</span> common.layers <span style="color: #51afef;">import</span> MatMul, SoftmaxWithLoss
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">SimpleCBOW</span>:
<span class="linenr"> 9: </span>    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, vocab_size, hidden_size):
<span class="linenr">10: </span>        <span style="color: #dcaeea;">V</span>, <span style="color: #dcaeea;">H</span> = vocab_size, hidden_size
<span class="linenr">11: </span>
<span class="linenr">12: </span>        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21021;&#22987;&#21270;&#27402;&#37325;</span>
<span class="linenr">13: </span>        <span style="color: #dcaeea;">W_in</span> = <span style="color: #da8548; font-weight: bold;">0.01</span> * np.random.randn(V, H).astype(<span style="color: #98be65;">'f'</span>)
<span class="linenr">14: </span>        <span style="color: #dcaeea;">W_out</span> = <span style="color: #da8548; font-weight: bold;">0.01</span> * np.random.randn(H, V).astype(<span style="color: #98be65;">'f'</span>)
<span class="linenr">15: </span>
<span class="linenr">16: </span>        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;&#21508;&#23652;</span>
<span class="linenr">17: </span>        <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">in_layer0</span> = MatMul(W_in)
<span class="linenr">18: </span>        <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">in_layer1</span> = MatMul(W_in)
<span class="linenr">19: </span>        <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">out_layer</span> = MatMul(W_out)
<span class="linenr">20: </span>        <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">loss_layer</span> = SoftmaxWithLoss()
<span class="linenr">21: </span>
<span class="linenr">22: </span>        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25226;&#20840;&#37096;&#30340;&#27402;&#37325;&#33287;&#26799;&#24230;&#25972;&#21512;&#25104;&#28165;&#21934;</span>
<span class="linenr">23: </span>        <span style="color: #dcaeea;">layers</span> = [<span style="color: #51afef;">self</span>.in_layer0, <span style="color: #51afef;">self</span>.in_layer1, <span style="color: #51afef;">self</span>.out_layer]
<span class="linenr">24: </span>        <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">params</span>, <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">grads</span> = [], []
<span class="linenr">25: </span>        <span style="color: #51afef;">for</span> layer <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">layers</span>:
<span class="linenr">26: </span>            <span style="color: #51afef;">self</span>.params += layer.params
<span class="linenr">27: </span>            <span style="color: #51afef;">self</span>.<span style="color: #dcaeea;">grads</span> += layer.grads
<span class="linenr">28: </span>
<span class="linenr">29: </span>        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22312;&#25104;&#21729;&#35722;&#25976;&#20013;&#65292;&#35373;&#23450;&#23383;&#35422;&#30340;&#20998;&#25955;&#24335;&#34920;&#31034;        self.word_vecs = W_in</span>
<span class="linenr">30: </span>
<span class="linenr">31: </span>    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">forward</span>(<span style="color: #51afef;">self</span>, contexts, target):
<span class="linenr">32: </span>        <span style="color: #dcaeea;">h0</span> = <span style="color: #51afef;">self</span>.in_layer0.forward(contexts[:, <span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">33: </span>        <span style="color: #dcaeea;">h1</span> = <span style="color: #51afef;">self</span>.in_layer1.forward(contexts[:, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">34: </span>        <span style="color: #dcaeea;">h</span> = (h0 + h1) * <span style="color: #da8548; font-weight: bold;">0.5</span>
<span class="linenr">35: </span>        <span style="color: #dcaeea;">score</span> = <span style="color: #51afef;">self</span>.out_layer.forward(h)
<span class="linenr">36: </span>        <span style="color: #dcaeea;">loss</span> = <span style="color: #51afef;">self</span>.loss_layer.forward(score, target)
<span class="linenr">37: </span>        <span style="color: #51afef;">return</span> loss
<span class="linenr">38: </span>
<span class="linenr">39: </span>    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">backward</span>(<span style="color: #51afef;">self</span>, dout=<span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr">40: </span>        ds = <span style="color: #51afef;">self</span>.loss_layer.backward(dout)
<span class="linenr">41: </span>        da = <span style="color: #51afef;">self</span>.out_layer.backward(ds)
<span class="linenr">42: </span>        da *= <span style="color: #da8548; font-weight: bold;">0.5</span>
<span class="linenr">43: </span>        <span style="color: #51afef;">self</span>.in_layer1.backward(da)
<span class="linenr">44: </span>        <span style="color: #51afef;">self</span>.in_layer0.backward(da)
<span class="linenr">45: </span>        <span style="color: #51afef;">return</span> <span style="color: #a9a1e1;">None</span>
<span class="linenr">46: </span>
</pre>
</div>
</div>
<ol class="org-ol">
<li><a id="org35e34f0"></a>forward()取得contexts與target兩個引數，回傳loss<br />
<div class="outline-text-4" id="text-8-3-1">
<ul class="org-ul">
<li>contexts為三維NumPy陣列，形狀為圖<a href="#org34063f1">15</a>中的(6, 2, 7)，表示有6個batch、上下文參考window size為2、人ne-hot encoding向量長度為7。<br /></li>
<li>target為二維，假設形狀為(6, 7)<br /></li>
</ul>
</div>
</li>
<li><a id="orga716497"></a>backward()執行反向傳播<br />
<div class="outline-text-4" id="text-8-3-2">
<p>
反向傳播由圖<a href="#org2ba4705">16</a>中的最右「1」開始，將之輸入Softmax with Loss層、再把Softmax with Loss層的反向輸出當作ds、將ds做為MatMul層的輸入。<br />
</p>

<div id="org2ba4705" class="figure">
<p><img src="images/2022-05-10_15-44-32.png" alt="2022-05-10_15-44-32.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 16: </span>Caption</p>
</div>
</div>
</li>
<li><a id="orgce47d6f"></a>學習<br />
<div class="outline-text-4" id="text-8-3-3">
<p>
這段找別的書來看，這本書用太多自己寫的code，麻煩，意義不太大<br />
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgd6b6509" class="outline-2">
<h2 id="orgd6b6509"><span class="section-number-2">9.</span> NLP實作: Fake News Classification</h2>
<div class="outline-text-2" id="text-9">
<ul class="org-ul">
<li>本段所有內容來自<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html">進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南</a><br /></li>
<li><a href="https://www.kaggle.com/c/fake-news-pair-classification-challenge">WSDM - Fake News Classification</a><br />
此競賽的目的在於想辦法自動找出假新聞以節省人工檢查的成本。資料集則是由中國的手機新聞應用：今日頭條的母公司字節跳動所提出的。（知名的抖音也是由該公司的產品）<sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup><br /></li>
</ul>
</div>
<div id="outline-container-org4bc1edd" class="outline-3">
<h3 id="org4bc1edd"><span class="section-number-3">9.1.</span> 資料集</h3>
<div class="outline-text-3" id="text-9-1">
<p>
訓練資料集（Training Set）約有 32 萬筆數據、測試資料集（Test Set）則約為 8 萬筆。<br />
</p>
<p width="500">
<img src="images/view-data-on-kaggle.jpg" alt="view-data-on-kaggle.jpg" width="500" /><br />
要了解此資料集，讓我們先專注在第一列（Row），大蒜與地溝油新聞的每一個欄位。<br />
</p>
<ul class="org-ul">
<li>第一欄位 title1_zh 代表的是「已知假新聞」 A 的中文標題：用大蒜鉴别地沟油的方法,怎么鉴别地沟油<br /></li>
<li>而第二欄位 title2_zh 則是一筆新的新聞 B 的中文標題，我們還不知道它的真偽：翻炒大蒜可鉴别地沟油<br /></li>
</ul>
<p>
要判斷第二欄中的新聞標題是否為真，我們可以把它跟已知的第一篇假新聞做比較，分為 3 個類別：<br />
</p>
<ul class="org-ul">
<li>unrelated：B 跟 A 沒有關係<br /></li>
<li>agreed：B 同意 A 的敘述<br /></li>
<li>disagreed：B 不同意 A 的敘述<br /></li>
</ul>
<p>
如果新聞 B 同意假新聞 A 的敘述的話，我們可以將 B 也視為一個假新聞；而如果 B 不同意假新聞 A 的敘述的話，我們可以放心地將 B 新聞釋出給一般大眾查看；如果 B 與 A 無關的話，可以考慮再進一步處理 B。<br />
第 3、 4 欄位則為新聞標題的英文翻譯。而因為該翻譯為 <b>機器翻譯</b> ， <b>不一定能 100% 正確反映本來中文新聞想表達的意思</b> ，因此接下來的文章會忽視這兩個欄位，只使用簡體中文的新聞標題來訓練 NLP 模型。<br />
</p>
</div>
</div>
<div id="outline-container-org7e74017" class="outline-3">
<h3 id="org7e74017"><span class="section-number-3">9.2.</span> 思路</h3>
<div class="outline-text-3" id="text-9-2">
<p>
現在任務目標: 要將有 32 萬筆數據的訓練資料集（Training Set）交給我們的 NLP 模型，讓它「閱讀」每一列裡頭的假新聞 A 與新聞 B 的標題並瞭解它們之間的關係（不相關、B 同意 A、B 不同意 A）。這是一個非常典型的機器學習（Machine Learning, ML）問題。我們當然希望不管使用什麼樣的模型，該模型都能夠幫我們減少人工檢查的成本，並同時最大化分類的準確度。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org6ac0247"></a>觀察資料集<br />
<div class="outline-text-4" id="text-9-2-1">
<p>
訓練集中的三個類別比例如下：<br />
</p>

<div id="org091a222" class="figure">
<p><img src="images/fake-news-classification-dist.jpg" alt="fake-news-classification-dist.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 17: </span>Caption</p>
</div>
<ul class="org-ul">
<li>這張圖顯示了訓練資料集（Training Set）裏頭各個分類所佔的比例。是一個常見的 Unbalanced Dataset：特定的分類佔了數據的大半比例。<br /></li>
<li>現在假設測試資料集（Test Set）的數據分佈跟訓練資料集相差不遠，且衡量一個分類模型的指標是準確度（Accuracy）：100 組成對新聞中，模型猜對幾組。<br /></li>
<li>這時候如果要你用一個簡單法則來分類所有成對新聞，並同時最大化準確度，你會怎麼做？<br /></li>
<li>對沒錯，就是全部猜 unrelated 就對了！<br /></li>
</ul>
<p>
此競賽採用Weighted Categorization Accuracy做為效能評估依據:<br />
\[ WeightedAccuracy(y, \hat{y}, \omega) = \frac{1}{n} \displaystyle{\sum_{i=1}^{n}} \frac{{\omega}_i(y_i=\hat{y}_i)}{\sum {\omega}_i} \]<br />
where<br />
</p>
<ul class="org-ul">
<li>\(y\) are ground truths<br /></li>
<li>\(\hat y\) are the predicted results<br /></li>
<li>\(\omega_i\) 為資料庫中第 \(i\) 個項目的權重<br /></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-orge030566" class="outline-3">
<h3 id="orge030566"><span class="section-number-3">9.3.</span> 資料前處理：讓機器能夠處理文字</h3>
<div class="outline-text-3" id="text-9-3">
<p>
即，如何將<br />
</p>
<p class="verse">
用大蒜鉴别地沟油的方法,怎么鉴别地沟油<br />
</p>
<p>
轉換成人腦不易理解，但很「機器友善」的數字序列（Sequence of Numbers）：<br />
</p>
<p class="verse">
[217, 1268, 32, 1178, 25, 489, 116]<br />
</p>
<p>
資料型態轉換流程:<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orga09a332"></a>讀入資料<br />
<div class="outline-text-4" id="text-9-3-1">
<p>
先讓我們用 Pandas 將訓練資料集讀取進來<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">2: </span><span style="color: #dcaeea;">train</span> = pd.read_csv(<span style="color: #98be65;">"./dataset/fakeNews.train.csv"</span>, index_col=<span style="color: #da8548; font-weight: bold;">0</span>, encoding=<span style="color: #98be65;">'utf-8'</span>, dtype=<span style="color: #c678dd;">str</span>)
<span class="linenr">3: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">train = pd.read_csv(TRAIN_CSV_PATH, index_col=0)</span>
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(train.head(<span style="color: #da8548; font-weight: bold;">3</span>))
</pre>
</div>

<pre class="example">
   tid1 tid2  ...                                          title2_en      label
id            ...
0     0    1  ...  Police disprove "bird's nest congress each per...  unrelated
3     2    3  ...  Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated
1     2    4  ...  The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated

[3 rows x 7 columns]
</pre>


<p>
為了畫面簡潔，讓我們只選取 2 個中文新聞標題以及分類結果（Label）的欄位：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">cols</span> = [<span style="color: #98be65;">'title1_zh'</span>,
<span class="linenr">2: </span>        <span style="color: #98be65;">'title2_zh'</span>,
<span class="linenr">3: </span>        <span style="color: #98be65;">'label'</span>]
<span class="linenr">4: </span><span style="color: #dcaeea;">train</span> = train.loc[:, cols]
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(train.columns)
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(train.head(<span style="color: #da8548; font-weight: bold;">3</span>))
</pre>
</div>

<pre class="example">
Index(['title1_zh', 'title2_zh', 'label'], dtype='object')
                            title1_zh                  title2_zh      label
id
0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗   警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京  unrelated
3   "你不来深圳，早晚你儿子也要来"，不出10年深圳人均GDP将超香港  深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小  unrelated
1   "你不来深圳，早晚你儿子也要来"，不出10年深圳人均GDP将超香港       GDP首超香港？深圳澄清：还差一点点……  unrelated
</pre>


<p>
有了必要的欄位以後，我們可以開始進行數據的前處理了。<br />
</p>
</div>
</li>
<li><a id="org0726122"></a>文本分詞（Text Segmentation）<br />
<div class="outline-text-4" id="text-9-3-2">
<p>
文本分詞（Text Segmentation）是一個將一連串文字切割成多個有意義的單位的步驟。這單位可以是<br />
</p>
<ul class="org-ul">
<li>一個中文漢字 / 英文字母（Character）<br /></li>
<li>一個中文詞彙 / 英文單字（Word）<br /></li>
<li>一個中文句子 / 英文句子（Sentence）<br /></li>
</ul>
<p>
以英文來說，Word Segmentation 十分容易。通常只要依照空白分割，就能得到一個有意義的詞彙列表了（在這邊讓我們先無視標點符號）：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'I am Meng Lee, a data scientist based in Tokyo.'</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">words</span> = text.split(<span style="color: #98be65;">' '</span>)
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(words)
</pre>
</div>
<pre class="example">
['I', 'am', 'Meng', 'Lee,', 'a', 'data', 'scientist', 'based', 'in', 'Tokyo.']
</pre>

<p>
但很明顯地，中文無法這樣做。這時候我們將藉助 Jieba 這個中文斷詞工具，來為一連串的文字做有意義的切割：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> jieba.posseg <span style="color: #51afef;">as</span> pseg
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'&#25105;&#26159;&#26446;&#23391;&#65292;&#22312;&#26481;&#20140;&#24037;&#20316;&#30340;&#25976;&#25818;&#31185;&#23416;&#23478;'</span>
<span class="linenr">4: </span><span style="color: #dcaeea;">words</span> = pseg.cut(text)
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>([word <span style="color: #51afef;">for</span> word <span style="color: #51afef;">in</span> words])
</pre>
</div>

<pre class="example">
[pair('我', 'r'), pair('是', 'v'), pair('李孟', 'nr'), pair('，', 'x'), pair('在', 'p'), pair('東京', 'ns'), pair('工作', 'vn'), pair('的', 'uj'), pair('數據', 'n'), pair('科學家', 'n')]
</pre>

<p>
如上所示，Jieba 將我們的中文文本切成有意義的詞彙列表，並為每個詞彙附上對應的詞性（Flag）。<br />
假設我們不需要標點符號，則只要將 flag == x 的詞彙去除即可。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> jieba.posseg <span style="color: #51afef;">as</span> pseg
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'&#25105;&#26159;&#26446;&#23391;&#65292;&#22312;&#26481;&#20140;&#24037;&#20316;&#30340;&#25976;&#25818;&#31185;&#23416;&#23478;'</span>
<span class="linenr">4: </span><span style="color: #dcaeea;">words</span> = pseg.cut(text)
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>([(x.word, x.flag) <span style="color: #51afef;">for</span> x <span style="color: #51afef;">in</span> words])
</pre>
</div>

<pre class="example">
[('我', 'r'), ('是', 'v'), ('李孟', 'nr'), ('，', 'x'), ('在', 'p'), ('東京', 'ns'), ('工作', 'vn'), ('的', 'uj'), ('數據', 'n'), ('科學家', 'n')]
</pre>


<p>
我們可以寫一個很簡單的 Jieba 斷詞函式，此函式能將輸入的文本 text 斷詞，並回傳除了標點符號以外的詞彙列表：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> jieba.posseg <span style="color: #51afef;">as</span> pseg
<span class="linenr">2: </span>
<span class="linenr">3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">jieba_tokenizer</span>(text):
<span class="linenr">4: </span>    <span style="color: #dcaeea;">words</span> = pseg.cut(text)
<span class="linenr">5: </span>    <span style="color: #51afef;">return</span> <span style="color: #98be65;">' '</span>.join([word <span style="color: #51afef;">for</span> word, flag <span style="color: #51afef;">in</span> words <span style="color: #51afef;">if</span> flag != <span style="color: #98be65;">'x'</span>])
<span class="linenr">6: </span>
<span class="linenr">7: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'&#25105;&#26159;&#26446;&#23391;&#65292;&#22312;&#26481;&#20140;&#24037;&#20316;&#30340;&#25976;&#25818;&#31185;&#23416;&#23478;'</span>
<span class="linenr">8: </span><span style="color: #c678dd;">print</span>(jieba_tokenizer(text))
</pre>
</div>

<pre class="example">
Building prefix dict from the default dictionary ...
Dumping model to file cache /var/folders/cb/t4v0kdcj6fj20jdzljy7y9240000gn/T/jieba.cache
Loading model cost 0.706 seconds.
Prefix dict has been built successfully.
我 是 李孟 在 東京 工作 的 數據 科學家
</pre>


<p>
我們可以利用 Pandas 的 apply 函式，將 jieba_tokenizer 套用到所有新聞標題 A 以及 B 之上，做文本分詞：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> jieba.posseg <span style="color: #51afef;">as</span> pseg
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">train</span> = pd.read_csv(<span style="color: #98be65;">"./dataset/fakeNews.train.csv"</span>, index_col=<span style="color: #da8548; font-weight: bold;">0</span>, encoding=<span style="color: #98be65;">'utf-8'</span>, dtype=<span style="color: #c678dd;">str</span>)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>cols = [<span style="color: #98be65;">'title1_zh'</span>,
<span class="linenr"> 7: </span>        <span style="color: #98be65;">'title2_zh'</span>,
<span class="linenr"> 8: </span>        <span style="color: #98be65;">'label'</span>]
<span class="linenr"> 9: </span>train = train.loc[:, cols]
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">jieba_tokenizer</span>(text):
<span class="linenr">12: </span>    words = pseg.cut(text)
<span class="linenr">13: </span>    <span style="color: #51afef;">return</span> <span style="color: #98be65;">' '</span>.join([word <span style="color: #51afef;">for</span> word, flag <span style="color: #51afef;">in</span> words <span style="color: #51afef;">if</span> flag != <span style="color: #98be65;">'x'</span>])
<span class="linenr">14: </span>
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(train.columns)
<span class="linenr">16: </span>train[<span style="color: #98be65;">'title1_tokenized'</span>] = train.loc[:, <span style="color: #98be65;">'title1_zh'</span>].astype(<span style="color: #c678dd;">str</span>).<span style="color: #c678dd;">apply</span>(jieba_tokenizer)
<span class="linenr">17: </span>train[<span style="color: #98be65;">'title2_tokenized'</span>] = train.loc[:, <span style="color: #98be65;">'title2_zh'</span>].astype(<span style="color: #c678dd;">str</span>).<span style="color: #c678dd;">apply</span>(jieba_tokenizer)
<span class="linenr">18: </span><span style="color: #c678dd;">print</span>(train.columns)
</pre>
</div>

<pre class="example">
Index(['title1_zh', 'title2_zh', 'label'], dtype='object')
Index(['title1_zh', 'title2_zh', 'label', 'title1_tokenized',
       'title2_tokenized'],
      dtype='object')
</pre>



<p>
新聞標題 A 的斷詞結果如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(train.iloc[:, [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">3</span>]].head())
</pre>
</div>

<pre class="example">
                            title1_zh                                 title1_tokenized
id
0       2017养老保险又新增两项，农村老人人人可申领，你领到了吗         2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗
3   "你不来深圳，早晚你儿子也要来"，不出10年深圳人均GDP将超香港  你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港
1   "你不来深圳，早晚你儿子也要来"，不出10年深圳人均GDP将超香港  你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港
2   "你不来深圳，早晚你儿子也要来"，不出10年深圳人均GDP将超香港  你 不 来 深圳 早晚 你 儿子 也 要 来 不出 10 年 深圳 人均 GDP 将 超 香港
9                "用大蒜鉴别地沟油的方法,怎么鉴别地沟油                       用 大蒜 鉴别 地沟油 的 方法 怎么 鉴别 地沟油
</pre>


<p>
新聞標題 B 的結果則為：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(train.iloc[:, [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">4</span>]].head())
</pre>
</div>

<pre class="example">
                     title2_zh                      title2_tokenized
id
0     警方辟谣“鸟巢大会每人领5万” 仍有老人坚持进京     警方 辟谣 鸟巢 大会 每人 领 5 万 仍 有 老人 坚持 进京
3    深圳GDP首超香港？深圳统计局辟谣：只是差距在缩小    深圳 GDP 首 超 香港 深圳 统计局 辟谣 只是 差距 在 缩小
1         GDP首超香港？深圳澄清：还差一点点……              GDP 首 超 香港 深圳 澄清 还 差 一点点
2   去年深圳GDP首超香港？深圳统计局辟谣：还差611亿  去年 深圳 GDP 首 超 香港 深圳 统计局 辟谣 还 差 611 亿
9      吃了30年食用油才知道，一片大蒜轻松鉴别地沟油     吃 了 30 年 食用油 才 知道 一片 大蒜 轻松 鉴别 地沟油
</pre>
</div>
</li>

<li><a id="orgc0dd142"></a>建立字典並將文本轉成數字序列<br />
<div class="outline-text-4" id="text-9-3-3">
<p>
當我們將完整的新聞標題切成一個個有意義的詞彙（Token）以後，下一步就是將這些詞彙轉換成一個數字序列，方便電腦處理。<br />
這些數字是所謂的索引（Index），分別對應到特定的詞彙。<br />
假設我們現在就只有一個新聞標題：<br />
</p>
<p class="verse">
狐狸被陌生人拍照<br />
</p>
<p>
我們首先會對此標題做斷詞，將句子分成多個有意義的詞彙：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> jieba.posseg <span style="color: #51afef;">as</span> pseg
<span class="linenr">2: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'&#29392;&#29432;&#34987;&#38476;&#29983;&#20154;&#25293;&#29031;'</span>
<span class="linenr">3: </span><span style="color: #dcaeea;">words</span> = pseg.cut(text)
<span class="linenr">4: </span><span style="color: #dcaeea;">words</span> = [w <span style="color: #51afef;">for</span> w, f <span style="color: #51afef;">in</span> words]
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(words)
</pre>
</div>

<pre class="example">
['狐狸', '被', '陌生人', '拍照']
</pre>

<p>
有了詞彙的列表以後，我們可以建立一個字典 word_index。<br />
該 dict 裏頭將上面的 4 個詞彙當作鍵值（Key），每個鍵值對應的值（Value）則為不重複的數字：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">word_index</span> = {
<span class="linenr">2: </span>    word: idx
<span class="linenr">3: </span>    <span style="color: #51afef;">for</span> idx, word <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(words)
<span class="linenr">4: </span>}
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(word_index)
</pre>
</div>

<pre class="example">
{'狐狸': 0, '被': 1, '陌生人': 2, '拍照': 3}
</pre>

<p>
有了這個字典以後，我們就能把該句子轉成一個數字序列：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(words)
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>([word_index[w] <span style="color: #51afef;">for</span> w <span style="color: #51afef;">in</span> words])
</pre>
</div>

<pre class="example">
['狐狸', '被', '陌生人', '拍照']
[0, 1, 2, 3]
</pre>


<p>
如果來了一個新的句子：<br />
</p>
<p class="verse">
陌生人被狐狸拍照<br />
</p>
<p>
我們也能利用手上已有的字典 word_index 如法炮製：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">text</span> = <span style="color: #98be65;">'&#38476;&#29983;&#20154;&#34987;&#29392;&#29432;&#25293;&#29031;'</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">words</span> = pseg.cut(text)
<span class="linenr">3: </span><span style="color: #dcaeea;">words</span> = [w <span style="color: #51afef;">for</span> w, f <span style="color: #51afef;">in</span> words]
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(words)
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>([word_index[w] <span style="color: #51afef;">for</span> w <span style="color: #51afef;">in</span> words])
</pre>
</div>

<pre class="example">
['陌生人', '被', '狐狸', '拍照']
[2, 1, 0, 3]
</pre>

<p>
在這個簡單的狐狸例子裡頭，word_index 就是我們的字典；我們利用該字典，將 1 句話轉成包含多個數字的序列，而每個數字實際上代表著一個 Token。<br />
</p>
</div>
</li>
<li><a id="orgcf16b49"></a>如何將新聞標題轉為數字序列<br />
<div class="outline-text-4" id="text-9-3-4">
<ol class="org-ol">
<li>將已被斷詞的新聞標題 A 以及新聞標題 B 全部倒在一起<br /></li>
<li>建立一個空字典<br /></li>
<li>查看所有新聞標題，裏頭每出現一個字典裡頭沒有的詞彙，就為該詞彙指定一個字典裡頭還沒出現的索引數字，並將該詞彙放入字典<br /></li>
<li>利用建好的字典，將每個新聞標題裡頭包含的詞彙轉換成數字<br /></li>
</ol>
</div>
<ol class="org-ol">
<li><a id="org51b3d98"></a>步驟1: 將已被斷詞的新聞標題 A 以及新聞標題 B 全部倒在一起<br />
<div class="outline-text-5" id="text-9-3-4-1">
<p>
上述步驟幾乎是NLP的必要動作，許多套件都提供了相應的function，如Keras<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> keras
<span class="linenr">2: </span><span style="color: #dcaeea;">MAX_NUM_WORDS</span> = <span style="color: #da8548; font-weight: bold;">10000</span>
<span class="linenr">3: </span><span style="color: #dcaeea;">tokenizer</span> = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS)
</pre>
</div>

<p>
Tokenizer 顧名思義，即是將一段文字轉換成一系列的詞彙（Tokens），並為其建立字典。這邊的 num_words=10000 代表我們限制字典只能包含 10,000 個詞彙，一旦字典達到這個大小以後，剩餘的新詞彙都會被視為 Unknown，以避免字典過於龐大。<br />
</p>

<p>
如同上述的步驟 1，我們得將新聞 A 及新聞 B 的標題全部聚集起來，為它們建立字典：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">corpus_x1</span> = train.title1_tokenized
<span class="linenr">2: </span><span style="color: #dcaeea;">corpus_x2</span> = train.title2_tokenized
<span class="linenr">3: </span><span style="color: #dcaeea;">corpus</span> = pd.concat([
<span class="linenr">4: </span>    corpus_x1, corpus_x2])
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(corpus.shape)
</pre>
</div>

<pre class="example">
(641104,)
</pre>


<p>
因為訓練集有大約 32 萬列（Row）的成對新聞（每一列包含 2 筆新聞：A &amp; B），因此將所有新聞放在一起的話，就有 2 倍的大小。而這些文本的集合在習慣上被稱作語料庫（Text Corpus），代表著我們有的所有文本數據。以下是我們語料庫的一小部分：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(pd.DataFrame(corpus.iloc[<span style="color: #da8548; font-weight: bold;">14</span>:<span style="color: #da8548; font-weight: bold;">19</span>], columns=[<span style="color: #98be65;">'title'</span>]))
</pre>
</div>

<pre class="example">
                                         title
id
15   飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 今天 最 催泪 的 一幕
17      飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 最 催泪 的 一幕
14                 男人 在 机舱 口 跪下 原来 一切 都 只 因为 爱
16   飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 今天 最 催泪 的 一幕
18  飞机 就要 起飞 一个 男人 在 机舱 口 跪下 这 是 见 过 最 催泪 的 一幕
</pre>
</div>
</li>
<li><a id="org19de9dd"></a>步驟2,3: 呼叫 tokenizer 為我們查看所有文本，並建立一個字典<br />
<div class="outline-text-5" id="text-9-3-4-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>tokenizer.fit_on_texts(corpus)
</pre>
</div>

<p>
以我們的語料庫大小來說，這大約需時 10 秒鐘。而等到 tokenizer 建好字典以後，我們可以進行上述第 4 個步驟，請 tokenizer 利用內部生成的字典分別將我們的新聞標題 A 與 新聞 B 轉換成數字序列：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">x1_train</span> = tokenizer.texts_to_sequences(corpus_x1)
<span class="linenr">2: </span><span style="color: #dcaeea;">x2_train</span> = tokenizer.texts_to_sequences(corpus_x2)
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(x1_train))
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(x1_train[:<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(pd.DataFrame(corpus.iloc[:<span style="color: #da8548; font-weight: bold;">1</span>], columns=[<span style="color: #98be65;">'title'</span>]))
</pre>
</div>

<pre class="example">
320552
[[217, 1268, 32, 1178, 5967, 25, 489, 2877, 116, 5559, 4, 1850, 2, 13]]
                                       title
id
0   2017 养老保险 又 新增 两项 农村 老人 人人 可 申领 你 领到 了 吗
</pre>


<p>
x1_train 為一個 Python list，裡頭包含了每一筆假新聞標題 A 對應的數字序列。<br />
讓我們利用 tokenizer.index_word 來將索引數字對應回本來的詞彙：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">for</span> seq <span style="color: #51afef;">in</span> x1_train[:<span style="color: #da8548; font-weight: bold;">1</span>]:
<span class="linenr">2: </span>    <span style="color: #c678dd;">print</span>([tokenizer.index_word[idx] <span style="color: #51afef;">for</span> idx <span style="color: #51afef;">in</span> seq])
</pre>
</div>

<pre class="example">
['2017', '养老保险', '又', '新增', '两项', '农村', '老人', '人人', '可', '申领', '你', '领到', '了', '吗']
</pre>
</div>
</li>
</ol>
</li>
<li><a id="orga9152e0"></a>序列的 Zero Padding<br />
<div class="outline-text-4" id="text-9-3-5">
<p>
雖然我們已經將每個新聞標題的文本轉為一行行的數字序列，你會發現每篇標題的序列長度並不相同：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">for</span> seq <span style="color: #51afef;">in</span> x1_train[:<span style="color: #da8548; font-weight: bold;">10</span>]:
<span class="linenr">2: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(seq), seq[:<span style="color: #da8548; font-weight: bold;">5</span>], <span style="color: #98be65;">' ...'</span>)
</pre>
</div>

<pre class="example" id="org4ee951c">
14 [217, 1268, 32, 1178, 5967]  ...
19 [4, 10, 47, 678, 2558]  ...
19 [4, 10, 47, 678, 2558]  ...
19 [4, 10, 47, 678, 2558]  ...
9 [31, 320, 3372, 3062, 1]  ...
19 [4, 10, 47, 678, 2558]  ...
6 [7, 2221, 1, 2072, 7]  ...
19 [4, 10, 47, 678, 2558]  ...
14 [1281, 1211, 427, 3, 3245]  ...
9 [31, 320, 3372, 3062, 1]  ...
</pre>
<p>
而為了方便之後的 NLP 模型處理（見循環神經網路一章），一般會設定一個 MAX_SEQUENCE_LENGTH 來讓所有序列的長度一致。長度超過此數字的序列尾巴會被刪掉；而針對原來長度不足的序列，我們則會在詞彙前面補零。Keras 一樣有個方便函式 pad_sequences 來幫助我們完成這件工作：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">MAX_SEQUENCE_LENGTH</span> = <span style="color: #da8548; font-weight: bold;">20</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">x1_train</span> = keras.preprocessing.sequence.pad_sequences(x1_train,
<span class="linenr"> 3: </span>                   maxlen=MAX_SEQUENCE_LENGTH)
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>x2_train = keras.preprocessing.sequence.pad_sequences(x2_train,
<span class="linenr"> 6: </span>                   maxlen=MAX_SEQUENCE_LENGTH)
<span class="linenr"> 7: </span><span style="color: #51afef;">for</span> seq <span style="color: #51afef;">in</span> x1_train[:<span style="color: #da8548; font-weight: bold;">3</span>]:
<span class="linenr"> 8: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(seq), seq[:<span style="color: #da8548; font-weight: bold;">5</span>], <span style="color: #98be65;">' ...'</span>)
<span class="linenr"> 9: </span><span style="color: #51afef;">for</span> seq <span style="color: #51afef;">in</span> x2_train[:<span style="color: #da8548; font-weight: bold;">3</span>]:
<span class="linenr">10: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(seq), seq[:<span style="color: #da8548; font-weight: bold;">5</span>], <span style="color: #98be65;">' ...'</span>)
<span class="linenr">11: </span><span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">'Zero Padding&#24460;x1_train[0]&#30340;&#20839;&#23481;:</span>{x1_train[0]}<span style="color: #98be65;">'</span>)
</pre>
</div>

<pre class="example">
20 [0 0 0 0 0]  ...
20 [  0   4  10  47 678]  ...
20 [  0   4  10  47 678]  ...
20 [0 0 0 0 0]  ...
20 [0 0 0 0 0]  ...
20 [0 0 0 0 0]  ...
Zero Padding後x1_train[0]的內容:[   0    0    0    0    0    0  217 1268   32 1178 5967   25  489 2877
  116 5559    4 1850    2   13]
</pre>

<p>
其中，x1_train[0]因為長度不足20(只有14)，故前6項被補0，所有的新聞標題也是:<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">for</span> seq <span style="color: #51afef;">in</span> x1_train + x2_train:
<span class="linenr">2: </span>    <span style="color: #51afef;">assert</span> <span style="color: #c678dd;">len</span>(seq) == <span style="color: #da8548; font-weight: bold;">20</span>
<span class="linenr">3: </span>
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#25152;&#26377;&#26032;&#32862;&#27161;&#38988;&#30340;&#24207;&#21015;&#38263;&#24230;&#30342;&#28858; 20 !"</span>)
</pre>
</div>

<pre class="example">
所有新聞標題的序列長度皆為 20 !
</pre>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(x1_train[:<span style="color: #da8548; font-weight: bold;">3</span>])
</pre>
</div>

<pre class="example">
[[   0    0    0    0    0    0  217 1268   32 1178 5967   25  489 2877
   116 5559    4 1850    2   13]
 [   0    4   10   47  678 2558    4  166   34   17   47 5150   63   15
   678 4502 3211   23  284 1181]
 [   0    4   10   47  678 2558    4  166   34   17   47 5150   63   15
   678 4502 3211   23  284 1181]]
</pre>

<p>
接下來要處理label<br />
</p>
</div>
</li>
<li><a id="org3b4aa88"></a>將正解做 One-hot Encoding<br />
<div class="outline-text-4" id="text-9-3-6">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(train.label[:<span style="color: #da8548; font-weight: bold;">5</span>])
</pre>
</div>

<pre class="example">
id
0    unrelated
3    unrelated
1    unrelated
2    unrelated
9       agreed
Name: label, dtype: object
</pre>

<p>
label 的處理相對簡單。跟新聞標題相同，我們一樣需要一個字典將分類的文字轉換成索引：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23450;&#32681;&#27599;&#19968;&#20491;&#20998;&#39006;&#23565;&#25033;&#21040;&#30340;&#32034;&#24341;&#25976;&#23383;</span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">label_to_index</span> = {
<span class="linenr"> 5: </span>    <span style="color: #98be65;">'unrelated'</span>: <span style="color: #da8548; font-weight: bold;">0</span>,
<span class="linenr"> 6: </span>    <span style="color: #98be65;">'agreed'</span>: <span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr"> 7: </span>    <span style="color: #98be65;">'disagreed'</span>: <span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr"> 8: </span>}
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#20998;&#39006;&#27161;&#31844;&#23565;&#25033;&#21040;&#21083;&#23450;&#32681;&#30340;&#25976;&#23383;</span>
<span class="linenr">11: </span><span style="color: #dcaeea;">y_train</span> = train.label.<span style="color: #c678dd;">apply</span>(
<span class="linenr">12: </span>    <span style="color: #51afef;">lambda</span> <span style="color: #dcaeea;">x</span>: label_to_index[x])
<span class="linenr">13: </span>
<span class="linenr">14: </span>y_train = np.asarray(y_train).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(y_train[:<span style="color: #da8548; font-weight: bold;">20</span>])
</pre>
</div>

<pre class="example">
[0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 2. 0. 0. 0. 0.]
</pre>


<p>
現在每個分類的文字標籤都已經被轉成對應的數字，接著讓我們利用 Keras 做 One Hot Encoding：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">from tensorflow.keras.utils import to_categorical</span>
<span class="linenr">2: </span><span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr">3: </span><span style="color: #dcaeea;">y_train</span> = np_utils.to_categorical(y_train)
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(y_train[:<span style="color: #da8548; font-weight: bold;">5</span>])
</pre>
</div>

<pre class="example">
[[1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [1. 0. 0.]
 [0. 1. 0.]]
</pre>

<p>
上述矩陣的每一列即為 1 個 label，而你可以看到現在每個 label 都從 1 個數字變成一個 3 維的向量（Vector）。每 1 維度則對應到 1 個分類：<br />
</p>
<ul class="org-ul">
<li>[1, 0, 0] 代表 label 為 unrelated<br /></li>
<li>[0, 1, 0] 代表 label 為 agreed<br /></li>
<li>[0, 0, 1] 代表 label 為 disagreed<br /></li>
</ul>
<p>
用這樣的方式表達 label 的好處是我們可以把分類結果想成機率分佈。[1, 0, 0] 就代表一組新聞標題 A、B 為 unrelated 的機率等於 100 %；假設預測的結果為[0.7, 0.2, 0.1]，則代表模型認為這 2 個新聞標題的關係有 70 % 的機率為 unrelated、20 % 的機率是 agreed 而 10 % 為 disagreed。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org2acd2ec"></a>Why One-Hot Encoding?<br />
<div class="outline-text-5" id="text-9-3-6-1">
<p>
轉換後有兩個優點<br />
1.能夠處理非連續型的分類變量。<br />
2.在一定程度上也擴充了特徵，讓特徵之間更稀疏。<br />
</p>
</div>
</li>
</ol>
</li>
<li><a id="org7d9165f"></a>切割訓練資料集 &amp; 驗證資料集<br />
<div class="outline-text-4" id="text-9-3-7">
<ul class="org-ul">
<li>驗證集: 我們會反覆在 Train / Valid Set 上訓練並測試模型，最後用 Test Set 一決生死，至於驗證集的必要，簡而言之，當你多次利用驗證資料集的預測結果以修正模型，並讓它在該資料集表現更好時，過適（Overfitting）的風險就已經產生了。<br /></li>
<li>測試集: 儘管你沒有直接讓模型看到驗證資料集（Validation Set）內的任何數據，你還是間接地洩漏了該資料集的重要資訊：你讓模型知道怎樣的參數設定會讓它在該資料集表現比較好，亦或表現較差。因此有一個完全跟模型訓練過程獨立的測試資料集（Test Set）就顯得重要許多了。<br /></li>
</ul>
<p width="500">
<img src="images/train-valid-test-split.png" alt="train-valid-test-split.png" width="500" /><br />
要切訓練資料集 / 驗證資料集，scikit-learn 中的 train_test_split 函式是一個不錯的選擇：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #dcaeea;">VALIDATION_RATIO</span> = <span style="color: #da8548; font-weight: bold;">0.1</span>
<span class="linenr"> 4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23567;&#24425;&#34507;</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">RANDOM_STATE</span> = <span style="color: #da8548; font-weight: bold;">9527</span>
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">x1_train</span>, <span style="color: #dcaeea;">x1_val</span>, <span style="color: #dcaeea;">x2_train</span>, <span style="color: #dcaeea;">x2_val</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_val</span> = train_test_split(
<span class="linenr"> 8: </span>        x1_train, x2_train, y_train,
<span class="linenr"> 9: </span>        test_size=VALIDATION_RATIO,
<span class="linenr">10: </span>        random_state=RANDOM_STATE
<span class="linenr">11: </span>)
</pre>
</div>

<p>
在這邊，我們分別將新聞標題 A x1_train、新聞標題 B x2_train 以及分類標籤 y_train 都分成兩個部分：訓練部分 &amp; 驗證部分。<br />
</p>

<p>
以假新聞 A 的標題 x1_train 為例，本來完整 32 萬筆的 x1_train 會被分為包含 90 % 數據的訓練資料集 x1_train 以及 10 % 的驗證資料集 x1_val。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Training Set"</span>)
<span class="linenr"> 2: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"-"</span> * <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr"> 3: </span><span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">"x1_train: </span>{x1_train.shape}<span style="color: #98be65;">"</span>)
<span class="linenr"> 4: </span><span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">"x2_train: </span>{x2_train.shape}<span style="color: #98be65;">"</span>)
<span class="linenr"> 5: </span><span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">"y_train : </span>{y_train.shape}<span style="color: #98be65;">"</span>)
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"-"</span> * <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr"> 8: </span><span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">"x1_val:   </span>{x1_val.shape}<span style="color: #98be65;">"</span>)
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">"x2_val:   </span>{x2_val.shape}<span style="color: #98be65;">"</span>)
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">"y_val :   </span>{y_val.shape}<span style="color: #98be65;">"</span>)
<span class="linenr">11: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"-"</span> * <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">12: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Test Set"</span>)
</pre>
</div>

<pre class="example" id="orgb865335">
Training Set
----------
x1_train: (288496, 20)
x2_train: (288496, 20)
y_train : (288496, 3)
----------
x1_val:   (32056, 20)
x2_val:   (32056, 20)
y_val :   (32056, 3)
----------
Test Set
</pre>
<p>
我們可以看到，切割後的訓練資料集有 288,488 筆資料。每一筆資料裡頭，成對新聞標題 A &amp; B 的長度皆為 20 個 Tokens，分類結果則有 3 個；驗證資料集的內容一模一樣，僅差在資料筆數較少（32,055 筆）。<br />
</p>

<p>
到此為此，資料前處理大功告成！<br />
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgd9cf56b" class="outline-3">
<h3 id="orgd9cf56b"><span class="section-number-3">9.4.</span> RNN</h3>
<div class="outline-text-3" id="text-9-4">
<p>
RNN 是一種有「記憶力」的神經網路，其最為人所知的形式如下：<br />
</p>
<p width="500">
<img src="images/rnn-static.png" alt="rnn-static.png" width="500" /><br />
如同上圖等號左側所示，RNN 跟一般深度學習中常見的前饋神經網路（Feedforward Neural Network, 後簡稱 FFNN）最不一樣的地方在於它有一個迴圈（Loop）。<br />
要了解這個迴圈在 RNN 裏頭怎麼運作，現在讓我們想像有一個輸入序列 X（Input Sequence）其長相如下：<br />
\[ X = [ x_0, x_1, x_2, \dots x_t ]\]<br />
</p>
<ol class="org-ol">
<li>不同於 FFNN，RNN 在第一個時間點 \(t_0\) 並不會直接把整個序列 \(X\) 讀入。反之，在第一個時間點 \(t_0\)，它只將該序列中的第一個元素 \(x_0\) 讀入中間的細胞 A。細胞 A 則會針對 \(x_0\) 做些處理以後，更新自己的「狀態」並輸出第一個結果 \(h_0\) 。<br /></li>
<li>在下個時間點 \(t_1\)，RNN 如法炮製，讀入序列 \(X\) 中的下一個元素 \(x_1\)，並利用剛剛處理完 \(x_0\) 得到的細胞狀態，處理 \(x_1\) 並更新自己的狀態（也被稱為記憶），接著輸出另個結果 \(h_1\)。<br /></li>
<li>剩下的 \(x_t\) 都會被以同樣的方式處理。但不管輸入的序列 \(X\) 有多長，RNN 的本體從頭到尾都是等號左邊的樣子：迴圈代表細胞 A 利用「上」一個時間點（比方說 \(t_1\)）儲存的狀態，來處理當下的輸入（比方說 \(x_2\) ）。<br /></li>
</ol>

<p>
但如果你將不同時間點（\(t_0\)、\(t_1\) &#x2026;）的 RNN 以及它的輸入一起截圖，並把所有截圖從左到右一字排開的話，就會長得像等號右邊的形式。將 RNN 以右邊的形式表示的話，你可以很清楚地了解，當輸入序列越長，向右展開的 RNN 也就越長。（模型也就需要訓練更久時間，這也是為何我們在資料前處理時設定了序列的最長長度）<br />
</p>

<p>
為了確保你 100 % 理解 RNN，讓我們假設剛剛的序列 X 實際上是一個內容如下的英文問句：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">X</span> = [ What, time, <span style="color: #51afef;">is</span>, it, ? ]
</pre>
</div>
<p>
而且 RNN 已經處理完前兩個元素 What 和 time 了。<br />
</p>

<p>
則接下來 RNN 會這樣處理剩下的句子：<br />
</p>
<p width="500">
<img src="images/rnn-animate.gif" alt="rnn-animate.gif" width="500" /><br />
就像你現在閱讀這段話一樣，你是由左到右逐字在大腦裡處理我現在寫的文字，同時不斷地更新你腦中的記憶狀態。<br />
</p>

<p>
每當下個詞彙映入眼中，你腦中的處理都會跟以下兩者相關：<br />
</p>
<ul class="org-ul">
<li>前面所有已讀的詞彙<br /></li>
<li>目前腦中的記憶狀態<br /></li>
</ul>

<p>
當然，實際人腦的閱讀機制更為複雜，但 RNN抓到這個處理精髓，利用內在迴圈以及細胞內的「記憶狀態」來處理序列資料。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orga169174"></a>RNN實作<br />
<div class="outline-text-4" id="text-9-4-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">state_t</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">2: </span><span style="color: #51afef;">for</span> input_t <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">input_sequence</span>:
<span class="linenr">3: </span>    output_t = f(input_t, state_t)
<span class="linenr">4: </span>    <span style="color: #dcaeea;">state_t</span> = output_t
</pre>
</div>
<p>
在 RNN 每次讀入任何新的序列數據前，細胞 A 中的記憶狀態 state_t 都會被初始化為 0。<br />
</p>

<p>
接著在每個時間點 t，RNN 會重複以下步驟：<br />
</p>

<ul class="org-ul">
<li>讀入 input_sequence 序列中的一個新元素 input_t<br /></li>
<li>利用 f 函式將當前細胞的狀態 state_t 以及輸入 input_t 做些處理產生 output_t<br /></li>
<li>輸出 output_t 並同時更新自己的狀態 state_t<br /></li>
</ul>

<p>
在 Keras 裏頭只要 2 行就可以建立一個 RNN layer：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">2: </span><span style="color: #dcaeea;">rnn</span> = layers.SimpleRNN()
</pre>
</div>

<div id="org857a077" class="figure">
<p><img src="images/nn-layers.jpg" alt="nn-layers.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 18: </span>RNN示例</p>
</div>
</div>
</li>
<li><a id="org6dd52aa"></a>LSTM<br />
<div class="outline-text-4" id="text-9-4-2">
<p>
如下的簡易RNN<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">state_t</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">2: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32048;&#32990; A &#26371;&#37325;&#35079;&#22519;&#34892;&#20197;&#19979;&#34389;&#29702;</span>
<span class="linenr">3: </span><span style="color: #51afef;">for</span> input_t <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">input_sequence</span>:
<span class="linenr">4: </span>    output_t = f(input_t, state_t)
<span class="linenr">5: </span>    <span style="color: #dcaeea;">state_t</span> = output_t
</pre>
</div>
<p>
要如何將細胞 A 當下的記憶 state_t 與輸入 input_t 結合，才能產生最有意義的輸出 output_t 呢？<br />
</p>

<p>
在 SimpleRNN 的細胞 A 裡頭，這個 f 的實作很簡單。而這導致其記憶狀態 state_t 沒辦法很好地「記住」前面處理過的序列元素，造成 RNN 在處理後來的元素時，就已經把前面重要的資訊給忘記了。(只有短期記憶，沒有長期記憶)<br />
</p>

<p>
長短期記憶（Long Short-Term Memory, 後簡稱 LSTM）就是被設計來解決 RNN 的這個問題。如下圖所示，你可以把 LSTM 想成是 RNN 中用來實現細胞 A 內部處理邏輯的一個特定方法：<br />
</p>

<p width="500">
<img src="images/lstm-cell.png" alt="lstm-cell.png" width="500" /><br />
基本上一個 LSTM 細胞裡頭會有 3 個閘門（Gates）來控制細胞在不同時間點的記憶狀態：<br />
</p>

<ul class="org-ul">
<li>Forget Gate：決定細胞是否要遺忘目前的記憶狀態<br /></li>
<li>Input Gate：決定目前輸入有沒有重要到值得處理<br /></li>
<li>Output Gate：決定更新後的記憶狀態有多少要輸出<br /></li>
</ul>

<p>
透過這些閘門控管機制，LSTM 可以將很久以前的記憶狀態儲存下來，在需要的時候再次拿出來使用。值得一提的是，這些閘門的參數也都是神經網路自己訓練出來的。<br />
</p>


<div id="org8668db5" class="figure">
<p><img src="images/lstm-cell-detailed.png" alt="lstm-cell-detailed.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 19: </span>LSTM 細胞頂端那條 cell state 正代表著細胞記憶的轉換過程</p>
</div>

<p>
想像 LSTM 細胞裡頭的記憶狀態是一個包裹，上面那條直線就代表著一個輸送帶。<br />
</p>

<p>
LSTM 可以把任意時間點的記憶狀態（包裹）放上該輸送帶，然後在未來的某個時間點將其原封不動地取下來使用。<br />
</p>

<div id="org2dec231" class="figure">
<p><img src="images/accumulation-conveyor-101.jpg" alt="accumulation-conveyor-101.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 20: </span>Caption</p>
</div>

<p>
因為這樣的機制，讓 LSTM 即使面對很長的序列數據也能有效處理，不遺忘以前的記憶。<br />
</p>

<p>
因為效果卓越，LSTM 非常廣泛地被使用。事實上，當有人跟你說他用 RNN 做了什麼 NLP 專案時，有 9 成機率他是使用 LSTM 或是 GRU（LSTM 的改良版，只使用 2 個閘門） 來實作，而不是使用最簡單的 SimpleRNN。<br />
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org5ca09a8" class="outline-3">
<h3 id="org5ca09a8"><span class="section-number-3">9.5.</span> 詞向量：將詞彙表達成有意義的向量¶</h3>
<div class="outline-text-3" id="text-9-5">
<p>
訓練資料集裡頭前 5 筆的假新聞標題 A：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">for</span> i, seq <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(x1_train[:<span style="color: #da8548; font-weight: bold;">5</span>]):
<span class="linenr">2: </span>    <span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">"&#26032;&#32862;&#27161;&#38988; </span>{i + 1}<span style="color: #98be65;">: "</span>)
<span class="linenr">3: </span>    <span style="color: #c678dd;">print</span>(seq)
<span class="linenr">4: </span>    <span style="color: #c678dd;">print</span>()
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(x1_train.shape)
</pre>
</div>

<pre class="example" id="orgcb6febf">
新聞標題 1:
[   0    0    0    0    0 4178 2972    9   80   87  717   18  474    4
  968    4  823   14 1436  721]

新聞標題 2:
[   0    0    0    0    0    0    0    0    0  411  308  809 3142   17
   90  434  191 3713    1    2]

新聞標題 3:
[   0    0    0    0    0    0    0    0    0    0    0 5212    5 8867
  793 1063 3626  642  337 1172]

新聞標題 4:
[   0    0    0    0    0    0    0    0    0    0    0    0    0    0
    0    0 2832  244  493  393]

新聞標題 5:
[   0    0    0    0    0    0    0    0  290  143 2523 2380   46 3120
   70  243  178 2238 3945 1082]

(288496, 20)
</pre>

<p>
而我們在訓練資料集則總共有 288,488 筆新聞標題，每筆標題如同剛剛所說的，是一個包含 20 個數字的序列。我們可以用 tokenizer 裡頭的字典 index_word 還原文本看看：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">for</span> i, seq <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(x1_train[:<span style="color: #da8548; font-weight: bold;">5</span>]):
<span class="linenr">2: </span>    <span style="color: #c678dd;">print</span>(f<span style="color: #98be65;">"&#26032;&#32862;&#27161;&#38988; </span>{i + 1}<span style="color: #98be65;">: "</span>)
<span class="linenr">3: </span>    <span style="color: #c678dd;">print</span>([tokenizer.index_word.get(idx, <span style="color: #98be65;">''</span>) <span style="color: #51afef;">for</span> idx <span style="color: #51afef;">in</span> seq])
<span class="linenr">4: </span>    <span style="color: #c678dd;">print</span>()
</pre>
</div>

<pre class="example" id="orgd6ad3f9">
新聞標題 1:
['', '', '', '', '', '营养师', '补充', '这', '4', '种', '营养', '能', '帮', '你', '降血压', '你', '一样', '都', '不吃', '么']

新聞標題 2:
['', '', '', '', '', '', '', '', '', '刘涛', '现场', '痛哭', '发飙', '要', '离婚', '直接', '把', '旁边', '的', '了']

新聞標題 3:
['', '', '', '', '', '', '', '', '', '', '', 'nba', '被', '球星', '诞生', '火箭', '骑士', '同', '抢', '交易']

新聞標題 4:
['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '海口', '飞机', '撒药治', '白蛾']

新聞標題 5:
['', '', '', '', '', '', '', '', '网', '曝', '杜', '海涛', '与', '沈梦辰', '已', '分手', '疑似', '女方', '劈', '腿']
</pre>
<p>
但事實上要讓神經網路能夠處理標題序列內的詞彙，我們要將它們表示成向量（更精準地說，是張量：Tensor），而不是一個單純數字。如果我們能做到這件事情，則 RNN 就能用以下的方式讀入我們的資料：<br />
</p>

<div id="orgc4c8a1e" class="figure">
<p><img src="images/rnn-process-vectors.gif" alt="rnn-process-vectors.gif" width="500" /><br />
</p>
<p><span class="figure-number">Figure 21: </span>Caption</p>
</div>

<p>
上圖中，在每個時間點被塞入 RNN 的「詞彙」不再是 1 個數字，而是一個 N 維向量（圖中 N 為 3），所以現在的問題變成：<br />
</p>

<p>
「要怎麼將一個詞彙表示成一個 N 維向量 ？」<br />
</p>

<p>
其中一個方法是我們隨便決定一個 N，然後為語料庫裡頭的每一個詞彙都指派一個隨機生成的 N 維向量。假設我們現在有 5 個詞彙：<br />
</p>
<ul class="org-ul">
<li>野狼<br /></li>
<li>老虎<br /></li>
<li>狗<br /></li>
<li>貓<br /></li>
<li>喵咪<br /></li>
</ul>

<p>
依照剛剛說的方法，我們可以設定 N = 2，並為每個詞彙隨機分配一個 2 維向量後將它們畫在一個平面空間裡頭：<br />
</p>


<div id="orgce9b6b4" class="figure">
<p><img src="images/2d-random-word-vector.jpg" alt="2d-random-word-vector.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 22: </span>Caption</p>
</div>

<p>
這些代表詞彙的向量被稱之為詞向量，但是你可以想像這樣的隨機轉換很沒意義。如圖<a href="#orgce9b6b4">22</a>我們就無法理解：<br />
</p>
<ul class="org-ul">
<li>為何「狗」是跟「老虎」而不是跟同為犬科的「野狼」比較接近？<br /></li>
<li>為何「貓」的維度 2 比「狗」高，但卻比「野狼」低？<br /></li>
<li>維度 2 的值的大小到底代表什麼意義？<br /></li>
<li>「喵咪」怎麼會在那裡？<br /></li>
</ul>

<p>
這是因為我們只是將詞彙隨機地轉換到 2 維空間，並沒有讓這些轉換的結果（向量）反應出詞彙本身的語意（Semantic）。<br />
</p>

<p>
一個理想的轉換應該是如圖<a href="#org2f34241">23</a>：<br />
</p>


<div id="org2f34241" class="figure">
<p><img src="images/2d-good-word-vector.jpg" alt="2d-good-word-vector.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 23: </span>Caption</p>
</div>

<p>
在這個 2 維空間裡頭，我們可以發現一個好的轉換有 2 個特性：<br />
</p>
<ul class="org-ul">
<li>距離有意義：「喵咪」與意思相近的詞彙「貓」距離接近，而與較不相關的「狗」距離較遠<br /></li>
<li>維度有意義：看看（狗, 貓）與（野狼, 老虎）這兩對組合，可以發現我們能將維度 1 解釋為貓科 VS 犬科；維度 2 解釋為寵物與野生動物<br /></li>
</ul>

<p>
如果我們能把語料庫（Corpus）裏頭的每個詞彙都表示成一個像是這樣有意義的詞向量，神經網路就能幫我們找到潛藏在大量詞彙中的語義關係，並進一步改善 NLP 任務的精準度。<br />
</p>

<p>
好消息是，大部分的情況我們並不需要自己手動設定每個詞彙的詞向量。我們可以隨機初始化所有詞向量（如前述的隨機轉換），並利用平常訓練神經網路的反向傳播算法（Backpropagation），讓神經網路自動學到一組適合當前 NLP 任務的詞向量（如上張圖的理想狀態）。<br />
</p>


<div id="org70f1bda" class="figure">
<p><img src="images/backpropagation-example.gif" alt="backpropagation-example.gif" width="500" /><br />
</p>
<p><span class="figure-number">Figure 24: </span>Caption</p>
</div>

<p>
如圖<a href="#org70f1bda">24</a>，反向傳播讓神經網路可以在訓練過程中修正參數，持續減少預測錯誤的可能性。<br />
</p>

<p>
在 NLP 裏頭，這種將一個詞彙或句子轉換成一個實數詞向量（Vectors of real numbers）的技術被稱之為詞嵌入（Word Embedding）。<br />
</p>

<p>
而在 Keras 裡頭，我們可以使用 Embedding 層來幫我們做到這件事情：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">2: </span><span style="color: #dcaeea;">embedding_layer</span> = layers.Embedding(
<span class="linenr">3: </span>    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)
</pre>
</div>

<ul class="org-ul">
<li>MAX_NUM_WORDS 是我們的字典大小（10,000 個詞彙）、NUM_EMBEDDING_DIM 則是詞向量的維度。常見的詞向量維度有 128、256 或甚至 1,024。<br /></li>
<li>Embedding 層一次接收 k 個長度任意的數字序列，並輸出 k 個長度相同的序列。輸出的序列中，每個元素不再是數字，而是一個 NUM_EMBEDDING_DIM 維的詞向量。<br /></li>
</ul>

<p>
假如我們將第一筆（也就是 k = 1）假新聞標題 A 丟入 Embedding 層，並設定 NUM_EMBEDDING_DIM 為 3 的話，原來的標題 A：<br />
</p>
<p class="verse">
新聞標題:<br />
[<br />
&#xa0;0,<br />
&#xa0;0,<br />
&#xa0;0,<br />
&#xa0;185,<br />
&#xa0;300,<br />
&#xa0;72,<br />
&#xa0;4029,<br />
&#xa0;37,<br />
&#xa0;1,<br />
&#xa0;121,<br />
&#xa0;250,<br />
&#xa0;95,<br />
&#xa0;30,<br />
&#xa0;511,<br />
&#xa0;92,<br />
&#xa0;2358,<br />
&#xa0;33,<br />
&#xa0;2565,<br />
&#xa0;19,<br />
&#xa0;55,<br />
<br />
]<br />
</p>
<p>
就會被轉換成類似以下的形式：<br />
</p>
<p class="verse">
新聞標題:<br />
[<br />
&#xa0;[0.212, 0.111, 0.666],<br />
&#xa0;[0.212, 0.111, 0.666],<br />
&#xa0;[0.212, 0.111, 0.666],<br />
&#xa0;[0.528, 0.344, 0.452],<br />
&#xa0;[0.163, 0.93, 0.58],<br />
&#xa0;[0.527, 0.262, 0.246],<br />
&#xa0;[0.077, 0.695, 0.776],<br />
&#xa0;[0.624, 0.962, 0.96],<br />
&#xa0;[0.456, 0.927, 0.404],<br />
&#xa0;[0.353, 0.119, 0.108],<br />
&#xa0;[0.805, 0.969, 0.725],<br />
&#xa0;[0.379, 0.265, 0.473],<br />
&#xa0;[0.436, 0.186, 0.738],<br />
&#xa0;[0.923, 0.287, 0.967],<br />
&#xa0;[0.477, 0.614, 0.838],<br />
&#xa0;[0.089, 0.328, 0.993],<br />
&#xa0;[0.887, 0.913, 0.885],<br />
&#xa0;[0.604, 0.118, 0.646],<br />
&#xa0;[0.907, 0.52, 0.437],<br />
&#xa0;[0.443, 0.432, 0.498],<br />
]<br />
</p>
<p>
序列裡頭的每個數字（即詞彙）都被轉換成一個 3 維的詞向量，而相同數字則當然都會對應到同一個詞向量（如前 3 個 0 所對應到的詞向量）。<br />
</p>

<div id="org4e95177" class="figure">
<p><img src="images/rnn-process-vectors.gif" alt="rnn-process-vectors.gif" width="500" /><br />
</p>
<p><span class="figure-number">Figure 25: </span>Caption</p>
</div>

<p>
有了這樣的轉換，我們就能將轉換後的詞向量丟入 RNN / LSTM 裏頭，讓模型逐步修正隨機初始化的詞向量，使得詞向量裡頭的值越來越有意義。<br />
</p>

<p>
有了兩個新聞標題的詞向量，接著讓我們瞧瞧能夠處理這些數據的神經網路架構吧！<br />
</p>
</div>
</div>
<div id="outline-container-orge7a3e01" class="outline-3">
<h3 id="orge7a3e01"><span class="section-number-3">9.6.</span> 一個神經網路，兩個新聞標題</h3>
<div class="outline-text-3" id="text-9-6">
<p>
一般來說，多數你見過的神經網路只會接受一個資料來源：<br />
</p>
<ul class="org-ul">
<li>輸入一張圖片，判斷是狗還是貓<br /></li>
<li>輸入一個音訊，將其轉成文字<br /></li>
<li>輸入一篇新聞，判斷是娛樂還是運動新聞<br /></li>
</ul>

<div id="org92a276d" class="figure">
<p><img src="images/one-data-source-nn.jpg" alt="one-data-source-nn.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 26: </span>Caption</p>
</div>


<p>
單一輸入的神經網路架構可以解決大部分的深度學習問題。但在這個 Kaggle 競賽裡頭，我們想要的是一個能夠讀入成對新聞標題，並判斷兩者之間關係的神經網路架構：<br />
</p>
<ul class="org-ul">
<li>不相關（unrelated）<br /></li>
<li>新聞 B 同意 A（agreed）<br /></li>
<li>新聞 B 不同意 A（disagreed）<br /></li>
</ul>
<p>
要怎麼做到這件事情呢？<br />
我們可以使用孿生神經網路（Siamese Network）架構：<br />
</p>


<div id="orgfff9548" class="figure">
<p><img src="images/siamese-network.jpg" alt="siamese-network.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 27: </span>使用孿生神經網路架構來處理同類型的 2 個新聞標題</p>
</div>

<p>
所謂孿生神網網路指的是：一部份的神經網路（紅框部分）被重複用來處理多個不同的資料來源（在本篇中為 2 篇不同的新聞標題）。<br />
</p>

<p>
而會想這樣做，是因為不管標題內容是新聞 A 還是新聞 B，其標題本身的語法 &amp; 語義結構大同小異。<br />
</p>


<p>
以這樣的觀點來看的話，我們並不需要兩個不同的 LSTM 來分別將新聞 A 以及新聞 B 的詞向量做有意義的轉換，而是只需要讓標題 A 與標題 B 共享一個 LSTM 即可。畢竟，標題 A 跟標題 B 的數據結構很像。<br />
</p>

<p>
如果我們只寫一個 Python 函式就能處理 2 個相同格式的輸入的話，為何要寫 2 個函式呢？孿生神經網路也是相同的概念。<br />
</p>
</div>
</div>
<div id="outline-container-org4e087a5" class="outline-3">
<h3 id="org4e087a5"><span class="section-number-3">9.7.</span> 建立模型</h3>
<div class="outline-text-3" id="text-9-7">
<p>
深度學習以及 NLP 領域的學問博大精深，但一般來說，當你想要實際動手寫出一個神經網路的時候，有 3 個基本步驟可以 follow：<br />
</p>
<ol class="org-ol">
<li>定義神經網路的架構<br /></li>
<li>決定如何衡量模型的表現<br /></li>
<li>訓練模型並挑選最好的結果<br /></li>
</ol>
<p width="500">
<img src="images/deep-learning-three-steps-with-keras.jpg" alt="deep-learning-three-steps-with-keras.jpg" width="500" /><br />
接下來你會看到，大約 80 % 的程式碼會花在實作第一個步驟。剩餘 2 個步驟在使用 Keras 的情況下非常容易就能實現；但後面我們也會談到，你將花 80 % 的時間在最後一個步驟上面。<br />
</p>

<p>
首先，先讓我們進入第一步驟。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="org3204967"></a>定義神經網路的架構<br />
<div class="outline-text-4" id="text-9-7-1">
<p>
在實作之前，先讓我們回顧一下前面段落看到的模型架構：<br />
</p>

<div id="orgd39fc7b" class="figure">
<p><img src="images/siamese-network1.jpg" alt="siamese-network1.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 28: </span>Caption</p>
</div>


<p>
從左到右掃過一遍，你可以很清楚地發現我們需要以下 5 個元素來完成這個模型：<br />
</p>
<ul class="org-ul">
<li>兩個新聞標題（兩個長度為 20 的數字序列）<br /></li>
<li>一個詞嵌入層：將數字序列轉換為詞向量序列<br /></li>
<li>一個 LSTM 層：讀入前層的詞向量並萃取標題語義<br /></li>
<li>一個串接層：將兩個新聞標題的處理結果（也是向量）串接成一個向量<br /></li>
<li>一個全連接層：將前層的向量轉換為 3 個分類的預測機率<br /></li>
</ul>
<p>
有些層我們已經在前面章節看過 Keras 的實現，比方說詞嵌入層以及 LSTM 層。剩下的串接層以及全連結層在 Keras 也都有現成的模組可供使用。<br />
</p>

<p>
另外值得一提的是，圖上的每個層（Layer）以及向量右下的灰字都對應了底下 Python 程式碼裡頭的變數名稱：<br />
</p>


<p>
因此，如果等等你不了解底下某個特定的變數所代表的意義，可以回來利用這張架構圖來釐清概念。<br />
</p>

<p>
以下就是此模型的 Keras 實作：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22522;&#26412;&#21443;&#25976;&#35373;&#32622;&#65292;&#26377;&#24190;&#20491;&#20998;&#39006;</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">NUM_CLASSES</span> = <span style="color: #da8548; font-weight: bold;">3</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22312;&#35486;&#26009;&#24235;&#35041;&#26377;&#22810;&#23569;&#35422;&#24409;</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">MAX_NUM_WORDS</span> = <span style="color: #da8548; font-weight: bold;">10000</span>
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19968;&#20491;&#27161;&#38988;&#26368;&#38263;&#26377;&#24190;&#20491;&#35422;&#24409;</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">MAX_SEQUENCE_LENGTH</span> = <span style="color: #da8548; font-weight: bold;">20</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19968;&#20491;&#35422;&#21521;&#37327;&#30340;&#32173;&#24230;</span>
<span class="linenr">11: </span><span style="color: #dcaeea;">NUM_EMBEDDING_DIM</span> = <span style="color: #da8548; font-weight: bold;">256</span>
<span class="linenr">12: </span>
<span class="linenr">13: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">LSTM &#36664;&#20986;&#30340;&#21521;&#37327;&#32173;&#24230;</span>
<span class="linenr">14: </span><span style="color: #dcaeea;">NUM_LSTM_UNITS</span> = <span style="color: #da8548; font-weight: bold;">128</span>
<span class="linenr">15: </span>
<span class="linenr">16: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;&#23423;&#29983; LSTM &#26550;&#27083;&#65288;Siamese LSTM&#65289;</span>
<span class="linenr">17: </span><span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> Input
<span class="linenr">18: </span><span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Embedding, \
<span class="linenr">19: </span>    LSTM, concatenate, Dense
<span class="linenr">20: </span><span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Model
<span class="linenr">21: </span>
<span class="linenr">22: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#21029;&#23450;&#32681; 2 &#20491;&#26032;&#32862;&#27161;&#38988; A &amp; B &#28858;&#27169;&#22411;&#36664;&#20837;</span>
<span class="linenr">23: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20841;&#20491;&#27161;&#38988;&#37117;&#26159;&#19968;&#20491;&#38263;&#24230;&#28858; 20 &#30340;&#25976;&#23383;&#24207;&#21015;</span>
<span class="linenr">24: </span><span style="color: #dcaeea;">top_input</span> = Input(
<span class="linenr">25: </span>    shape=(MAX_SEQUENCE_LENGTH, ),
<span class="linenr">26: </span>    dtype=<span style="color: #98be65;">'int32'</span>)
<span class="linenr">27: </span>bm_input = Input(
<span class="linenr">28: </span>    shape=(MAX_SEQUENCE_LENGTH, ),
<span class="linenr">29: </span>    dtype=<span style="color: #98be65;">'int32'</span>)
<span class="linenr">30: </span>
<span class="linenr">31: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35422;&#23884;&#20837;&#23652;</span>
<span class="linenr">32: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32147;&#36942;&#35422;&#23884;&#20837;&#23652;&#30340;&#36681;&#25563;&#65292;&#20841;&#20491;&#26032;&#32862;&#27161;&#38988;&#37117;&#35722;&#25104;</span>
<span class="linenr">33: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19968;&#20491;&#35422;&#21521;&#37327;&#30340;&#24207;&#21015;&#65292;&#32780;&#27599;&#20491;&#35422;&#21521;&#37327;&#30340;&#32173;&#24230;</span>
<span class="linenr">34: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28858; 256</span>
<span class="linenr">35: </span>embedding_layer = Embedding(
<span class="linenr">36: </span>    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)
<span class="linenr">37: </span>top_embedded = embedding_layer(
<span class="linenr">38: </span>    top_input)
<span class="linenr">39: </span>bm_embedded = embedding_layer(
<span class="linenr">40: </span>    bm_input)
<span class="linenr">41: </span>
<span class="linenr">42: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">LSTM &#23652;</span>
<span class="linenr">43: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20841;&#20491;&#26032;&#32862;&#27161;&#38988;&#32147;&#36942;&#27492;&#23652;&#24460;</span>
<span class="linenr">44: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28858;&#19968;&#20491; 128 &#32173;&#24230;&#21521;&#37327;</span>
<span class="linenr">45: </span>shared_lstm = LSTM(NUM_LSTM_UNITS)
<span class="linenr">46: </span>top_output = shared_lstm(top_embedded)
<span class="linenr">47: </span>bm_output = shared_lstm(bm_embedded)
<span class="linenr">48: </span>
<span class="linenr">49: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20018;&#25509;&#23652;&#23559;&#20841;&#20491;&#26032;&#32862;&#27161;&#38988;&#30340;&#32080;&#26524;&#20018;&#25509;&#21934;&#19968;&#21521;&#37327;</span>
<span class="linenr">50: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26041;&#20415;&#36319;&#20840;&#36899;&#32080;&#23652;&#30456;&#36899;</span>
<span class="linenr">51: </span>merged = concatenate(
<span class="linenr">52: </span>    [top_output, bm_output],
<span class="linenr">53: </span>    axis=-<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">54: </span>
<span class="linenr">55: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20840;&#36899;&#25509;&#23652;&#25645;&#37197; Softmax Activation</span>
<span class="linenr">56: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21487;&#20197;&#22238;&#20659; 3 &#20491;&#25104;&#23565;&#27161;&#38988;</span>
<span class="linenr">57: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23660;&#26044;&#21508;&#39006;&#21029;&#30340;&#21487;&#33021;&#27231;&#29575;</span>
<span class="linenr">58: </span>dense =  Dense(
<span class="linenr">59: </span>    units=NUM_CLASSES,
<span class="linenr">60: </span>    activation=<span style="color: #98be65;">'softmax'</span>)
<span class="linenr">61: </span>predictions = dense(merged)
<span class="linenr">62: </span>
<span class="linenr">63: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25105;&#20497;&#30340;&#27169;&#22411;&#23601;&#26159;&#23559;&#25976;&#23383;&#24207;&#21015;&#30340;&#36664;&#20837;&#65292;&#36681;&#25563;</span>
<span class="linenr">64: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25104; 3 &#20491;&#20998;&#39006;&#30340;&#27231;&#29575;&#30340;&#25152;&#26377;&#27493;&#39519; / &#23652;&#30340;&#32317;&#21644;</span>
<span class="linenr">65: </span>model = Model(
<span class="linenr">66: </span>    inputs=[top_input, bm_input],
<span class="linenr">67: </span>    outputs=predictions)
<span class="linenr">68: </span>
</pre>
</div>

<pre class="example">
2022-05-23 10:42:00.872716: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
</pre>


<p>
也可以透過keras的function將模型的架構圖畫出來<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">from keras.utils import plot_model</span>
<span class="linenr">2: </span><span style="color: #51afef;">from</span> keras.utils.vis_utils <span style="color: #51afef;">import</span> plot_model
<span class="linenr">3: </span>plot_model(
<span class="linenr">4: </span>    model,
<span class="linenr">5: </span>    to_file=<span style="color: #98be65;">'siameseModel.png'</span>,
<span class="linenr">6: </span>    show_shapes=<span style="color: #a9a1e1;">True</span>,
<span class="linenr">7: </span>    show_layer_names=<span style="color: #a9a1e1;">False</span>,
<span class="linenr">8: </span>    rankdir=<span style="color: #98be65;">'LR'</span>)
</pre>
</div>


<div id="orgec0cd7c" class="figure">
<p><img src="images/siameseModel.png" alt="siameseModel.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 29: </span>Caption</p>
</div>



<p>
除了模型架構以外，我們還可以看到所有層的輸入 / 輸出張量（Tensor）的維度。在 Keras 裏頭，張量的第 1 個維度通常為樣本數（比方說 5 則新聞標題），而 None 則代表可以指定任意值。<br />
</p>

<p>
最重要的是，這個用 Keras 定義出來的模型，跟我們之前想像中的孿生神經網路可以說是一模一樣：<br />
</p>
</div>
</li>
<li><a id="orgd57f97e"></a>全連接層<br />
<div class="outline-text-4" id="text-9-7-2">
<p>
全連接層顧名思義，代表該層的每個神經元（Neuron）都會跟前一層的所有神經元享有連結：<br />
</p>
<p width="500">
<img src="images/fully-connected.jpg" alt="fully-connected.jpg" width="500" /><br />
而為了確認我們計算的參數量無誤，還可以使用 model.summary() 來看每一層的參數量以及輸出的張量（Tensor）長相：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(model.summary())
</pre>
</div>

<pre class="example" id="orga7671fd">
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, 20)]         0           []

 input_2 (InputLayer)           [(None, 20)]         0           []

 embedding (Embedding)          (None, 20, 256)      2560000     ['input_1[0][0]',
                                                                  'input_2[0][0]']

 lstm (LSTM)                    (None, 128)          197120      ['embedding[0][0]',
                                                                  'embedding[1][0]']

 concatenate (Concatenate)      (None, 256)          0           ['lstm[0][0]',
                                                                  'lstm[1][0]']

 dense (Dense)                  (None, 3)            771         ['concatenate[0][0]']

==================================================================================================
Total params: 2,757,891
Trainable params: 2,757,891
Non-trainable params: 0
__________________________________________________________________________________________________
None
</pre>


<p>
全連接層在最下面。而因為其與前一層「緊密」連接的緣故，它在 Keras 裏頭被稱為 Dense 層。它也是最早出現、最簡單的神經網路層之一。<br />
</p>

<p>
Param # 則紀錄了每一層所包含的模型參數（Parameters）。在機器學習的過程中，這些參數都會不斷地被調整，直到能讓模型能做出很好的預測。詞嵌入層有最多的參數，因為我們要為 字典裡頭的每個詞彙都建立一個 256 維度的詞向量，因此參數量為 10,000 * 256。<br />
</p>

<p>
這張表另外一個值得注意的地方是所有層的 Output Shape 的第一個維度都是 None。而 None 代表著可以是任意的數字。<br />
</p>

<p>
在 Keras 裡頭，第一個維度代表著樣本數（#Samples），比方說前 9,527 筆新聞標題 A 的數字序列的 shape 應該要是 （9527, 20）：<br />
</p>

<p>
而之所以每層的樣本數為 None 是因為 Keras 為了因應在不同場合會丟入不同數量的樣本需求。比方說，在訓練時你可能會一次丟 32 筆資料給模型訓練，但在預測的時候一次只丟 16 筆資料。<br />
</p>
</div>
</li>

<li><a id="org17734ba"></a>Softmax 函式<br />
<div class="outline-text-4" id="text-9-7-3">
<p>
Softmax 函式一般都會被用在整個神經網路的最後一層上面，比方說我們這次的全連接層。<br />
</p>

<p>
Softmax 函式能將某層中的所有神經元裡頭的數字作正規化（Normalization）：將它們全部壓縮到 0 到 1 之間的範圍，並讓它們的和等於 1。<br />
</p>


<div id="org8e96c35" class="figure">
<p><img src="images/softmax-and-fully-connectead.jpg" alt="softmax-and-fully-connectead.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 30: </span>Softmax 能將多個數字作正規化，讓它們的和為 1</p>
</div>


<p>
因為<br />
</p>

<ul class="org-ul">
<li>所有數值都位於 0 到 1 之間<br /></li>
<li>所有數值相加等於 1<br /></li>
</ul>

<p>
這兩個條件恰好是機率（Probability）的定義，Softmax 函式的運算結果可以讓我們將每個神經元的值解釋為對應分類（Class）的發生機率。<br />
</p>

<p>
以我們的假新聞分類任務來說的話，每個值就各代表以下分類的發生機率：<br />
</p>

<ul class="org-ul">
<li>不相關： 0.46<br /></li>
<li>新聞 B 同意新聞 A：0.34<br /></li>
<li>新聞 B 不同意新聞 B：0.20<br /></li>
</ul>

<p>
如果現在是在做預測且我們只能選出一個分類當作答案的話，我們可以說這次的分類結果最有可能是「不相關」這個類別，因為其發生機率最高。<br />
</p>

<p>
在定義好模型以後，我們就可以進入下個步驟：定義衡量模型好壞的指標。<br />
</p>
<p width="500">
<img src="images/loss-function.jpg" alt="loss-function.jpg" width="500" /><br />
圖<a href="#org5c21e44">43</a>的拋物線即為損失函數 J(w)。當參數 w 有不同值時，損失函數的值也有所不同。模型會持續修正參數 w 以期最小化損失函數 （圖片來源）<br />
</p>

<p>
那你會問，在假新聞分類裡頭，我們應該使用什麼損失函數呢？<br />
</p>

<p>
我們在將正解做 One-hot Encoding 一節有稍微提到，我們會希望<br />
</p>

<ul class="org-ul">
<li>正確的分類的機率分佈 P1（例：[1, 0, 0]）<br /></li>
<li>模型預測出的機率分佈 P2（例：[0.7, 0.2, 0.1]）<br /></li>
</ul>

<p>
這 2 個機率分佈的「差距」越小越好。而能計算 2 個機率分佈之間的差距的交叉熵（Cross Entropy）就是這次的分類問題中最適合的損失函數。<br />
</p>
</div>
</li>

<li><a id="org04f49d5"></a>評估模型效能<br />
<div class="outline-text-4" id="text-9-7-4">
<p>
為了讓機器自動「學習」，我們得給它一個損失函數（Loss Function）。<br />
</p>

<p>
給定一個正確解答 y 以及模型預測的結果 y_head，我們的模型透過損失函數就能自動計算出現在的預測結果跟正解的差距為多少。<br />
</p>

<p>
透過損失函數的回饋，模型會盡全力修正參數，以期將此損失函數的值下降到最低（也就是讓預測結果 y_head 跟正解 y 越來越接近）。<br />
</p>

<p width="500">
<img src="images/cross-entropy.jpg" alt="cross-entropy.jpg" width="500" /><br />
在 Keras 裏頭，我們可以這樣定義模型的損失函數：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>model.<span style="color: #c678dd;">compile</span>(
<span class="linenr">2: </span>    optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">3: </span>    loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr">4: </span>    metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>

<p>
categorical_crossentropy 即是我們剛剛所說的交叉熵，而 accuracy 則是準確度，會被我們用來在訓練過程中了解模型的表現情況。<br />
</p>

<p>
精準度的定義為：<br />
</p>
<p class="verse">
# 模型預測正確的樣本數<br />
--------------&#x2013;&#x2014;<br />
#     總樣本數<br />
</p>
<p>
雖然有了交叉熵來當作我們模型的損失函數，但是實際上模型要如何更新裡頭的參數呢？我們需要一個優化器（Optimizer）來做到這件事情。<br />
</p>

<div id="org461dfc3" class="figure">
<p><img src="images/loss-function-learning.gif" alt="loss-function-learning.gif" width="500" /><br />
</p>
<p><span class="figure-number">Figure 31: </span>不同優化器透過調整參數來降低損失函數的情形，就像是在想辦法往溜滑梯的低處滑一樣</p>
</div>


<p>
雖然我們有很多種優化器，但它們基本上都是從梯度下降法（Gradient Descent）延伸而來。<br />
</p>

<p>
在上圖的不同位置，梯度下降法會重新計算每個參數對損失函數的梯度（斜率）。接著梯度下降法會利用該梯度來修正參數，使得使用新參數算出來的損失函數的值能夠持續往下降。<br />
</p>

<p>
不同優化器則有各自往下滑的秘方，比方說自動調整 Learning rate。<br />
</p>

<p>
現在就先讓我們使用 RMSProp 優化器。而在有了損失函數以及優化器以後，我們就可以正式開始訓練模型了！<br />
</p>
</div>
</li>

<li><a id="org8df9728"></a>訓練模型並挑選最好的結果<br />
<div class="outline-text-4" id="text-9-7-5">
<p>
這步驟很直觀，我們就是實際使用 model.fit 來訓練剛剛定義出來的孿生 LSTM 模型：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27770;&#23450;&#19968;&#27425;&#35201;&#25918;&#22810;&#23569;&#25104;&#23565;&#27161;&#38988;&#32102;&#27169;&#22411;&#35347;&#32244;</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">BATCH_SIZE</span> = <span style="color: #da8548; font-weight: bold;">512</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27770;&#23450;&#27169;&#22411;&#35201;&#30475;&#25972;&#20491;&#35347;&#32244;&#36039;&#26009;&#38598;&#24190;&#36941;</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">NUM_EPOCHS</span> = <span style="color: #da8548; font-weight: bold;">10</span>
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#38555;&#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">history</span> = model.fit(
<span class="linenr"> 9: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20837;&#26159;&#20841;&#20491;&#38263;&#24230;&#28858; 20 &#30340;&#25976;&#23383;&#24207;&#21015;</span>
<span class="linenr">10: </span>    x=[x1_train, x2_train],
<span class="linenr">11: </span>    y=y_train,
<span class="linenr">12: </span>    batch_size=BATCH_SIZE,
<span class="linenr">13: </span>    epochs=NUM_EPOCHS,
<span class="linenr">14: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27599;&#20491; epoch &#23436;&#24460;&#35336;&#31639;&#39511;&#35657;&#36039;&#26009;&#38598;</span>
<span class="linenr">15: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19978;&#30340; Loss &#20197;&#21450;&#28310;&#30906;&#24230;</span>
<span class="linenr">16: </span>    validation_data=(
<span class="linenr">17: </span>        [x1_val, x2_val],
<span class="linenr">18: </span>        y_val
<span class="linenr">19: </span>    ),
<span class="linenr">20: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27599;&#20491; epoch &#38568;&#27231;&#35519;&#25972;&#35347;&#32244;&#36039;&#26009;&#38598;</span>
<span class="linenr">21: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35041;&#38957;&#30340;&#25976;&#25818;&#20197;&#35731;&#35347;&#32244;&#36942;&#31243;&#26356;&#31337;&#23450;</span>
<span class="linenr">22: </span>    shuffle=<span style="color: #a9a1e1;">True</span>
<span class="linenr">23: </span>)
<span class="linenr">24: </span>
</pre>
</div>

<pre class="example" id="org08e8280">
Epoch 1/10
1/564 [..............................] - ETA: 24:28 - loss: 1.0995 - accuracy: 0.2832
2/564 [..............................] - ETA: 1:55 - loss: 1.0636 - accuracy: 0.4668
loss: 0.2228 - accuracy: 0.9103
...
564/564 [==============================] - 185s 329ms/step - loss: 0.2228 - accuracy: 0.9103 - val_loss: 0.3631 - val_accuracy: 0.8526
</pre>

<p>
這邊特別值得拿出來提的是以下兩個參數：<br />
</p>
<ul class="org-ul">
<li>BATCH_SIZE<br /></li>
<li>NUM_EPOCHS<br /></li>
</ul>
<p>
依照我們前面對損失函數（Loss Function）的說明，理論上模型是把訓練資料集裡頭的 32 萬筆資料全部看完一遍之後，再更新一次參數以降低損失函數。<br />
</p>

<p>
但是這樣太曠日廢時，訓練可能要花很久才能完成。<br />
</p>
</div>
</li>
<li><a id="org0bb4aab"></a>BATCH<br />
<div class="outline-text-4" id="text-9-7-6">
<p>
實務上都是每次只放入幾筆訓練數據，讓模型看完這些資料後就做一次參數的更新。而這個「幾筆」，就是 BATCH_SIZE。<br />
</p>

<p>
依照 BATCH_SIZE 的大小，梯度下降（Gradient Descent, 後稱 GD）可以概括為 3 個類別：<br />
</p>
<ul class="org-ul">
<li>GD（BATCH_SIZE = 訓練資料集大小，且這時不稱為 batch）<br /></li>
<li>Mini-batch GD（BATCH_SIZE 通常為一個較小的 2 的倍數）<br /></li>
<li>SGD（BATCH_SIZE = 1）<br /></li>
</ul>
<p width="500">
<img src="images/sgd-vs-mini-batch.jpg" alt="sgd-vs-mini-batch.jpg" width="500" /><br />
 想像損失函數是個越往裡面值就越低的碗，梯度下降就是要想辦法到達中心點 （圖片來源）<br />
</p>

<p>
如上圖所示，下方的 GD 因為在每次更新參數前都會看完訓練資料集裡頭所有的數據，因此它更新參數的方向是最可靠的。但要往前走一步就就得看完 32 萬筆數據，未免成本也太大。<br />
</p>

<p>
另一個極端是上方的 SGD：模型每看完 1 個訓練數據就嘗試更新權重，而因為單一一筆訓練數據並不能很好地代表整個訓練資料集，前進的方向非常不穩定。<br />
</p>

<div id="orga77af4c" class="figure">
<p><img src="images/mini-batch.jpg" alt="mini-batch.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 32: </span>Caption</p>
</div>


<p>
因此我們常常採用的是中庸之道： Mini-batch GD 的方式來訓練模型，而這靠的是指定 model.fit 函式裡頭的 batch_size。<br />
</p>

<p>
NUM_EPOCHS 則很容易理解：你希望模型不只將 32 萬筆的訓練數據都看過一遍，而是每一筆資料還要多看過好幾次，以讓模型確確實實地從它們身上學到東西。NUM_EPOCHS = 10 的意思就代表模型會重複看整個訓練資料集 10 次。<br />
</p>

<p>
接著讓我們看看 Keras 的訓練過程：<br />
</p>


<div id="orgad01731" class="figure">
<p><img src="images/training-process.jpg" alt="training-process.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 33: </span>Caption</p>
</div>


<p>
因為模型的目標就是要最小化損失函數（Loss Function），你可以觀察到當模型看過越多訓練資料集（Training Set）的數據以後，損失值（loss）就越低，分類的準確度（acc）則越高。<br />
</p>

<p>
這代表我們的模型越來越熟悉訓練資料集裡頭的數據，因此在訓練資料集裡頭的表現越來越好。<br />
</p>

<p>
如果依照準確度以及損失值分別畫圖的話則會長這樣：<br />
</p>

<div id="org912dc24" class="figure">
<p><img src="images/training-result.jpg" alt="training-result.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 34: </span>Caption</p>
</div>


<p>
很明顯地，我們的神經網路有過適（Overfittng）的問題：儘管在訓練資料集表現得非常好（準確度超過 90 %、損失小於 0.2），在從沒看過的驗證資料集的表現就相對遜色不少。且在第 6 個 epoch 之後驗證資料集的準確度 val_acc 就沒什麼在上升，驗證集的損失 val_loss 則已經逐漸上升。<br />
</p>

<p>
這代表模型利用從訓練資料集學到的模式（Pattern）還無法非常精準地預測沒見過的事物。<br />
</p>

<p>
如同我們在這章節一開頭所說的，雖然第 3 步驟：「訓練模型並挑選最好的結果」的 Keras 實作非常簡單（基本上就是 model.fit( &#x2026;)），但實際上在一個機器學習 / 深度學習專案裡頭，你將會花 80 % 的時間在這個步驟裡頭調整參數，想辦法找到一個最棒的模型。<br />
</p>

<p>
儘管如此，我們現在最想知道的還是這個模型在真實世界（也就是測試資料集）到底能表現多好，因此先讓我們試著拿這個簡單模型來做預測吧！<br />
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org7bc49b4" class="outline-3">
<h3 id="org7bc49b4"><span class="section-number-3">9.8.</span> 進行預測並提交結果</h3>
<div class="outline-text-3" id="text-9-8">
<p>
就跟我們對訓練 / 驗證資料集做的資料前處理一樣，要對測試資料集（Test Set）做預測，我們得先將裡頭的文本數據通通轉換成能夠丟進模型的數字序列資料。<br />
</p>
</div>
<ol class="org-ol">
<li><a id="orgb68a984"></a>首先，讓我們把測試資料集讀取進來：<br />
<div class="outline-text-4" id="text-9-8-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">2: </span><span style="color: #dcaeea;">test</span> = pd.read_csv(<span style="color: #98be65;">"./dataset/fakenews.train.csv"</span>, index_col=<span style="color: #da8548; font-weight: bold;">0</span>, encoding=<span style="color: #98be65;">'utf-8'</span>, dtype=<span style="color: #c678dd;">str</span>)
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(test.head(<span style="color: #da8548; font-weight: bold;">3</span>))
</pre>
</div>

<pre class="example">
   tid1 tid2  ...                                          title2_en      label
id            ...
0     0    1  ...  Police disprove "bird's nest congress each per...  unrelated
3     2    3  ...  Shenzhen's GDP outstrips Hong Kong? Shenzhen S...  unrelated
1     2    4  ...  The GDP overtopped Hong Kong? Shenzhen clarifi...  unrelated

[3 rows x 7 columns]
</pre>


<p>
測試資料集跟訓練資料集的唯一差別只在沒有 label 欄位，因此我們只需要將當初在資料前處理章節使用的步驟原封不動地套用在測試資料集即可。<br />
</p>

<p>
你可以趁機複習一下有哪些步驟：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#27493;&#39519;&#20998;&#21029;&#23565;&#26032;&#32862;&#27161;&#38988; A&#12289;B&#12288;&#36914;&#34892;&#25991;&#26412;&#26039;&#35422; / Word Segmentation</span>
<span class="linenr"> 2: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">jieba&#22312;&#36889;&#35041;&#26371;&#20986;&#21839;&#38988;&#65292;&#35201;&#21152;&#19978;astype(str)</span>
<span class="linenr"> 3: </span><span style="color: #dcaeea;">test</span>[<span style="color: #98be65;">'title1_tokenized'</span>] = test.loc[:, <span style="color: #98be65;">'title1_zh'</span>].astype(<span style="color: #c678dd;">str</span>).<span style="color: #c678dd;">apply</span>(jieba_tokenizer)
<span class="linenr"> 4: </span><span style="color: #dcaeea;">test</span>[<span style="color: #98be65;">'title2_tokenized'</span>] = test.loc[:, <span style="color: #98be65;">'title2_zh'</span>].astype(<span style="color: #c678dd;">str</span>).<span style="color: #c678dd;">apply</span>(jieba_tokenizer)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#35422;&#24409;&#24207;&#21015;&#36681;&#28858;&#32034;&#24341;&#25976;&#23383;&#30340;&#24207;&#21015;</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">x1_test</span> = tokenizer.texts_to_sequences(test.title1_tokenized)
<span class="linenr"> 8: </span><span style="color: #dcaeea;">x2_test</span> = tokenizer.texts_to_sequences(test.title2_tokenized)
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28858;&#25976;&#23383;&#24207;&#21015;&#21152;&#20837; zero padding</span>
<span class="linenr">11: </span><span style="color: #dcaeea;">x1_test</span> = keras.preprocessing.sequence.pad_sequences(
<span class="linenr">12: </span>        x1_test, maxlen=MAX_SEQUENCE_LENGTH)
<span class="linenr">13: </span>x2_test = keras.preprocessing.sequence.pad_sequences(
<span class="linenr">14: </span>        x2_test, maxlen=MAX_SEQUENCE_LENGTH)
<span class="linenr">15: </span>
<span class="linenr">16: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;&#24050;&#35347;&#32244;&#30340;&#27169;&#22411;&#20570;&#38928;&#28204;</span>
<span class="linenr">17: </span>predictions = model.predict([x1_test, x2_test])
<span class="linenr">18: </span>
<span class="linenr">19: </span><span style="color: #c678dd;">print</span>(predictions[<span style="color: #da8548; font-weight: bold;">5</span>])
</pre>
</div>

<pre class="example">
[8.7921888e-01 8.2681845e-06 1.2077282e-01]
</pre>




<p>
跟我們之前討論過的一樣，模型針對每一筆成對新聞標題的輸入，會回傳給我們 3 個分類的機率值。<br />
</p>

<p>
現在，我們只要將機率值最大的類別當作答案，並將這個結果轉回對應的文本標籤即可上傳到 Kaggle：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">index_to_label</span> = {<span style="color: #dcaeea;">v</span>: k <span style="color: #51afef;">for</span> k, v <span style="color: #51afef;">in</span> label_to_index.items()}
<span class="linenr">2: </span>
<span class="linenr">3: </span>test[<span style="color: #98be65;">'Category'</span>] = [index_to_label[idx] <span style="color: #51afef;">for</span> idx <span style="color: #51afef;">in</span> np.argmax(predictions, axis=<span style="color: #da8548; font-weight: bold;">1</span>)]
<span class="linenr">4: </span>
<span class="linenr">5: </span>submission = test.loc[:, [<span style="color: #98be65;">'Category'</span>]].reset_index()
<span class="linenr">6: </span>
<span class="linenr">7: </span>submission.columns = [<span style="color: #98be65;">'Id'</span>, <span style="color: #98be65;">'Category'</span>]
<span class="linenr">8: </span><span style="color: #c678dd;">print</span>(submission.head())
</pre>
</div>

<pre class="example">
   Id   Category
0   0  unrelated
1   3  unrelated
2   1  unrelated
3   2  unrelated
4   9     agreed
</pre>


<p>
得到上面的 DataFrame 以後，我們可以將其儲存成 CSV 並上傳到 kaggle，而結果如下：<br />
</p>

<div id="org4fdae32" class="figure">
<p><img src="images/first-submission-result.jpg" alt="first-submission-result.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 35: </span>NLP 模型第一次的結果</p>
</div>



<p>
如果你還記得我們在用直覺找出第一條底線的章節內容的話，就會知道這並不是應該多好的預測結果，但的確比多數票決好了一點點。<br />
</p>

<p>
不過不需要操之過急，因為任何機器學習專案都是一個持續重複改善的迴圈。在第一次預測就做出完美結果的情況很少，重點是持續改善。<br />
</p>



<p>
在第一次提交結果以後，我們還可以做非常多事情來嘗試改善模型效能：<br />
</p>

<ul class="org-ul">
<li>改變字典詞彙量、序列長度<br /></li>
<li>改變詞向量的維度<br /></li>
<li>嘗試預先訓練的詞向量如 ELMo、GloVe<br /></li>
<li>調整 LSTM 層的輸出維度<br /></li>
<li>使用不同優化器、調整 Learning rate<br /></li>
<li>改變神經網路架構如使用 GRU 層<br /></li>
<li>&#x2026;<br /></li>
</ul>

<p>
能改善準確度的方式不少，但因為牽涉範圍太廣，請容許我把它們留給你當做回家作業。<br />
</p>

<p>
走到這裡代表你已經完整地經歷了一個 NLP 專案所需要的大部分步驟。在下一節．讓我們回顧一下在這趟旅程中你所學到的東西。<br />
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org74d739f" class="outline-2">
<h2 id="org74d739f"><span class="section-number-2">10.</span> 3 門推薦的線上課程</h2>
<div class="outline-text-2" id="text-10">
<ul class="org-ul">
<li>台大電機系李宏毅教授的深度學習課程: 奠定理論基礎<br /></li>
<li>Coursera 的 Deep Learning 專項課程: 理論 70 % + 實作 30 %<br /></li>
<li>[[<a href="https://www.oreilly.com/library/view/deep-learning-with/9781617294433VE/">https://www.oreilly.com/library/view/deep-learning-with/9781617294433VE/</a>][Deep Learning with Python: 注重程式實作<br /></li>
</ul>
</div>
</div>

<div id="outline-container-org7390409" class="outline-2">
<h2 id="org7390409"><span class="section-number-2">11.</span> 工具</h2>
<div class="outline-text-2" id="text-11">
</div>
<div id="outline-container-org5469080" class="outline-3">
<h3 id="org5469080"><span class="section-number-3">11.1.</span> 中研院CkipTagger中文處理工具</h3>
<div class="outline-text-3" id="text-11-1">
</div>
<ol class="org-ol">
<li><a id="org5d78872"></a>官網: <a href="https://ckip.iis.sinica.edu.tw/">https://ckip.iis.sinica.edu.tw/</a><br />
<div class="outline-text-4" id="text-11-1-1">
<ul class="org-ul">
<li>CKIP <a href="https://github.com/ckiplab/ckip-transformers">Transformers</a> — 繁體中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然語言處理工具。<br /></li>
<li>CKIP <a href="https://github.com/ckiplab/ckiptagger">Tagger</a> — 新的開源斷詞、詞性標注、實體辨識系統。<br /></li>
</ul>
</div>
</li>
<li><a id="orge00430c"></a>簡介<br />
<div class="outline-text-4" id="text-11-1-2">
<p>
根據該工具的文件說明，這套CkipTagger中文處理工具，不只提供繁體中文斷詞的功能，也加入詞性標注和18類專有名詞的實體辨識（Named entity recognition）等功能，甚至當以多達5萬句的ASBC 4.0漢語語料庫測試集，來進行中文斷詞測試時，CkipTagger表現遠高於中國的結巴，中研院在中文斷詞準確度可達到97.49%，相較之下，中國的結巴只有90.51%<sup><a id="fnr.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup>。<br />
</p>

<p>
新版本也提供幾大特色，包括了加強斷詞表現、可以不自動刪／改字，並且能夠支援不限長度的句子，另外，新版也加入使用者自訂功能，提供參考／強制 詞典的自訂功能<sup><a id="fnr.7.100" class="footref" href="#fn.7" role="doc-backlink">7</a></sup>。<br />
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org4bd992d" class="outline-3">
<h3 id="org4bd992d"><span class="section-number-3">11.2.</span> 結巴Jieba1</h3>
<div class="outline-text-3" id="text-11-2">
</div>
<ol class="org-ol">
<li><a id="org96b76f6"></a>使用教學<br />
<div class="outline-text-4" id="text-11-2-1">
<ul class="org-ul">
<li><a href="https://blog.kennycoder.io/2020/02/12/Python-%E7%9F%A5%E5%90%8DJieba%E4%B8%AD%E6%96%87%E6%96%B7%E8%A9%9E%E5%B7%A5%E5%85%B7%E6%95%99%E5%AD%B8/">Python - 知名 Jieba 中文斷詞工具教學 </a><br /></li>
</ul>
</div>
</li>
<li><a id="org8769efa"></a>Jieba 原理介紹<sup><a id="fnr.8" class="footref" href="#fn.8" role="doc-backlink">8</a></sup><br />
<div class="outline-text-4" id="text-11-2-2">
<p>
Jieba 斷詞主要是结合：<br />
</p>
<ul class="org-ul">
<li>規則斷詞: 主要是透過詞典，在對句子進行斷詞的時候，將句子的每個字與詞典中的詞進行匹配，找到則斷詞，否則無法斷詞。<br /></li>
<li>統計斷詞: 主要是看如果相連的字在不同的文本中出現的次數越多，就推斷這相連的字很可能就是一個詞。因此就可以利用字與字相鄰出现的頻率來做統計。當高於某一個臨界值時，便可認為此字組是一個詞語。<br /></li>
</ul>
</div>
</li>

<li><a id="org4bb4607"></a>Jieba 斷詞模式<sup><a id="fnr.8.100" class="footref" href="#fn.8" role="doc-backlink">8</a></sup><br />
<div class="outline-text-4" id="text-11-2-3">
<p>
最著名的功能就是提供斷詞模式，主要分為<br />
</p>
<ul class="org-ul">
<li>精確模式: 將句子最精確的切開，適合文本分析<br /></li>
<li>全模式: 把句子中所有的可以成詞的詞語都斷出来，速度非常快。<br /></li>
<li>搜索引擎模式: 在精確模式的基礎上，對長的詞語再次切分，提高召回率，適合用於搜索引擎分詞。<br /></li>
</ul>
</div>
</li>
</ol>
</div>
<div id="outline-container-org30c8b8c" class="outline-3">
<h3 id="org30c8b8c"><span class="section-number-3">11.3.</span> NLTK</h3>
<div class="outline-text-3" id="text-11-3">
<p>
<a href="NLTK.html">NLTK</a>: NLP的處理工具<br />
</p>
</div>
</div>
</div>

<div id="outline-container-org23cdcb5" class="outline-2">
<h2 id="org23cdcb5"><span class="section-number-2">12.</span> 語料集</h2>
<div class="outline-text-2" id="text-12">
<p>
自然語言中，雖然算法、模型很重要，但沒有資料驅動還是什麼都做不了，尤其是文字不像圖片一樣不分國界，每個地方用的語言不同，所需要的資料也就不同，英文因為是世界語言，加上論文的發表大部分以研究英文為主，所以資料當然是最多，不過幸運地，中文的語料也算蠻多的，這邊列出幾個好用的中文資料集或預訓練模型<sup><a id="fnr.9" class="footref" href="#fn.9" role="doc-backlink">9</a></sup><br />
</p>
</div>
<div id="outline-container-org1b2794b" class="outline-3">
<h3 id="org1b2794b"><span class="section-number-3">12.1.</span> NLP-Chinese-Corpus</h3>
<div class="outline-text-3" id="text-12-1">
<p>
<a href="https://github.com/brightmart/nlp_chinese_corpus">NLP_Chinese-Corpus</a> 是在github上由brightmart開源的中文語料統整，裡面包含了一些維基百科、新聞、社區問答等語料，比較多是通用型的中文語料，可以當作詞向量訓練的材料，或者其它應用<br />
</p>
</div>
</div>
<div id="outline-container-orgb9f1bf6" class="outline-3">
<h3 id="orgb9f1bf6"><span class="section-number-3">12.2.</span> PTT-Gossiping-Corpus</h3>
<div class="outline-text-3" id="text-12-2">
<p>
<a href="https://github.com/zake7749/Gossiping-Chinese-Corpus">PTT-Gossiping-Corpus</a>是由zake7749在github以及Kaggle上提供的繁體中文問答語料集，一個問句對應到一句鄉民回答，對於想實作中文Sequence2Sequence的朋友，會是蠻不錯的實驗語料<br />
圖片截自zake7749/Gossiping-Chinese-Corpus 中的範例說明<br />
</p>
</div>
</div>
<div id="outline-container-orgc241b58" class="outline-3">
<h3 id="orgc241b58"><span class="section-number-3">12.3.</span> Wiki-corpus</h3>
<div class="outline-text-3" id="text-12-3">
<p>
繁體中文的語料，通常就會想到PTT以及中文維基百科，而現在獲取中文維基百科的手段有很多種，維基百科本身也有提供API，大家可以視個人需求來拿到資料<br />
</p>
</div>
</div>
<div id="outline-container-org8ca2f70" class="outline-3">
<h3 id="org8ca2f70"><span class="section-number-3">12.4.</span> Tencent AI LAB</h3>
<div class="outline-text-3" id="text-12-4">
<p>
騰訊AI實驗室非常佛心的釋出相當多有用的中文資料集，像是Dialogue Research中包含一些在聊天機器人領域上實用的中文語料，有興趣的朋友可以參考我這篇論文的介紹：Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration此外，騰訊AI也有提供預訓練的中文詞向量模型，有興趣的朋友也能試試看<br />
</p>
</div>
</div>
<div id="outline-container-org5cea815" class="outline-3">
<h3 id="org5cea815"><span class="section-number-3">12.5.</span> BERT</h3>
<div class="outline-text-3" id="text-12-5">
<p>
BERT就像詞向量模型一樣，需要大量的語料進行預訓練，好處是我們這種一般大眾可以直接使用一些科技巨擘所釋出的預訓練模型，像是Google, HuggingFace, 還有一些開源的github項目都有提供中文的BERT預訓練模型<br />
</p>
</div>
</div>
</div>

<div id="outline-container-org235fe07" class="outline-2">
<h2 id="org235fe07"><span class="section-number-2">13.</span> NLP 應用</h2>
<div class="outline-text-2" id="text-13">
</div>
<div id="outline-container-org2e42a76" class="outline-3">
<h3 id="org2e42a76"><span class="section-number-3">13.1.</span> 領域</h3>
<div class="outline-text-3" id="text-13-1">
<ul class="org-ul">
<li>文字朗讀（Text to speech）/語音合成（Speech synthesis）<br /></li>
<li>語音識別（Speech recognition）<br /></li>
<li>中文自動分詞（Chinese word segmentation）<br /></li>
<li>詞性標註（Part-of-speech tagging）<br /></li>
<li>句法分析（Parsing）<br /></li>
<li>自然語言生成（Natural language generation）<br /></li>
<li>文字分類（Text categorization）<br /></li>
<li>資訊檢索（Information retrieval）<br /></li>
<li>資訊抽取（Information extraction）<br /></li>
<li>文字校對（Text-proofing）<br /></li>
<li>問答系統（Question answering）<br /></li>
<li>機器翻譯（Machine translation）<br /></li>
<li>自動摘要（Automatic summarization）<br /></li>
<li>文字蘊涵（Textual entailment）<br /></li>
<li>命名實體辨識（Named entity recognition）<br /></li>
</ul>
</div>
</div>
<div id="outline-container-orgde68e67" class="outline-3">
<h3 id="orgde68e67"><span class="section-number-3">13.2.</span> 範例</h3>
<div class="outline-text-3" id="text-13-2">
</div>
<ol class="org-ol">
<li><a id="org8341039"></a>NLP 應用 1:TextClassification 文本分類<br />
<div class="outline-text-4" id="text-13-2-1">
<p>
TextClassification 文本分類的意義<br />
二元分類:分成好與惡<br />
多元分類:分成不同類型的文章或喜好度<br />
TextClassification 文本分類<br />
IMDb-Movie-Review IMDb 網路電影影評資料集 Sentiment Analysis on IMDb<br />
</p>

<ul class="org-ul">
<li>Sentiment Analysis on IMDb: <a href="https://paperswithcode.com/sota/sentiment-analysis-on-imdb">https://paperswithcode.com/sota/sentiment-analysis-on-imdb</a><br /></li>
<li>情緒分析 sentiment analysis<br /></li>
</ul>
<p>
Sentiment analysis (opinion mining or emotion AI)<br />
<a href="https://en.wikipedia.org/wiki/Sentiment_analysis">https://en.wikipedia.org/wiki/Sentiment_analysis</a><br />
很多線上社群網站會蒐集使用者的資料，並且分析使用者行為，<br />
像是知名的 Facebook 在前幾年開始做「情緒分析(sentiment analysis)」，<br />
情緒分析是以文字分析、自然語言處理 NLP 的方法，找出使用者的評價、情緒，進而預測出使用者行為來進行商業決策，<br />
像這樣一連串利用情緒分析帶來的商業價值是相當可觀的。<br />
</p>
</div>
</li>

<li><a id="org72b53e2"></a>NLP 應用 1:TextClassification 文本分類 技術主題<br />
<div class="outline-text-4" id="text-13-2-2">
<p>
TF-IDF 與機器學習實現<br />
<a href="https://blog.csdn.net/crazy_scott/article/details/80830399">https://blog.csdn.net/crazy_scott/article/details/80830399</a><br />
</p>

<p>
使用 TensorFlow Estimators<br />
<a href="https://ruder.io/text-classification-tensorflow-estimators/">https://ruder.io/text-classification-tensorflow-estimators/</a><br />
</p>

<p>
使用 TF-IDF and ELMo<br />
<a href="https://www.kaggle.com/saikumar587/imdb-text-classification-tf-idf-and-elmo">https://www.kaggle.com/saikumar587/imdb-text-classification-tf-idf-and-elmo</a><br />
</p>

<p>
使用 BERT (2018)<br />
Sentiment Analysis of IMDb Movie Reviews Using BERT<br />
BERT Text Classification in 3 Lines of Code Using Keras<br />
<a href="https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358">https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358</a><br />
</p>


<p>
使用 XLNet(2019)<br />
XLNet: Generalized Autoregressive Pretraining for Language Understanding<br />
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le<br />
(Submitted on 19 Jun 2019)<br />
<a href="https://arxiv.org/abs/1906.08237">https://arxiv.org/abs/1906.08237</a><br />
<a href="https://github.com/zihangdai/xlnet">https://github.com/zihangdai/xlnet</a><br />
</p>

<p>
範例程式<br />
<a href="https://colab.research.google.com/github/zihangdai/xlnet/blob/master/notebooks/colab_imdb_gpu.ipynb">https://colab.research.google.com/github/zihangdai/xlnet/blob/master/notebooks/colab_imdb_gpu.ipynb</a><br />
</p>
</div>
</li>

<li><a id="org72f1f68"></a>NLP 應用 2:Text Generation 文本生成<br />
<div class="outline-text-4" id="text-13-2-3">
<p>
TextGeneration 文本生成:作詞機器人<br />
範例程式<br />
參考資料<br />
sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python<br />
<a href="https://github.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python">https://github.com/sudharsan13296/Hands-On-Deep-Learning-Algorithms-with-Python</a><br />
</p>

<p>
Hands-On-Deep-Learning-Algorithms-with-Python/<br />
</p>
<ol class="org-ol">
<li>Generating Song Lyrics Using RNN/4.06 Generating Song Lyrics Using RNN.ipynb<br /></li>
</ol>
</div>
</li>

<li><a id="orge734026"></a>NLP 應用 3: IMDB<br />
<ol class="org-ol">
<li><a id="orgabc45a2"></a>IMDB 正評負評<br />
<div class="outline-text-5" id="text-13-2-4-1">
<ul class="org-ul">
<li>應用：PTT 裡的噓/推/→<br /></li>
<li>中文斷詞<br /></li>
<li>拿 google 的店家評論來判斷星級<br /></li>
</ul>
</div>
</li>
<li><a id="orge80a7f8"></a>計算 Vector 的關聯性<br />
<div class="outline-text-5" id="text-13-2-4-2">
<ul class="org-ul">
<li>cos simility<br /></li>
<li>但是 one-hot vector 間無法計算 cos simility，因為兩向量內積為 1，所以要用 word vector representation<br /></li>
<li>在文字處理的狀況下，所謂兩個 vector 的相關指的是兩句話的相似性<br /></li>
<li>用來自動生成考題(vocabulary testing)或生成答案，找人家 train 好的 word emgedding matrix 來做，生成 word vector<br /></li>
<li>可以拿英文 train 好的 model 來做中文嗎？(transfer learning)<br /></li>
</ul>
</div>
</li>
<li><a id="org2cd82e2"></a>Text Classification<br />
<div class="outline-text-5" id="text-13-2-4-3">
<ul class="org-ul">
<li><p>
tf.keras 技術者們必讀！深度學習攻略手冊 施威銘研究室 著 旗標科技  2020-02-13<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 2: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot</span>(history_dict, keys, title=<span style="color: #a9a1e1;">None</span>, xyLabel=[], ylim=(), size=()):
<span class="linenr"> 3: </span>    lineType = (<span style="color: #98be65;">'-'</span>, <span style="color: #98be65;">'--'</span>, <span style="color: #98be65;">'.'</span>, <span style="color: #98be65;">':'</span>)    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32218;&#26781;&#30340;&#27171;&#24335;, &#30059;&#22810;&#26781;&#32218;&#26178;&#26371;&#20381;&#24207;&#25505;&#29992;</span>
<span class="linenr"> 4: </span>    <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(ylim)==<span style="color: #da8548; font-weight: bold;">2</span>: plt.ylim(*ylim)    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450; y &#36600;&#26368;&#23567;&#20540;&#21450;&#26368;&#22823;&#20540;</span>
<span class="linenr"> 5: </span>    <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(size)==<span style="color: #da8548; font-weight: bold;">2</span>: plt.gcf().set_size_inches(*size)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">size&#38928;&#35373;&#28858; (6,4)</span>
<span class="linenr"> 6: </span>    epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(history_dict[keys[<span style="color: #da8548; font-weight: bold;">0</span>]])+<span style="color: #da8548; font-weight: bold;">1</span>)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;&#26377;&#24190;&#36913;&#26399;&#30340;&#36039;&#26009;</span>
<span class="linenr"> 7: </span>    <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(keys)):   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36208;&#35370;&#27599;&#19968;&#20491; key (&#20363;&#22914; 'loss' &#25110; 'acc' &#31561;)</span>
<span class="linenr"> 8: </span>        plt.plot(epochs, history_dict[keys[i]], lineType[i])  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#20986;&#32218;&#26781;</span>
<span class="linenr"> 9: </span>    <span style="color: #51afef;">if</span> title:   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26159;&#21542;&#39023;&#31034;&#27161;&#38988;&#27396;</span>
<span class="linenr">10: </span>        plt.title(title)
<span class="linenr">11: </span>    <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(xyLabel)==<span style="color: #da8548; font-weight: bold;">2</span>:  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26159;&#21542;&#39023;&#31034; x, y &#36600;&#30340;&#35498;&#26126;&#25991;&#23383;</span>
<span class="linenr">12: </span>        plt.xlabel(xyLabel[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">13: </span>        plt.ylabel(xyLabel[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">14: </span>    plt.legend(keys, loc=<span style="color: #98be65;">'best'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#22294;&#20363; (&#26371;&#20197; key &#28858;&#27599;&#26781;&#32218;&#30340;&#35498;&#26126;)</span>
<span class="linenr">15: </span>    plt.show()  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#20986;&#30059;&#22909;&#30340;&#22294;</span>
<span class="linenr">16: </span>
<span class="linenr">17: </span>
<span class="linenr">18: </span><span style="color: #51afef;">from</span> tensorflow.keras.datasets <span style="color: #51afef;">import</span> imdb    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592; &#24478; keras.datasets &#22871;&#20214;&#20013;&#21295;&#20837; imdb</span>
<span class="linenr">19: </span>(a_train, b_train),(a_test, b_test)= imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36617;&#20837; IMDB</span>
<span class="linenr">20: </span>
<span class="linenr">21: </span><span style="color: #51afef;">from</span> tensorflow.keras.preprocessing.text <span style="color: #51afef;">import</span> Tokenizer
<span class="linenr">22: </span>
<span class="linenr">23: </span>tok = Tokenizer(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)           <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#25351;&#23450;&#23383;&#20856;&#30340;&#32317;&#23383;&#25976;</span>
<span class="linenr">24: </span>x_train = tok.sequences_to_matrix(a_train) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#35347;&#32244;&#27171;&#26412;&#20570; k-hot &#32232;&#30908;</span>
<span class="linenr">25: </span>x_test  = tok.sequences_to_matrix(a_test)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#28204;&#35430;&#27171;&#26412;&#20570; k-hot &#32232;&#30908;</span>
<span class="linenr">26: </span>
<span class="linenr">27: </span>y_train = b_train.astype(<span style="color: #98be65;">'float32'</span>)   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#35347;&#32244;&#27161;&#31844;&#36681;&#28858;&#28014;&#40670;&#21521;&#37327;</span>
<span class="linenr">28: </span>y_test  = b_test.astype(<span style="color: #98be65;">'float32'</span>)    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#28204;&#35430;&#27161;&#31844;&#36681;&#28858;&#28014;&#40670;&#21521;&#37327;</span>
<span class="linenr">29: </span>
<span class="linenr">30: </span><span style="color: #51afef;">from</span> tensorflow.keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">31: </span><span style="color: #51afef;">from</span> tensorflow.keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">32: </span>
<span class="linenr">33: </span>model = Sequential()                       <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#24314;&#31435;&#27169;&#22411;&#29289;&#20214;</span>
<span class="linenr">34: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_dim=<span style="color: #da8548; font-weight: bold;">10000</span>))  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#36664;&#20837;&#23652;</span>
<span class="linenr">35: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>))    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#38577;&#34255;&#23652;</span>
<span class="linenr">36: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#36664;&#20986;&#23652;</span>
<span class="linenr">37: </span>
<span class="linenr">38: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20013;&#38291;&#29992;&#20840;&#36899;&#25509;&#23652;&#65292;&#20854;&#23526;&#20063;&#21487;&#20197;&#29992;CNN&#25110;RNN</span>
<span class="linenr">39: </span>model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">40: </span>              optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">41: </span>              metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr">42: </span>
<span class="linenr">43: </span>history = model.fit(x_train, y_train,
<span class="linenr">44: </span>                    batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#27599;&#25209;&#27425; 512 &#31558;&#27171;&#26412;</span>
<span class="linenr">45: </span>                    epochs=<span style="color: #da8548; font-weight: bold;">10</span>,       <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#20849;&#35347;&#32244; 10 &#36913;&#26399;</span>
<span class="linenr">46: </span>                    verbose = <span style="color: #da8548; font-weight: bold;">2</span>,     <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#39023;&#31034;&#31934;&#31777;&#35338;&#24687; (&#28961;&#36914;&#24230;&#26781;)</span>
<span class="linenr">47: </span>                    validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>)
<span class="linenr">48: </span>                             <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8593;&#30001;&#35347;&#32244;&#36039;&#26009;&#24460;&#38754;&#20999;&#20986; 20% &#20570;&#28858;&#39511;&#35657;&#29992;</span>
<span class="linenr">49: </span>
<span class="linenr">50: </span>g1 = plot(history.history,
<span class="linenr">51: </span>       (<span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>),          <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#27511;&#21490;&#36039;&#26009;&#20013;&#30340; key</span>
<span class="linenr">52: </span>       <span style="color: #98be65;">'Training &amp; Validation Loss'</span>,  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#32218;&#22294;&#30340;&#27161;&#38988;</span>
<span class="linenr">53: </span>       (<span style="color: #98be65;">'Epoch'</span>,<span style="color: #98be65;">'Loss'</span>))              <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;x,y &#36600;&#30340;&#21517;&#31281;</span>
<span class="linenr">54: </span>plt.savefig()
<span class="linenr">55: </span>plot(history.history,
<span class="linenr">56: </span>       (<span style="color: #98be65;">'acc'</span>, <span style="color: #98be65;">'val_acc'</span>),            <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#27511;&#21490;&#36039;&#26009;&#20013;&#30340; key</span>
<span class="linenr">57: </span>       <span style="color: #98be65;">'Training &amp; Validation Acc'</span>,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#32218;&#22294;&#30340;&#27161;&#38988;</span>
<span class="linenr">58: </span>       (<span style="color: #98be65;">'Epoch'</span>,<span style="color: #98be65;">'Acc'</span>))               <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;x,y &#36600;&#30340;&#21517;&#31281;</span>
</pre>
</div></li>

<li><p>
中間多塞幾層<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot</span>(history_dict, keys, title=<span style="color: #a9a1e1;">None</span>, xyLabel=[], ylim=(), size=()):
<span class="linenr"> 4: </span>    lineType = (<span style="color: #98be65;">'-'</span>, <span style="color: #98be65;">'--'</span>, <span style="color: #98be65;">'.'</span>, <span style="color: #98be65;">':'</span>)    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32218;&#26781;&#30340;&#27171;&#24335;, &#30059;&#22810;&#26781;&#32218;&#26178;&#26371;&#20381;&#24207;&#25505;&#29992;</span>
<span class="linenr"> 5: </span>    <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(ylim)==<span style="color: #da8548; font-weight: bold;">2</span>: plt.ylim(*ylim)    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450; y &#36600;&#26368;&#23567;&#20540;&#21450;&#26368;&#22823;&#20540;</span>
<span class="linenr"> 6: </span>    <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(size)==<span style="color: #da8548; font-weight: bold;">2</span>: plt.gcf().set_size_inches(*size)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">size&#38928;&#35373;&#28858; (6,4)</span>
<span class="linenr"> 7: </span>    epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(history_dict[keys[<span style="color: #da8548; font-weight: bold;">0</span>]])+<span style="color: #da8548; font-weight: bold;">1</span>)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;&#26377;&#24190;&#36913;&#26399;&#30340;&#36039;&#26009;</span>
<span class="linenr"> 8: </span>    <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(keys)):   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36208;&#35370;&#27599;&#19968;&#20491; key (&#20363;&#22914; 'loss' &#25110; 'acc' &#31561;)</span>
<span class="linenr"> 9: </span>        plt.plot(epochs, history_dict[keys[i]], lineType[i])  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#20986;&#32218;&#26781;</span>
<span class="linenr">10: </span>    <span style="color: #51afef;">if</span> title:   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26159;&#21542;&#39023;&#31034;&#27161;&#38988;&#27396;</span>
<span class="linenr">11: </span>        plt.title(title)
<span class="linenr">12: </span>    <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(xyLabel)==<span style="color: #da8548; font-weight: bold;">2</span>:  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26159;&#21542;&#39023;&#31034; x, y &#36600;&#30340;&#35498;&#26126;&#25991;&#23383;</span>
<span class="linenr">13: </span>        plt.xlabel(xyLabel[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">14: </span>        plt.ylabel(xyLabel[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">15: </span>    plt.legend(keys, loc=<span style="color: #98be65;">'best'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#22294;&#20363; (&#26371;&#20197; key &#28858;&#27599;&#26781;&#32218;&#30340;&#35498;&#26126;)</span>
<span class="linenr">16: </span>    plt.show()  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#20986;&#30059;&#22909;&#30340;&#22294;</span>
<span class="linenr">17: </span>
<span class="linenr">18: </span>
<span class="linenr">19: </span><span style="color: #51afef;">from</span> tensorflow.keras.datasets <span style="color: #51afef;">import</span> imdb    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592; &#24478; keras.datasets &#22871;&#20214;&#20013;&#21295;&#20837; imdb</span>
<span class="linenr">20: </span>(a_train, b_train),(a_test, b_test)= imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36617;&#20837; IMDB</span>
<span class="linenr">21: </span>
<span class="linenr">22: </span><span style="color: #51afef;">from</span> tensorflow.keras.preprocessing.text <span style="color: #51afef;">import</span> Tokenizer
<span class="linenr">23: </span>
<span class="linenr">24: </span>tok = Tokenizer(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)           <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#25351;&#23450;&#23383;&#20856;&#30340;&#32317;&#23383;&#25976;</span>
<span class="linenr">25: </span>x_train = tok.sequences_to_matrix(a_train) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#35347;&#32244;&#27171;&#26412;&#20570; k-hot &#32232;&#30908;</span>
<span class="linenr">26: </span>x_test  = tok.sequences_to_matrix(a_test)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#28204;&#35430;&#27171;&#26412;&#20570; k-hot &#32232;&#30908;</span>
<span class="linenr">27: </span>
<span class="linenr">28: </span>y_train = b_train.astype(<span style="color: #98be65;">'float32'</span>)   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#35347;&#32244;&#27161;&#31844;&#36681;&#28858;&#28014;&#40670;&#21521;&#37327;</span>
<span class="linenr">29: </span>y_test  = b_test.astype(<span style="color: #98be65;">'float32'</span>)    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#28204;&#35430;&#27161;&#31844;&#36681;&#28858;&#28014;&#40670;&#21521;&#37327;</span>
<span class="linenr">30: </span>
<span class="linenr">31: </span><span style="color: #51afef;">from</span> tensorflow.keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">32: </span><span style="color: #51afef;">from</span> tensorflow.keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">33: </span><span style="color: #51afef;">from</span> tensorflow.keras <span style="color: #51afef;">import</span> layers
<span class="linenr">34: </span>
<span class="linenr">35: </span>model = Sequential()                       <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#24314;&#31435;&#27169;&#22411;&#29289;&#20214;</span>
<span class="linenr">36: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_dim=<span style="color: #da8548; font-weight: bold;">10000</span>))  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#36664;&#20837;&#23652;</span>
<span class="linenr">37: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>))    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#38577;&#34255;&#23652;</span>
<span class="linenr">38: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21152;&#20837;&#31532;&#20108;&#23652;&#65292;&#35442;&#23652;&#30340; output neurons &#25976;&#30446;=64&#65292;&#21855;&#21205;&#20989;&#24335;&#28858;&#38928;&#35373; relu</span>
<span class="linenr">39: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20006;&#23565;&#35442;&#23652;&#21443;&#25976;&#20570; l1-regularization&#65292;&#20854;&#20418;&#25976;&#28858; 0.01</span>
<span class="linenr">40: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, kernel_regularizer=tf.keras.regularizers.l1(<span style="color: #da8548; font-weight: bold;">0.01</span>)))
<span class="linenr">41: </span>
<span class="linenr">42: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21152;&#20837;&#31532;&#19977;&#23652;&#65292;&#35442;&#23652;&#30340; output neurons &#25976;&#30446;=64&#65292;&#21855;&#21205;&#20989;&#24335;&#28858;&#38928;&#35373; relu</span>
<span class="linenr">43: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20006;&#23565;&#35442;&#23652;&#21443;&#25976;&#20570; l2-regularization&#65292;&#20854;&#20418;&#25976;&#28858; 0.01</span>
<span class="linenr">44: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, bias_regularizer=tf.keras.regularizers.l2(<span style="color: #da8548; font-weight: bold;">0.01</span>)))
<span class="linenr">45: </span>
<span class="linenr">46: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21152;&#20837;&#31532;&#22235;&#23652;&#65292;&#35442;&#23652;&#30340; output neurons &#25976;&#30446;=64&#65292;&#21855;&#21205;&#20989;&#24335;&#28858;&#38928;&#35373; relu</span>
<span class="linenr">47: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35442;&#23652;&#21443;&#25976;&#21021;&#22987;&#21270;&#26159;&#29992;&#27491;&#20132;&#31574;&#30053;</span>
<span class="linenr">48: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, kernel_initializer=<span style="color: #98be65;">'orthogonal'</span>))
<span class="linenr">49: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21152;&#20837;&#31532;&#20116;&#23652;&#65292;&#35442;&#23652;&#30340; output neurons &#25976;&#30446;=64&#65292;&#21855;&#21205;&#20989;&#24335;&#28858;&#38928;&#35373; relu</span>
<span class="linenr">50: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35442;&#23652;&#20559;&#31227;&#21021;&#22987;&#21270;&#26159;&#29992;&#24120;&#25976;&#31574;&#30053;&#65288;&#30342;&#35373;&#28858;2.0&#65289;</span>
<span class="linenr">51: </span>model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, bias_initializer=tf.keras.initializers.Constant(<span style="color: #da8548; font-weight: bold;">2.0</span>)))
<span class="linenr">52: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#36664;&#20986;&#23652;</span>
<span class="linenr">53: </span>
<span class="linenr">54: </span>model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">55: </span>              optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">56: </span>              metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr">57: </span>
<span class="linenr">58: </span>history = model.fit(x_train, y_train,
<span class="linenr">59: </span>                    batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#27599;&#25209;&#27425; 512 &#31558;&#27171;&#26412;</span>
<span class="linenr">60: </span>                    epochs=<span style="color: #da8548; font-weight: bold;">10</span>,       <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#20849;&#35347;&#32244; 10 &#36913;&#26399;</span>
<span class="linenr">61: </span>                    verbose = <span style="color: #da8548; font-weight: bold;">2</span>,     <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#39023;&#31034;&#31934;&#31777;&#35338;&#24687; (&#28961;&#36914;&#24230;&#26781;)</span>
<span class="linenr">62: </span>                    validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>)
<span class="linenr">63: </span>                             <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8593;&#30001;&#35347;&#32244;&#36039;&#26009;&#24460;&#38754;&#20999;&#20986; 20% &#20570;&#28858;&#39511;&#35657;&#29992;</span>
<span class="linenr">64: </span>plt.clf()
<span class="linenr">65: </span>plt.plot(history.history,
<span class="linenr">66: </span>       (<span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>),          <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#27511;&#21490;&#36039;&#26009;&#20013;&#30340; key</span>
<span class="linenr">67: </span>       <span style="color: #98be65;">'Training &amp; Validation Loss'</span>,  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#32218;&#22294;&#30340;&#27161;&#38988;</span>
<span class="linenr">68: </span>       (<span style="color: #98be65;">'Epoch'</span>,<span style="color: #98be65;">'Loss'</span>))              <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;x,y &#36600;&#30340;&#21517;&#31281;</span>
<span class="linenr">69: </span>
<span class="linenr">70: </span>plt.savefig(<span style="color: #98be65;">'images/plt2020-a.jpg'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">71: </span>plt.clf()
<span class="linenr">72: </span>plt.plot(history.history,
<span class="linenr">73: </span>       (<span style="color: #98be65;">'acc'</span>, <span style="color: #98be65;">'val_acc'</span>),            <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#27511;&#21490;&#36039;&#26009;&#20013;&#30340; key</span>
<span class="linenr">74: </span>       <span style="color: #98be65;">'Training &amp; Validation Acc'</span>,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#32218;&#22294;&#30340;&#27161;&#38988;</span>
<span class="linenr">75: </span>       (<span style="color: #98be65;">'Epoch'</span>,<span style="color: #98be65;">'Acc'</span>))               <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;x,y &#36600;&#30340;&#21517;&#31281;</span>
<span class="linenr">76: </span>plt.savefig(<span style="color: #98be65;">'images/plt2020-b.jpg'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">77: </span>
<span class="linenr">78: </span>
<span class="linenr">79: </span>
</pre>
</div></li>
</ul>

<p>
[[<img src="images/plt2020-a.jpg" alt="plt2020-a.jpg" />]]<br />
[[<img src="images/plt2020-b.jpg" alt="plt2020-b.jpg" />]]<br />
</p>
</div>
</li>
<li><a id="orgd4903a6"></a>多元分類:依據不同主題分類各種新聞<br />
<div class="outline-text-5" id="text-13-2-4-4">
<ul class="org-ul">
<li>路透社新聞：共 47 類，答案部份用 one-hot<br /></li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span><span style="color: #51afef;">from</span> tensorflow.keras.datasets <span style="color: #51afef;">import</span> reuters  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21295;&#20837; reuters &#36039;&#26009;&#38598;</span>
<span class="linenr">  2: </span>
<span class="linenr">  3: </span>(a_train, b_train),(<span style="color: #dcaeea;">a_test</span>, <span style="color: #dcaeea;">b_test</span>) = reuters.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">  4: </span>
<span class="linenr">  5: </span><span style="color: #51afef;">from</span> tensorflow.keras.preprocessing.text <span style="color: #51afef;">import</span> Tokenizer  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#21295;&#20837; Tokenizer &#39006;&#21029;</span>
<span class="linenr">  6: </span>
<span class="linenr">  7: </span>tok = Tokenizer(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)           <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#25351;&#23450;&#23383;&#20856;&#30340;&#32317;&#23383;&#25976;</span>
<span class="linenr">  8: </span>x_train = tok.sequences_to_matrix(a_train) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#35347;&#32244;&#27171;&#26412;&#20570; multi-hot &#32232;&#30908;</span>
<span class="linenr">  9: </span>x_test  = tok.sequences_to_matrix(a_test)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#28204;&#35430;&#27171;&#26412;&#20570; multi-hot &#32232;&#30908;</span>
<span class="linenr"> 10: </span>
<span class="linenr"> 11: </span><span style="color: #51afef;">from</span> tensorflow.keras.utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr"> 12: </span>
<span class="linenr"> 13: </span>y_train = to_categorical(b_train)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#35347;&#32244;&#27161;&#31844;&#36681;&#28858; one-hot &#32232;&#30908;</span>
<span class="linenr"> 14: </span>y_test  = to_categorical(b_test)   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#28204;&#35430;&#27161;&#31844;&#36681;&#28858; one-hot &#32232;&#30908;</span>
<span class="linenr"> 15: </span>
<span class="linenr"> 16: </span><span style="color: #51afef;">from</span> tensorflow.keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr"> 17: </span><span style="color: #51afef;">from</span> tensorflow.keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr"> 18: </span>
<span class="linenr"> 19: </span>model = Sequential()
<span class="linenr"> 20: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>, input_dim=<span style="color: #da8548; font-weight: bold;">10000</span>))
<span class="linenr"> 21: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 22: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">46</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr"> 23: </span>
<span class="linenr"> 24: </span>model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr"> 25: </span>              optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr"> 26: </span>              metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr"> 27: </span>
<span class="linenr"> 28: </span>history = model.fit(x_train, y_train,
<span class="linenr"> 29: </span>                    batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr"> 30: </span>                    epochs=<span style="color: #da8548; font-weight: bold;">20</span>,            <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592; &#35347;&#32244; 20 &#20491;&#36913;&#26399;</span>
<span class="linenr"> 31: </span>                    verbose=<span style="color: #da8548; font-weight: bold;">2</span>,     <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#39023;&#31034;&#31934;&#31777;&#35338;&#24687; (&#28961;&#36914;&#24230;&#26781;)</span>
<span class="linenr"> 32: </span>                    validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592; &#30001;&#35347;&#32244;&#36039;&#26009;&#20999;&#20986; 20% &#20570;&#39511;&#35657;</span>
<span class="linenr"> 33: </span>
<span class="linenr"> 34: </span><span style="color: #51afef;">import</span> util2 <span style="color: #51afef;">as</span> u
<span class="linenr"> 35: </span>u.plot(history.history, (<span style="color: #98be65;">'acc'</span>, <span style="color: #98be65;">'val_acc'</span>), <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#32362;&#35069;&#28310;&#30906;&#29575;&#30340;&#27511;&#21490;&#32218;&#22294;</span>
<span class="linenr"> 36: </span>       <span style="color: #98be65;">'Training &amp; Validation Acc'</span>, (<span style="color: #98be65;">'Epoch'</span>,<span style="color: #98be65;">'Acc'</span>))
<span class="linenr"> 37: </span>
<span class="linenr"> 38: </span><span style="color: #dcaeea;">loss</span>, <span style="color: #dcaeea;">acc</span> = model.evaluate(x_test, y_test, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#35413;&#20272;&#35347;&#32244;&#25104;&#25928;</span>
<span class="linenr"> 39: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#35413;&#20272;&#28204;&#35430;&#36039;&#26009;&#30340;&#28310;&#30906;&#29575; ='</span>, acc)
<span class="linenr"> 40: </span>
<span class="linenr"> 41: </span><span style="color: #51afef;">from</span> tensorflow.keras.datasets <span style="color: #51afef;">import</span> reuters  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21295;&#20837; reuters &#36039;&#26009;&#38598;</span>
<span class="linenr"> 42: </span>
<span class="linenr"> 43: </span>(a_train, b_train),(a_test, b_test) = reuters.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr"> 44: </span>
<span class="linenr"> 45: </span><span style="color: #51afef;">from</span> tensorflow.keras.preprocessing.text <span style="color: #51afef;">import</span> Tokenizer  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#21295;&#20837; Tokenizer &#39006;&#21029;</span>
<span class="linenr"> 46: </span>
<span class="linenr"> 47: </span>tok = Tokenizer(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)           <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#25351;&#23450;&#23383;&#20856;&#30340;&#32317;&#23383;&#25976;</span>
<span class="linenr"> 48: </span>x_train = tok.sequences_to_matrix(a_train) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#35347;&#32244;&#27171;&#26412;&#20570; multi-hot &#32232;&#30908;</span>
<span class="linenr"> 49: </span>x_test  = tok.sequences_to_matrix(a_test)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#28204;&#35430;&#27171;&#26412;&#20570; multi-hot &#32232;&#30908;</span>
<span class="linenr"> 50: </span>
<span class="linenr"> 51: </span><span style="color: #51afef;">from</span> tensorflow.keras.utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr"> 52: </span>
<span class="linenr"> 53: </span>y_train = to_categorical(b_train)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#35347;&#32244;&#27161;&#31844;&#36681;&#28858; one-hot &#32232;&#30908;</span>
<span class="linenr"> 54: </span>y_test  = to_categorical(b_test)   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#28204;&#35430;&#27161;&#31844;&#36681;&#28858; one-hot &#32232;&#30908;</span>
<span class="linenr"> 55: </span>
<span class="linenr"> 56: </span><span style="color: #51afef;">from</span> tensorflow.keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr"> 57: </span><span style="color: #51afef;">from</span> tensorflow.keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr"> 58: </span>
<span class="linenr"> 59: </span>model = Sequential()
<span class="linenr"> 60: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>, input_dim=<span style="color: #da8548; font-weight: bold;">10000</span>))
<span class="linenr"> 61: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 62: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">46</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr"> 63: </span>
<span class="linenr"> 64: </span>model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr"> 65: </span>              optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr"> 66: </span>              metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr"> 67: </span>
<span class="linenr"> 68: </span>history = model.fit(x_train, y_train,
<span class="linenr"> 69: </span>                    batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27599;&#25209;&#27425; 512 &#31558;&#27171;&#26412;</span>
<span class="linenr"> 70: </span>                    epochs=<span style="color: #da8548; font-weight: bold;">10</span>,        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21482;&#35347;&#32244; 10 &#36913;&#26399;</span>
<span class="linenr"> 71: </span>                    verbose=<span style="color: #da8548; font-weight: bold;">0</span>)      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19981;&#39023;&#31034;&#35338;&#24687;</span>
<span class="linenr"> 72: </span>
<span class="linenr"> 73: </span>loss, acc = model.evaluate(x_test, y_test, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#35413;&#20272;&#35347;&#32244;&#25104;&#25928;</span>
<span class="linenr"> 74: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#35413;&#20272;&#28204;&#35430;&#36039;&#26009;&#30340;&#28310;&#30906;&#29575; ='</span>, acc)
<span class="linenr"> 75: </span>
<span class="linenr"> 76: </span><span style="color: #51afef;">from</span> tensorflow.keras.datasets <span style="color: #51afef;">import</span> reuters  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21295;&#20837; reuters &#36039;&#26009;&#38598;</span>
<span class="linenr"> 77: </span>
<span class="linenr"> 78: </span>(a_train, b_train),(a_test, b_test) = reuters.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span><span style="color: #51afef;">from</span> tensorflow.keras.preprocessing.text <span style="color: #51afef;">import</span> Tokenizer  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#21295;&#20837; Tokenizer &#39006;&#21029;</span>
<span class="linenr"> 81: </span>
<span class="linenr"> 82: </span>tok = Tokenizer(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)           <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#25351;&#23450;&#23383;&#20856;&#30340;&#32317;&#23383;&#25976;</span>
<span class="linenr"> 83: </span>x_train = tok.sequences_to_matrix(a_train) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#35347;&#32244;&#27171;&#26412;&#20570; multi-hot &#32232;&#30908;</span>
<span class="linenr"> 84: </span>x_test  = tok.sequences_to_matrix(a_test)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#23559;&#28204;&#35430;&#27171;&#26412;&#20570; multi-hot &#32232;&#30908;</span>
<span class="linenr"> 85: </span>
<span class="linenr"> 86: </span><span style="color: #51afef;">from</span> tensorflow.keras.utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr"> 87: </span>
<span class="linenr"> 88: </span>y_train = b_train  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#30452;&#25509;&#20351;&#29992;&#36617;&#20837;&#30340;&#21407;&#22987;&#27161;&#31844; (&#24050;&#28858; NumPy &#21521;&#37327;)</span>
<span class="linenr"> 89: </span>y_test  = b_test   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#30452;&#25509;&#20351;&#29992;&#36617;&#20837;&#30340;&#21407;&#22987;&#27161;&#31844; (&#24050;&#28858; NumPy &#21521;&#37327;)</span>
<span class="linenr"> 90: </span>
<span class="linenr"> 91: </span><span style="color: #51afef;">from</span> tensorflow.keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr"> 92: </span><span style="color: #51afef;">from</span> tensorflow.keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr"> 93: </span>
<span class="linenr"> 94: </span>model = Sequential()
<span class="linenr"> 95: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>, input_dim=<span style="color: #da8548; font-weight: bold;">10000</span>))
<span class="linenr"> 96: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 97: </span>model.add(Dense(<span style="color: #da8548; font-weight: bold;">46</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr"> 98: </span>
<span class="linenr"> 99: </span>model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'sparse_categorical_crossentropy'</span>,
<span class="linenr">100: </span>              optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">101: </span>              metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr">102: </span>
<span class="linenr">103: </span>history = model.fit(x_train, y_train,
<span class="linenr">104: </span>                    batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27599;&#25209;&#27425; 512 &#31558;&#27171;&#26412;</span>
<span class="linenr">105: </span>                    epochs=<span style="color: #da8548; font-weight: bold;">10</span>,        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21482;&#35347;&#32244; 10 &#36913;&#26399;</span>
<span class="linenr">106: </span>                    verbose=<span style="color: #da8548; font-weight: bold;">0</span>)      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19981;&#39023;&#31034;&#35338;&#24687;</span>
<span class="linenr">107: </span>
<span class="linenr">108: </span>loss, acc = model.evaluate(x_test, y_test, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#8592;&#35413;&#20272;&#35347;&#32244;&#25104;&#25928;</span>
<span class="linenr">109: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#35413;&#20272;&#28204;&#35430;&#36039;&#26009;&#30340;&#28310;&#30906;&#29575; ='</span>, acc)
<span class="linenr">110: </span>
<span class="linenr">111: </span>
</pre>
</div>
</div>
</li>
</ol>
</li>
</ol>
</div>
</div>

<div id="outline-container-org2c43469" class="outline-2">
<h2 id="org2c43469"><span class="section-number-2">14.</span> 文本分類:IMDB 電影評論</h2>
<div class="outline-text-2" id="text-14">
<p>
<a href="https://www.tensorflow.org/tutorials/keras/text_classification">https://www.tensorflow.org/tutorials/keras/text_classification</a><br />
中文版<br />
</p>

<p>
<a href="https://geektutu.com/post/tf2doc-ml-basic-text.html">https://geektutu.com/post/tf2doc-ml-basic-text.html</a><br />
<a href="https://blog.csdn.net/weixin_41753033/article/details/90815215">https://blog.csdn.net/weixin_41753033/article/details/90815215</a><br />
<a href="https://www.jishuwen.com/d/2O0H/zh-tw">https://www.jishuwen.com/d/2O0H/zh-tw</a><br />
</p>

<p>
資料集:網路電影資料庫(Internet Movie Database)<br />
包含 50,000 條影評文本。<br />
從該資料集切割出的 25,000 條評論用作訓練，<br />
另外 25,000 條用作測試。<br />
訓練集與測試集是*平衡的（balanced）*，意味著它們包含相等數量的積極和消極評論。<br />
</p>

<p>
此範例評論文本<br />
將影評分為*積極（positive）*或*消極（nagetive）*兩類。<br />
這是一個二元（binary）分類問題，一種重要且應用廣泛的機器學習問題。<br />
</p>

<p>
此範例使用 [tf.keras]，它是一個 Tensorflow 中用於構建和訓練模型的高級 API。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr">  2: </span><span style="color: #51afef;">from</span> tensorflow <span style="color: #51afef;">import</span> keras
<span class="linenr">  3: </span>
<span class="linenr">  4: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  5: </span><span style="color: #c678dd;">print</span>(tf.__version__)
<span class="linenr">  6: </span>
<span class="linenr">  7: </span>
<span class="linenr">  8: </span><span style="color: #dcaeea;">imdb</span> = keras.datasets.imdb
<span class="linenr">  9: </span>
<span class="linenr"> 10: </span>(train_data, train_labels), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_labels</span>) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr"> 11: </span>
<span class="linenr"> 12: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Training entries: {}, labels: {}"</span>.<span style="color: #c678dd;">format</span>(<span style="color: #c678dd;">len</span>(train_data), <span style="color: #c678dd;">len</span>(train_labels)))
<span class="linenr"> 13: </span>
<span class="linenr"> 14: </span><span style="color: #c678dd;">print</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 15: </span>
<span class="linenr"> 16: </span><span style="color: #c678dd;">len</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>]), <span style="color: #c678dd;">len</span>(train_data[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr"> 17: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19968;&#20491;&#26144;&#23556;&#21934;&#35422;&#21040;&#25972;&#25976;&#32034;&#24341;&#30340;&#35422;&#20856;</span>
<span class="linenr"> 18: </span>word_index = imdb.get_word_index()
<span class="linenr"> 19: </span>
<span class="linenr"> 20: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20445;&#30041;&#31532;&#19968;&#20491;&#32034;&#24341;</span>
<span class="linenr"> 21: </span>word_index = {k:(v+<span style="color: #da8548; font-weight: bold;">3</span>) <span style="color: #51afef;">for</span> k,v <span style="color: #51afef;">in</span> word_index.items()}
<span class="linenr"> 22: </span>word_index[<span style="color: #98be65;">"&lt;PAD&gt;"</span>] = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 23: </span>word_index[<span style="color: #98be65;">"&lt;START&gt;"</span>] = <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 24: </span>word_index[<span style="color: #98be65;">"&lt;UNK&gt;"</span>] = <span style="color: #da8548; font-weight: bold;">2</span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">unknown</span>
<span class="linenr"> 25: </span>word_index[<span style="color: #98be65;">"&lt;UNUSED&gt;"</span>] = <span style="color: #da8548; font-weight: bold;">3</span>
<span class="linenr"> 26: </span>
<span class="linenr"> 27: </span>reverse_word_index = <span style="color: #c678dd;">dict</span>([(value, key) <span style="color: #51afef;">for</span> (key, value) <span style="color: #51afef;">in</span> word_index.items()])
<span class="linenr"> 28: </span>
<span class="linenr"> 29: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">decode_review</span>(text):
<span class="linenr"> 30: </span>    <span style="color: #51afef;">return</span> <span style="color: #98be65;">' '</span>.join([reverse_word_index.get(i, <span style="color: #98be65;">'?'</span>) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> text])
<span class="linenr"> 31: </span>
<span class="linenr"> 32: </span>decode_review(train_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 33: </span>
<span class="linenr"> 34: </span>train_data = keras.preprocessing.sequence.pad_sequences(train_data,
<span class="linenr"> 35: </span>                                                        value=word_index[<span style="color: #98be65;">"&lt;PAD&gt;"</span>],
<span class="linenr"> 36: </span>                                                        padding=<span style="color: #98be65;">'post'</span>,
<span class="linenr"> 37: </span>                                                        maxlen=<span style="color: #da8548; font-weight: bold;">256</span>)
<span class="linenr"> 38: </span>
<span class="linenr"> 39: </span>test_data = keras.preprocessing.sequence.pad_sequences(test_data,
<span class="linenr"> 40: </span>                                                       value=word_index[<span style="color: #98be65;">"&lt;PAD&gt;"</span>],
<span class="linenr"> 41: </span>                                                       padding=<span style="color: #98be65;">'post'</span>,
<span class="linenr"> 42: </span>                                                       maxlen=<span style="color: #da8548; font-weight: bold;">256</span>)
<span class="linenr"> 43: </span>
<span class="linenr"> 44: </span><span style="color: #c678dd;">len</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>]), <span style="color: #c678dd;">len</span>(train_data[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr"> 45: </span>
<span class="linenr"> 46: </span><span style="color: #c678dd;">print</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 47: </span>
<span class="linenr"> 48: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27083;&#24314;&#27169;&#22411;</span>
<span class="linenr"> 49: </span>
<span class="linenr"> 50: </span>vocab_size = <span style="color: #da8548; font-weight: bold;">10000</span>
<span class="linenr"> 51: </span>
<span class="linenr"> 52: </span>model = keras.Sequential()
<span class="linenr"> 53: </span>model.add(keras.layers.Embedding(vocab_size, <span style="color: #da8548; font-weight: bold;">16</span>))
<span class="linenr"> 54: </span>model.add(keras.layers.GlobalAveragePooling1D())
<span class="linenr"> 55: </span>model.add(keras.layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 56: </span>model.add(keras.layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr"> 57: </span>
<span class="linenr"> 58: </span>model.summary()
<span class="linenr"> 59: </span>
<span class="linenr"> 60: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#37197;&#32622;&#27169;&#22411;</span>
<span class="linenr"> 61: </span>
<span class="linenr"> 62: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'adam'</span>,
<span class="linenr"> 63: </span>              loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 64: </span>              metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr"> 65: </span>
<span class="linenr"> 66: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;&#39511;&#35657;&#38598;</span>
<span class="linenr"> 67: </span>
<span class="linenr"> 68: </span>x_val = train_data[:<span style="color: #da8548; font-weight: bold;">10000</span>]
<span class="linenr"> 69: </span>partial_x_train = train_data[<span style="color: #da8548; font-weight: bold;">10000</span>:]
<span class="linenr"> 70: </span>
<span class="linenr"> 71: </span>y_val = train_labels[:<span style="color: #da8548; font-weight: bold;">10000</span>]
<span class="linenr"> 72: </span>partial_y_train = train_labels[<span style="color: #da8548; font-weight: bold;">10000</span>:]
<span class="linenr"> 73: </span>
<span class="linenr"> 74: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr"> 75: </span>
<span class="linenr"> 76: </span>history = model.fit(partial_x_train,
<span class="linenr"> 77: </span>                    partial_y_train,
<span class="linenr"> 78: </span>                    epochs=<span style="color: #da8548; font-weight: bold;">40</span>,
<span class="linenr"> 79: </span>                    batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr"> 80: </span>                    validation_data=(x_val, y_val),
<span class="linenr"> 81: </span>                    verbose=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 82: </span>
<span class="linenr"> 83: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435; &#28310;&#30906;&#29575;&#65288;accuracy&#65289;&#21644;&#25613;&#22833;&#20540;&#65288;loss&#65289;&#38568;&#26178;&#38291;&#35722;&#21270;&#30340;&#22294;&#34920;</span>
<span class="linenr"> 84: </span>
<span class="linenr"> 85: </span>history_dict = history.history
<span class="linenr"> 86: </span>history_dict.keys()
<span class="linenr"> 87: </span>
<span class="linenr"> 88: </span>
<span class="linenr"> 89: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 90: </span>
<span class="linenr"> 91: </span>acc = history_dict[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr"> 92: </span>val_acc = history_dict[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr"> 93: </span>loss = history_dict[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 94: </span>val_loss = history_dict[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 95: </span>
<span class="linenr"> 96: </span>epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 97: </span>
<span class="linenr"> 98: </span>plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr"> 99: </span>plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">100: </span>plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">101: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">102: </span>plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">103: </span>plt.legend()
<span class="linenr">104: </span>
<span class="linenr">105: </span>plt.show()
<span class="linenr">106: </span>
<span class="linenr">107: </span>plt.clf()   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28165;&#38500;&#25976;&#23383;</span>
<span class="linenr">108: </span>
<span class="linenr">109: </span>plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">110: </span>plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">111: </span>plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">112: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">113: </span>plt.ylabel(<span style="color: #98be65;">'Accuracy'</span>)
<span class="linenr">114: </span>plt.legend()
<span class="linenr">115: </span>
<span class="linenr">116: </span>plt.show()
<span class="linenr">117: </span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgcae434e" class="outline-2">
<h2 id="orgcae434e"><span class="section-number-2">15.</span> 官方範例</h2>
<div class="outline-text-2" id="text-15">
</div>
<div id="outline-container-org6699b3c" class="outline-3">
<h3 id="org6699b3c"><span class="section-number-3">15.1.</span> 文本分類 Text classification::電影評論</h3>
<div class="outline-text-3" id="text-15-1">
<p>
<a href="https://www.tensorflow.org/tutorials/keras/text_classification">https://www.tensorflow.org/tutorials/keras/text_classification</a><br />
IMDB 資料集已經打包在 Tensorflow `tfds` 中。<br />
該資料集已經經過預處理，<br />
評論（單詞序列）已經被轉換為整數序列，其中每個整數表示字典中的特定單詞。<br />
</p>

<p>
The IMDB movie reviews dataset comes packaged in `tfds`.<br />
It has already been preprocessed so that the reviews (sequences of words) have been converted to sequences of integers,<br />
where each integer represents a specific word in a dictionary.<br />
</p>

<p>
The following code downloads the IMDB dataset to your machine (or uses a cached copy if you&rsquo;ve already downloaded it):<br />
To encode your own text see the [Loading text tutorial](../load_data/text.ipynb)<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">-*- coding: utf-8 -*-</span>
<span class="linenr">  2: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29872;&#22659;&#35373;&#23450;Setup</span>
<span class="linenr">  3: </span>
<span class="linenr">  4: </span>
<span class="linenr">  5: </span><span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr">  6: </span><span style="color: #51afef;">from</span> tensorflow <span style="color: #51afef;">import</span> keras
<span class="linenr">  7: </span>
<span class="linenr">  8: </span><span style="color: #51afef;">import</span> tensorflow_datasets <span style="color: #51afef;">as</span> tfds
<span class="linenr">  9: </span>tfds.disable_progress_bar()
<span class="linenr"> 10: </span>
<span class="linenr"> 11: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 12: </span><span style="color: #c678dd;">print</span>(tf.__version__)
<span class="linenr"> 13: </span>
<span class="linenr"> 14: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Download the IMDB dataset  &#19979;&#36617; IMDB &#36039;&#26009;&#38598;</span>
<span class="linenr"> 15: </span>(train_data, test_data), <span style="color: #dcaeea;">info</span> = tfds.load(
<span class="linenr"> 16: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Use the version pre-encoded with an ~8k vocabulary.</span>
<span class="linenr"> 17: </span>    <span style="color: #98be65;">'imdb_reviews/subwords8k'</span>,
<span class="linenr"> 18: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Return the train/test datasets as a tuple.</span>
<span class="linenr"> 19: </span>    split = (tfds.Split.TRAIN, tfds.Split.TEST),
<span class="linenr"> 20: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Return (example, label) pairs from the dataset (instead of a dictionary).</span>
<span class="linenr"> 21: </span>    as_supervised=<span style="color: #a9a1e1;">True</span>,
<span class="linenr"> 22: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Also return the `info` structure.</span>
<span class="linenr"> 23: </span>    with_info=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 24: </span>
<span class="linenr"> 25: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Try the encoder</span>
<span class="linenr"> 26: </span>
<span class="linenr"> 27: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">The dataset `info` includes the text encoder (a `tfds.features.text.SubwordTextEncoder`).</span>
<span class="linenr"> 28: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">### tfds.features.text.SubwordTextEncoder</span>
<span class="linenr"> 29: </span><span style="color: #5B6268;">### </span><span style="color: #5B6268;">https://www.tensorflow.org/datasets/api_docs/python/tfds/features/text/SubwordTextEncoder</span>
<span class="linenr"> 30: </span>
<span class="linenr"> 31: </span>
<span class="linenr"> 32: </span>encoder = info.features[<span style="color: #98be65;">'text'</span>].encoder
<span class="linenr"> 33: </span>
<span class="linenr"> 34: </span><span style="color: #c678dd;">print</span> (<span style="color: #98be65;">'Vocabulary size: {}'</span>.<span style="color: #c678dd;">format</span>(encoder.vocab_size))
<span class="linenr"> 35: </span>
<span class="linenr"> 36: </span><span style="color: #98be65;">"""This text encoder will reversibly encode any string:"""</span>
<span class="linenr"> 37: </span>
<span class="linenr"> 38: </span>sample_string = <span style="color: #98be65;">'Hello TensorFlow.'</span>
<span class="linenr"> 39: </span>
<span class="linenr"> 40: </span>encoded_string = encoder.encode(sample_string)
<span class="linenr"> 41: </span><span style="color: #c678dd;">print</span> (<span style="color: #98be65;">'Encoded string is {}'</span>.<span style="color: #c678dd;">format</span>(encoded_string))
<span class="linenr"> 42: </span>
<span class="linenr"> 43: </span>original_string = encoder.decode(encoded_string)
<span class="linenr"> 44: </span><span style="color: #c678dd;">print</span> (<span style="color: #98be65;">'The original string: "{}"'</span>.<span style="color: #c678dd;">format</span>(original_string))
<span class="linenr"> 45: </span>
<span class="linenr"> 46: </span><span style="color: #51afef;">assert</span> original_string == sample_string
<span class="linenr"> 47: </span>
<span class="linenr"> 48: </span>
<span class="linenr"> 49: </span><span style="color: #5B6268;">## </span><span style="color: #5B6268;">The encoder encodes the string by breaking it into subwords or characters if the word is not in its dictionary.</span>
<span class="linenr"> 50: </span><span style="color: #5B6268;">## </span><span style="color: #5B6268;">So the more a string resembles the dataset, the shorter the encoded representation will be."""</span>
<span class="linenr"> 51: </span>
<span class="linenr"> 52: </span><span style="color: #51afef;">for</span> ts <span style="color: #51afef;">in</span> encoded_string:
<span class="linenr"> 53: </span>  <span style="color: #c678dd;">print</span> (<span style="color: #98be65;">'{} ----&gt; {}'</span>.<span style="color: #c678dd;">format</span>(ts, encoder.decode([ts])))
<span class="linenr"> 54: </span>
<span class="linenr"> 55: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Explore the data</span>
<span class="linenr"> 56: </span>
<span class="linenr"> 57: </span><span style="color: #5B6268;">## </span><span style="color: #5B6268;">The dataset comes preprocessed: each example is an array of integers representing the words of the movie review.</span>
<span class="linenr"> 58: </span><span style="color: #5B6268;">## </span><span style="color: #5B6268;">The text of reviews have been converted to integers, where each integer represents a specific word-piece in the dictionary.</span>
<span class="linenr"> 59: </span><span style="color: #5B6268;">## </span><span style="color: #5B6268;">Each label is an integer value of either 0 or 1, where 0 is a negative review, and 1 is a positive review.</span>
<span class="linenr"> 60: </span>
<span class="linenr"> 61: </span><span style="color: #51afef;">for</span> train_example, train_label <span style="color: #51afef;">in</span> train_data.take(<span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr"> 62: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Encoded text:'</span>, train_example[:<span style="color: #da8548; font-weight: bold;">10</span>].numpy())
<span class="linenr"> 63: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Label:'</span>, train_label.numpy())
<span class="linenr"> 64: </span>
<span class="linenr"> 65: </span><span style="color: #5B6268;">## </span><span style="color: #5B6268;">The `info` structure contains the encoder/decoder. The encoder can be used to recover the original text:"""</span>
<span class="linenr"> 66: </span>
<span class="linenr"> 67: </span>encoder.decode(train_example)
<span class="linenr"> 68: </span>
<span class="linenr"> 69: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Data Pre-processing</span>
<span class="linenr"> 70: </span>```
<span class="linenr"> 71: </span><span style="color: #5B6268;">## </span><span style="color: #5B6268;">Prepare the data for training</span>
<span class="linenr"> 72: </span><span style="color: #5B6268;">## </span><span style="color: #5B6268;">You will want to create batches of training data for your model.</span>
<span class="linenr"> 73: </span><span style="color: #5B6268;">## </span><span style="color: #5B6268;">The reviews are all different lengths, so use `padded_batch` to zero pad the sequences while batching:</span>
<span class="linenr"> 74: </span>
<span class="linenr"> 75: </span>BUFFER_SIZE = <span style="color: #da8548; font-weight: bold;">1000</span>
<span class="linenr"> 76: </span>
<span class="linenr"> 77: </span>train_batches = (
<span class="linenr"> 78: </span>    train_data
<span class="linenr"> 79: </span>    .shuffle(BUFFER_SIZE)
<span class="linenr"> 80: </span>    .padded_batch(<span style="color: #da8548; font-weight: bold;">32</span>))
<span class="linenr"> 81: </span>
<span class="linenr"> 82: </span>test_batches = (
<span class="linenr"> 83: </span>    test_data
<span class="linenr"> 84: </span>    .padded_batch(<span style="color: #da8548; font-weight: bold;">32</span>))
<span class="linenr"> 85: </span>
<span class="linenr"> 86: </span><span style="color: #5B6268;">## </span><span style="color: #5B6268;">Each batch will have a shape of `(batch_size, sequence_length)` because the padding is dynamic each batch will have a different length:"""</span>
<span class="linenr"> 87: </span>
<span class="linenr"> 88: </span><span style="color: #51afef;">for</span> example_batch, label_batch <span style="color: #51afef;">in</span> train_batches.take(<span style="color: #da8548; font-weight: bold;">2</span>):
<span class="linenr"> 89: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Batch shape:"</span>, example_batch.shape)
<span class="linenr"> 90: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"label shape:"</span>, label_batch.shape)
<span class="linenr"> 91: </span>
<span class="linenr"> 92: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27083;&#24314;&#27169;&#22411;Build the model</span>
<span class="linenr"> 93: </span>
<span class="linenr"> 94: </span>model = keras.Sequential([
<span class="linenr"> 95: </span>  keras.layers.Embedding(encoder.vocab_size, <span style="color: #da8548; font-weight: bold;">16</span>),
<span class="linenr"> 96: </span>  keras.layers.GlobalAveragePooling1D(),
<span class="linenr"> 97: </span>  keras.layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>)])
<span class="linenr"> 98: </span>
<span class="linenr"> 99: </span>model.summary()
<span class="linenr">100: </span>
<span class="linenr">101: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#37197;&#32622;&#27169;&#22411;</span>
<span class="linenr">102: </span>
<span class="linenr">103: </span>model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'adam'</span>,
<span class="linenr">104: </span>              loss=tf.losses.BinaryCrossentropy(from_logits=<span style="color: #a9a1e1;">True</span>),
<span class="linenr">105: </span>              metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">106: </span>
<span class="linenr">107: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;&#27169;&#22411;Train the model</span>
<span class="linenr">108: </span>
<span class="linenr">109: </span>history = model.fit(train_batches,
<span class="linenr">110: </span>                    epochs=<span style="color: #da8548; font-weight: bold;">10</span>,
<span class="linenr">111: </span>                    validation_data=test_batches,
<span class="linenr">112: </span>                    validation_steps=<span style="color: #da8548; font-weight: bold;">30</span>)
<span class="linenr">113: </span>
<span class="linenr">114: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35413;&#20272;&#27169;&#22411;Evaluate the model</span>
<span class="linenr">115: </span>
<span class="linenr">116: </span><span style="color: #dcaeea;">loss</span>, <span style="color: #dcaeea;">accuracy</span> = model.evaluate(test_batches)
<span class="linenr">117: </span>
<span class="linenr">118: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Loss: "</span>, loss)
<span class="linenr">119: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Accuracy: "</span>, accuracy)
<span class="linenr">120: </span>
<span class="linenr">121: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435; &#28310;&#30906;&#29575;&#65288;accuracy&#65289;&#21644;&#25613;&#22833;&#20540;&#65288;loss&#65289;&#38568;&#26178;&#38291;&#35722;&#21270;&#30340;&#22294;&#34920;</span>
<span class="linenr">122: </span>
<span class="linenr">123: </span>history_dict = history.history
<span class="linenr">124: </span>history_dict.keys()
<span class="linenr">125: </span>
<span class="linenr">126: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">127: </span>
<span class="linenr">128: </span>acc = history_dict[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr">129: </span>val_acc = history_dict[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr">130: </span>loss = history_dict[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">131: </span>val_loss = history_dict[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">132: </span>
<span class="linenr">133: </span>epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">134: </span>
<span class="linenr">135: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">"bo" is for "blue dot"</span>
<span class="linenr">136: </span>plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">137: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">b is for "solid blue line"</span>
<span class="linenr">138: </span>plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">139: </span>plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">140: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">141: </span>plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">142: </span>plt.legend()
<span class="linenr">143: </span>
<span class="linenr">144: </span>plt.show()
<span class="linenr">145: </span>
<span class="linenr">146: </span>plt.clf()   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">clear figure</span>
<span class="linenr">147: </span>
<span class="linenr">148: </span>plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">149: </span>plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">150: </span>plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">151: </span>plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">152: </span>plt.ylabel(<span style="color: #98be65;">'Accuracy'</span>)
<span class="linenr">153: </span>plt.legend(loc=<span style="color: #98be65;">'lower right'</span>)
<span class="linenr">154: </span>
<span class="linenr">155: </span>plt.show()
<span class="linenr">156: </span>
<span class="linenr">157: </span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org8b3bcc6" class="outline-2">
<h2 id="org8b3bcc6"><span class="section-number-2">16.</span> 練習</h2>
<div class="outline-text-2" id="text-16">
</div>
<div id="outline-container-org20b1a65" class="outline-3">
<h3 id="org20b1a65"><span class="section-number-3">16.1.</span> SPAM email detection 垃圾郵件偵測</h3>
<div class="outline-text-3" id="text-16-1">
<ul class="org-ul">
<li>Linear &amp; Tree-based models for Email Sentiment Classification<br /></li>
<li><a href="https://www.kaggle.com/azunre/tl-for-nlp-section2-1-2-4-emails">https://www.kaggle.com/azunre/tl-for-nlp-section2-1-2-4-emails</a><br /></li>
</ul>
</div>
</div>
<div id="outline-container-org20c263d" class="outline-3">
<h3 id="org20c263d"><span class="section-number-3">16.2.</span> fake News detection 假新聞偵測</h3>
<div class="outline-text-3" id="text-16-2">
<ul class="org-ul">
<li>Using ELMo for &ldquo;fake news&rdquo; detection/classification<br /></li>
<li><a href="https://www.kaggle.com/azunre/tl-for-nlp-sections4-2-4-4-fake-news-elmo">https://www.kaggle.com/azunre/tl-for-nlp-sections4-2-4-4-fake-news-elmo</a><br /></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org1442f78" class="outline-2">
<h2 id="org1442f78"><span class="section-number-2">17.</span> 教學資源</h2>
<div class="outline-text-2" id="text-17">
<ul class="org-ul">
<li>[超讚]<a href="https://leemeng.tw/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html">進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南</a><br /></li>
<li><a href="https://aiacademy.tw/what-is-nlp-natural-language-processing/">斷開中文的鎖鍊！自然語言處理 (NLP)是什麼？</a><br /></li>
<li><a href="https://www.twblogs.net/a/5eb8fcf386ec4d39f335f108">Python 結巴分詞——自然語言處理之中文分詞器</a><br /></li>
</ul>
</div>
<div id="outline-container-org1f3c942" class="outline-3">
<h3 id="org1f3c942"><span class="section-number-3">17.1.</span> paper</h3>
<div class="outline-text-3" id="text-17-1">
</div>
<ol class="org-ol">
<li><a id="org2b0120b"></a>Deep Learning and NLP<br />
<div class="outline-text-4" id="text-17-1-1">
<p>
Deep Learning for Source Code Modeling and Generation: Models, Applications and Challenges<br />
Triet H. M. Le, Hao Chen, M. Ali Babar<br />
<a href="https://arxiv.org/abs/2002.05442">https://arxiv.org/abs/2002.05442</a><br />
</p>
</div>
</li>
</ol>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.cnblogs.com/iloveai/p/cs224d-lecture1-note.html">NLP簡介</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.twblogs.net/a/5f047c6c9644181341a1ea76">自然語言處理總概括</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.gushiciku.cn/pl/pUAS/zh-hk">NLP中的Mask全解</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="http://puremonkey2010.blogspot.com/2017/09/toolkit-keras-imdb.html">Keras - IMDb 網路電影資料集 </a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://aiacademy.tw/what-is-nlp-natural-language-processing/">斷開中文的鎖鍊！自然語言處理 (NLP)是什麼？</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
進入 NLP 世界的最佳橋樑：寫給所有人的自然語言處理與深度學習入門指南]]<br />
</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7" role="doc-backlink">7</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.ithome.com.tw/news/132838">千呼萬喚十多年！中研院終於開源釋出國產自動化中文斷詞工具，正式採用GPL 3.0釋出</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.8" class="footnum" href="#fnr.8" role="doc-backlink">8</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://blog.kennycoder.io/2020/02/12/Python-%E7%9F%A5%E5%90%8DJieba%E4%B8%AD%E6%96%87%E6%96%B7%E8%A9%9E%E5%B7%A5%E5%85%B7%E6%95%99%E5%AD%B8/">Python - 知名 Jieba 中文斷詞工具教學 </a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.9" class="footnum" href="#fnr.9" role="doc-backlink">9</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://medium.com/nlp-tsupei/%E5%BE%9E%E9%9B%B6%E9%96%8B%E5%A7%8B%E5%AD%B8%E7%BF%92%E8%87%AA%E7%84%B6%E8%AA%9E%E8%A8%80%E8%99%95%E7%90%86-a729bccecda">中文自然語言處理整理</a><br />
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2022-07-04 Mon 15:21</p>
</div>
</body>
</html>
