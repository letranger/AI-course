#+title: AI應用實例
#+TAGS: AI
#+OPTIONS: toc:2 ^:nil num:5
#+PROPERTY: header-args :eval never-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/white.css" />
#+EXCLUDE_TAGS: noexport
#+latex:\newpage

* AI v.s. Security

** 釣魚網站偵測實戰

[[https://github.com/PacktPublishing/Mastering-Machine-Learning-for-Penetration-Testing/tree/master/Chapter02][github data]]

*** 資料集
UCI Machine Learning Repository (Phishing Websites Data Set).
https://archive.ics.uci.edu/ml/datasets/Phishing+Websites

The dataset is provided as an arff file

處理過的資料集:
{30 Attributes (having_IP_Address URL_Length, abnormal_URL and so on)}+ {1 Attribute (Result)}

-1,1,1,1,-1,-1,-1,-1,-1,1,1,-1,1,-1,1,-1,-1,-1,0,1,1,1,1,-1,-1,-1,-1,1,1,-1,-1

真正要能上線跑的演算法不多，因為會面臨資料量太大(流量)的問題，會導致記憶體不足....

*** papers

- Mohammad, Rami, McCluskey, T.L. and Thabtah, Fadi (2012). An Assessment of Features Related to Phishing Websites using an Automated Technique. In: International Conferece For Internet Technology And Secured Transactions. ICITST 2012 . IEEE, London, UK, pp. 492-497. ISBN 978-1-4673-5325-0
- Mohammad, Rami, Thabtah, Fadi Abdeljaber and McCluskey, T.L. (2014). Predicting phishing websites based on self-structuring neural network. Neural Computing and Applications, 25 (2). pp. 443-458. ISSN 0941-0643
- Mohammad, Rami, McCluskey, T.L. and Thabtah, Fadi Abdeljaber (2014). Intelligent Rule based Phishing Websites Classification. IET Information Security, 8 (3). pp. 153-160. ISSN 1751-8709

*** 使用 LogisticRegression

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import numpy as np
  from sklearn import *
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import accuracy_score

  training_data = np.genfromtxt('dataset.csv', delimiter=',', dtype=np.int32)
  inputs = training_data[:,:-1]
  outputs = training_data[:, -1]

  training_inputs = inputs[:2000]
  training_outputs = outputs[:2000]
  testing_inputs = inputs[2000:]
  testing_outputs = outputs[2000:]

  classifier = LogisticRegression()
  classifier.fit(training_inputs, training_outputs)
  predictions = classifier.predict(testing_inputs)
  accuracy = 100.0 * accuracy_score(testing_outputs, predictions)
  print ("The accuracy of your Logistic Regression on testing data is: " + str(accuracy))
#+END_SRC

*** 使用 DecisionTreeClassifier

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from sklearn import tree
  from sklearn.metrics import accuracy_score
  import numpy as np

  training_data = np.genfromtxt('dataset.csv', delimiter=',', dtype=np.int32)
  inputs = training_data[:,:-1]
  outputs = training_data[:, -1]

  training_inputs = inputs[:2000]
  training_outputs = outputs[:2000]
  testing_inputs = inputs[2000:]
  testing_outputs = outputs[2000:]

  classifier = tree.DecisionTreeClassifier()
  classifier.fit(training_inputs, training_outputs)
  predictions = classifier.predict(testing_inputs)
  accuracy = 100.0 * accuracy_score(testing_outputs, predictions)
  print ("The accuracy of your decision tree on testing data is: " + str(accuracy))
#+END_SRC

** Text Classification

[[https://github.com/MyDearGreatTeacher/TensorSecurity/blob/master/code/AI_security/3_TextClassification%E8%88%87%E5%9E%83%E5%9C%BE%E7%9F%AD%E4%BF%A1%E9%A0%90%E6%B8%AC.md][Text classification github]]

二元分類: binary classification

spam detection[email, SMS]

*** papers

- MS Spam Collection Dataset, Collection of SMS messages tagged as spam or legitimate, https://www.kaggle.com/uciml/sms-spam-collection-dataset/data
- The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.
- 2009 年博士論文, A CORPUS LINGUISTICS STUDY OF SMS TEXT MESSAGING, CAROLINE TAGG, https://etheses.bham.ac.uk/id/eprint/253/1/Tagg09PhD.pdf

*** 資料集

https://github.com/CorkyMaigre/sms-spam-ml/blob/master/dataset/SMSSpamCollection

#+BEGIN_SRC sh
  ham	Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...
  ham	Ok lar... Joking wif u oni...
  spam	Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's
  ham	U dun say so early hor... U c already then say...
  ham	Nah I don't think he goes to usf, he lives around here though
  spam	FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv
  ham	Even my brother is not like to speak with me. They treat me like aids patent.
  ham	As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune
  spam	WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.
  spam	Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030
  ham	I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.
  spam	SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info
  spam	URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18
  ham	I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.
  ham	I HAVE A DATE ON SUNDAY WITH WILL!!
  spam	XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL
  ham	Oh k...i'm watching here:)
  ham	Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.
  ham	Fine if that�s the way u feel. That�s the way its gota b
  spam	England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+
  ham	Is that seriously how you spell his name?
  ham	I‘m going to try for 2 months ha ha only joking
  ham	So ü pay first lar... Then when is da stock comin...
  ham	Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?
  ham	Ffffffffff. Alright no way I can meet up with you sooner?
  ham	Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol
  ham	Lol your always so convincing.
  ham	Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?
  ham	I'm back &amp; we're packing the car now, I'll let you know if there's room
  ham	Ahhh. Work. I vaguely remember that! What does it feel like? Lol
  ham	Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us
  ham	Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You?

#+END_SRC

*** 使用 LogisticRegression

- Hands-on-Machine-Learning-for-Cyber-Security/Chapter05/sms_spam.py /
- https://github.com/PacktPublishing/Hands-on-Machine-Learning-for-Cyber-Security/blob/master/Chapter05/sms_spam.py

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import pandas as pd
  import numpy as np
  from sklearn.feature_extraction.text import TfidfVectorizer
  from sklearn.linear_model.logistic import LogisticRegression
  from sklearn.model_selection import train_test_split, cross_val_score

  dataframe = pd.read_csv('SMSSpamCollectionDataSet', delimiter='\t',header=None)

  X_train_dataset, X_test_dataset, y_train_dataset, y_test_dataset = train_test_split(dataframe[1],dataframe[0])

  vectorizer = TfidfVectorizer()
  X_train_dataset = vectorizer.fit_transform(X_train_dataset)

  classifier_log = LogisticRegression()
  classifier_log.fit(X_train_dataset, y_train_dataset)

  X_test_dataset = vectorizer.transform( ['URGENT! Your Mobile No 1234 was awarded a Prize', 'Hey honey, whats up?'] )

  predictions_logistic = classifier.predict(X_test_dataset)
  print(predictions)
#+END_SRC

*** TensorFlow_RNN for 垃圾短信預測

TensorFlow 機器學習實戰指南 (美)尼克‧麥克盧爾
 9.2 用 TensorFlow 實現 RNN 模型進行垃圾短信預測
 https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition


#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import os
  import re
  import io
  import requests
  import numpy as np
  import matplotlib.pyplot as plt
  import tensorflow as tf
  from zipfile import ZipFile
  from tensorflow.python.framework import ops
  ops.reset_default_graph()

  # Start a graph
  sess = tf.Session()

  # Set RNN parameters
  epochs = 20
  batch_size = 250
  max_sequence_length = 25
  rnn_size = 10
  embedding_size = 50
  min_word_frequency = 10
  learning_rate = 0.0005
  dropout_keep_prob = tf.placeholder(tf.float32)


  # Download or open data
  data_dir = 'temp'
  data_file = 'text_data.txt'
  if not os.path.exists(data_dir):
      os.makedirs(data_dir)

  if not os.path.isfile(os.path.join(data_dir, data_file)):
      zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'
      r = requests.get(zip_url)
      z = ZipFile(io.BytesIO(r.content))
      file = z.read('SMSSpamCollection')
      # Format Data
      text_data = file.decode()
      text_data = text_data.encode('ascii', errors='ignore')
      text_data = text_data.decode().split('\n')

      # Save data to text file
      with open(os.path.join(data_dir, data_file), 'w') as file_conn:
          for text in text_data:
              file_conn.write("{}\n".format(text))
  else:
      # Open data from text file
      text_data = []
      with open(os.path.join(data_dir, data_file), 'r') as file_conn:
          for row in file_conn:
              text_data.append(row)
      text_data = text_data[:-1]

  text_data = [x.split('\t') for x in text_data if len(x) >= 1]
  [text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]


  # Create a text cleaning function
  def clean_text(text_string):
      text_string = re.sub(r'([^\s\w]|_|[0-9])+', '', text_string)
      text_string = " ".join(text_string.split())
      text_string = text_string.lower()
      return text_string


  # Clean texts
  text_data_train = [clean_text(x) for x in text_data_train]

  # Change texts into numeric vectors
  vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,
                                                                       min_frequency=min_word_frequency)
  text_processed = np.array(list(vocab_processor.fit_transform(text_data_train)))

  # Shuffle and split data
  text_processed = np.array(text_processed)
  text_data_target = np.array([1 if x == 'ham' else 0 for x in text_data_target])
  shuffled_ix = np.random.permutation(np.arange(len(text_data_target)))
  x_shuffled = text_processed[shuffled_ix]
  y_shuffled = text_data_target[shuffled_ix]

  # Split train/test set
  ix_cutoff = int(len(y_shuffled)*0.80)
  x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]
  y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]
  vocab_size = len(vocab_processor.vocabulary_)
  print("Vocabulary Size: {:d}".format(vocab_size))
  print("80-20 Train Test split: {:d} -- {:d}".format(len(y_train), len(y_test)))

  # Create placeholders
  x_data = tf.placeholder(tf.int32, [None, max_sequence_length])
  y_output = tf.placeholder(tf.int32, [None])

  # Create embedding
  embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))
  embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data)

  # Define the RNN cell
  # tensorflow change >= 1.0, rnn is put into tensorflow.contrib directory. Prior version not test.
  if tf.__version__[0] >= '1':
      cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)
  else:
      cell = tf.nn.rnn_cell.BasicRNNCell(num_units=rnn_size)

  output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)
  output = tf.nn.dropout(output, dropout_keep_prob)

  # Get output of RNN sequence
  output = tf.transpose(output, [1, 0, 2])
  last = tf.gather(output, int(output.get_shape()[0]) - 1)

  weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))
  bias = tf.Variable(tf.constant(0.1, shape=[2]))
  logits_out = tf.matmul(last, weight) + bias

  # Loss function
  losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output)
  loss = tf.reduce_mean(losses)

  accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, 1), tf.cast(y_output, tf.int64)), tf.float32))

  optimizer = tf.train.RMSPropOptimizer(learning_rate)
  train_step = optimizer.minimize(loss)

  init = tf.global_variables_initializer()
  sess.run(init)

  train_loss = []
  test_loss = []
  train_accuracy = []
  test_accuracy = []
  # Start training
  for epoch in range(epochs):

      # Shuffle training data
      shuffled_ix = np.random.permutation(np.arange(len(x_train)))
      x_train = x_train[shuffled_ix]
      y_train = y_train[shuffled_ix]
      num_batches = int(len(x_train)/batch_size) + 1
      # TO DO CALCULATE GENERATIONS ExACTLY
      for i in range(num_batches):
          # Select train data
          min_ix = i * batch_size
          max_ix = np.min([len(x_train), ((i+1) * batch_size)])
          x_train_batch = x_train[min_ix:max_ix]
          y_train_batch = y_train[min_ix:max_ix]

          # Run train step
          train_dict = {x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob:0.5}
          sess.run(train_step, feed_dict=train_dict)

      # Run loss and accuracy for training
      temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)
      train_loss.append(temp_train_loss)
      train_accuracy.append(temp_train_acc)

      # Run Eval Step
      test_dict = {x_data: x_test, y_output: y_test, dropout_keep_prob:1.0}
      temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)
      test_loss.append(temp_test_loss)
      test_accuracy.append(temp_test_acc)
      print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}'.format(epoch+1, temp_test_loss, temp_test_acc))

  # Plot loss over time
  epoch_seq = np.arange(1, epochs+1)
  plt.plot(epoch_seq, train_loss, 'k--', label='Train Set')
  plt.plot(epoch_seq, test_loss, 'r-', label='Test Set')
  plt.title('Softmax Loss')
  plt.xlabel('Epochs')
  plt.ylabel('Softmax Loss')
  plt.legend(loc='upper left')
  plt.show()

  # Plot accuracy over time
  plt.plot(epoch_seq, train_accuracy, 'k--', label='Train Set')
  plt.plot(epoch_seq, test_accuracy, 'r-', label='Test Set')
  plt.title('Test Accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend(loc='upper left')
  plt.show()
#+END_SRC

** AI and Botnet Detection

[[https://github.com/MyDearGreatTeacher/TensorSecurity/tree/master/code/AI_security/%E7%99%BC%E5%B1%95%E8%B6%A8%E5%8B%A2/Botnet][Botnet github]]
IOT honey pot

*** 案例分析

Hands-On Artificial Intelligence for Cybersecurity
Alessandro Parisi

*** 資料集

https://github.com/MyDearGreatTeacher/AI201909/blob/master/data/network-logs.csv

!wget https://raw.githubusercontent.com/MyDearGreatTeacher/AI201909/master/data/network-logs.csv



#+BEGIN_SRC csv
  REMOTE_PORT	LATENCY	THROUGHPUT	ANOMALY
  21	15.94287532	16.20299807	0
  20	12.66645095	15.89908374	1
  80	13.89454962	12.95800822	0
  21	13.62081292	15.45947525	0
  21	15.70548485	15.33956527	0
  23	15.59318973	15.61238106	0
  21	15.48906755	15.64087368	0
  80	15.52704801	15.63568031	0
  21	14.07506707	15.76531533	0
  ......
#+END_SRC

#+BEGIN_SRC csv
  延遲（Latency）：一個封包從來源端送出後，到目的端接收到這個封包，中間所花的時間。
  頻寬（Bandwidth）：傳輸媒介的最大吞吐量（throughput）。

  https://blog.gtwang.org/web-development/network-lantency-and-bandwidth/
#+END_SRC

*** 基本統計分析
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  !wget https://raw.githubusercontent.com/MyDearGreatTeacher/AI201909/master/data/network-logs.csv

  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  %matplotlib inline

  dataset = pd.read_csv('network-logs.csv')
  hist_dist = dataset[['LATENCY', 'THROUGHPUT']].hist(grid=False, figsize=(10,4))

  data = dataset[['LATENCY', 'THROUGHPUT']].values

  plt.scatter(data[:, 0], data[:, 1], alpha=0.6)
  plt.xlabel('LATENCY')
  plt.ylabel('THROUGHPUT')
  plt.title('DATA FLOW')
  plt.show()
#+END_SRC

*** 機器學習
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import numpy as np
  import pandas as pd

  from sklearn.linear_model import *
  from sklearn.tree import *
  from sklearn.naive_bayes import *
  from sklearn.neighbors import *
  from sklearn.metrics import accuracy_score

  from sklearn.model_selection import train_test_split

  import matplotlib.pyplot as plt
  %matplotlib inline

  # Load the data.
  dataset = pd.read_csv('network-logs.csv')


  samples = dataset.iloc[:, [1, 2]].values #只取第1、2欄的資料當features
  targets = dataset['ANOMALY'].values

  training_samples, testing_samples, training_targets, testing_targets = train_test_split(
           samples, targets, test_size=0.3, random_state=0)

#+END_SRC

接下來就可以套用各種分類演算法

**** 使用 k-Nearest Neighbors model

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  knc = KNeighborsClassifier(n_neighbors=2)
  knc.fit(training_samples,training_targets)
  knc_prediction = knc.predict(testing_samples)
  knc_accuracy = 100.0 * accuracy_score(testing_targets, knc_prediction)
  print ("K-Nearest Neighbours accuracy: " + str(knc_accuracy))


#+END_SRC

**** 使用 Decision tree model

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  dtc = DecisionTreeClassifier(random_state=0)
  dtc.fit(training_samples,training_targets)
  dtc_prediction = dtc.predict(testing_samples)
  dtc_accuracy = 100.0 * accuracy_score(testing_targets, dtc_prediction)
  print ("Decision Tree accuracy: " + str(dtc_accuracy))
#+END_SRC

**** 使用 Gaussian Naive Bayes model

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  gnb = GaussianNB()
  gnb.fit(training_samples,training_targets)
  gnb_prediction = gnb.predict(testing_samples)
  gnb_accuracy = 100.0 * accuracy_score(testing_targets, gnb_prediction)
  print ("Gaussian Naive Bayes accuracy: " + str(gnb_accuracy))

#+END_SRC

*** 結果

- K-Nearest Neighbours accuracy: 95.90163934426229
- Decision Tree accuracy: 96.72131147540983
- Gaussian Naive Bayes accuracy: 98.36065573770492



#+BEGIN_SRC python -r -n :results output :exports both :eval no
#+END_SRC

#+BEGIN_SRC python -r -n :results output :exports both :eval no
#+END_SRC

#+BEGIN_SRC python -r -n :results output :exports both :eval no
#+END_SRC

#+BEGIN_SRC python -r -n :results output :exports both :eval no
#+END_SRC

#+BEGIN_SRC python -r -n :results output :exports both :eval no
#+END_SRC

#+BEGIN_SRC python -r -n :results output :exports both :eval no
#+END_SRC

#+BEGIN_SRC python -r -n :results output :exports both :eval no
#+END_SRC

#+latex:\newpage
