<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-06-27 Mon 11:16 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>非監督式學習</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/white.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">非監督式學習</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orge753a05">1. 非監督式學習</a>
<ul>
<li><a href="#org7c39d19">1.1. 目的</a></li>
<li><a href="#org031b0fb">1.2. 非監督式學習的任務</a></li>
</ul>
</li>
<li><a href="#org60cde3d">2. 非監督式學習的常見演算法</a>
<ul>
<li><a href="#org4e2eb66">2.1. 降維</a></li>
<li><a href="#orgaad941e">2.2. 分群</a></li>
<li><a href="#org87b9d42">2.3. 異常檢測與新穎檢測</a></li>
<li><a href="#orgd9a76b6">2.4. 視覺化與降維</a></li>
<li><a href="#org7b4eb3e">2.5. 關聯規則學習</a></li>
</ul>
</li>
<li><a href="#NS-KM-Image">3. K-Means</a>
<ul>
<li><a href="#org74ce26f">3.1. 簡介</a></li>
<li><a href="#NS-KM-Image">3.2. 應用: 壓縮影像</a></li>
</ul>
</li>
<li><a href="#NS-Hie-cluster">4. Hierarchical clustering</a>
<ul>
<li><a href="#org0200bb6">4.1. 聚合式階層分群法 (Bottom-up, agglomerative)</a></li>
<li><a href="#orgc3f3fcf">4.2. 聚合式分群作業</a></li>
<li><a href="#org3c62906">4.3. 分裂式階層分群法 (Top-down, divisible) :</a></li>
</ul>
</li>
<li><a href="#org7ad3855">5. 信用卡詐欺偵測</a>
<ul>
<li><a href="#orgcfbc26f">5.1. 資料取得</a></li>
<li><a href="#org7bd6bed">5.2. 資料準備</a></li>
<li><a href="#orga87da2f">5.3. 模型準備</a></li>
<li><a href="#orgb8e0dc2">5.4. 模型訓練-1</a></li>
<li><a href="#org6473ec3">5.5. 評估指標</a></li>
<li><a href="#orgcb2ad05">5.6. 模型訓練#2</a></li>
<li><a href="#org0e4cbe0">5.7. Stack</a></li>
</ul>
</li>
<li><a href="#org98ea72b">6. 降維</a>
<ul>
<li><a href="#org0118d3d">6.1. 讀入資料</a></li>
<li><a href="#orgeeb3a12">6.2. 線性投影</a></li>
<li><a href="#orgfe85077">6.3. 主成分分析</a></li>
<li><a href="#org5090417">6.4. 奇異值分解</a></li>
<li><a href="#orgd96c9a1">6.5. 隨機投影</a></li>
<li><a href="#orge218aff">6.6. Isomap</a></li>
<li><a href="#orgf4bf3c5">6.7. 多維標度(Multidimensional Scaling)</a></li>
<li><a href="#org284fa6a">6.8. 局部線性嵌入法(Locally Linear Embedding)</a></li>
<li><a href="#orgbcc024e">6.9. t-Distributed Stochastic Neighbor Embedding</a></li>
<li><a href="#org0217ac3">6.10. 字典學習</a></li>
<li><a href="#org4e83c17">6.11. 獨立成份分析</a></li>
</ul>
</li>
<li><a href="#orgdae28b2">7. 異常偵測</a>
<ul>
<li><a href="#org90c3d62">7.1. 準備資料</a></li>
<li><a href="#org68169b7">7.2. 定義異常評分函數</a></li>
<li><a href="#org5bf591d">7.3. 評估指標：畫圖</a></li>
<li><a href="#orgf7fa99e">7.4. PCA異常偵測</a></li>
<li><a href="#org8146501">7.5. Sparse PCA異常偵測</a></li>
<li><a href="#org136e498">7.6. Kernel PCA異常偵測</a></li>
<li><a href="#org56a1deb">7.7. 高斯隨機投影異常偵測</a></li>
<li><a href="#org4bb93d3">7.8. 稀疏隨機投影異常偵測</a></li>
<li><a href="#orgadc6f96">7.9. 字典學習異常偵測</a></li>
<li><a href="#org822f120">7.10. ICA異常偵測</a></li>
<li><a href="#orgb5ef812">7.11. 結論</a></li>
</ul>
</li>
<li><a href="#org248186a">8. 分群</a>
<ul>
<li><a href="#org1f2b4d9">8.1. K-Means</a></li>
<li><a href="#orga2cab67">8.2. Hierarchical Clustering</a></li>
<li><a href="#org4dc877d">8.3. DBSCAN</a></li>
</ul>
</li>
<li><a href="#orgd93d89a">9. 群組區隔</a></li>
</ul>
</div>
</div>

<div id="outline-container-orge753a05" class="outline-2">
<h2 id="orge753a05"><span class="section-number-2">1.</span> 非監督式學習</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org7c39d19" class="outline-3">
<h3 id="org7c39d19"><span class="section-number-3">1.1.</span> 目的</h3>
<div class="outline-text-3" id="text-1-1">
<p>
非監督式學習演算法嚐試學習 <b>資料的基礎結構</b> ，而非 <b>做出預測</b> 。
</p>
<ul class="org-ul">
<li>將網站訪客進行分類: 40%為男性、愛看漫畫、通常晚上造訪網站&#x2026;.</li>
<li><p>
將一堆圖片分類：cat、automobile、truck、frog、ship&#x2026;
</p>

<div id="org08f1be4" class="figure">
<p><img src="images/2022-04-30_10-57-36.jpg" alt="2022-04-30_10-57-36.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 1: </span>Caption</p>
</div></li>
</ul>
</div>
</div>

<div id="outline-container-org031b0fb" class="outline-3">
<h3 id="org031b0fb"><span class="section-number-3">1.2.</span> 非監督式學習的任務</h3>
<div class="outline-text-3" id="text-1-2">
<p>
如何協助儘量保留非監督式學習資料的重要特徵以利將來進行有效辨識<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>：
</p>
<ul class="org-ul">
<li>降維(dimensionality reduction): 目的是在不損失過多資訊的前提下簡化資料，將多個特徵合併成一個。例如，汽車的里程數與車齡就有合併的依據。</li>
<li>異常檢測(Anamaly Detection): 例如，找出不尋常的信用卡交易以防止詐騙、找出製程中有缺陷的產品、將資料組中的離群值挑出來再傳給另一個演算法</li>
<li><p>
新穎檢測(Novelty Detection): 檢測與訓練組中所有實例(instance)看起來不一樣的新實例。前提是訓練組的數據非常乾淨（clean）。例如，有1000張狗的照片，裡面只有10張吉娃娃，則Novelty就 <b>不應該</b> 把這10張標為novelty，而是找出裡面不小心誤放的貓的照片。
</p>

<div id="org30464d2" class="figure">
<p><img src="images/2022-04-30_11-35-44.jpg" alt="2022-04-30_11-35-44.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 2: </span>Caption</p>
</div></li>
<li>關聯規則學習(association rule learning): 超市中售出貨物間的關連，可以將常常一起買的物品擺近一點</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org60cde3d" class="outline-2">
<h2 id="org60cde3d"><span class="section-number-2">2.</span> 非監督式學習的常見演算法</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org4e2eb66" class="outline-3">
<h3 id="org4e2eb66"><span class="section-number-3">2.1.</span> 降維</h3>
<div class="outline-text-3" id="text-2-1">
<p>
There are two major branches of dimensionality reduction. The first is known as linear projection, which involves linearly projecting data from a high- dimensional space to a low-dimensional space. This includes techniques such as<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>:
</p>
<ul class="org-ul">
<li>principal component analysis</li>
<li>singular value decomposition</li>
<li>random projection.</li>
</ul>

<p>
The second is known as manifold learning, which is also referred to as nonlinear dimensionality reduction. This involves techniques such as<sup><a id="fnr.2.100" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>:
</p>
<ul class="org-ul">
<li>isomap: which learns the curved distance (also called the geodesic distance) between points rather than the Euclidean distance</li>
<li>multidimensional scaling (MDS)</li>
<li>locally linear embedding (LLE)</li>
<li>t-distributed stochastic neighbor embedding (t-SNE)</li>
<li>dictionary learning</li>
<li>random trees embedding</li>
<li>independent component analysis</li>
</ul>

<p>
維度縮減演算法(dimensionality reduction algorithm)將原始的高維度輸入資料映射到一個低維度空間，同時過濾掉與整體資料不那麼相關的特徵，並儘可能保留資料中令人感興趣的特徵。主要有兩個分支：線性投影、非線性投影:
</p>
</div>
<div id="outline-container-orge8e555b" class="outline-4">
<h4 id="orge8e555b"><span class="section-number-4">2.1.1.</span> 線性投影</h4>
<div class="outline-text-4" id="text-2-1-1">
</div>
<ol class="org-ol">
<li><a id="orge7e6a3d"></a>主成份分析(Principal component analysis, PCA)<br />
<div class="outline-text-5" id="text-2-1-1-1">
<p>
PCA有數種變形：mini-batch變形式PCA(incremental PCA)、非線性變形(kernel PCA)、稀疏變形(sparse PCA)
</p>
</div>
</li>
<li><a id="org0ca5c76"></a>奇異值分解(Singular value decomposition, SVD)<br />
<div class="outline-text-5" id="text-2-1-1-2">
<p>
降低原來特徵所組成的矩陣的秩（rank)，使得原來的矩陣可以使用擁有較小的秩的矩陣所組成的線性組合來表示。
</p>
</div>
</li>
<li><a id="orgd384cd4"></a>隨機投影(Random projection)<br />
<div class="outline-text-5" id="text-2-1-1-3">
<p>
由高維投影至低維空間，但同時保留點與點間的矩離，可以使用隨機高斯矩陣（random Gaussian matrix)或隨機稀疏矩陣(random sparse matrix)來實現。
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-org2a7f3dc" class="outline-4">
<h4 id="org2a7f3dc"><span class="section-number-4">2.1.2.</span> 流形學習(Manifold learning)</h4>
<div class="outline-text-4" id="text-2-1-2">
</div>
<ol class="org-ol">
<li><a id="org209eb0b"></a>Isomap<br />
<div class="outline-text-5" id="text-2-1-2-1">
<p>
透過估算點與粌近點的捷線(geodesic)或曲線距離(curved distance)，而非使用歐式距離(Euclidean distance)來學習資料流形的內蘊幾何。
</p>
</div>
</li>
<li><a id="org2a40cab"></a>t-distributed stochastic neighbor embedding(t-SNE)<br />
<div class="outline-text-5" id="text-2-1-2-2">
<p>
將高維度空間的資料嵌入至二維或三維的空間
</p>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-orgaad941e" class="outline-3">
<h3 id="orgaad941e"><span class="section-number-3">2.2.</span> 分群</h3>
<div class="outline-text-3" id="text-2-2">
<ul class="org-ul">
<li>K-Means</li>
<li>DBSCAN</li>
<li>階層式分群分析(Hierarchical Cluster Analysis, HCA)</li>
</ul>
</div>
</div>
<div id="outline-container-org87b9d42" class="outline-3">
<h3 id="org87b9d42"><span class="section-number-3">2.3.</span> 異常檢測與新穎檢測</h3>
<div class="outline-text-3" id="text-2-3">
<ul class="org-ul">
<li>One-class SVM</li>
<li>孤立森林(Isolation Forest)</li>
</ul>
</div>
</div>
<div id="outline-container-orgd9a76b6" class="outline-3">
<h3 id="orgd9a76b6"><span class="section-number-3">2.4.</span> 視覺化與降維</h3>
<div class="outline-text-3" id="text-2-4">
<ul class="org-ul">
<li>主成份分析(Principal Component Analysis, PCA)</li>
<li>kernel PCA</li>
<li>局部線性嵌入法(Locally Linear Embedding, LLE)</li>
<li>t-隨機隣近嵌入法(t-Distributed Stochastic Neighbor Emgedding, t-SNE)</li>
</ul>
</div>
</div>
<div id="outline-container-org7b4eb3e" class="outline-3">
<h3 id="org7b4eb3e"><span class="section-number-3">2.5.</span> 關聯規則學習</h3>
<div class="outline-text-3" id="text-2-5">
<ul class="org-ul">
<li>先驗</li>
<li>Eclat</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-NS-KM-Image" class="outline-2">
<h2 id="NS-KM-Image"><span class="section-number-2">3.</span> K-Means</h2>
<div class="outline-text-2" id="text-NS-KM-Image">
</div>
<div id="outline-container-org74ce26f" class="outline-3">
<h3 id="org74ce26f"><span class="section-number-3">3.1.</span> 簡介</h3>
<div class="outline-text-3" id="text-3-1">
<p>
從資料中找出 K 個分類的非監督式機器學習演算法
</p>
<div class="org-src-container">
<pre class="src src-python">sklearn.datasets.samples_generator <span style="color: #51afef;">import</span> make_blobs
<span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y_true</span> = make_blobs(n_samples=<span style="color: #da8548; font-weight: bold;">300</span>, centers=<span style="color: #da8548; font-weight: bold;">4</span>, cluster_std=<span style="color: #da8548; font-weight: bold;">0.60</span>, random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
plt.scatter(X[:, <span style="color: #da8548; font-weight: bold;">0</span>], X[:, <span style="color: #da8548; font-weight: bold;">1</span>], s=<span style="color: #da8548; font-weight: bold;">50</span>);
plt.show()&#8232;
</pre>
</div>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> sklearn.cluster <span style="color: #51afef;">import</span> KMeans&#8232;
<span style="color: #dcaeea;">kmeans</span> = KMeans(n_clusters=<span style="color: #da8548; font-weight: bold;">3</span>)&#8232;
kmeans.fit(X)&#8232;cluster = kmeans.predict(X)&#8232;
plt.scatter(X[:,<span style="color: #da8548; font-weight: bold;">0</span>], X[:,<span style="color: #da8548; font-weight: bold;">1</span>], c=cluster, cmap=plt.cm.Set1)&#8232;
plt.show()
</pre>
</div>
</div>
</div>

<div id="outline-container-NS-KM-Image" class="outline-3">
<h3 id="NS-KM-Image"><span class="section-number-3">3.2.</span> 應用: 壓縮影像</h3>
<div class="outline-text-3" id="text-NS-KM-Image">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38656;&#23433;&#35037; pillow &#25165;&#33021;&#35712; JPEG</span>
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> matplotlib <span style="color: #51afef;">import</span> image
<span class="linenr"> 4: </span><span style="color: #51afef;">from</span> sklearn.cluster <span style="color: #51afef;">import</span> MiniBatchKMeans
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">K &#20540; (&#35201;&#20445;&#30041;&#30340;&#38991;&#33394;&#25976;&#37327;)</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">K</span> = <span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#21462;&#22294;&#29255;</span>
<span class="linenr">10: </span><span style="color: #dcaeea;">image</span> = image.imread(r<span style="color: #98be65;">'./images/Photo42.jpg'</span>)
<span class="linenr">11: </span><span style="color: #dcaeea;">w</span>, <span style="color: #dcaeea;">h</span>, <span style="color: #dcaeea;">d</span> = <span style="color: #c678dd;">tuple</span>(image.shape)
<span class="linenr">12: </span><span style="color: #c678dd;">print</span>(w,h,d)
<span class="linenr">13: </span><span style="color: #dcaeea;">image_data</span> = np.reshape(image, (w * h, d))/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">14: </span><span style="color: #c678dd;">print</span>(image_data.shape)
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(image_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">16: </span><span style="color: #c678dd;">print</span>(image_data[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">17: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#38991;&#33394;&#20998;&#39006;&#28858; K &#31278;</span>
<span class="linenr">18: </span><span style="color: #dcaeea;">kmeans</span> = MiniBatchKMeans(n_clusters=K, batch_size=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">19: </span>labels = kmeans.fit_predict(image_data)
<span class="linenr">20: </span><span style="color: #c678dd;">print</span>(labels[:<span style="color: #da8548; font-weight: bold;">10</span>])
<span class="linenr">21: </span>centers = kmeans.cluster_centers_
<span class="linenr">22: </span><span style="color: #c678dd;">print</span>(centers[:<span style="color: #da8548; font-weight: bold;">10</span>])
<span class="linenr">23: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26681;&#25818;&#20998;&#39006;&#23559;&#38991;&#33394;&#23531;&#20837;&#26032;&#30340;&#24433;&#20687;&#38499;&#21015;</span>
<span class="linenr">24: </span>image_compressed = np.zeros(image.shape)
<span class="linenr">25: </span>label_idx = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">26: </span><span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(w):
<span class="linenr">27: </span>  <span style="color: #51afef;">for</span> j <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(h):
<span class="linenr">28: </span>    image_compressed[i][<span style="color: #dcaeea;">j</span>] = centers[labels[label_idx]]
<span class="linenr">29: </span>    label_idx += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">30: </span>
<span class="linenr">31: </span>plt.imsave(r<span style="color: #98be65;">'images/compressTest.jpg'</span>, image_compressed)
</pre>
</div>

<pre class="example">
480 640 3
(307200, 3)
[0.2        0.16470588 0.23921569]
[0.21176471 0.17647059 0.25098039]
[1 1 1 1 1 1 1 1 1 1]
[[0.61803922 0.37591036 0.37204482]
 [0.23843137 0.15398693 0.18973856]]
</pre>


<div id="org1e3f008" class="figure">
<p><img src="images/compressTest.jpg" alt="compressTest.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 3: </span>Caption</p>
</div>
</div>
</div>
</div>

<div id="outline-container-NS-Hie-cluster" class="outline-2">
<h2 id="NS-Hie-cluster"><span class="section-number-2">4.</span> Hierarchical clustering</h2>
<div class="outline-text-2" id="text-NS-Hie-cluster">
<p>
階層式分群法（Hierarchical Clustering）透過一種階層架構的方式，將資料層層反覆地進行分裂或聚合，以產生最後的樹狀結構，常見的方式有兩種：
</p>
</div>

<div id="outline-container-org0200bb6" class="outline-3">
<h3 id="org0200bb6"><span class="section-number-3">4.1.</span> 聚合式階層分群法 (Bottom-up, agglomerative)</h3>
<div class="outline-text-3" id="text-4-1">
<p>
如果採用聚合的方式，階層式分群法可由樹狀結構的底部開始，將資料或群聚逐次合併。
定義兩個群聚之間的距離
</p>
<ul class="org-ul">
<li>單一連結聚合演算法(single-linkage agglomerative algorithm)：群聚與群聚間的距離可以定義為不同群聚中最接近兩點間的距離。</li>
<li>完整連結聚合演算法(complete-linkage agglomerative algorithm）：群聚間的距離定義為不同群聚中最遠兩點間的距離，這樣可以保證這兩個集合合併後, 任何一對的距離不會大於 d。</li>
<li>平均連結聚合演算法(average-linkage agglomerative algorithm)：群聚間的距離定義為不同群聚間各點與各點間距離總和的平均。沃德法（Ward&rsquo;s method）：群聚間的距離定義為在將兩群合併後，各點到合併後的群中心的距離平方和。</li>
</ul>
</div>

<div id="outline-container-orgb1f07ff" class="outline-4">
<h4 id="orgb1f07ff"><span class="section-number-4">4.1.1.</span> 分群</h4>
<div class="outline-text-4" id="text-4-1-1">
<ol class="org-ol">
<li>Agglomerative Clustering Sample</li>
</ol>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> sklearn.cluster <span style="color: #51afef;">import</span> AgglomerativeClustering
<span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">randomly chosen dataset</span>
<span style="color: #dcaeea;">X</span> = np.array([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>],
              [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">0</span>]])

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">here we need to mention the number of clusters</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">otherwise the result will be a single cluster</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">containing all the data</span>
<span style="color: #dcaeea;">clustering</span> = AgglomerativeClustering(n_clusters = <span style="color: #da8548; font-weight: bold;">2</span>).fit(X)

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">print the class labels</span>
<span style="color: #c678dd;">print</span>(clustering.labels_)
</pre>
</div>

<pre class="example">
[1 1 1 0 0 0]
</pre>

<ol class="org-ol">
<li>畫圖</li>
</ol>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span style="color: #51afef;">import</span> scipy.cluster.hierarchy <span style="color: #51afef;">as</span> sch

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">randomly chosen dataset</span>
<span style="color: #dcaeea;">X</span> = np.array([[<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>],
              [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">2</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>], [<span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">0</span>]])
<span style="color: #dcaeea;">y</span> = np.array([<span style="color: #98be65;">'cat3'</span>, <span style="color: #98be65;">'cat1'</span>, <span style="color: #98be65;">'cat2'</span>, <span style="color: #98be65;">'dog2'</span>, <span style="color: #98be65;">'dog3'</span>, <span style="color: #98be65;">'cat3'</span>])

<span style="color: #dcaeea;">dis</span>=sch.linkage(X,metric=<span style="color: #98be65;">'euclidean'</span>, method=<span style="color: #98be65;">'ward'</span>)
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">metric: &#36317;&#38626;&#30340;&#35336;&#31639;&#26041;&#24335;</span>
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">method: &#32676;&#33287;&#32676;&#20043;&#38291;&#30340;&#35336;&#31639;&#26041;&#24335;&#65292;&#8221;single&#8221;, &#8220;complete&#8221;, &#8220;average&#8221;, &#8220;weighted&#8221;, &#8220;centroid&#8221;, &#8220;median&#8221;, &#8220;ward&#8221;</span>

sch.dendrogram(dis, labels = y)
plt.title(<span style="color: #98be65;">'Hierarchical Clustering'</span>)
plt.show()

</pre>
</div>

<pre class="example">
[1 1 1 2 2 2]
[1 1 1 2 2 2]
</pre>
</div>
</div>

<div id="outline-container-org62f9b82" class="outline-4">
<h4 id="org62f9b82"><span class="section-number-4">4.1.2.</span> 利用距離決定群數，或直接給定群數。</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
建構好聚落樹狀圖後，我們可以依照距離的切割來進行分類，也可以直接給定想要分類的群數，讓系統自動切割到相對應的距離。
</p>
<ul class="org-ul">
<li><p>
距離切割
所給出的樹狀圖，y軸代表距離，我們可以用特徵之間的距離進行分群的切割。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #dcaeea;">max_dis</span>=<span style="color: #da8548; font-weight: bold;">5</span>
<span style="color: #dcaeea;">clusters</span>=sch.fcluster(dis,max_dis,criterion=<span style="color: #98be65;">'distance'</span>)
</pre>
</div></li>
<li><p>
直接給定群數
同時，我們也可以像sklearn一樣，直接給定我們所想要分出的群數。
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #dcaeea;">k</span>=<span style="color: #da8548; font-weight: bold;">5</span>
<span style="color: #dcaeea;">clusters</span>=sch.fcluster(dis,k,criterion=<span style="color: #98be65;">'maxclust'</span>)
</pre>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-org9338c05" class="outline-4">
<h4 id="org9338c05"><span class="section-number-4">4.1.3.</span> 如何評估最佳分群數:K</h4>
<div class="outline-text-4" id="text-4-1-3">
<ul class="org-ul">
<li><a href="https://jimmy-huang.medium.com/kmeans%E5%88%86%E7%BE%A4%E6%BC%94%E7%AE%97%E6%B3%95-%E8%88%87-silhouette-%E8%BC%AA%E5%BB%93%E5%88%86%E6%9E%90-8be17e634589">Kmeans分群演算法 與 Silhouette 輪廓分析</a></li>
<li><a href="https://www.geeksforgeeks.org/implementing-agglomerative-clustering-using-sklearn/">Implementing Agglomerative Clustering using Sklearn</a></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgc3f3fcf" class="outline-3">
<h3 id="orgc3f3fcf"><span class="section-number-3">4.2.</span> 聚合式分群作業</h3>
<div class="outline-text-3" id="text-4-2">
<p>
請利用聚合式分群的模型幫 <b>鳶尾花</b> 分類，
</p>
<ol class="org-ol">
<li>將階層圖畫出來</li>
<li>將K值設為3，輸出分群組果</li>
</ol>
</div>
</div>

<div id="outline-container-org3c62906" class="outline-3">
<h3 id="org3c62906"><span class="section-number-3">4.3.</span> 分裂式階層分群法 (Top-down, divisible) :</h3>
<div class="outline-text-3" id="text-4-3">
<p>
如果採用分裂的方式，則由樹狀結構的頂端開始，將群聚逐次分裂。
Divisive clustering : Also known as top-down approach. This algorithm also does not require to prespecify the number of clusters. Top-down clustering requires a method for splitting a cluster that contains the whole data and proceeds by splitting clusters recursively until individual data have been splitted into singleton cluster.
</p>
</div>
</div>
</div>

<div id="outline-container-org7ad3855" class="outline-2">
<h2 id="org7ad3855"><span class="section-number-2">5.</span> 信用卡詐欺偵測</h2>
<div class="outline-text-2" id="text-5">
<ul class="org-ul">
<li>資料來源: Hands-On Unsupervised Learning Using Python: How to Build Applied Machine Learning Solutions from Ulabled Data</li>
<li>Code: <a href="https://github.com/aapatel09/handson-unsupervised-learning/blob/master/02_end_to_end_machine_learning_project.ipynb">https://github.com/aapatel09/handson-unsupervised-learning/blob/master/02_end_to_end_machine_learning_project.ipynb</a></li>
</ul>
</div>
<div id="outline-container-orgcfbc26f" class="outline-3">
<h3 id="orgcfbc26f"><span class="section-number-3">5.1.</span> 資料取得</h3>
<div class="outline-text-3" id="text-5-1">
</div>
<div id="outline-container-org321bae5" class="outline-4">
<h4 id="org321bae5"><span class="section-number-4">5.1.1.</span> for Google colab</h4>
<div class="outline-text-4" id="text-5-1-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Define functions to connect to Google and change directories</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">connectDrive</span>():
<span class="linenr"> 3: </span>    <span style="color: #51afef;">from</span> google.colab <span style="color: #51afef;">import</span> drive
<span class="linenr"> 4: </span>    drive.mount(<span style="color: #98be65;">'/content/drive'</span>, force_remount=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">changeDirectory</span>(path):
<span class="linenr"> 7: </span>    <span style="color: #51afef;">import</span> os
<span class="linenr"> 8: </span>    original_path = os.getcwd()
<span class="linenr"> 9: </span>    os.chdir(path)
<span class="linenr">10: </span>    new_path = os.getcwd()
<span class="linenr">11: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Original path: "</span>,original_path)
<span class="linenr">12: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"New path: "</span>,new_path)
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Connect to Google Drive</span>
<span class="linenr">15: </span>connectDrive()
<span class="linenr">16: </span>
<span class="linenr">17: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Change path</span>
<span class="linenr">18: </span>changeDirectory(<span style="color: #98be65;">"/content/drive/My Drive/github/handson-unsupervised-learning/"</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org5c51fce" class="outline-4">
<h4 id="org5c51fce"><span class="section-number-4">5.1.2.</span> Import Libraries</h4>
<div class="outline-text-4" id="text-5-1-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #83898d;">'''Main'''</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> os
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #98be65;">'''Data Viz'''</span>
<span class="linenr"> 7: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 8: </span><span style="color: #51afef;">import</span> matplotlib <span style="color: #51afef;">as</span> mpl
<span class="linenr"> 9: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr">10: </span><span style="color: #dcaeea;">color</span> = sns.color_palette()
<span class="linenr">11: </span>
<span class="linenr">12: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">%matplotlib inline</span>
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #83898d;">'''Data Prep'''</span>
<span class="linenr">15: </span><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> preprocessing <span style="color: #51afef;">as</span> pp
<span class="linenr">16: </span><span style="color: #51afef;">from</span> scipy.stats <span style="color: #51afef;">import</span> pearsonr
<span class="linenr">17: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">18: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> StratifiedKFold
<span class="linenr">19: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> log_loss
<span class="linenr">20: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> precision_recall_curve, average_precision_score
<span class="linenr">21: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> roc_curve, auc, roc_auc_score
<span class="linenr">22: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> confusion_matrix, classification_report
<span class="linenr">23: </span>
<span class="linenr">24: </span><span style="color: #98be65;">'''Algos'''</span>
<span class="linenr">25: </span><span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr">26: </span><span style="color: #51afef;">from</span> sklearn.ensemble <span style="color: #51afef;">import</span> RandomForestClassifier
<span class="linenr">27: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">import xgboost as xgb</span>
<span class="linenr">28: </span><span style="color: #51afef;">import</span> lightgbm <span style="color: #51afef;">as</span> lgb
</pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org7bd6bed" class="outline-3">
<h3 id="org7bd6bed"><span class="section-number-3">5.2.</span> 資料準備</h3>
<div class="outline-text-3" id="text-5-2">
</div>
<div id="outline-container-org78c07ed" class="outline-4">
<h4 id="org78c07ed"><span class="section-number-4">5.2.1.</span> 取得資料</h4>
<div class="outline-text-4" id="text-5-2-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Acquire Data</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">data</span> = pd.read_csv(<span style="color: #98be65;">"https://raw.githubusercontent.com/aapatel09/handson-unsupervised-learning/master/datasets/credit_card_data/correlationMatrix.csv"</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-org9d3143e" class="outline-4">
<h4 id="org9d3143e"><span class="section-number-4">5.2.2.</span> 資料探索</h4>
<div class="outline-text-4" id="text-5-2-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #c678dd;">print</span>(data.shape)
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(data.head())
<span class="linenr">3: </span><span style="color: #c678dd;">print</span>(data.describe())
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(data.columns)
</pre>
</div>

<pre class="example" id="orgd4bb1bb">
(30, 31)
  Unnamed: 0  Time    V1    V2    V3    V4    V5    V6    V7    V8    V9   V10   V11   V12  ...   V16   V17   V18   V19   V20   V21   V22   V23   V24   V25   V26   V27   V28  Amount
0       Time  1.00  0.12 -0.01 -0.42 -0.11  0.17 -0.06  0.08 -0.04 -0.01  0.03 -0.25  0.12  ...  0.01 -0.07  0.09  0.03 -0.05  0.04  0.14  0.05 -0.02 -0.23 -0.04 -0.01 -0.01   -0.01
1         V1  0.12  1.00 -0.00 -0.00 -0.00  0.00  0.00  0.00 -0.00  0.00  0.00  0.00  0.00  ...  0.00 -0.00  0.00  0.00  0.00 -0.00  0.00  0.00 -0.00 -0.00 -0.00  0.00  0.00   -0.23
2         V2 -0.01 -0.00  1.00  0.00 -0.00  0.00  0.00 -0.00 -0.00 -0.00 -0.00  0.00 -0.00  ...  0.00 -0.00  0.00  0.00  0.00  0.00  0.00  0.00 -0.00  0.00  0.00 -0.00 -0.00   -0.53
3         V3 -0.42 -0.00  0.00  1.00 -0.00 -0.00  0.00  0.00 -0.00  0.00  0.00  0.00  0.00  ...  0.00  0.00  0.00  0.00  0.00 -0.00 -0.00 -0.00  0.00  0.00 -0.00  0.00  0.00   -0.21
4         V4 -0.11 -0.00 -0.00 -0.00  1.00 -0.00 -0.00 -0.00  0.00  0.00 -0.00 -0.00 -0.00  ... -0.00 -0.00 -0.00 -0.00 -0.00 -0.00  0.00  0.00  0.00  0.00 -0.00 -0.00 -0.00    0.10

[5 rows x 31 columns]
            Time         V1         V2         V3         V4         V5         V6         V7  ...        V22        V23        V24        V25        V26        V27        V28     Amount
count  30.000000  30.000000  30.000000  30.000000  30.000000  30.000000  30.000000  30.000000  ...  30.000000  30.000000  30.000000  30.000000  30.000000  30.000000  30.000000  30.000000
mean    0.006333   0.029667   0.015333   0.012333   0.033000   0.026000   0.038667   0.049333  ...   0.036000   0.031333   0.033000   0.024000   0.032000   0.034000   0.033333   0.013667
std     0.224599   0.189454   0.209593   0.204765   0.184711   0.200062   0.186358   0.194173  ...   0.184234   0.184311   0.182684   0.189202   0.182972   0.182541   0.182593   0.257166
min    -0.420000  -0.230000  -0.530000  -0.420000  -0.110000  -0.390000  -0.060000   0.000000  ...  -0.060000  -0.110000  -0.020000  -0.230000  -0.040000  -0.010000  -0.010000  -0.530000
25%    -0.067500   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000  ...   0.000000   0.000000   0.000000  -0.000000   0.000000   0.000000  -0.000000  -0.060000
50%    -0.010000   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000  ...   0.000000   0.000000   0.000000   0.000000  -0.000000   0.000000   0.000000  -0.000000
75%     0.047500  -0.000000   0.000000   0.000000  -0.000000   0.000000   0.000000   0.000000  ...   0.000000   0.000000  -0.000000   0.000000   0.000000   0.000000   0.000000   0.030000
max     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   1.000000  ...   1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   1.000000   1.000000

[8 rows x 30 columns]
Index(['Unnamed: 0', 'Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8',
       'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18',
       'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',
       'Amount'],
      dtype='object')
</pre>
</div>
<ol class="org-ol">
<li><a id="orgcf1eca2"></a>找出非數值資料<br />
<div class="outline-text-5" id="text-5-2-2-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">3: </span><span style="color: #dcaeea;">data</span> = pd.read_csv(<span style="color: #98be65;">"./credit_card.csv"</span>)
<span class="linenr">4: </span><span style="color: #dcaeea;">nanCounter</span> = pd.isna(data).<span style="color: #c678dd;">sum</span>() <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23559;np.isnan&#20197;pd.isna&#21462;&#20195;</span>
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(nanCounter)
</pre>
</div>

<pre class="example" id="orga862dc9">
Time      0
V1        0
V2        0
V3        0
V4        0
V5        0
V6        0
V7        0
V8        0
V9        0
V10       0
V11       0
V12       0
V13       0
V14       0
V15       0
V16       0
V17       0
V18       0
V19       0
V20       0
V21       0
V22       0
V23       0
V24       0
V25       0
V26       0
V27       0
V28       0
Amount    0
Class     0
dtype: int64
</pre>
</div>
</li>
<li><a id="org18efb11"></a>找出缺漏值<br />
<div class="outline-text-5" id="text-5-2-2-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">distinctCounter</span> = data.<span style="color: #c678dd;">apply</span>(<span style="color: #51afef;">lambda</span> x: <span style="color: #c678dd;">len</span>(x.unique()))
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(distinctCounter)
</pre>
</div>

<pre class="example" id="orgbc308b9">
Time      124592
V1        275663
V2        275663
V3        275663
V4        275663
V5        275663
V6        275663
V7        275663
V8        275663
V9        275663
V10       275663
V11       275663
V12       275663
V13       275663
V14       275663
V15       275663
V16       275663
V17       275663
V18       275663
V19       275663
V20       275663
V21       275663
V22       275663
V23       275663
V24       275663
V25       275663
V26       275663
V27       275663
V28       275663
Amount     32767
Class          2
dtype: int64
</pre>
</div>
</li>
</ol>
</div>
<div id="outline-container-orge7cbb9a" class="outline-4">
<h4 id="orge7cbb9a"><span class="section-number-4">5.2.3.</span> 建立feature matrix和labels array</h4>
<div class="outline-text-4" id="text-5-2-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Generate feature matrix and labels array</span>
<span class="linenr">2: </span><span style="color: #dcaeea;">dataX</span> = data.copy().drop([<span style="color: #98be65;">'Class'</span>],axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">3: </span>dataY = data[<span style="color: #98be65;">'Class'</span>].copy()
<span class="linenr">4: </span><span style="color: #c678dd;">print</span>(dataX.describe())
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(dataY.head())
</pre>
</div>

<pre class="example" id="orgcae363e">
                Time            V1            V2            V3            V4            V5  ...           V24           V25           V26           V27           V28         Amount
count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000
mean    94813.859575  1.168375e-15  3.416908e-16 -1.379537e-15  2.074095e-15  9.604066e-16  ...  4.473266e-15  5.340915e-16  1.683437e-15 -3.660091e-16 -1.227390e-16      88.349619
std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00  1.380247e+00  ...  6.056471e-01  5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109
min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00 -1.137433e+02  ... -2.836627e+00 -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000
25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01 -6.915971e-01  ... -3.545861e-01 -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000
50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02 -5.433583e-02  ...  4.097606e-02  1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000
75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01  6.119264e-01  ...  4.395266e-01  3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000
max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01  3.480167e+01  ...  4.584549e+00  7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000

[8 rows x 30 columns]
0    0
1    0
2    0
3    0
4    0
Name: Class, dtype: int64
</pre>
</div>
</div>
<div id="outline-container-orgd61b1f4" class="outline-4">
<h4 id="orgd61b1f4"><span class="section-number-4">5.2.4.</span> 特徵工程與特徵選擇</h4>
<div class="outline-text-4" id="text-5-2-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">correlationMatrix</span> = pd.DataFrame(data=[],index=dataX.columns,columns=dataX.columns)
<span class="linenr"> 2: </span><span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> dataX.columns:
<span class="linenr"> 3: </span>    <span style="color: #51afef;">for</span> j <span style="color: #51afef;">in</span> dataX.columns:
<span class="linenr"> 4: </span>        correlationMatrix.loc[i,j] = np.<span style="color: #c678dd;">round</span>(pearsonr(dataX.loc[:,i],dataX.loc[:,j])[<span style="color: #da8548; font-weight: bold;">0</span>],<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>count_classes = pd.value_counts(data[<span style="color: #98be65;">'Class'</span>],sort=<span style="color: #a9a1e1;">True</span>).sort_index()
<span class="linenr"> 7: </span>ax = sns.barplot(x=count_classes.index, y=[<span style="color: #c678dd;">tuple</span>(count_classes/<span style="color: #c678dd;">len</span>(data))[<span style="color: #da8548; font-weight: bold;">0</span>],<span style="color: #c678dd;">tuple</span>(count_classes/<span style="color: #c678dd;">len</span>(data))[<span style="color: #da8548; font-weight: bold;">1</span>]])
<span class="linenr"> 8: </span>ax.set_title(<span style="color: #98be65;">'Frequency Percentage by Class'</span>)
<span class="linenr"> 9: </span>ax.set_xlabel(<span style="color: #98be65;">'Class'</span>)
<span class="linenr">10: </span>ax.set_ylabel(<span style="color: #98be65;">'Frequency Percentage'</span>)
<span class="linenr">11: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">12: </span>plt.savefig(<span style="color: #98be65;">'images/creditCardFreq.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="orgf339e34" class="figure">
<p><img src="images/creditCardFreq.png" alt="creditCardFreq.png" width="500" />
</p>
<p><span class="figure-number">Figure 4: </span>Caption</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orga87da2f" class="outline-3">
<h3 id="orga87da2f"><span class="section-number-3">5.3.</span> 模型準備</h3>
<div class="outline-text-3" id="text-5-3">
</div>
<div id="outline-container-org69279be" class="outline-4">
<h4 id="org69279be"><span class="section-number-4">5.3.1.</span> 切分訓練集與測試集</h4>
<div class="outline-text-4" id="text-5-3-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(dataX,
<span class="linenr">2: </span>                                    dataY, test_size=<span style="color: #da8548; font-weight: bold;">0.33</span>,
<span class="linenr">3: </span>                                    random_state=<span style="color: #da8548; font-weight: bold;">2018</span>, stratify=dataY)
<span class="linenr">4: </span>
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(X_train))
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(X_test))
</pre>
</div>

<pre class="example">
190820
93987
</pre>
</div>
</div>
<div id="outline-container-org5a08ca8" class="outline-4">
<h4 id="org5a08ca8"><span class="section-number-4">5.3.2.</span> 選擇成本函數(loss function)</h4>
<div class="outline-text-4" id="text-5-3-2">
<p>
這是監督式分類問題，可以使用二元分類對數損失來計算實際label與模型預測二者間的交叉熵。
\[ log loss=-\frac{1}{N}\sum_{i=1}^N{\sum^M_{j=1}{y_{i,j}log(P_{i,j})} \]
其中
</p>
<ul class="org-ul">
<li>N為資料數量</li>
<li>M為label數量</li>
<li>若第 \(i\) 項為 \(j\) 類別， \(y_{i,j}\) 值為1，反之為0</li>
<li>\(P_{i,j}\) 為預測類別項目 \(i\) 為 \(j\) 類別的機率</li>
</ul>
</div>
</div>
<div id="outline-container-orgeafc80b" class="outline-4">
<h4 id="orgeafc80b"><span class="section-number-4">5.3.3.</span> k-Fold交叉驗證</h4>
<div class="outline-text-4" id="text-5-3-3">
<p>
為了有效評估「模型演算法預測未曾見過的樣本」的表現成效，訓練集可再進一步拆成訓練集與驗證集，可以用 <i>k-fold</i> 交叉驗證來實作。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">k_fold</span> = StratifiedKFold(n_splits=<span style="color: #da8548; font-weight: bold;">5</span>,shuffle=<span style="color: #a9a1e1;">True</span>,random_state=<span style="color: #da8548; font-weight: bold;">2018</span>)
<span class="linenr">2: </span><span style="color: #c678dd;">print</span>(k_fold)
</pre>
</div>

<pre class="example">
StratifiedKFold(n_splits=5, random_state=2018, shuffle=True)
</pre>
</div>
</div>

<div id="outline-container-orgec86339" class="outline-4">
<h4 id="orgec86339"><span class="section-number-4">5.3.4.</span> Feature scaling</h4>
</div>
</div>
<div id="outline-container-orgb8e0dc2" class="outline-3">
<h3 id="orgb8e0dc2"><span class="section-number-3">5.4.</span> 模型訓練-1</h3>
<div class="outline-text-3" id="text-5-4">
<p>
先從最簡單的分類法做起：Logistic Regression分類
</p>
</div>
<div id="outline-container-orgf10dbf9" class="outline-4">
<h4 id="orgf10dbf9"><span class="section-number-4">5.4.1.</span> 模型1: Logistic Regression</h4>
<div class="outline-text-4" id="text-5-4-1">
<ul class="org-ul">
<li>懲罰項設為L2對離群值較不敏感，且會給全部特徵值分配非零權重，較為穩定；L1則會對最重要的特徵值分配最高權重，其他特徵值則給予接近零的權重，較不穩定。</li>
<li>C為正規化的強度，正規化主要用來解決過度擬合的問題。C為一正浮點數，其值越小，正規化強度越強，預設值為1。</li>
<li>模型class_weight設為balanced，目的在告訴演算法這個訓練集的label類別分佈不平衡，在訓練時演算法就會對正向label加大權重。</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#36229;&#21443;&#25976;</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr"> 3: </span><span style="color: #dcaeea;">penalty</span> = <span style="color: #98be65;">'l2'</span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">C</span> = <span style="color: #da8548; font-weight: bold;">1.0</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">class_weight</span> = <span style="color: #98be65;">'balanced'</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">solver</span> = <span style="color: #98be65;">'liblinear'</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">n_jobs</span> = <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #dcaeea;">logReg</span> = LogisticRegression(penalty=penalty, C=C,
<span class="linenr">11: </span>            class_weight=class_weight, random_state=random_state, solver=solver, n_jobs=n_jobs)
<span class="linenr">12: </span><span style="color: #c678dd;">print</span>(logReg)
</pre>
</div>

<pre class="example">
LogisticRegression(class_weight='balanced', n_jobs=1, random_state=2018,
                   solver='liblinear')
</pre>
</div>
</div>

<div id="outline-container-orgde1c744" class="outline-4">
<h4 id="orgde1c744"><span class="section-number-4">5.4.2.</span> 訓練模型</h4>
<div class="outline-text-4" id="text-5-4-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">trainingScores</span> = []
<span class="linenr"> 2: </span><span style="color: #dcaeea;">cvScores</span> = []
<span class="linenr"> 3: </span><span style="color: #dcaeea;">predictionsBasedOnKFolds</span> = pd.DataFrame(data=[], index=y_train.index,columns=[<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>model = logReg
<span class="linenr"> 6: </span><span style="color: #c678dd;">print</span>(model)
<span class="linenr"> 7: </span><span style="color: #51afef;">for</span> train_index, cv_index <span style="color: #51afef;">in</span> k_fold.split(np.zeros(<span style="color: #c678dd;">len</span>(X_train)),y_train.ravel()):
<span class="linenr"> 8: </span>    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]
<span class="linenr"> 9: </span>    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]
<span class="linenr">10: </span>
<span class="linenr">11: </span>    model.fit(X_train_fold, y_train_fold)
<span class="linenr">12: </span>    loglossTraining = log_loss(y_train_fold, model.predict_proba(X_train_fold)[:,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">13: </span>    trainingScores.append(loglossTraining)
<span class="linenr">14: </span>
<span class="linenr">15: </span>    predictionsBasedOnKFolds.loc[X_cv_fold.index,:] = model.predict_proba(X_cv_fold)
<span class="linenr">16: </span>    loglossCV = log_loss(y_cv_fold, predictionsBasedOnKFolds.loc[X_cv_fold.index,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">17: </span>    cvScores.append(loglossCV)
<span class="linenr">18: </span>
<span class="linenr">19: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Training Log Loss: '</span>, loglossTraining)
<span class="linenr">20: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'CV Log Loss: '</span>, loglossCV)
<span class="linenr">21: </span>
<span class="linenr">22: </span>loglossLogisticRegression = log_loss(y_train, predictionsBasedOnKFolds.loc[:,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">23: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Logistic Regression Log Loss: '</span>, loglossLogisticRegression)
</pre>
</div>

<p>
完整版程式碼(trainCreditCard-1.py)
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">!/usr/bin/env python3</span>
<span class="linenr"> 2: </span><span style="color: #83898d;">'''Main'''</span>
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 5: </span><span style="color: #51afef;">import</span> os
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #98be65;">'''Data Viz'''</span>
<span class="linenr"> 8: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 9: </span><span style="color: #51afef;">import</span> matplotlib <span style="color: #51afef;">as</span> mpl
<span class="linenr">10: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr">11: </span><span style="color: #dcaeea;">color</span> = sns.color_palette()
<span class="linenr">12: </span>
<span class="linenr">13: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">%matplotlib inline</span>
<span class="linenr">14: </span>
<span class="linenr">15: </span><span style="color: #83898d;">'''Data Prep'''</span>
<span class="linenr">16: </span><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> preprocessing <span style="color: #51afef;">as</span> pp
<span class="linenr">17: </span><span style="color: #51afef;">from</span> scipy.stats <span style="color: #51afef;">import</span> pearsonr
<span class="linenr">18: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">19: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> StratifiedKFold
<span class="linenr">20: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> log_loss
<span class="linenr">21: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> precision_recall_curve, average_precision_score
<span class="linenr">22: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> roc_curve, auc, roc_auc_score
<span class="linenr">23: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> confusion_matrix, classification_report
<span class="linenr">24: </span>
<span class="linenr">25: </span><span style="color: #98be65;">'''Algos'''</span>
<span class="linenr">26: </span><span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr">27: </span><span style="color: #51afef;">from</span> sklearn.ensemble <span style="color: #51afef;">import</span> RandomForestClassifier
<span class="linenr">28: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">import xgboost as xgb</span>
<span class="linenr">29: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">import lightgbm as lgb</span>
<span class="linenr">30: </span>
<span class="linenr">31: </span>
<span class="linenr">32: </span><span style="color: #dcaeea;">data</span> = pd.read_csv(<span style="color: #98be65;">"./credit_card.csv"</span>)
<span class="linenr">33: </span><span style="color: #dcaeea;">nanCounter</span> = pd.isna(data).<span style="color: #c678dd;">sum</span>() <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23559;np.isnan&#20197;pd.isna&#21462;&#20195;</span>
<span class="linenr">34: </span>
<span class="linenr">35: </span><span style="color: #dcaeea;">dataX</span> = data.copy().drop([<span style="color: #98be65;">'Class'</span>],axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">36: </span>dataY = data[<span style="color: #98be65;">'Class'</span>].copy()
<span class="linenr">37: </span>
<span class="linenr">38: </span><span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(dataX,
<span class="linenr">39: </span>                                    dataY, test_size=<span style="color: #da8548; font-weight: bold;">0.33</span>,
<span class="linenr">40: </span>                                    random_state=<span style="color: #da8548; font-weight: bold;">2018</span>, stratify=dataY)
<span class="linenr">41: </span>
<span class="linenr">42: </span>k_fold = StratifiedKFold(n_splits=<span style="color: #da8548; font-weight: bold;">5</span>,shuffle=<span style="color: #a9a1e1;">True</span>,random_state=<span style="color: #da8548; font-weight: bold;">2018</span>)
<span class="linenr">43: </span>
<span class="linenr">44: </span><span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr">45: </span>penalty = <span style="color: #98be65;">'l2'</span>
<span class="linenr">46: </span>C = <span style="color: #da8548; font-weight: bold;">1.0</span>
<span class="linenr">47: </span>class_weight = <span style="color: #98be65;">'balanced'</span>
<span class="linenr">48: </span>random_state = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr">49: </span>solver = <span style="color: #98be65;">'liblinear'</span>
<span class="linenr">50: </span>n_jobs = <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">51: </span>
<span class="linenr">52: </span>logReg = LogisticRegression(penalty=penalty, C=C,
<span class="linenr">53: </span>            class_weight=class_weight, random_state=random_state, solver=solver, n_jobs=n_jobs)
<span class="linenr">54: </span>
<span class="linenr">55: </span>trainingScores = []
<span class="linenr">56: </span>cvScores = []
<span class="linenr">57: </span>predictionsBasedOnKFolds = pd.DataFrame(data=[], index=y_train.index,columns=[<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">58: </span>
<span class="linenr">59: </span>model = logReg
<span class="linenr">60: </span><span style="color: #c678dd;">print</span>(model)
<span class="linenr">61: </span><span style="color: #51afef;">for</span> train_index, cv_index <span style="color: #51afef;">in</span> k_fold.split(np.zeros(<span style="color: #c678dd;">len</span>(X_train)),y_train.ravel()):
<span class="linenr">62: </span>    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]
<span class="linenr">63: </span>    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]
<span class="linenr">64: </span>
<span class="linenr">65: </span>    model.fit(X_train_fold, y_train_fold)
<span class="linenr">66: </span>    loglossTraining = log_loss(y_train_fold, model.predict_proba(X_train_fold)[:,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">67: </span>    trainingScores.append(loglossTraining)
<span class="linenr">68: </span>
<span class="linenr">69: </span>    predictionsBasedOnKFolds.loc[X_cv_fold.index,:] = model.predict_proba(X_cv_fold)
<span class="linenr">70: </span>    loglossCV = log_loss(y_cv_fold, predictionsBasedOnKFolds.loc[X_cv_fold.index,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">71: </span>    cvScores.append(loglossCV)
<span class="linenr">72: </span>
<span class="linenr">73: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Training Log Loss: '</span>, loglossTraining)
<span class="linenr">74: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'CV Log Loss: '</span>, loglossCV)
<span class="linenr">75: </span>
<span class="linenr">76: </span>loglossLogisticRegression = log_loss(y_train, predictionsBasedOnKFolds.loc[:,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">77: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Logistic Regression Log Loss: '</span>, loglossLogisticRegression)
<span class="linenr">78: </span>
</pre>
</div>

<pre class="example" id="orgbb77fb0">
LogisticRegression(class_weight='balanced', n_jobs=1, random_state=2018,
                   solver='liblinear')
Training Log Loss:  0.10939361490760419
CV Log Loss:  0.10853402466643607
Training Log Loss:  0.10453309543196382
CV Log Loss:  0.10404365007856392
Training Log Loss:  0.11558188743919326
CV Log Loss:  0.11799026783957066
Training Log Loss:  0.11560666592384615
CV Log Loss:  0.11818686208380477
Training Log Loss:  0.09707169357423985
CV Log Loss:  0.0969591251780277
Logistic Regression Log Loss:  0.10914278596928062
</pre>

<p>
正常而言，Training Log Loss應該會略低於CV Log Loss，二者的值相近，表示未發生過擬合狀況（Training Log Loss很低但CV Log Loss很高）。
</p>
</div>
</div>
</div>
<div id="outline-container-org6473ec3" class="outline-3">
<h3 id="org6473ec3"><span class="section-number-3">5.5.</span> 評估指標</h3>
<div class="outline-text-3" id="text-5-5">
<ul class="org-ul">
<li>召回率(recall): 找出了幾筆在訓練集中的詐欺交易？</li>
<li>精確率(precision): 被模型標示為詐欺的交易中，有幾筆為真的詐欺交易</li>
</ul>
</div>
<div id="outline-container-org262abd0" class="outline-4">
<h4 id="org262abd0"><span class="section-number-4">5.5.1.</span> 混淆矩陣(Confusion Matrix)</h4>
<div class="outline-text-4" id="text-5-5-1">
<p>
此例的label分類高度不平衡，使用混淆矩陣意義不大。若預測所有的交易均非詐欺，則結果會有284315筆真陰性、492筆偽陰性、0筆真陽性、0筆偽陽性，精確度為0。
</p>
</div>
</div>
<div id="outline-container-org223e945" class="outline-4">
<h4 id="org223e945"><span class="section-number-4">5.5.2.</span> 精確率-召回率曲線</h4>
<div class="outline-text-4" id="text-5-5-2">
<p>
對於類別不平衡的資料集，比較好的效能評估方案為精準率與召回率。
</p>
</div>
</div>
<div id="outline-container-orgbed7f5a" class="outline-4">
<h4 id="orgbed7f5a"><span class="section-number-4">5.5.3.</span> precision=真陽性個數/(真陽性個數+偽陽性個數)</h4>
<div class="outline-text-4" id="text-5-5-3">
<p>
即，所有的陽性預測中，有多少是對的預測？
</p>
</div>
</div>
<div id="outline-container-org9f669f2" class="outline-4">
<h4 id="org9f669f2"><span class="section-number-4">5.5.4.</span> recall=真陽性個數/(真陽性個數+偽陰性個數)</h4>
<div class="outline-text-4" id="text-5-5-4">
<p>
即，模型能捕捉到多少個詐欺交易？
</p>
</div>
</div>
<div id="outline-container-orgaa38606" class="outline-4">
<h4 id="orgaa38606"><span class="section-number-4">5.5.5.</span> 取捨</h4>
<div class="outline-text-4" id="text-5-5-5">
<ul class="org-ul">
<li>高recall低precision: 雖然預測中會有很多真的詐欺，但也會出現太多誤判</li>
<li>低precision高recall: 因為標記許多詐欺案例，因此能捕捉到許多詐欺交易，但也有許多被詐欺交易的case並不是真的詐欺</li>
<li>權衡：precision-recall curve，可以在每個門檻值下計算出最佳的average precision</li>
</ul>
</div>
</div>
<div id="outline-container-org2552749" class="outline-4">
<h4 id="org2552749"><span class="section-number-4">5.5.6.</span> 接收者操作特徵(Receiver Operating Characteristic)</h4>
<div class="outline-text-4" id="text-5-5-6">
<p>
ROC將真陽性率當Y軸、將偽陽性率當X軸，真陽性率也可以被當成敏感度，而偽陽性率也乭以被當作l-specificity。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">preds</span> = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,<span style="color: #da8548; font-weight: bold;">1</span>]], axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 2: </span>preds.columns = [<span style="color: #98be65;">'trueLabel'</span>,<span style="color: #98be65;">'prediction'</span>]
<span class="linenr"> 3: </span>predictionsBasedOnKFoldsLogisticRegression = preds.copy()
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">precision</span>, <span style="color: #dcaeea;">recall</span>, <span style="color: #dcaeea;">thresholds</span> = precision_recall_curve(preds[<span style="color: #98be65;">'trueLabel'</span>],
<span class="linenr"> 6: </span>                                                       preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr"> 7: </span>average_precision = average_precision_score(preds[<span style="color: #98be65;">'trueLabel'</span>],
<span class="linenr"> 8: </span>                                            preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr"> 9: </span>plt.step(recall, precision, color=<span style="color: #98be65;">'k'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.7</span>, where=<span style="color: #98be65;">'post'</span>)
<span class="linenr">10: </span>plt.fill_between(recall, precision, step=<span style="color: #98be65;">'post'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, color=<span style="color: #98be65;">'k'</span>)
<span class="linenr">11: </span>
<span class="linenr">12: </span>plt.xlabel(<span style="color: #98be65;">'Recall'</span>)
<span class="linenr">13: </span>plt.ylabel(<span style="color: #98be65;">'Precision'</span>)
<span class="linenr">14: </span>plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr">15: </span>plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr">16: </span>plt.title(<span style="color: #98be65;">'Precision-Recall curve: Average Precision = {0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(average_precision))
<span class="linenr">17: </span>plt.savefig(<span style="color: #98be65;">'images/prec-recall.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">18: </span>fpr, tpr, thresholds = roc_curve(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">19: </span>areaUnderROC = auc(fpr, tpr)
<span class="linenr">20: </span>plt.figure()
<span class="linenr">21: </span>plt.plot(fpr, tpr, color=<span style="color: #98be65;">'r'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, label=<span style="color: #98be65;">'ROC curve'</span>)
<span class="linenr">22: </span>plt.plot([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], color=<span style="color: #98be65;">'k'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, linestyle=<span style="color: #98be65;">'--'</span>)
<span class="linenr">23: </span>plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr">24: </span>plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr">25: </span>plt.xlabel(<span style="color: #98be65;">'False Positive Rate'</span>)
<span class="linenr">26: </span>plt.ylabel(<span style="color: #98be65;">'True Positive Rate'</span>)
<span class="linenr">27: </span>plt.title(<span style="color: #98be65;">'Receiver operating characteristic: \</span>
<span class="linenr">28: </span><span style="color: #98be65;">          Area under the curve = {0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(areaUnderROC))
<span class="linenr">29: </span>plt.legend(loc=<span style="color: #98be65;">"lower right"</span>)
<span class="linenr">30: </span>plt.savefig(<span style="color: #98be65;">'images/auROC.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">31: </span>
</pre>
</div>
<p>
由圖<a href="#org11c066d">5</a>可以看出此模型能達到近80%的recall(捕捉到了80%的詐欺交易)，以及近乎70%的precision(所有被標記為詐欺的case中，有70%為真的詐欺，但仍有30$交易被不正確的標記為詐欺)
</p>
<p width="500">
<img src="images/prec-recall.png" alt="prec-recall.png" width="500" />
圖<a href="#org58cc940">5</a>的auROC曲線充許我們在盡可能保持偽陽率低的情況下，決定有多少的詐欺案例能被捕捉到。
</p>

<div id="org58cc940" class="figure">
<p><img src="images/auROC.png" alt="auROC.png" width="500" />
</p>
<p><span class="figure-number">Figure 5: </span>Caption</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgcb2ad05" class="outline-3">
<h3 id="orgcb2ad05"><span class="section-number-3">5.6.</span> 模型訓練#2</h3>
<div class="outline-text-3" id="text-5-6">
</div>
<div id="outline-container-orgda3a82c" class="outline-4">
<h4 id="orgda3a82c"><span class="section-number-4">5.6.1.</span> 模型#2: 隨機森林</h4>
<div class="outline-text-4" id="text-5-6-1">
</div>
<ol class="org-ol">
<li><a id="org504a2c2"></a>設定超參數<br />
<div class="outline-text-5" id="text-5-6-1-1">
<ul class="org-ul">
<li>n_estimators = 10: 建立十顆樹並將這十顆樹的預測結果平均</li>
<li>這個case有30個特徵值，每顆樹會取總特徵值數的平方根值作為特徵數量，此例每顆樹會取5個特徵值</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">n_estimators</span> = <span style="color: #da8548; font-weight: bold;">10</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">max_features</span> = <span style="color: #98be65;">'auto'</span>
<span class="linenr"> 3: </span><span style="color: #dcaeea;">max_depth</span> = <span style="color: #a9a1e1;">None</span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">min_samples_split</span> = <span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">min_samples_leaf</span> = <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">min_weight_fraction_leaf</span> = <span style="color: #da8548; font-weight: bold;">0.0</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">max_leaf_nodes</span> = <span style="color: #a9a1e1;">None</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">bootstrap</span> = <span style="color: #a9a1e1;">True</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">oob_score</span> = <span style="color: #a9a1e1;">False</span>
<span class="linenr">10: </span><span style="color: #dcaeea;">n_jobs</span> = -<span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">11: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr">12: </span><span style="color: #dcaeea;">class_weight</span> = <span style="color: #98be65;">'balanced'</span>
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #dcaeea;">RFC</span> = RandomForestClassifier(n_estimators=n_estimators,
<span class="linenr">15: </span>        max_features=max_features, max_depth=max_depth,
<span class="linenr">16: </span>        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
<span class="linenr">17: </span>        min_weight_fraction_leaf=min_weight_fraction_leaf,
<span class="linenr">18: </span>        max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap,
<span class="linenr">19: </span>        oob_score=oob_score, n_jobs=n_jobs, random_state=random_state,
<span class="linenr">20: </span>        class_weight=class_weight)
</pre>
</div>
</div>
</li>
<li><a id="org0f19bf7"></a>訓練模型<br />
<div class="outline-text-5" id="text-5-6-1-2">
<p>
執行k-fold交叉驗證五次，每次用4/5訓練集做為訓練、1/5作為預測
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">trainingScores</span> = []
<span class="linenr"> 2: </span><span style="color: #dcaeea;">cvScores</span> = []
<span class="linenr"> 3: </span><span style="color: #dcaeea;">predictionsBasedOnKFolds</span> = pd.DataFrame(data=[],
<span class="linenr"> 4: </span>                                        index=y_train.index,columns=[<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>model = RFC
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #51afef;">for</span> train_index, cv_index <span style="color: #51afef;">in</span> k_fold.split(np.zeros(<span style="color: #c678dd;">len</span>(X_train)),
<span class="linenr"> 9: </span>                                          y_train.ravel()):
<span class="linenr">10: </span>    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \
<span class="linenr">11: </span>        X_train.iloc[cv_index,:]
<span class="linenr">12: </span>    y_train_fold, y_cv_fold = y_train.iloc[train_index], \
<span class="linenr">13: </span>        y_train.iloc[cv_index]
<span class="linenr">14: </span>
<span class="linenr">15: </span>    model.fit(X_train_fold, y_train_fold)
<span class="linenr">16: </span>    loglossTraining = log_loss(y_train_fold, \
<span class="linenr">17: </span>                                model.predict_proba(X_train_fold)[:,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">18: </span>    trainingScores.append(loglossTraining)
<span class="linenr">19: </span>
<span class="linenr">20: </span>    predictionsBasedOnKFolds.loc[X_cv_fold.index,:] = \
<span class="linenr">21: </span>        model.predict_proba(X_cv_fold)
<span class="linenr">22: </span>    loglossCV = log_loss(y_cv_fold, \
<span class="linenr">23: </span>        predictionsBasedOnKFolds.loc[X_cv_fold.index,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">24: </span>    cvScores.append(loglossCV)
<span class="linenr">25: </span>
<span class="linenr">26: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Training Log Loss: '</span>, loglossTraining)
<span class="linenr">27: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'CV Log Loss: '</span>, loglossCV)
<span class="linenr">28: </span>
<span class="linenr">29: </span>loglossRandomForestsClassifier = log_loss(y_train,
<span class="linenr">30: </span>                                          predictionsBasedOnKFolds.loc[:,<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">31: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Random Forests Log Loss: '</span>, loglossRandomForestsClassifier)
</pre>
</div>

<pre class="example" id="org1284b5e">
Training Log Loss:  0.0004566021382946814
CV Log Loss:  0.009090297680965084
Training Log Loss:  0.0004082945215246006
CV Log Loss:  0.01262610549231437
Training Log Loss:  0.00038853441430403824
CV Log Loss:  0.010809571632524845
Training Log Loss:  0.0003910895192496393
CV Log Loss:  0.006617990788005817
Training Log Loss:  0.00042118560357566983
CV Log Loss:  0.013463844755711074
Random Forests Log Loss:  0.010521562069904237
</pre>
<ul class="org-ul">
<li>可以看出Training Log Loss明顯低於CV Log Loss，表示可能有過擬合的現象</li>
<li>但這兩個Loss指標仍明顯優於Logistic Regression模型(大概為後者的1/10)</li>
</ul>
</div>
</li>
<li><a id="org73474dd"></a>評估結果<br />
<div class="outline-text-5" id="text-5-6-1-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>plt.cla()
<span class="linenr"> 2: </span><span style="color: #dcaeea;">preds</span> = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,<span style="color: #da8548; font-weight: bold;">1</span>]], axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 3: </span>preds.columns = [<span style="color: #98be65;">'trueLabel'</span>,<span style="color: #98be65;">'prediction'</span>]
<span class="linenr"> 4: </span>predictionsBasedOnKFoldsRandomForests = preds.copy()
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">precision</span>, <span style="color: #dcaeea;">recall</span>, <span style="color: #dcaeea;">thresholds</span> = precision_recall_curve(preds[<span style="color: #98be65;">'trueLabel'</span>],
<span class="linenr"> 7: </span>                                                       preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr"> 8: </span>average_precision = average_precision_score(preds[<span style="color: #98be65;">'trueLabel'</span>],
<span class="linenr"> 9: </span>                                            preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">10: </span>
<span class="linenr">11: </span>plt.step(recall, precision, color=<span style="color: #98be65;">'k'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.7</span>, where=<span style="color: #98be65;">'post'</span>)
<span class="linenr">12: </span>plt.fill_between(recall, precision, step=<span style="color: #98be65;">'post'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, color=<span style="color: #98be65;">'k'</span>)
<span class="linenr">13: </span>
<span class="linenr">14: </span>plt.xlabel(<span style="color: #98be65;">'Recall'</span>)
<span class="linenr">15: </span>plt.ylabel(<span style="color: #98be65;">'Precision'</span>)
<span class="linenr">16: </span>plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr">17: </span>plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr">18: </span>
<span class="linenr">19: </span>plt.title(<span style="color: #98be65;">'Precision-Recall curve: Average Precision = {0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(
<span class="linenr">20: </span>          average_precision))
<span class="linenr">21: </span>plt.savefig(<span style="color: #98be65;">'images/prec-recall-2.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">22: </span>
<span class="linenr">23: </span>fpr, tpr, thresholds = roc_curve(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">24: </span>areaUnderROC = auc(fpr, tpr)
<span class="linenr">25: </span>
<span class="linenr">26: </span>plt.figure()
<span class="linenr">27: </span>plt.plot(fpr, tpr, color=<span style="color: #98be65;">'r'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, label=<span style="color: #98be65;">'ROC curve'</span>)
<span class="linenr">28: </span>plt.plot([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], color=<span style="color: #98be65;">'k'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, linestyle=<span style="color: #98be65;">'--'</span>)
<span class="linenr">29: </span>plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr">30: </span>plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr">31: </span>plt.xlabel(<span style="color: #98be65;">'False Positive Rate'</span>)
<span class="linenr">32: </span>plt.ylabel(<span style="color: #98be65;">'True Positive Rate'</span>)
<span class="linenr">33: </span>plt.title(<span style="color: #98be65;">'Receiver operating characteristic: \</span>
<span class="linenr">34: </span><span style="color: #98be65;">          Area under the curve = {0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(
<span class="linenr">35: </span>          areaUnderROC))
<span class="linenr">36: </span>plt.legend(loc=<span style="color: #98be65;">"lower right"</span>)
<span class="linenr">37: </span>plt.savefig(<span style="color: #98be65;">'images/auROC-2.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="orge9da7b2" class="figure">
<p><img src="images/prec-recall-2.png" alt="prec-recall-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 6: </span>Caption</p>
</div>
<ul class="org-ul">
<li>但隨機森林的auROC為0.92，不如Logistic Regression模型的0.97</li>
</ul>

<div id="orgc9b3834" class="figure">
<p><img src="images/auROC-2.png" alt="auROC-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 7: </span>Caption</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-org36ff08d" class="outline-4">
<h4 id="org36ff08d"><span class="section-number-4">5.6.2.</span> 模型#3: Gradient Boosting Machine (XGBoost)</h4>
<div class="outline-text-4" id="text-5-6-2">
<p>
Gradient boosting有兩種版本: XBGoost和微軟的LightGBM(效能較快)
</p>
</div>
<ol class="org-ol">
<li><a id="orgb04d7c1"></a>設定超參數<br />
<div class="outline-text-5" id="text-5-6-2-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">params_xGB</span> = {
<span class="linenr"> 2: </span>    <span style="color: #98be65;">'nthread'</span>:<span style="color: #da8548; font-weight: bold;">16</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">number of cores</span>
<span class="linenr"> 3: </span>    <span style="color: #98be65;">'learning rate'</span>: <span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to 1, default 0.3</span>
<span class="linenr"> 4: </span>    <span style="color: #98be65;">'gamma'</span>: <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to infinity, default 0</span>
<span class="linenr"> 5: </span>        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">increase to reduce complexity (increase bias, reduce variance)</span>
<span class="linenr"> 6: </span>    <span style="color: #98be65;">'max_depth'</span>: <span style="color: #da8548; font-weight: bold;">6</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 1 to infinity, default 6</span>
<span class="linenr"> 7: </span>    <span style="color: #98be65;">'min_child_weight'</span>: <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to infinity, default 1</span>
<span class="linenr"> 8: </span>    <span style="color: #98be65;">'max_delta_step'</span>: <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to infinity, default 0</span>
<span class="linenr"> 9: </span>    <span style="color: #98be65;">'subsample'</span>: <span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to 1, default 1</span>
<span class="linenr">10: </span>        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">subsample ratio of the training examples</span>
<span class="linenr">11: </span>    <span style="color: #98be65;">'colsample_bytree'</span>: <span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to 1, default 1</span>
<span class="linenr">12: </span>        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">subsample ratio of features</span>
<span class="linenr">13: </span>    <span style="color: #98be65;">'objective'</span>:<span style="color: #98be65;">'binary:logistic'</span>,
<span class="linenr">14: </span>    <span style="color: #98be65;">'num_class'</span>:<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">15: </span>    <span style="color: #98be65;">'eval_metric'</span>:<span style="color: #98be65;">'logloss'</span>,
<span class="linenr">16: </span>    <span style="color: #98be65;">'seed'</span>:<span style="color: #da8548; font-weight: bold;">2018</span>,
<span class="linenr">17: </span>    <span style="color: #98be65;">'silent'</span>:<span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">18: </span>}
</pre>
</div>
</div>
</li>
<li><a id="org0f579f7"></a>訓練模型<br />
<div class="outline-text-5" id="text-5-6-2-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">trainingScores</span> = []
<span class="linenr"> 2: </span><span style="color: #dcaeea;">cvScores</span> = []
<span class="linenr"> 3: </span><span style="color: #dcaeea;">predictionsBasedOnKFolds</span> = pd.DataFrame(data=[],
<span class="linenr"> 4: </span>                                    index=y_train.index,columns=[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #51afef;">for</span> train_index, cv_index <span style="color: #51afef;">in</span> k_fold.split(np.zeros(<span style="color: #c678dd;">len</span>(X_train)), y_train.ravel()):
<span class="linenr"> 7: </span>    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]
<span class="linenr"> 8: </span>    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>    dtrain = xgb.DMatrix(data=X_train_fold, label=y_train_fold)
<span class="linenr">11: </span>    dCV = xgb.DMatrix(data=X_cv_fold)
<span class="linenr">12: </span>
<span class="linenr">13: </span>    bst = xgb.cv(params_xGB, dtrain, num_boost_round=<span style="color: #da8548; font-weight: bold;">2000</span>,
<span class="linenr">14: </span>                 nfold=<span style="color: #da8548; font-weight: bold;">5</span>, early_stopping_rounds=<span style="color: #da8548; font-weight: bold;">200</span>, verbose_eval=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr">15: </span>
<span class="linenr">16: </span>    best_rounds = np.argmin(bst[<span style="color: #98be65;">'test-logloss-mean'</span>])
<span class="linenr">17: </span>    bst = xgb.train(params_xGB, dtrain, best_rounds)
<span class="linenr">18: </span>
<span class="linenr">19: </span>    loglossTraining = log_loss(y_train_fold, bst.predict(dtrain))
<span class="linenr">20: </span>    trainingScores.append(loglossTraining)
<span class="linenr">21: </span>
<span class="linenr">22: </span>    predictionsBasedOnKFolds.loc[X_cv_fold.index,<span style="color: #98be65;">'prediction'</span>] = bst.predict(dCV)
<span class="linenr">23: </span>    loglossCV = log_loss(y_cv_fold, predictionsBasedOnKFolds.loc[X_cv_fold.index,<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">24: </span>    cvScores.append(loglossCV)
<span class="linenr">25: </span>
<span class="linenr">26: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Training Log Loss: '</span>, loglossTraining)
<span class="linenr">27: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'CV Log Loss: '</span>, loglossCV)
<span class="linenr">28: </span>
<span class="linenr">29: </span>loglossXGBoostGradientBoosting = log_loss(y_train, predictionsBasedOnKFolds.loc[:,<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">30: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'XGBoost Gradient Boosting Log Loss: '</span>, loglossXGBoostGradientBoosting)
</pre>
</div>
<p>
完整程式版(trainXGBoost.py)
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #83898d;">'''Main'''</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> os
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #98be65;">'''Data Viz'''</span>
<span class="linenr"> 7: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 8: </span><span style="color: #51afef;">import</span> matplotlib <span style="color: #51afef;">as</span> mpl
<span class="linenr"> 9: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr">10: </span><span style="color: #dcaeea;">color</span> = sns.color_palette()
<span class="linenr">11: </span>
<span class="linenr">12: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">%matplotlib inline</span>
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #83898d;">'''Data Prep'''</span>
<span class="linenr">15: </span><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> preprocessing <span style="color: #51afef;">as</span> pp
<span class="linenr">16: </span><span style="color: #51afef;">from</span> scipy.stats <span style="color: #51afef;">import</span> pearsonr
<span class="linenr">17: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">18: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> StratifiedKFold
<span class="linenr">19: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> log_loss
<span class="linenr">20: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> precision_recall_curve, average_precision_score
<span class="linenr">21: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> roc_curve, auc, roc_auc_score
<span class="linenr">22: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> confusion_matrix, classification_report
<span class="linenr">23: </span>
<span class="linenr">24: </span><span style="color: #98be65;">'''Algos'''</span>
<span class="linenr">25: </span><span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr">26: </span><span style="color: #51afef;">from</span> sklearn.ensemble <span style="color: #51afef;">import</span> RandomForestClassifier
<span class="linenr">27: </span><span style="color: #51afef;">import</span> xgboost <span style="color: #51afef;">as</span> xgb
<span class="linenr">28: </span><span style="color: #51afef;">import</span> lightgbm <span style="color: #51afef;">as</span> lgb
<span class="linenr">29: </span>
<span class="linenr">30: </span>
<span class="linenr">31: </span><span style="color: #dcaeea;">data</span> = pd.read_csv(<span style="color: #98be65;">"./credit_card.csv"</span>)
<span class="linenr">32: </span><span style="color: #dcaeea;">nanCounter</span> = pd.isna(data).<span style="color: #c678dd;">sum</span>() <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23559;np.isnan&#20197;pd.isna&#21462;&#20195;</span>
<span class="linenr">33: </span>
<span class="linenr">34: </span><span style="color: #dcaeea;">dataX</span> = data.copy().drop([<span style="color: #98be65;">'Class'</span>],axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">35: </span>dataY = data[<span style="color: #98be65;">'Class'</span>].copy()
<span class="linenr">36: </span>
<span class="linenr">37: </span><span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(dataX,
<span class="linenr">38: </span>                                    dataY, test_size=<span style="color: #da8548; font-weight: bold;">0.33</span>,
<span class="linenr">39: </span>                                    random_state=<span style="color: #da8548; font-weight: bold;">2018</span>, stratify=dataY)
<span class="linenr">40: </span>
<span class="linenr">41: </span>k_fold = StratifiedKFold(n_splits=<span style="color: #da8548; font-weight: bold;">5</span>,shuffle=<span style="color: #a9a1e1;">True</span>,random_state=<span style="color: #da8548; font-weight: bold;">2018</span>)
<span class="linenr">42: </span>
<span class="linenr">43: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36229;&#21443;&#25976;</span>
<span class="linenr">44: </span>params_xGB = {
<span class="linenr">45: </span>    <span style="color: #98be65;">'nthread'</span>:<span style="color: #da8548; font-weight: bold;">16</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">number of cores</span>
<span class="linenr">46: </span>    <span style="color: #98be65;">'learning_rate'</span>:<span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to 1, default 0.3</span>
<span class="linenr">47: </span>    <span style="color: #98be65;">'gamma'</span>:<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to infinity, default 0</span>
<span class="linenr">48: </span>        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">increase to reduce complexity (increase bias, reduce variance)</span>
<span class="linenr">49: </span>    <span style="color: #98be65;">'max_depth'</span>: <span style="color: #da8548; font-weight: bold;">6</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 1 to infinity, default 6</span>
<span class="linenr">50: </span>    <span style="color: #98be65;">'min_child_weight'</span>: <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to infinity, default 1</span>
<span class="linenr">51: </span>    <span style="color: #98be65;">'max_delta_step'</span>: <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to infinity, default 0</span>
<span class="linenr">52: </span>    <span style="color: #98be65;">'subsample'</span>: <span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to 1, default 1</span>
<span class="linenr">53: </span>        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">subsample ratio of the training examples</span>
<span class="linenr">54: </span>    <span style="color: #98be65;">'colsample_bytree'</span>: <span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">range 0 to 1, default 1</span>
<span class="linenr">55: </span>        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">subsample ratio of features</span>
<span class="linenr">56: </span>    <span style="color: #98be65;">'objective'</span>:<span style="color: #98be65;">'binary:logistic'</span>,
<span class="linenr">57: </span>    <span style="color: #98be65;">'num_class'</span>:<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">58: </span>    <span style="color: #98be65;">'eval_metric'</span>:<span style="color: #98be65;">'logloss'</span>,
<span class="linenr">59: </span>    <span style="color: #98be65;">'seed'</span>:<span style="color: #da8548; font-weight: bold;">2018</span>,
<span class="linenr">60: </span>    <span style="color: #98be65;">'silent'</span>:<span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">61: </span>}
<span class="linenr">62: </span>
<span class="linenr">63: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr">64: </span>trainingScores = []
<span class="linenr">65: </span>cvScores = []
<span class="linenr">66: </span>predictionsBasedOnKFolds = pd.DataFrame(data=[],
<span class="linenr">67: </span>                                    index=y_train.index,columns=[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">68: </span>
<span class="linenr">69: </span><span style="color: #51afef;">for</span> train_index, cv_index <span style="color: #51afef;">in</span> k_fold.split(np.zeros(<span style="color: #c678dd;">len</span>(X_train)), y_train.ravel()):
<span class="linenr">70: </span>    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], X_train.iloc[cv_index,:]
<span class="linenr">71: </span>    y_train_fold, y_cv_fold = y_train.iloc[train_index], y_train.iloc[cv_index]
<span class="linenr">72: </span>
<span class="linenr">73: </span>    dtrain = xgb.DMatrix(data=X_train_fold, label=y_train_fold)
<span class="linenr">74: </span>    dCV = xgb.DMatrix(data=X_cv_fold)
<span class="linenr">75: </span>
<span class="linenr">76: </span>    bst = xgb.cv(params_xGB, dtrain, num_boost_round=<span style="color: #da8548; font-weight: bold;">2000</span>,
<span class="linenr">77: </span>                 nfold=<span style="color: #da8548; font-weight: bold;">5</span>, early_stopping_rounds=<span style="color: #da8548; font-weight: bold;">200</span>, verbose_eval=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr">78: </span>
<span class="linenr">79: </span>    best_rounds = np.argmin(bst[<span style="color: #98be65;">'test-logloss-mean'</span>])
<span class="linenr">80: </span>    bst = xgb.train(params_xGB, dtrain, best_rounds)
<span class="linenr">81: </span>
<span class="linenr">82: </span>    loglossTraining = log_loss(y_train_fold, bst.predict(dtrain))
<span class="linenr">83: </span>    trainingScores.append(loglossTraining)
<span class="linenr">84: </span>
<span class="linenr">85: </span>    predictionsBasedOnKFolds.loc[X_cv_fold.index,<span style="color: #98be65;">'prediction'</span>] = bst.predict(dCV)
<span class="linenr">86: </span>    loglossCV = log_loss(y_cv_fold, predictionsBasedOnKFolds.loc[X_cv_fold.index,<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">87: </span>    cvScores.append(loglossCV)
<span class="linenr">88: </span>
<span class="linenr">89: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Training Log Loss: '</span>, loglossTraining)
<span class="linenr">90: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'CV Log Loss: '</span>, loglossCV)
<span class="linenr">91: </span>
<span class="linenr">92: </span>loglossXGBoostGradientBoosting = log_loss(y_train, predictionsBasedOnKFolds.loc[:,<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">93: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'XGBoost Gradient Boosting Log Loss: '</span>, loglossXGBoostGradientBoosting)
<span class="linenr">94: </span>
<span class="linenr">95: </span>
</pre>
</div>

<pre class="example" id="orgc18db81">
Training Log Loss:  0.0009698748139954727
CV Log Loss:  0.0023983441021591216
Training Log Loss:  0.000872351723518117
CV Log Loss:  0.0031189630212595408
Training Log Loss:  0.0007123358367720159
CV Log Loss:  0.002299779731184769
Training Log Loss:  0.0009144685956787081
CV Log Loss:  0.0026294304116304865
Training Log Loss:  0.0005772056222738951
CV Log Loss:  0.003686224309306172
XGBoost Gradient Boosting Log Loss:  0.002826548315108018
</pre>
<p>
會跑很久&#x2026;.中間會出現Warning
Training Log Loss與CV Log Loss都較Random Forest有巨大改善
</p>
</div>
</li>
<li><a id="orge3f894e"></a>評估結果<br />
<div class="outline-text-5" id="text-5-6-2-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>plt.cla()
<span class="linenr"> 2: </span><span style="color: #dcaeea;">preds</span> = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,<span style="color: #98be65;">'prediction'</span>]], axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 3: </span>preds.columns = [<span style="color: #98be65;">'trueLabel'</span>,<span style="color: #98be65;">'prediction'</span>]
<span class="linenr"> 4: </span>predictionsBasedOnKFoldsXGBoostGradientBoosting = preds.copy()
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">precision</span>, <span style="color: #dcaeea;">recall</span>, <span style="color: #dcaeea;">thresholds</span> = \
<span class="linenr"> 7: </span>    precision_recall_curve(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr"> 8: </span>average_precision = \
<span class="linenr"> 9: </span>    average_precision_score(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">10: </span>
<span class="linenr">11: </span>plt.step(recall, precision, color=<span style="color: #98be65;">'k'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.7</span>, where=<span style="color: #98be65;">'post'</span>)
<span class="linenr">12: </span>plt.fill_between(recall, precision, step=<span style="color: #98be65;">'post'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, color=<span style="color: #98be65;">'k'</span>)
<span class="linenr">13: </span>
<span class="linenr">14: </span>plt.xlabel(<span style="color: #98be65;">'Recall'</span>)
<span class="linenr">15: </span>plt.ylabel(<span style="color: #98be65;">'Precision'</span>)
<span class="linenr">16: </span>plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr">17: </span>plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr">18: </span>
<span class="linenr">19: </span>plt.title(<span style="color: #98be65;">'Precision-Recall curve: Average Precision = {0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(
<span class="linenr">20: </span>          average_precision))
<span class="linenr">21: </span>plt.savefig(<span style="color: #98be65;">'images/prec-recall-3.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">22: </span>
<span class="linenr">23: </span>fpr, tpr, thresholds = roc_curve(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">24: </span>areaUnderROC = auc(fpr, tpr)
<span class="linenr">25: </span>
<span class="linenr">26: </span>plt.figure()
<span class="linenr">27: </span>plt.plot(fpr, tpr, color=<span style="color: #98be65;">'r'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, label=<span style="color: #98be65;">'ROC curve'</span>)
<span class="linenr">28: </span>plt.plot([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], color=<span style="color: #98be65;">'k'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, linestyle=<span style="color: #98be65;">'--'</span>)
<span class="linenr">29: </span>plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr">30: </span>plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr">31: </span>plt.xlabel(<span style="color: #98be65;">'False Positive Rate'</span>)
<span class="linenr">32: </span>plt.ylabel(<span style="color: #98be65;">'True Positive Rate'</span>)
<span class="linenr">33: </span>plt.title(<span style="color: #98be65;">'Receiver operating characteristic: \</span>
<span class="linenr">34: </span><span style="color: #98be65;">        Area under the curve = {0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(areaUnderROC))
<span class="linenr">35: </span>plt.legend(loc=<span style="color: #98be65;">"lower right"</span>)
<span class="linenr">36: </span>plt.savefig(<span style="color: #98be65;">'images/auROC-3.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">37: </span>
</pre>
</div>


<div id="org09e07a7" class="figure">
<p><img src="images/prec-recall-3.png" alt="prec-recall-3.png" width="500" />
</p>
<p><span class="figure-number">Figure 8: </span>Caption</p>
</div>
<ul class="org-ul">
<li>但隨機森林的auROC為0.92，不如Logistic Regression模型的0.97</li>
</ul>

<div id="org3a29c73" class="figure">
<p><img src="images/auROC-3.png" alt="auROC-3.png" width="500" />
</p>
<p><span class="figure-number">Figure 9: </span>Caption</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-orge5e4bf9" class="outline-4">
<h4 id="orge5e4bf9"><span class="section-number-4">5.6.3.</span> 模型#4: Gradient Boosting Machine (LightGBM)</h4>
<div class="outline-text-4" id="text-5-6-3">
</div>
<ol class="org-ol">
<li><a id="org6b7ac37"></a>讀資料<br />
<div class="outline-text-5" id="text-5-6-3-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #83898d;">'''Main'''</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> os
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #98be65;">'''Data Viz'''</span>
<span class="linenr"> 7: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 8: </span><span style="color: #51afef;">import</span> matplotlib <span style="color: #51afef;">as</span> mpl
<span class="linenr"> 9: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr">10: </span><span style="color: #dcaeea;">color</span> = sns.color_palette()
<span class="linenr">11: </span>
<span class="linenr">12: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">%matplotlib inline</span>
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #83898d;">'''Data Prep'''</span>
<span class="linenr">15: </span><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> preprocessing <span style="color: #51afef;">as</span> pp
<span class="linenr">16: </span><span style="color: #51afef;">from</span> scipy.stats <span style="color: #51afef;">import</span> pearsonr
<span class="linenr">17: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">18: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> StratifiedKFold
<span class="linenr">19: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> log_loss
<span class="linenr">20: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> precision_recall_curve, average_precision_score
<span class="linenr">21: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> roc_curve, auc, roc_auc_score
<span class="linenr">22: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> confusion_matrix, classification_report
<span class="linenr">23: </span>
<span class="linenr">24: </span><span style="color: #98be65;">'''Algos'''</span>
<span class="linenr">25: </span><span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr">26: </span><span style="color: #51afef;">from</span> sklearn.ensemble <span style="color: #51afef;">import</span> RandomForestClassifier
<span class="linenr">27: </span><span style="color: #51afef;">import</span> xgboost <span style="color: #51afef;">as</span> xgb
<span class="linenr">28: </span><span style="color: #51afef;">import</span> lightgbm <span style="color: #51afef;">as</span> lgb
<span class="linenr">29: </span>
<span class="linenr">30: </span>
<span class="linenr">31: </span><span style="color: #dcaeea;">data</span> = pd.read_csv(<span style="color: #98be65;">"./credit_card.csv"</span>)
<span class="linenr">32: </span><span style="color: #dcaeea;">nanCounter</span> = pd.isna(data).<span style="color: #c678dd;">sum</span>() <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23559;np.isnan&#20197;pd.isna&#21462;&#20195;</span>
<span class="linenr">33: </span>
<span class="linenr">34: </span><span style="color: #dcaeea;">dataX</span> = data.copy().drop([<span style="color: #98be65;">'Class'</span>],axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">35: </span>dataY = data[<span style="color: #98be65;">'Class'</span>].copy()
<span class="linenr">36: </span>
<span class="linenr">37: </span><span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(dataX,
<span class="linenr">38: </span>                                    dataY, test_size=<span style="color: #da8548; font-weight: bold;">0.33</span>,
<span class="linenr">39: </span>                                    random_state=<span style="color: #da8548; font-weight: bold;">2018</span>, stratify=dataY)
<span class="linenr">40: </span>
<span class="linenr">41: </span>k_fold = StratifiedKFold(n_splits=<span style="color: #da8548; font-weight: bold;">5</span>,shuffle=<span style="color: #a9a1e1;">True</span>,random_state=<span style="color: #da8548; font-weight: bold;">2018</span>)
<span class="linenr">42: </span>
</pre>
</div>
</div>
</li>

<li><a id="orgc50f3ff"></a>設定超參數<br />
<div class="outline-text-5" id="text-5-6-3-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #dcaeea;">params_lightGB</span> = {
<span class="linenr"> 2: </span>    <span style="color: #98be65;">'task'</span>: <span style="color: #98be65;">'train'</span>,
<span class="linenr"> 3: </span>    <span style="color: #98be65;">'num_class'</span>:<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr"> 4: </span>    <span style="color: #98be65;">'boosting'</span>: <span style="color: #98be65;">'gbdt'</span>,
<span class="linenr"> 5: </span>    <span style="color: #98be65;">'objective'</span>: <span style="color: #98be65;">'binary'</span>,
<span class="linenr"> 6: </span>    <span style="color: #98be65;">'metric'</span>: <span style="color: #98be65;">'binary_logloss'</span>,
<span class="linenr"> 7: </span>    <span style="color: #98be65;">'metric_freq'</span>:<span style="color: #da8548; font-weight: bold;">50</span>,
<span class="linenr"> 8: </span>    <span style="color: #98be65;">'is_training_metric'</span>:<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 9: </span>    <span style="color: #98be65;">'max_depth'</span>:<span style="color: #da8548; font-weight: bold;">4</span>,
<span class="linenr">10: </span>    <span style="color: #98be65;">'num_leaves'</span>: <span style="color: #da8548; font-weight: bold;">31</span>,
<span class="linenr">11: </span>    <span style="color: #98be65;">'learning_rate'</span>: <span style="color: #da8548; font-weight: bold;">0.01</span>,
<span class="linenr">12: </span>    <span style="color: #98be65;">'feature_fraction'</span>: <span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">13: </span>    <span style="color: #98be65;">'bagging_fraction'</span>: <span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">14: </span>    <span style="color: #98be65;">'bagging_freq'</span>: <span style="color: #da8548; font-weight: bold;">0</span>,
<span class="linenr">15: </span>    <span style="color: #98be65;">'bagging_seed'</span>: <span style="color: #da8548; font-weight: bold;">2018</span>,
<span class="linenr">16: </span>    <span style="color: #98be65;">'verbose'</span>: -<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">17: </span>    <span style="color: #98be65;">'num_threads'</span>:<span style="color: #da8548; font-weight: bold;">16</span>
<span class="linenr">18: </span>}
</pre>
</div>
</div>
</li>

<li><a id="orgc369d62"></a>訓練模型<br />
<div class="outline-text-5" id="text-5-6-3-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">trainingScores</span> = []
<span class="linenr"> 3: </span><span style="color: #dcaeea;">cvScores</span> = []
<span class="linenr"> 4: </span><span style="color: #dcaeea;">predictionsBasedOnKFolds</span> = pd.DataFrame(data=[],
<span class="linenr"> 5: </span>                                index=y_train.index,columns=[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #51afef;">for</span> train_index, cv_index <span style="color: #51afef;">in</span> k_fold.split(np.zeros(<span style="color: #c678dd;">len</span>(X_train)),
<span class="linenr"> 8: </span>                                          y_train.ravel()):
<span class="linenr"> 9: </span>    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \
<span class="linenr">10: </span>        X_train.iloc[cv_index,:]
<span class="linenr">11: </span>    y_train_fold, y_cv_fold = y_train.iloc[train_index], \
<span class="linenr">12: </span>        y_train.iloc[cv_index]
<span class="linenr">13: </span>
<span class="linenr">14: </span>    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)
<span class="linenr">15: </span>    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)
<span class="linenr">16: </span>    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=<span style="color: #da8548; font-weight: bold;">2000</span>,
<span class="linenr">17: </span>                   valid_sets=lgb_eval, early_stopping_rounds=<span style="color: #da8548; font-weight: bold;">200</span>)
<span class="linenr">18: </span>
<span class="linenr">19: </span>    loglossTraining = log_loss(y_train_fold, \
<span class="linenr">20: </span>                gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))
<span class="linenr">21: </span>    trainingScores.append(loglossTraining)
<span class="linenr">22: </span>
<span class="linenr">23: </span>    predictionsBasedOnKFolds.loc[X_cv_fold.index,<span style="color: #98be65;">'prediction'</span>] = \
<span class="linenr">24: </span>        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration)
<span class="linenr">25: </span>    loglossCV = log_loss(y_cv_fold, \
<span class="linenr">26: </span>        predictionsBasedOnKFolds.loc[X_cv_fold.index,<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">27: </span>    cvScores.append(loglossCV)
<span class="linenr">28: </span>
<span class="linenr">29: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Training Log Loss: '</span>, loglossTraining)
<span class="linenr">30: </span>    <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'CV Log Loss: '</span>, loglossCV)
<span class="linenr">31: </span>
<span class="linenr">32: </span>loglossLightGBMGradientBoosting = \
<span class="linenr">33: </span>    log_loss(y_train, predictionsBasedOnKFolds.loc[:,<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">34: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'LightGBM Gradient Boosting Log Loss: '</span>, loglossLightGBMGradientBoosting)
</pre>
</div>

<pre class="example" id="org2986f14">
Training Log Loss:  0.0003947673377921407
CV Log Loss:  0.002662918492530588
Training Log Loss:  0.0004295118076208872
CV Log Loss:  0.002917044383361585
Training Log Loss:  0.0005351430169195703
CV Log Loss:  0.0026303365660141687
Training Log Loss:  0.0006123639756391712
CV Log Loss:  0.003627373853307207
LightGBM Gradient Boosting Log Loss:  0.002816067500582217
</pre>
</div>
</li>
<li><a id="orgacbc0f4"></a>評估結果<br />
<div class="outline-text-5" id="text-5-6-3-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>plt.cla()
<span class="linenr"> 2: </span><span style="color: #dcaeea;">reds</span> = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,<span style="color: #98be65;">'prediction'</span>]], axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 3: </span>preds.columns = [<span style="color: #98be65;">'trueLabel'</span>,<span style="color: #98be65;">'prediction'</span>]
<span class="linenr"> 4: </span>predictionsBasedOnKFoldsLightGBMGradientBoosting = preds.copy()
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">precision</span>, <span style="color: #dcaeea;">recall</span>, <span style="color: #dcaeea;">thresholds</span> = \
<span class="linenr"> 7: </span>    precision_recall_curve(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr"> 8: </span>average_precision = \
<span class="linenr"> 9: </span>    average_precision_score(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">10: </span>
<span class="linenr">11: </span>plt.step(recall, precision, color=<span style="color: #98be65;">'k'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.7</span>, where=<span style="color: #98be65;">'post'</span>)
<span class="linenr">12: </span>plt.fill_between(recall, precision, step=<span style="color: #98be65;">'post'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, color=<span style="color: #98be65;">'k'</span>)
<span class="linenr">13: </span>plt.savefig(<span style="color: #98be65;">'images/prec-recall-4.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">14: </span>plt.xlabel(<span style="color: #98be65;">'Recall'</span>)
<span class="linenr">15: </span>plt.ylabel(<span style="color: #98be65;">'Precision'</span>)
<span class="linenr">16: </span>plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr">17: </span>plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr">18: </span>
<span class="linenr">19: </span>plt.title(<span style="color: #98be65;">'Precision-Recall curve: Average Precision = {0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(
<span class="linenr">20: </span>          average_precision))
<span class="linenr">21: </span>
<span class="linenr">22: </span>fpr, tpr, thresholds = roc_curve(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'prediction'</span>])
<span class="linenr">23: </span>areaUnderROC = auc(fpr, tpr)
<span class="linenr">24: </span>
<span class="linenr">25: </span>plt.figure()
<span class="linenr">26: </span>plt.plot(fpr, tpr, color=<span style="color: #98be65;">'r'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, label=<span style="color: #98be65;">'ROC curve'</span>)
<span class="linenr">27: </span>plt.plot([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], color=<span style="color: #98be65;">'k'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, linestyle=<span style="color: #98be65;">'--'</span>)
<span class="linenr">28: </span>plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr">29: </span>plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr">30: </span>plt.xlabel(<span style="color: #98be65;">'False Positive Rate'</span>)
<span class="linenr">31: </span>plt.ylabel(<span style="color: #98be65;">'True Positive Rate'</span>)
<span class="linenr">32: </span>plt.title(<span style="color: #98be65;">'Receiver operating characteristic: \</span>
<span class="linenr">33: </span><span style="color: #98be65;">Area under the curve = {0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(areaUnderROC))
<span class="linenr">34: </span>plt.legend(loc=<span style="color: #98be65;">"lower right"</span>)
<span class="linenr">35: </span>plt.savefig(<span style="color: #98be65;">'images/auROC-4.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<p>
結果跑不出來，用session跑，除錯有問題，看不到過程
</p>

<div id="orgd1522b8" class="figure">
<p><img src="images/prec-recall-4.png" alt="prec-recall-4.png" width="500" />
</p>
<p><span class="figure-number">Figure 10: </span>Caption</p>
</div>
<ul class="org-ul">
<li>但隨機森林的auROC為0.92，不如Logistic Regression模型的0.97</li>
</ul>

<div id="org7b66ec4" class="figure">
<p><img src="images/auROC-4.png" alt="auROC-4.png" width="500" />
</p>
<p><span class="figure-number">Figure 11: </span>Caption</p>
</div>
</div>
</li>
</ol>
</div>
</div>
<div id="outline-container-org0e4cbe0" class="outline-3">
<h3 id="org0e4cbe0"><span class="section-number-3">5.7.</span> Stack</h3>
<div class="outline-text-3" id="text-5-7">
<p>
可以將不同家族的模型Stack來改善單一模型的效能，從每個單一模型中取得k-fold交叉驗證的預測結果（第一層預測），將這些預測結果加到原始訓練資料集中，再採用k-fold交叉驗證，利用原始的特徵和第一層預測資料集進行訓練。
</p>
</div>
<div id="outline-container-org2100f37" class="outline-4">
<h4 id="org2100f37"><span class="section-number-4">5.7.1.</span> 合併</h4>
<div class="outline-text-4" id="text-5-7-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Without XGBoost</span>
<span class="linenr"> 2: </span><span style="color: #dcaeea;">predictionsBasedOnKFoldsFourModels</span> = pd.DataFrame(data=[],index=y_train.index)
<span class="linenr"> 3: </span>predictionsBasedOnKFoldsFourModels = predictionsBasedOnKFoldsFourModels.join(
<span class="linenr"> 4: </span>    predictionsBasedOnKFoldsLogisticRegression[<span style="color: #98be65;">'prediction'</span>].astype(<span style="color: #c678dd;">float</span>), \
<span class="linenr"> 5: </span>    how=<span style="color: #98be65;">'left'</span>).join(predictionsBasedOnKFoldsRandomForests[<span style="color: #98be65;">'prediction'</span>] \
<span class="linenr"> 6: </span>    .astype(<span style="color: #c678dd;">float</span>),how=<span style="color: #98be65;">'left'</span>,rsuffix=<span style="color: #98be65;">"2"</span>).join( \
<span class="linenr"> 7: </span>    predictionsBasedOnKFoldsLightGBMGradientBoosting[<span style="color: #98be65;">'prediction'</span>].astype(<span style="color: #c678dd;">float</span>), \
<span class="linenr"> 8: </span>    how=<span style="color: #98be65;">'left'</span>,rsuffix=<span style="color: #98be65;">"4"</span>)
<span class="linenr"> 9: </span>predictionsBasedOnKFoldsFourModels.columns = \
<span class="linenr">10: </span>    [<span style="color: #98be65;">'predsLR'</span>,<span style="color: #98be65;">'predsRF'</span>,<span style="color: #98be65;">'predsLightGBM'</span>]
<span class="linenr">11: </span>
<span class="linenr">12: </span>predictionsBasedOnKFoldsFourModels = pd.DataFrame(data=[],index=y_train.index)
<span class="linenr">13: </span>predictionsBasedOnKFoldsFourModels = predictionsBasedOnKFoldsFourModels.join(
<span class="linenr">14: </span>    predictionsBasedOnKFoldsLogisticRegression[<span style="color: #98be65;">'prediction'</span>].astype(<span style="color: #c678dd;">float</span>), \
<span class="linenr">15: </span>    how=<span style="color: #98be65;">'left'</span>).join(predictionsBasedOnKFoldsRandomForests[<span style="color: #98be65;">'prediction'</span>] \
<span class="linenr">16: </span>    .astype(<span style="color: #c678dd;">float</span>),how=<span style="color: #98be65;">'left'</span>,rsuffix=<span style="color: #98be65;">"2"</span>).join( \
<span class="linenr">17: </span>    predictionsBasedOnKFoldsXGBoostGradientBoosting[<span style="color: #98be65;">'prediction'</span>].astype(<span style="color: #c678dd;">float</span>), \
<span class="linenr">18: </span>    how=<span style="color: #98be65;">'left'</span>,rsuffix=<span style="color: #98be65;">"3"</span>).join( \
<span class="linenr">19: </span>    predictionsBasedOnKFoldsLightGBMGradientBoosting[<span style="color: #98be65;">'prediction'</span>].astype(<span style="color: #c678dd;">float</span>), \
<span class="linenr">20: </span>    how=<span style="color: #98be65;">'left'</span>,rsuffix=<span style="color: #98be65;">"4"</span>)
<span class="linenr">21: </span>predictionsBasedOnKFoldsFourModels.columns = \
<span class="linenr">22: </span>    [<span style="color: #98be65;">'predsLR'</span>,<span style="color: #98be65;">'predsRF'</span>,<span style="color: #98be65;">'predsXGB'</span>,<span style="color: #98be65;">'predsLightGBM'</span>]
<span class="linenr">23: </span>X_trainWithPredictions = \
<span class="linenr">24: </span>    X_train.merge(predictionsBasedOnKFoldsFourModels,
<span class="linenr">25: </span>                  left_index=<span style="color: #a9a1e1;">True</span>,right_index=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">26: </span>params_lightGB = {
<span class="linenr">27: </span>    <span style="color: #98be65;">'task'</span>: <span style="color: #98be65;">'train'</span>,
<span class="linenr">28: </span>    <span style="color: #98be65;">'num_class'</span>:<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">29: </span>    <span style="color: #98be65;">'boosting'</span>: <span style="color: #98be65;">'gbdt'</span>,
<span class="linenr">30: </span>    <span style="color: #98be65;">'objective'</span>: <span style="color: #98be65;">'binary'</span>,
<span class="linenr">31: </span>    <span style="color: #98be65;">'metric'</span>: <span style="color: #98be65;">'binary_logloss'</span>,
<span class="linenr">32: </span>    <span style="color: #98be65;">'metric_freq'</span>:<span style="color: #da8548; font-weight: bold;">50</span>,
<span class="linenr">33: </span>    <span style="color: #98be65;">'is_training_metric'</span>:<span style="color: #a9a1e1;">False</span>,
<span class="linenr">34: </span>    <span style="color: #98be65;">'max_depth'</span>:<span style="color: #da8548; font-weight: bold;">4</span>,
<span class="linenr">35: </span>    <span style="color: #98be65;">'num_leaves'</span>: <span style="color: #da8548; font-weight: bold;">31</span>,
<span class="linenr">36: </span>    <span style="color: #98be65;">'learning_rate'</span>: <span style="color: #da8548; font-weight: bold;">0.01</span>,
<span class="linenr">37: </span>    <span style="color: #98be65;">'feature_fraction'</span>: <span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">38: </span>    <span style="color: #98be65;">'bagging_fraction'</span>: <span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">39: </span>    <span style="color: #98be65;">'bagging_freq'</span>: <span style="color: #da8548; font-weight: bold;">0</span>,
<span class="linenr">40: </span>    <span style="color: #98be65;">'bagging_seed'</span>: <span style="color: #da8548; font-weight: bold;">2018</span>,
<span class="linenr">41: </span>    <span style="color: #98be65;">'verbose'</span>: -<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">42: </span>    <span style="color: #98be65;">'num_threads'</span>:<span style="color: #da8548; font-weight: bold;">16</span>
<span class="linenr">43: </span>}
</pre>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org98ea72b" class="outline-2">
<h2 id="org98ea72b"><span class="section-number-2">6.</span> 降維</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-org0118d3d" class="outline-3">
<h3 id="org0118d3d"><span class="section-number-3">6.1.</span> 讀入資料</h3>
<div class="outline-text-3" id="text-6-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #83898d;">'''Main'''</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> os, time, pickle, gzip
<span class="linenr"> 5: </span><span style="color: #51afef;">import</span> datetime
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span><span style="color: #98be65;">'''Data Prep'''</span>
<span class="linenr"> 8: </span><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> preprocessing <span style="color: #51afef;">as</span> pp
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #98be65;">'''Data Viz'''</span>
<span class="linenr">11: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">12: </span><span style="color: #51afef;">import</span> matplotlib <span style="color: #51afef;">as</span> mpl
<span class="linenr">13: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr">14: </span><span style="color: #dcaeea;">color</span> = sns.color_palette()
<span class="linenr">15: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Load the datasets</span>
<span class="linenr">16: </span><span style="color: #dcaeea;">current_path</span> = os.getcwd()
<span class="linenr">17: </span><span style="color: #c678dd;">file</span> = os.path.sep.join([<span style="color: #98be65;">''</span>, <span style="color: #98be65;">'dataset'</span>, <span style="color: #98be65;">'mnist.pkl.gz'</span>])
<span class="linenr">18: </span>
<span class="linenr">19: </span><span style="color: #dcaeea;">f</span> = gzip.<span style="color: #c678dd;">open</span>(current_path+<span style="color: #c678dd;">file</span>, <span style="color: #98be65;">'rb'</span>)
<span class="linenr">20: </span><span style="color: #dcaeea;">train_set</span>, <span style="color: #dcaeea;">validation_set</span>, <span style="color: #dcaeea;">test_set</span> = pickle.load(f, encoding=<span style="color: #98be65;">'latin1'</span>)
<span class="linenr">21: </span>f.close()
<span class="linenr">22: </span>
<span class="linenr">23: </span>X_train, y_train = train_set[<span style="color: #da8548; font-weight: bold;">0</span>], train_set[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">24: </span>X_validation, y_validation = validation_set[<span style="color: #da8548; font-weight: bold;">0</span>], validation_set[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">25: </span>X_test, y_test = test_set[<span style="color: #da8548; font-weight: bold;">0</span>], test_set[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">26: </span>
<span class="linenr">27: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create Pandas DataFrames from the datasets</span>
<span class="linenr">28: </span>train_index = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #c678dd;">len</span>(X_train))
<span class="linenr">29: </span>validation_index = <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(X_train), \
<span class="linenr">30: </span>                         <span style="color: #c678dd;">len</span>(X_train)+<span style="color: #c678dd;">len</span>(X_validation))
<span class="linenr">31: </span>test_index = <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(X_train)+<span style="color: #c678dd;">len</span>(X_validation), \
<span class="linenr">32: </span>                   <span style="color: #c678dd;">len</span>(X_train)+<span style="color: #c678dd;">len</span>(X_validation)+<span style="color: #c678dd;">len</span>(X_test))
<span class="linenr">33: </span>
<span class="linenr">34: </span>X_train = pd.DataFrame(data=X_train,index=train_index)
<span class="linenr">35: </span>y_train = pd.Series(data=y_train,index=train_index)
<span class="linenr">36: </span>
<span class="linenr">37: </span>X_validation = pd.DataFrame(data=X_validation,index=validation_index)
<span class="linenr">38: </span>y_validation = pd.Series(data=y_validation,index=validation_index)
<span class="linenr">39: </span>
<span class="linenr">40: </span>X_test = pd.DataFrame(data=X_test,index=test_index)
<span class="linenr">41: </span>y_test = pd.Series(data=y_test,index=test_index)
<span class="linenr">42: </span>
<span class="linenr">43: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">one_hot</span>(series):
<span class="linenr">44: </span>    label_binarizer = pp.LabelBinarizer()
<span class="linenr">45: </span>    label_binarizer.fit(<span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">max</span>(series)+<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">46: </span>    <span style="color: #51afef;">return</span> label_binarizer.transform(series)
<span class="linenr">47: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Define reversal of one-hot encoder function</span>
<span class="linenr">48: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">reverse_one_hot</span>(originalSeries, newSeries):
<span class="linenr">49: </span>    label_binarizer = pp.LabelBinarizer()
<span class="linenr">50: </span>    label_binarizer.fit(<span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">max</span>(originalSeries)+<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">51: </span>    <span style="color: #51afef;">return</span> label_binarizer.inverse_transform(newSeries)
<span class="linenr">52: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create one-hot vectors for the labels</span>
<span class="linenr">53: </span>y_train_oneHot = one_hot(y_train)
<span class="linenr">54: </span>y_validation_oneHot = one_hot(y_validation)
<span class="linenr">55: </span>y_test_oneHot = one_hot(y_test)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgeeb3a12" class="outline-3">
<h3 id="orgeeb3a12"><span class="section-number-3">6.2.</span> 線性投影</h3>
</div>
<div id="outline-container-orgfe85077" class="outline-3">
<h3 id="orgfe85077"><span class="section-number-3">6.3.</span> 主成分分析</h3>
<div class="outline-text-3" id="text-6-3">
</div>
<div id="outline-container-org6ff2bca" class="outline-4">
<h4 id="org6ff2bca"><span class="section-number-4">6.3.1.</span> PCA</h4>
<div class="outline-text-4" id="text-6-3-1">
<p>
PCA會找資料在低維度空間的表示方法，同時盡可能保留資料的變異性。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Principal Component Analysis</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">784</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">whiten</span> = <span style="color: #a9a1e1;">False</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">pca</span> = PCA(n_components=n_components, whiten=whiten, \
<span class="linenr"> 9: </span>          random_state=random_state)
<span class="linenr">10: </span>
<span class="linenr">11: </span>X_train_PCA = pca.fit_transform(X_train)
<span class="linenr">12: </span>X_train_PCA = pd.DataFrame(data=X_train_PCA, index=train_index)
<span class="linenr">13: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Percentage of Variance Captured by 784 principal components</span>
<span class="linenr">14: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Variance Explained by all 784 principal components: "</span>, \
<span class="linenr">15: </span>      <span style="color: #c678dd;">sum</span>(pca.explained_variance_ratio_))
<span class="linenr">16: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Percentage of Variance Captured by X principal components</span>
<span class="linenr">17: </span>importanceOfPrincipalComponents = \
<span class="linenr">18: </span>    pd.DataFrame(data=pca.explained_variance_ratio_)
<span class="linenr">19: </span>importanceOfPrincipalComponents = importanceOfPrincipalComponents.T
<span class="linenr">20: </span>
<span class="linenr">21: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Variance Captured by First 10 Principal Components: '</span>,
<span class="linenr">22: </span>      importanceOfPrincipalComponents.loc[:,<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">9</span>].<span style="color: #c678dd;">sum</span>(axis=<span style="color: #da8548; font-weight: bold;">1</span>).values)
<span class="linenr">23: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Variance Captured by First 20 Principal Components: '</span>,
<span class="linenr">24: </span>      importanceOfPrincipalComponents.loc[:,<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">19</span>].<span style="color: #c678dd;">sum</span>(axis=<span style="color: #da8548; font-weight: bold;">1</span>).values)
<span class="linenr">25: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Variance Captured by First 50 Principal Components: '</span>,
<span class="linenr">26: </span>      importanceOfPrincipalComponents.loc[:,<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">49</span>].<span style="color: #c678dd;">sum</span>(axis=<span style="color: #da8548; font-weight: bold;">1</span>).values)
<span class="linenr">27: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Variance Captured by First 100 Principal Components: '</span>,
<span class="linenr">28: </span>      importanceOfPrincipalComponents.loc[:,<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">99</span>].<span style="color: #c678dd;">sum</span>(axis=<span style="color: #da8548; font-weight: bold;">1</span>).values)
<span class="linenr">29: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Variance Captured by First 200 Principal Components: '</span>,
<span class="linenr">30: </span>      importanceOfPrincipalComponents.loc[:,<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">199</span>].<span style="color: #c678dd;">sum</span>(axis=<span style="color: #da8548; font-weight: bold;">1</span>).values)
<span class="linenr">31: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Variance Captured by First 300 Principal Components: '</span>,
<span class="linenr">32: </span>      importanceOfPrincipalComponents.loc[:,<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">299</span>].<span style="color: #c678dd;">sum</span>(axis=<span style="color: #da8548; font-weight: bold;">1</span>).values)
</pre>
</div>

<pre class="example">
Variance Explained by all 784 principal components:  0.9999999571231906
Variance Captured by First 10 Principal Components:  [0.48876157]
Variance Captured by First 20 Principal Components:  [0.6439795]
Variance Captured by First 50 Principal Components:  [0.8248605]
Variance Captured by First 100 Principal Components:  [0.91465825]
Variance Captured by First 200 Principal Components:  [0.9665006]
Variance Captured by First 300 Principal Components:  [0.9862488]
</pre>

<p>
由結果看，若將MNIST的原始784個特徵值縮減至300個，仍有近99%的解釋力，即，能捕捉到99%的變異量。PCA能讓我們縮減原始資料的維度，同時保持最多的顯著資訊。
</p>

<p>
如果只拿第1、第二個主成份特徵來進行預測，圖示結果如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Define scatterplot function</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">scatterPlot</span>(xDF, yDF, algoName):
<span class="linenr"> 3: </span>    <span style="color: #dcaeea;">tempDF</span> = pd.DataFrame(data=xDF.loc[:,<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">1</span>], index=xDF.index)
<span class="linenr"> 4: </span>    tempDF = pd.concat((tempDF,yDF), axis=<span style="color: #da8548; font-weight: bold;">1</span>, join=<span style="color: #98be65;">"inner"</span>)
<span class="linenr"> 5: </span>    tempDF.columns = [<span style="color: #98be65;">"First Vector"</span>, <span style="color: #98be65;">"Second Vector"</span>, <span style="color: #98be65;">"Label"</span>]
<span class="linenr"> 6: </span>    sns.lmplot(x=<span style="color: #98be65;">"First Vector"</span>, y=<span style="color: #98be65;">"Second Vector"</span>, hue=<span style="color: #98be65;">"Label"</span>, \
<span class="linenr"> 7: </span>               data=tempDF, fit_reg=<span style="color: #a9a1e1;">False</span>)
<span class="linenr"> 8: </span>    ax = plt.gca()
<span class="linenr"> 9: </span>    ax.set_title(<span style="color: #98be65;">"Separation of Observations using "</span>+algoName)
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">View scatterplot</span>
<span class="linenr">12: </span>scatterPlot(X_train_PCA, y_train, <span style="color: #98be65;">"PCA"</span>)
<span class="linenr">13: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">14: </span>plt.savefig(<span style="color: #98be65;">'images/PCA-MNIST-1.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="orgf487d3d" class="figure">
<p><img src="images/PCA-MNIST-1.png" alt="PCA-MNIST-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 12: </span>Caption</p>
</div>

<p>
由上圖可以看出PCA光找出最有價值的兩個特徵值就能對大致區分數0~9的不同類別，這在非監督式學習中是大分有用的。當資料集有數百萬個特徵、數十億筆資籵時，PCA可以大幅減少機器學習的訓練時間。
</p>
</div>
</div>
<div id="outline-container-org0fb5d13" class="outline-4">
<h4 id="org0fb5d13"><span class="section-number-4">6.3.2.</span> Incremental PCA</h4>
<div class="outline-text-4" id="text-6-3-2">
<p>
當資枓集大到無法載入記憶體時，可以小批次的遞增使用PCA，將資料集逐批送入記憶體，其結果與PCA相仿。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Incremental PCA</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> IncrementalPCA
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">784</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">batch_size</span> = none
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">incrementalPCA</span> = IncrementalPCA(n_components=n_components, \
<span class="linenr"> 9: </span>                                batch_size=batch_size)
<span class="linenr">10: </span>
<span class="linenr">11: </span>X_train_incrementalPCA = incrementalPCA.fit_transform(X_train)
<span class="linenr">12: </span>X_train_incrementalPCA = \
<span class="linenr">13: </span>    pd.DataFrame(data=X_train_incrementalPCA, index=train_index)
<span class="linenr">14: </span>
<span class="linenr">15: </span>X_validation_incrementalPCA = incrementalPCA.transform(X_validation)
<span class="linenr">16: </span>X_validation_incrementalPCA = \
<span class="linenr">17: </span>    pd.DataFrame(data=X_validation_incrementalPCA, index=validation_index)
<span class="linenr">18: </span>
<span class="linenr">19: </span>scatterPlot(X_train_incrementalPCA, y_train, <span style="color: #98be65;">"Incremental PCA"</span>)
<span class="linenr">20: </span>plt.savefig(<span style="color: #98be65;">'images/PCA-MNIST-2.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="org9a8d43e" class="figure">
<p><img src="images/PCA-MNIST-2.png" alt="PCA-MNIST-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 13: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-org91c5379" class="outline-4">
<h4 id="org91c5379"><span class="section-number-4">6.3.3.</span> Sparse PCA</h4>
<div class="outline-text-4" id="text-6-3-3">
<p>
一般的PCA希望儘量縮小特徵空間，提高空間中資枓點的密度。但有些機器學習可能需要讓資料點的密度更稀疏，此時可使用Sparse PCA，其稀疏程度中aplha控制。
</p>
<ul class="org-ul">
<li>計算速度會較慢，故只取10000個樣本訓練</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Sparse PCA</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> SparsePCA
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">alpha</span> = <span style="color: #da8548; font-weight: bold;">0.0001</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">n_jobs</span> = -<span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #dcaeea;">sparsePCA</span> = SparsePCA(n_components=n_components, \
<span class="linenr">11: </span>                alpha=alpha, random_state=random_state, n_jobs=n_jobs)
<span class="linenr">12: </span>
<span class="linenr">13: </span>sparsePCA.fit(X_train.loc[:<span style="color: #da8548; font-weight: bold;">10000</span>,:])
<span class="linenr">14: </span>X_train_sparsePCA = sparsePCA.transform(X_train)
<span class="linenr">15: </span>X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=train_index)
<span class="linenr">16: </span>
<span class="linenr">17: </span>X_validation_sparsePCA = sparsePCA.transform(X_validation)
<span class="linenr">18: </span>X_validation_sparsePCA = \
<span class="linenr">19: </span>    pd.DataFrame(data=X_validation_sparsePCA, index=validation_index)
<span class="linenr">20: </span>
<span class="linenr">21: </span>scatterPlot(X_train_sparsePCA, y_train, <span style="color: #98be65;">"Sparse PCA"</span>)
<span class="linenr">22: </span>plt.savefig(<span style="color: #98be65;">'images/PCA-MNIST-3.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">23: </span>
</pre>
</div>


<div id="orga4d4e4a" class="figure">
<p><img src="images/PCA-MNIST-3.png" alt="PCA-MNIST-3.png" width="500" />
</p>
<p><span class="figure-number">Figure 14: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-org4fcdaff" class="outline-4">
<h4 id="org4fcdaff"><span class="section-number-4">6.3.4.</span> Kernel PCA</h4>
<div class="outline-text-4" id="text-6-3-4">
<p>
非線性投影PCA，透過學習相似度函數(kernel function)，kernel PCA找出大多數資枓點聚集的隱含特徵空間，使用kernel PCA需要設定預期的成分數量、kernel的型態、kernel的係數(gamma)，常見的kernel PCA有radial basis function kernel、RBF kernel。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Kernel PCA</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> KernelPCA
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">kernel</span> = <span style="color: #98be65;">'rbf'</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">gamma</span> = <span style="color: #a9a1e1;">None</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">n_jobs</span> = <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #dcaeea;">kernelPCA</span> = KernelPCA(n_components=n_components, kernel=kernel, \
<span class="linenr">12: </span>                      gamma=gamma, n_jobs=n_jobs, random_state=random_state)
<span class="linenr">13: </span>
<span class="linenr">14: </span>kernelPCA.fit(X_train.loc[:<span style="color: #da8548; font-weight: bold;">10000</span>,:])
<span class="linenr">15: </span>X_train_kernelPCA = kernelPCA.transform(X_train)
<span class="linenr">16: </span>X_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA,index=train_index)
<span class="linenr">17: </span>
<span class="linenr">18: </span>X_validation_kernelPCA = kernelPCA.transform(X_validation)
<span class="linenr">19: </span>X_validation_kernelPCA = \
<span class="linenr">20: </span>    pd.DataFrame(data=X_validation_kernelPCA, index=validation_index)
<span class="linenr">21: </span>
<span class="linenr">22: </span>scatterPlot(X_train_kernelPCA, y_train, <span style="color: #98be65;">"Kernel PCA"</span>)
<span class="linenr">23: </span>plt.savefig(<span style="color: #98be65;">'images/PCA-MNIST-4.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<pre class="example">
Python 3.8.12 (default, Oct 12 2021, 06:23:56)
[Clang 10.0.0 ] :: Anaconda, Inc. on darwin
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt;
</pre>



<div id="orgb70a903" class="figure">
<p><img src="images/PCA-MNIST-4.png" alt="PCA-MNIST-4.png" width="500" />
</p>
<p><span class="figure-number">Figure 15: </span>Caption</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org5090417" class="outline-3">
<h3 id="org5090417"><span class="section-number-3">6.4.</span> 奇異值分解</h3>
<div class="outline-text-3" id="text-6-4">
<p>
目的在減少原始特徵值矩陣的秩
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Singular Value Decomposition</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> TruncatedSVD
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">200</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">algorithm</span> = <span style="color: #98be65;">'randomized'</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">n_iter</span> = <span style="color: #da8548; font-weight: bold;">5</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #dcaeea;">svd</span> = TruncatedSVD(n_components=n_components, algorithm=algorithm, \
<span class="linenr">11: </span>                   n_iter=n_iter, random_state=random_state)
<span class="linenr">12: </span>
<span class="linenr">13: </span>X_train_svd = svd.fit_transform(X_train)
<span class="linenr">14: </span>X_train_svd = pd.DataFrame(data=X_train_svd, index=train_index)
<span class="linenr">15: </span>
<span class="linenr">16: </span>X_validation_svd = svd.transform(X_validation)
<span class="linenr">17: </span>X_validation_svd = pd.DataFrame(data=X_validation_svd, index=validation_index)
<span class="linenr">18: </span>
<span class="linenr">19: </span>scatterPlot(X_train_svd, y_train, <span style="color: #98be65;">"Singular Value Decomposition"</span>)
<span class="linenr">20: </span>plt.savefig(<span style="color: #98be65;">'images/SVD-MNIST.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<div id="org7f69b90" class="figure">
<p><img src="images/SVD-MNIST.png" alt="SVD-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 16: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-orgd96c9a1" class="outline-3">
<h3 id="orgd96c9a1"><span class="section-number-3">6.5.</span> 隨機投影</h3>
<div class="outline-text-3" id="text-6-5">
<p>
也是線性投影，將高維度空間裡的點嵌到較低維度的空間中，但仍維持點與點間的距離。有兩種版本：
</p>
<ul class="org-ul">
<li>高斯隨機投影</li>
<li>稀疏隨機投影</li>
</ul>
</div>
<div id="outline-container-org6c13eac" class="outline-4">
<h4 id="org6c13eac"><span class="section-number-4">6.5.1.</span> 高斯隨機投影(Gaussian Random Projection)</h4>
<div class="outline-text-4" id="text-6-5-1">
<p>
可以指定在縮減的特徵空間中想要擁有的元素數量(eps值), eps控制了嵌入的品質，其值越高、維度數量也越高。
</p>
<ul class="org-ul">
<li>實驗結果：改eps後看不出來圖的差異&#x2026;.</li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org2bedfa7"></a>pes=0.4<br />
<div class="outline-text-5" id="text-6-5-1-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Gaussian Random Projection</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.random_projection <span style="color: #51afef;">import</span> GaussianRandomProjection
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #98be65;">'auto'</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">eps</span> = <span style="color: #da8548; font-weight: bold;">0.01</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">GRP</span> = GaussianRandomProjection(n_components=n_components, eps=eps, \
<span class="linenr">10: </span>                               random_state=random_state)
<span class="linenr">11: </span>
<span class="linenr">12: </span>X_train_GRP = GRP.fit_transform(X_train)
<span class="linenr">13: </span>X_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)
<span class="linenr">14: </span>
<span class="linenr">15: </span>X_validation_GRP = GRP.transform(X_validation)
<span class="linenr">16: </span>X_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)
<span class="linenr">17: </span>
<span class="linenr">18: </span>scatterPlot(X_train_GRP, y_train, <span style="color: #98be65;">"Gaussian Random Projection"</span>)
<span class="linenr">19: </span>plt.savefig(<span style="color: #98be65;">'images/GRP-MNIST-1.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="org993e9a7" class="figure">
<p><img src="images/GRP-MNIST-1.png" alt="GRP-MNIST-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 17: </span>Caption</p>
</div>
</div>
</li>
<li><a id="org4aceefe"></a>eps=0.8<br />
<div class="outline-text-5" id="text-6-5-1-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Gaussian Random Projection</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.random_projection <span style="color: #51afef;">import</span> GaussianRandomProjection
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #98be65;">'auto'</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">eps</span> = <span style="color: #da8548; font-weight: bold;">0.95</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">GRP</span> = GaussianRandomProjection(n_components=n_components, eps=eps, \
<span class="linenr">10: </span>                               random_state=random_state)
<span class="linenr">11: </span>
<span class="linenr">12: </span>X_train_GRP = GRP.fit_transform(X_train)
<span class="linenr">13: </span>X_train_GRP = pd.DataFrame(data=X_train_GRP, index=train_index)
<span class="linenr">14: </span>
<span class="linenr">15: </span>X_validation_GRP = GRP.transform(X_validation)
<span class="linenr">16: </span>X_validation_GRP = pd.DataFrame(data=X_validation_GRP, index=validation_index)
<span class="linenr">17: </span>
<span class="linenr">18: </span>scatterPlot(X_train_GRP, y_train, <span style="color: #98be65;">"Gaussian Random Projection"</span>)
<span class="linenr">19: </span>plt.savefig(<span style="color: #98be65;">'images/GRP-MNIST-2.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="org441aef2" class="figure">
<p><img src="images/GRP-MNIST-2.png" alt="GRP-MNIST-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 18: </span>Caption</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-orgefb5b90" class="outline-4">
<h4 id="orgefb5b90"><span class="section-number-4">6.5.2.</span> 稀疏矩陣投影</h4>
<div class="outline-text-4" id="text-6-5-2">
<p>
在轉換過程中保留了一定程度的資料點稀疏度，也較有效率
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Sparse Random Projection</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.random_projection <span style="color: #51afef;">import</span> SparseRandomProjection
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #98be65;">'auto'</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">density</span> = <span style="color: #98be65;">'auto'</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">eps</span> = <span style="color: #da8548; font-weight: bold;">0.5</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">dense_output</span> = <span style="color: #a9a1e1;">False</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #dcaeea;">SRP</span> = SparseRandomProjection(n_components=n_components, \
<span class="linenr">12: </span>        density=density, eps=eps, dense_output=dense_output, \
<span class="linenr">13: </span>        random_state=random_state)
<span class="linenr">14: </span>
<span class="linenr">15: </span>X_train_SRP = SRP.fit_transform(X_train)
<span class="linenr">16: </span>X_train_SRP = pd.DataFrame(data=X_train_SRP, index=train_index)
<span class="linenr">17: </span>
<span class="linenr">18: </span>X_validation_SRP = SRP.transform(X_validation)
<span class="linenr">19: </span>X_validation_SRP = pd.DataFrame(data=X_validation_SRP, index=validation_index)
<span class="linenr">20: </span>
<span class="linenr">21: </span>scatterPlot(X_train_SRP, y_train, <span style="color: #98be65;">"Sparse Random Projection"</span>)
<span class="linenr">22: </span>plt.savefig(<span style="color: #98be65;">'images/SRP-MNIST.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<pre class="example">
fig = plt.figure(figsize=figsize)
</pre>



<div id="orgbfcb9b8" class="figure">
<p><img src="images/SRP-MNIST.png" alt="SRP-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 19: </span>Caption</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orge218aff" class="outline-3">
<h3 id="orge218aff"><span class="section-number-3">6.6.</span> Isomap</h3>
<div class="outline-text-3" id="text-6-6">
<p>
非線性投影，基本的流形學習方法為isometric mapping，簡稱isomap。Isomap透過計算點與點間的成對距離（曲線距離或捷線距離，而非歐幾里德距離）來學習能代表原始特徵集的一個新低維度embedding。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Isomap</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.manifold <span style="color: #51afef;">import</span> Isomap
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_neighbors</span> = <span style="color: #da8548; font-weight: bold;">5</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">10</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">n_jobs</span> = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">isomap</span> = Isomap(n_neighbors=n_neighbors, \
<span class="linenr">10: </span>                n_components=n_components, n_jobs=n_jobs)
<span class="linenr">11: </span>
<span class="linenr">12: </span>isomap.fit(X_train.loc[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">5000</span>,:])
<span class="linenr">13: </span>X_train_isomap = isomap.transform(X_train)
<span class="linenr">14: </span>X_train_isomap = pd.DataFrame(data=X_train_isomap, index=train_index)
<span class="linenr">15: </span>
<span class="linenr">16: </span>X_validation_isomap = isomap.transform(X_validation)
<span class="linenr">17: </span>X_validation_isomap = pd.DataFrame(data=X_validation_isomap, \
<span class="linenr">18: </span>                                   index=validation_index)
<span class="linenr">19: </span>
<span class="linenr">20: </span>scatterPlot(X_train_isomap, y_train, <span style="color: #98be65;">"Isomap"</span>)
<span class="linenr">21: </span>plt.savefig(<span style="color: #98be65;">'images/ISOMAP-MNIST.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="org3f45130" class="figure">
<p><img src="images/ISOMAP-MNIST.png" alt="ISOMAP-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 20: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-orgf4bf3c5" class="outline-3">
<h3 id="orgf4bf3c5"><span class="section-number-3">6.7.</span> 多維標度(Multidimensional Scaling)</h3>
<div class="outline-text-3" id="text-6-7">
<p>
MDS，學習原始資料集點點與點間的相似度，將相似度塑模至低維度空間
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Multidimensional Scaling</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.manifold <span style="color: #51afef;">import</span> MDS
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">n_init</span> = <span style="color: #da8548; font-weight: bold;">12</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">max_iter</span> = <span style="color: #da8548; font-weight: bold;">1200</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">metric</span> = <span style="color: #a9a1e1;">True</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">n_jobs</span> = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr">10: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span><span style="color: #dcaeea;">mds</span> = MDS(n_components=n_components, n_init=n_init, max_iter=max_iter, \
<span class="linenr">13: </span>          metric=metric, n_jobs=n_jobs, random_state=random_state)
<span class="linenr">14: </span>
<span class="linenr">15: </span>X_train_mds = mds.fit_transform(X_train.loc[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">1000</span>,:])
<span class="linenr">16: </span>X_train_mds = pd.DataFrame(data=X_train_mds, index=train_index[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">1001</span>])
<span class="linenr">17: </span>
<span class="linenr">18: </span>scatterPlot(X_train_mds, y_train, <span style="color: #98be65;">"Multidimensional Scaling"</span>)
<span class="linenr">19: </span>plt.savefig(<span style="color: #98be65;">'images/MDS-MNIST.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="org2637f66" class="figure">
<p><img src="images/MDS-MNIST.png" alt="MDS-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 21: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-org284fa6a" class="outline-3">
<h3 id="org284fa6a"><span class="section-number-3">6.8.</span> 局部線性嵌入法(Locally Linear Embedding)</h3>
<div class="outline-text-3" id="text-6-8">
<p>
LLE透過以下方式來找出高維資枓中的非線性結構
</p>
<ul class="org-ul">
<li>分割資料成為較小的子集（包含數個點的鄰近區域）</li>
<li>將每個子集塑模成一個線性的embedding</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Locally Linear Embedding (LLE)</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.manifold <span style="color: #51afef;">import</span> LocallyLinearEmbedding
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_neighbors</span> = <span style="color: #da8548; font-weight: bold;">10</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">method</span> = <span style="color: #98be65;">'modified'</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">n_jobs</span> = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #dcaeea;">lle</span> = LocallyLinearEmbedding(n_neighbors=n_neighbors, \
<span class="linenr">12: </span>        n_components=n_components, method=method, \
<span class="linenr">13: </span>        random_state=random_state, n_jobs=n_jobs)
<span class="linenr">14: </span>
<span class="linenr">15: </span>lle.fit(X_train.loc[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">5000</span>,:])
<span class="linenr">16: </span>X_train_lle = lle.transform(X_train)
<span class="linenr">17: </span>X_train_lle = pd.DataFrame(data=X_train_lle, index=train_index)
<span class="linenr">18: </span>
<span class="linenr">19: </span>X_validation_lle = lle.transform(X_validation)
<span class="linenr">20: </span>X_validation_lle = pd.DataFrame(data=X_validation_lle, index=validation_index)
<span class="linenr">21: </span>
<span class="linenr">22: </span>scatterPlot(X_train_lle, y_train, <span style="color: #98be65;">"Locally Linear Embedding"</span>)
<span class="linenr">23: </span>plt.savefig(<span style="color: #98be65;">'images/LLE-MNIST.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<div id="org5254e6a" class="figure">
<p><img src="images/LLE-MNIST.png" alt="LLE-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 22: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-orgbcc024e" class="outline-3">
<h3 id="orgbcc024e"><span class="section-number-3">6.9.</span> t-Distributed Stochastic Neighbor Embedding</h3>
<div class="outline-text-3" id="text-6-9">
<p>
t-SNE建立兩個機率分佈來將高維資料點塑模至二維或三維空間，並使在此空間中彼此相似的點靠近、不相似的點疏遠。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">t-SNE</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.manifold <span style="color: #51afef;">import</span> TSNE
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">learning_rate</span> = <span style="color: #da8548; font-weight: bold;">300</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">perplexity</span> = <span style="color: #da8548; font-weight: bold;">30</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">early_exaggeration</span> = <span style="color: #da8548; font-weight: bold;">12</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">init</span> = <span style="color: #98be65;">'random'</span>
<span class="linenr">10: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span><span style="color: #dcaeea;">tSNE</span> = TSNE(n_components=n_components, learning_rate=learning_rate, \
<span class="linenr">13: </span>            perplexity=perplexity, early_exaggeration=early_exaggeration, \
<span class="linenr">14: </span>            init=init, random_state=random_state)
<span class="linenr">15: </span>
<span class="linenr">16: </span>X_train_tSNE = tSNE.fit_transform(X_train_PCA.loc[:<span style="color: #da8548; font-weight: bold;">5000</span>,:<span style="color: #da8548; font-weight: bold;">9</span>])
<span class="linenr">17: </span>X_train_tSNE = pd.DataFrame(data=X_train_tSNE, index=train_index[:<span style="color: #da8548; font-weight: bold;">5001</span>])
<span class="linenr">18: </span>
<span class="linenr">19: </span>scatterPlot(X_train_tSNE, y_train, <span style="color: #98be65;">"t-SNE"</span>)
<span class="linenr">20: </span>plt.savefig(<span style="color: #98be65;">'images/tSNE-MNIST.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>


<div id="org04059ed" class="figure">
<p><img src="images/tSNE-MNIST.png" alt="tSNE-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 23: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-org0217ac3" class="outline-3">
<h3 id="org0217ac3"><span class="section-number-3">6.10.</span> 字典學習</h3>
<div class="outline-text-3" id="text-6-10">
<p>
不依賴幾何指標或距離指標
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Mini-batch dictionary learning</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> MiniBatchDictionaryLearning
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">50</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">alpha</span> = <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">batch_size</span> = <span style="color: #da8548; font-weight: bold;">200</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">n_iter</span> = <span style="color: #da8548; font-weight: bold;">25</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #dcaeea;">miniBatchDictLearning</span> = MiniBatchDictionaryLearning( \
<span class="linenr">12: </span>                        n_components=n_components, alpha=alpha, \
<span class="linenr">13: </span>                        batch_size=batch_size, n_iter=n_iter, \
<span class="linenr">14: </span>                        random_state=random_state)
<span class="linenr">15: </span>
<span class="linenr">16: </span>miniBatchDictLearning.fit(X_train.loc[:,:<span style="color: #da8548; font-weight: bold;">10000</span>])
<span class="linenr">17: </span>X_train_miniBatchDictLearning = miniBatchDictLearning.fit_transform(X_train)
<span class="linenr">18: </span>X_train_miniBatchDictLearning = pd.DataFrame( \
<span class="linenr">19: </span>    data=X_train_miniBatchDictLearning, index=train_index)
<span class="linenr">20: </span>
<span class="linenr">21: </span>X_validation_miniBatchDictLearning = \
<span class="linenr">22: </span>    miniBatchDictLearning.transform(X_validation)
<span class="linenr">23: </span>X_validation_miniBatchDictLearning = \
<span class="linenr">24: </span>    pd.DataFrame(data=X_validation_miniBatchDictLearning, \
<span class="linenr">25: </span>    index=validation_index)
<span class="linenr">26: </span>
<span class="linenr">27: </span>scatterPlot(X_train_miniBatchDictLearning, y_train, \
<span class="linenr">28: </span>            <span style="color: #98be65;">"Mini-batch Dictionary Learning"</span>)
<span class="linenr">29: </span>plt.savefig(<span style="color: #98be65;">'images/DIC-MNIST.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>



<div id="org42ca4e6" class="figure">
<p><img src="images/DIC-MNIST.png" alt="DIC-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 24: </span>Caption</p>
</div>
</div>
</div>

<div id="outline-container-org4e83c17" class="outline-3">
<h3 id="org4e83c17"><span class="section-number-3">6.11.</span> 獨立成份分析</h3>
<div class="outline-text-3" id="text-6-11">
<p>
Independent component analysis
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Independent Component Analysis</span>
<span class="linenr"> 2: </span>plt.cla()
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> FastICA
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">25</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">algorithm</span> = <span style="color: #98be65;">'parallel'</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">whiten</span> = <span style="color: #a9a1e1;">True</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">max_iter</span> = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #dcaeea;">fastICA</span> = FastICA(n_components=n_components, algorithm=algorithm, \
<span class="linenr">12: </span>                  whiten=whiten, max_iter=max_iter, random_state=random_state)
<span class="linenr">13: </span>
<span class="linenr">14: </span>X_train_fastICA = fastICA.fit_transform(X_train)
<span class="linenr">15: </span>X_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=train_index)
<span class="linenr">16: </span>
<span class="linenr">17: </span>X_validation_fastICA = fastICA.transform(X_validation)
<span class="linenr">18: </span>X_validation_fastICA = pd.DataFrame(data=X_validation_fastICA, \
<span class="linenr">19: </span>                                    index=validation_index)
<span class="linenr">20: </span>
<span class="linenr">21: </span>scatterPlot(X_train_fastICA, y_train, <span style="color: #98be65;">"Independent Component Analysis"</span>)
<span class="linenr">22: </span>plt.savefig(<span style="color: #98be65;">'images/ICA-MNIST.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
</pre>
</div>

<div id="orgac17ace" class="figure">
<p><img src="images/ICA-MNIST.png" alt="ICA-MNIST.png" width="500" />
</p>
<p><span class="figure-number">Figure 25: </span>Caption</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgdae28b2" class="outline-2">
<h2 id="orgdae28b2"><span class="section-number-2">7.</span> 異常偵測</h2>
<div class="outline-text-2" id="text-7">
<p>
在現實的狀況下，詐欺的樣式會隨時間改變，如果只依賴訓練集的label來判斷，時間一久效能就會下降。故需要非監督式學習的詐欺偵測系統來協助。
</p>
</div>
<div id="outline-container-org90c3d62" class="outline-3">
<h3 id="org90c3d62"><span class="section-number-3">7.1.</span> 準備資料</h3>
<div class="outline-text-3" id="text-7-1">
<p>
共有284807筆信用卡交易、其中有492筆詐欺交易(class=1)
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Import libraries</span>
<span class="linenr"> 2: </span><span style="color: #83898d;">'''Main'''</span>
<span class="linenr"> 3: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 5: </span><span style="color: #51afef;">import</span> os, time
<span class="linenr"> 6: </span><span style="color: #51afef;">import</span> pickle, gzip
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #98be65;">'''Data Viz'''</span>
<span class="linenr"> 9: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">10: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr">11: </span><span style="color: #dcaeea;">color</span> = sns.color_palette()
<span class="linenr">12: </span><span style="color: #51afef;">import</span> matplotlib <span style="color: #51afef;">as</span> mpl
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #98be65;">'''Data Prep and Model Evaluation'''</span>
<span class="linenr">15: </span><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> preprocessing <span style="color: #51afef;">as</span> pp
<span class="linenr">16: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">17: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> precision_recall_curve, average_precision_score
<span class="linenr">18: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> roc_curve, auc, roc_auc_score
<span class="linenr">19: </span>
<span class="linenr">20: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">21: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">22: </span><span style="color: #dcaeea;">data</span> = pd.read_csv(<span style="color: #98be65;">"./credit_card.csv"</span>)
<span class="linenr">23: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Count total fraud</span>
<span class="linenr">24: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Number of fraudulent transactions:"</span>, data[<span style="color: #98be65;">'Class'</span>].<span style="color: #c678dd;">sum</span>())
<span class="linenr">25: </span>
<span class="linenr">26: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Suppress warnings</span>
<span class="linenr">27: </span>pd.set_option(<span style="color: #98be65;">'mode.chained_assignment'</span>, <span style="color: #a9a1e1;">None</span>)
<span class="linenr">28: </span>
<span class="linenr">29: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Split to train and test and scale features</span>
<span class="linenr">30: </span>dataX = data.drop([<span style="color: #98be65;">'Class'</span>],axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">31: </span>dataY = data.loc[:,<span style="color: #98be65;">'Class'</span>].copy()
<span class="linenr">32: </span>
<span class="linenr">33: </span><span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = \
<span class="linenr">34: </span>    train_test_split(dataX, dataY, test_size=<span style="color: #da8548; font-weight: bold;">0.33</span>, \
<span class="linenr">35: </span>                    random_state=<span style="color: #da8548; font-weight: bold;">2018</span>, stratify=dataY)
<span class="linenr">36: </span>
<span class="linenr">37: </span>featuresToScale = X_train.columns
<span class="linenr">38: </span>sX = pp.StandardScaler(copy=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">39: </span>X_train.loc[:,featuresToScale] = sX.fit_transform(X_train.loc[:,featuresToScale])
<span class="linenr">40: </span>X_test.loc[:,featuresToScale] = sX.transform(X_test.loc[:,featuresToScale])
</pre>
</div>

<pre class="example">
Number of fraudulent transactions: 492
</pre>
</div>
</div>
<div id="outline-container-org68169b7" class="outline-3">
<h3 id="org68169b7"><span class="section-number-3">7.2.</span> 定義異常評分函數</h3>
<div class="outline-text-3" id="text-7-2">
<p>
降維演算法在縮減維度時，會試圖將重建誤差最小化；對於信用卡交易資料來說，那些難以被塑模的交易會產生最大的重建誤差。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Calculate reconstruction error</span>
<span class="linenr">2: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">anomalyScores</span>(originalDF, reducedDF):
<span class="linenr">3: </span>    <span style="color: #dcaeea;">loss</span> = np.<span style="color: #c678dd;">sum</span>((np.array(originalDF)-np.array(reducedDF))**<span style="color: #da8548; font-weight: bold;">2</span>, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">4: </span>    loss = pd.Series(data=loss,index=originalDF.index)
<span class="linenr">5: </span>    loss = (loss-np.<span style="color: #c678dd;">min</span>(loss))/(np.<span style="color: #c678dd;">max</span>(loss)-np.<span style="color: #c678dd;">min</span>(loss))
<span class="linenr">6: </span>    <span style="color: #51afef;">return</span> loss
</pre>
</div>
</div>
</div>

<div id="outline-container-org5bf591d" class="outline-3">
<h3 id="org5bf591d"><span class="section-number-3">7.3.</span> 評估指標：畫圖</h3>
<div class="outline-text-3" id="text-7-3">
<p>
使用precision-recall曲線、average precision和auROC做為評估指標。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Plot results</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">setPlot</span>():
<span class="linenr"> 3: </span>    <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 4: </span>    <span style="color: #51afef;">from</span> matplotlib <span style="color: #51afef;">import</span> rcParams
<span class="linenr"> 5: </span>    rcParams.update({<span style="color: #98be65;">'figure.autolayout'</span>: <span style="color: #a9a1e1;">True</span>})
<span class="linenr"> 6: </span>    plt.<span style="color: #dcaeea;">rcParams</span>[<span style="color: #98be65;">'font.sans-serif'</span>] = [<span style="color: #98be65;">'Arial Unicode MS'</span>]
<span class="linenr"> 7: </span>    plt.<span style="color: #dcaeea;">rcParams</span>[<span style="color: #98be65;">'axes.unicode_minus'</span>] = <span style="color: #a9a1e1;">False</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">plotResults</span>(trueLabels, anomalyScores, returnPreds = <span style="color: #a9a1e1;">False</span>, imgName=<span style="color: #98be65;">''</span>):
<span class="linenr">10: </span>    plt.cla()
<span class="linenr">11: </span>    setPlot()
<span class="linenr">12: </span>    preds = pd.concat([trueLabels, anomalyScores], axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">13: </span>    preds.columns = [<span style="color: #98be65;">'trueLabel'</span>, <span style="color: #98be65;">'anomalyScore'</span>]
<span class="linenr">14: </span>    precision, recall, thresholds = \
<span class="linenr">15: </span>        precision_recall_curve(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'anomalyScore'</span>])
<span class="linenr">16: </span>    average_precision = \
<span class="linenr">17: </span>        average_precision_score(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'anomalyScore'</span>])
<span class="linenr">18: </span>
<span class="linenr">19: </span>    plt.step(recall, precision, color=<span style="color: #98be65;">'k'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.7</span>, where=<span style="color: #98be65;">'post'</span>)
<span class="linenr">20: </span>    plt.fill_between(recall, precision, step=<span style="color: #98be65;">'post'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, color=<span style="color: #98be65;">'k'</span>)
<span class="linenr">21: </span>
<span class="linenr">22: </span>    plt.xlabel(<span style="color: #98be65;">'Recall'</span>)
<span class="linenr">23: </span>    plt.ylabel(<span style="color: #98be65;">'Precision'</span>)
<span class="linenr">24: </span>    plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr">25: </span>    plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr">26: </span>
<span class="linenr">27: </span>    plt.title(<span style="color: #98be65;">'Precision-Recall curve: &#24179;&#22343;&#31934;&#30906;&#29575;:{0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(average_precision))
<span class="linenr">28: </span>
<span class="linenr">29: </span>    plt.savefig(<span style="color: #98be65;">'images/'</span>+imgName+<span style="color: #98be65;">'-1.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>, bbox_inches=<span style="color: #98be65;">'tight'</span>)
<span class="linenr">30: </span>
<span class="linenr">31: </span>    fpr, tpr, thresholds = roc_curve(preds[<span style="color: #98be65;">'trueLabel'</span>], \
<span class="linenr">32: </span>                                     preds[<span style="color: #98be65;">'anomalyScore'</span>])
<span class="linenr">33: </span>    areaUnderROC = auc(fpr, tpr)
<span class="linenr">34: </span>    plt.cla()
<span class="linenr">35: </span>    setPlot()
<span class="linenr">36: </span>    plt.plot(fpr, tpr, color=<span style="color: #98be65;">'r'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, label=<span style="color: #98be65;">'ROC curve'</span>)
<span class="linenr">37: </span>    plt.plot([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], color=<span style="color: #98be65;">'k'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, linestyle=<span style="color: #98be65;">'--'</span>)
<span class="linenr">38: </span>    plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr">39: </span>    plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr">40: </span>    plt.xlabel(<span style="color: #98be65;">'False Positive Rate'</span>)
<span class="linenr">41: </span>    plt.ylabel(<span style="color: #98be65;">'True Positive Rate'</span>)
<span class="linenr">42: </span>    plt.title(<span style="color: #98be65;">'Receiver operating characteristic: &#26354;&#32218;&#20197;&#19979;&#38754;&#31309;:{0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(areaUnderROC))
<span class="linenr">43: </span>    plt.legend(loc=<span style="color: #98be65;">"lower right"</span>)
<span class="linenr">44: </span>    plt.savefig(<span style="color: #98be65;">'images/'</span>+imgName+<span style="color: #98be65;">'-2.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>, bbox_inches=<span style="color: #98be65;">'tight'</span>)
<span class="linenr">45: </span>    <span style="color: #51afef;">if</span> returnPreds==<span style="color: #a9a1e1;">True</span>:
<span class="linenr">46: </span>        <span style="color: #51afef;">return</span> preds
<span class="linenr">47: </span>
<span class="linenr">48: </span>
<span class="linenr">49: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">View scatterplot</span>
<span class="linenr">50: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">scatterPlot</span>(xDF, yDF, algoName, imgName=<span style="color: #98be65;">''</span>):
<span class="linenr">51: </span>    plt.cla()
<span class="linenr">52: </span>    setPlot()
<span class="linenr">53: </span>    tempDF = pd.DataFrame(data=xDF.loc[:,<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">1</span>], index=xDF.index)
<span class="linenr">54: </span>    tempDF = pd.concat((tempDF,yDF), axis=<span style="color: #da8548; font-weight: bold;">1</span>, join=<span style="color: #98be65;">"inner"</span>)
<span class="linenr">55: </span>    tempDF.columns = [<span style="color: #98be65;">"First Vector"</span>, <span style="color: #98be65;">"Second Vector"</span>, <span style="color: #98be65;">"Label"</span>]
<span class="linenr">56: </span>    sns.lmplot(x=<span style="color: #98be65;">"First Vector"</span>, y=<span style="color: #98be65;">"Second Vector"</span>, hue=<span style="color: #98be65;">"Label"</span>, \
<span class="linenr">57: </span>               data=tempDF, fit_reg=<span style="color: #a9a1e1;">False</span>)
<span class="linenr">58: </span>    ax = plt.gca()
<span class="linenr">59: </span>    ax.set_title(<span style="color: #98be65;">"&#28436;&#31639;&#27861;:"</span>+algoName)
<span class="linenr">60: </span>    plt.savefig(<span style="color: #98be65;">'images/'</span>+imgName+<span style="color: #98be65;">'.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>, bbox_inches=<span style="color: #98be65;">'tight'</span>)
</pre>
</div>
</div>
</div>
<div id="outline-container-orgf7fa99e" class="outline-3">
<h3 id="orgf7fa99e"><span class="section-number-3">7.4.</span> PCA異常偵測</h3>
<div class="outline-text-3" id="text-7-4">
<p>
使用PCA模型來重建信用卡交易、計算重交的交易與原始交易的差異，那些PCA重建的較差的交易就是異常(可能為詐欺)。對於PCA來說，保留越多主成份、越有助於PCA學習到原始交易的資料結構，但若保留太多主成份，PCA可能太容易重建原始交易，反而讓所有的重建誤差都變小。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">30 principal components</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">30</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20445;&#30041;30o&#22266;&#20027;&#25104;&#20221;</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">whiten</span> = <span style="color: #a9a1e1;">False</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">pca</span> = PCA(n_components=n_components, whiten=whiten, \
<span class="linenr"> 9: </span>          random_state=random_state)
<span class="linenr">10: </span>
<span class="linenr">11: </span>X_train_PCA = pca.fit_transform(X_train)
<span class="linenr">12: </span>X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)
<span class="linenr">13: </span>
<span class="linenr">14: </span>X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)
<span class="linenr">15: </span>X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, index=X_train.index)
<span class="linenr">16: </span>
<span class="linenr">17: </span>scatterPlot(X_train_PCA, y_train, <span style="color: #98be65;">'AD-PCA'</span>, <span style="color: #98be65;">'AD-PCA'</span>)
<span class="linenr">18: </span>anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)
<span class="linenr">19: </span>preds = plotResults(y_train, anomalyScoresPCA, <span style="color: #a9a1e1;">True</span>, <span style="color: #98be65;">'AD-PCA'</span>)
<span class="linenr">20: </span>
</pre>
</div>


<div id="org3d62e10" class="figure">
<p><img src="images/AD-PCA.png" alt="AD-PCA.png" width="500" />
</p>
<p><span class="figure-number">Figure 26: </span>Caption</p>
</div>


<div id="orgae93cb7" class="figure">
<p><img src="images/AD-PCA-1.png" alt="AD-PCA-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 27: </span>Caption</p>
</div>
<p width="500">
<img src="images/AD-PCA-2.png" alt="AD-PCA-2.png" width="500" />
平均精確率不到1%，太差，必須不斷實驗找出最佳的PCA成份(<a href="http://bit.ly/2Gd4v7e">http://bit.ly/2Gd4v7e</a>)
</p>
</div>
<div id="outline-container-org3fdaa78" class="outline-4">
<h4 id="org3fdaa78"><span class="section-number-4">7.4.1.</span> 最後找出27個</h4>
<div class="outline-text-4" id="text-7-4-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">27 principal components</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">27</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">whiten</span> = <span style="color: #a9a1e1;">False</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">pca</span> = PCA(n_components=n_components, whiten=whiten, \
<span class="linenr"> 9: </span>          random_state=random_state)
<span class="linenr">10: </span>
<span class="linenr">11: </span>X_train_PCA = pca.fit_transform(X_train)
<span class="linenr">12: </span>X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)
<span class="linenr">13: </span>
<span class="linenr">14: </span>X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)
<span class="linenr">15: </span>X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, \
<span class="linenr">16: </span>                                   index=X_train.index)
<span class="linenr">17: </span>
<span class="linenr">18: </span>scatterPlot(X_train_PCA, y_train, <span style="color: #98be65;">'AD-PCA'</span>, <span style="color: #98be65;">'AD-PCA1'</span>)
<span class="linenr">19: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">View plot</span>
<span class="linenr">20: </span>anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)
<span class="linenr">21: </span>preds = plotResults(y_train, anomalyScoresPCA, <span style="color: #a9a1e1;">True</span>, <span style="color: #98be65;">'AD-PCA1'</span>)
</pre>
</div>

<div id="orge4aa0a1" class="figure">
<p><img src="images/AD-PCA1.png" alt="AD-PCA1.png" width="500" />
</p>
<p><span class="figure-number">Figure 28: </span>Caption</p>
</div>


<div id="org06ee917" class="figure">
<p><img src="images/AD-PCA1-1.png" alt="AD-PCA1-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 29: </span>Caption</p>
</div>

<div id="orga2d2984" class="figure">
<p><img src="images/AD-PCA1-2.png" alt="AD-PCA1-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 30: </span>Caption</p>
</div>
</div>
</div>
<div id="outline-container-orgf58488e" class="outline-4">
<h4 id="orgf58488e"><span class="section-number-4">7.4.2.</span> 分析結果</h4>
<div class="outline-text-4" id="text-7-4-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Analyze results</span>
<span class="linenr">2: </span>preds.sort_values(by=<span style="color: #98be65;">"anomalyScore"</span>,ascending=<span style="color: #a9a1e1;">False</span>,inplace=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">3: </span>cutoff = <span style="color: #da8548; font-weight: bold;">350</span>
<span class="linenr">4: </span>predsTop = preds[:cutoff]
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Precision: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr">6: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr">7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Recall: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr">8: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/y_train.<span style="color: #c678dd;">sum</span>(),<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr">9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #c678dd;">sum</span>())
</pre>
</div>

<pre class="example">
Precision:  0.75
Recall:  0.79
Fraud Caught out of 330 Cases: 261
</pre>
</div>
</div>
<div id="outline-container-org03bc688" class="outline-4">
<h4 id="org03bc688"><span class="section-number-4">7.4.3.</span> &lt;完整&gt;</h4>
<div class="outline-text-4" id="text-7-4-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Import libraries</span>
<span class="linenr">  2: </span>
<span class="linenr">  3: </span><span style="color: #83898d;">'''Main'''</span>
<span class="linenr">  4: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  5: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">  6: </span><span style="color: #51afef;">import</span> os, time
<span class="linenr">  7: </span><span style="color: #51afef;">import</span> pickle, gzip
<span class="linenr">  8: </span>
<span class="linenr">  9: </span><span style="color: #98be65;">'''Data Viz'''</span>
<span class="linenr"> 10: </span><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 11: </span><span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span class="linenr"> 12: </span><span style="color: #dcaeea;">color</span> = sns.color_palette()
<span class="linenr"> 13: </span><span style="color: #51afef;">import</span> matplotlib <span style="color: #51afef;">as</span> mpl
<span class="linenr"> 14: </span>
<span class="linenr"> 15: </span><span style="color: #98be65;">'''Data Prep and Model Evaluation'''</span>
<span class="linenr"> 16: </span><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> preprocessing <span style="color: #51afef;">as</span> pp
<span class="linenr"> 17: </span><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 18: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> precision_recall_curve, average_precision_score
<span class="linenr"> 19: </span><span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> roc_curve, auc, roc_auc_score
<span class="linenr"> 20: </span>
<span class="linenr"> 21: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 22: </span><span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 23: </span><span style="color: #dcaeea;">data</span> = pd.read_csv(<span style="color: #98be65;">"./credit_card.csv"</span>)
<span class="linenr"> 24: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Count total fraud</span>
<span class="linenr"> 25: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Number of fraudulent transactions:"</span>, data[<span style="color: #98be65;">'Class'</span>].<span style="color: #c678dd;">sum</span>())
<span class="linenr"> 26: </span>
<span class="linenr"> 27: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Suppress warnings</span>
<span class="linenr"> 28: </span>pd.set_option(<span style="color: #98be65;">'mode.chained_assignment'</span>, <span style="color: #a9a1e1;">None</span>)
<span class="linenr"> 29: </span>
<span class="linenr"> 30: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Split to train and test and scale features</span>
<span class="linenr"> 31: </span>dataX = data.drop([<span style="color: #98be65;">'Class'</span>],axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 32: </span>dataY = data.loc[:,<span style="color: #98be65;">'Class'</span>].copy()
<span class="linenr"> 33: </span>
<span class="linenr"> 34: </span><span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = \
<span class="linenr"> 35: </span>    train_test_split(dataX, dataY, test_size=<span style="color: #da8548; font-weight: bold;">0.33</span>, \
<span class="linenr"> 36: </span>                    random_state=<span style="color: #da8548; font-weight: bold;">2018</span>, stratify=dataY)
<span class="linenr"> 37: </span>
<span class="linenr"> 38: </span>featuresToScale = X_train.columns
<span class="linenr"> 39: </span>sX = pp.StandardScaler(copy=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 40: </span>X_train.loc[:,featuresToScale] = sX.fit_transform(X_train.loc[:,featuresToScale])
<span class="linenr"> 41: </span>X_test.loc[:,featuresToScale] = sX.transform(X_test.loc[:,featuresToScale])
<span class="linenr"> 42: </span>
<span class="linenr"> 43: </span>
<span class="linenr"> 44: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">anomalyScores</span>(originalDF, reducedDF):
<span class="linenr"> 45: </span>    loss = np.<span style="color: #c678dd;">sum</span>((np.array(originalDF)-np.array(reducedDF))**<span style="color: #da8548; font-weight: bold;">2</span>, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 46: </span>    loss = pd.Series(data=loss,index=originalDF.index)
<span class="linenr"> 47: </span>    loss = (loss-np.<span style="color: #c678dd;">min</span>(loss))/(np.<span style="color: #c678dd;">max</span>(loss)-np.<span style="color: #c678dd;">min</span>(loss))
<span class="linenr"> 48: </span>    <span style="color: #51afef;">return</span> loss
<span class="linenr"> 49: </span>
<span class="linenr"> 50: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Plot results</span>
<span class="linenr"> 51: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">setPlot</span>():
<span class="linenr"> 52: </span>    <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 53: </span>    <span style="color: #51afef;">from</span> matplotlib <span style="color: #51afef;">import</span> rcParams
<span class="linenr"> 54: </span>    rcParams.update({<span style="color: #98be65;">'figure.autolayout'</span>: <span style="color: #a9a1e1;">True</span>})
<span class="linenr"> 55: </span>    plt.rcParams[<span style="color: #98be65;">'font.sans-serif'</span>] = [<span style="color: #98be65;">'Arial Unicode MS'</span>]
<span class="linenr"> 56: </span>    plt.rcParams[<span style="color: #98be65;">'axes.unicode_minus'</span>] = <span style="color: #a9a1e1;">False</span>
<span class="linenr"> 57: </span>
<span class="linenr"> 58: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">plotResults</span>(trueLabels, anomalyScores, returnPreds = <span style="color: #a9a1e1;">False</span>, imgName=<span style="color: #98be65;">''</span>):
<span class="linenr"> 59: </span>    plt.cla()
<span class="linenr"> 60: </span>    setPlot()
<span class="linenr"> 61: </span>    preds = pd.concat([trueLabels, anomalyScores], axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 62: </span>    preds.columns = [<span style="color: #98be65;">'trueLabel'</span>, <span style="color: #98be65;">'anomalyScore'</span>]
<span class="linenr"> 63: </span>    precision, recall, thresholds = \
<span class="linenr"> 64: </span>        precision_recall_curve(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'anomalyScore'</span>])
<span class="linenr"> 65: </span>    average_precision = \
<span class="linenr"> 66: </span>        average_precision_score(preds[<span style="color: #98be65;">'trueLabel'</span>],preds[<span style="color: #98be65;">'anomalyScore'</span>])
<span class="linenr"> 67: </span>
<span class="linenr"> 68: </span>    plt.step(recall, precision, color=<span style="color: #98be65;">'k'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.7</span>, where=<span style="color: #98be65;">'post'</span>)
<span class="linenr"> 69: </span>    plt.fill_between(recall, precision, step=<span style="color: #98be65;">'post'</span>, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, color=<span style="color: #98be65;">'k'</span>)
<span class="linenr"> 70: </span>
<span class="linenr"> 71: </span>    plt.xlabel(<span style="color: #98be65;">'Recall'</span>)
<span class="linenr"> 72: </span>    plt.ylabel(<span style="color: #98be65;">'Precision'</span>)
<span class="linenr"> 73: </span>    plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr"> 74: </span>    plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr"> 75: </span>
<span class="linenr"> 76: </span>    plt.title(<span style="color: #98be65;">'Precision-Recall curve: &#24179;&#22343;&#31934;&#30906;&#29575;:{0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(average_precision))
<span class="linenr"> 77: </span>
<span class="linenr"> 78: </span>    plt.savefig(<span style="color: #98be65;">'images/'</span>+imgName+<span style="color: #98be65;">'-1.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>, bbox_inches=<span style="color: #98be65;">'tight'</span>)
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span>    fpr, tpr, thresholds = roc_curve(preds[<span style="color: #98be65;">'trueLabel'</span>], \
<span class="linenr"> 81: </span>                                     preds[<span style="color: #98be65;">'anomalyScore'</span>])
<span class="linenr"> 82: </span>    areaUnderROC = auc(fpr, tpr)
<span class="linenr"> 83: </span>    plt.cla()
<span class="linenr"> 84: </span>    setPlot()
<span class="linenr"> 85: </span>    plt.plot(fpr, tpr, color=<span style="color: #98be65;">'r'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, label=<span style="color: #98be65;">'ROC curve'</span>)
<span class="linenr"> 86: </span>    plt.plot([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>], color=<span style="color: #98be65;">'k'</span>, lw=<span style="color: #da8548; font-weight: bold;">2</span>, linestyle=<span style="color: #98be65;">'--'</span>)
<span class="linenr"> 87: </span>    plt.xlim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.0</span>])
<span class="linenr"> 88: </span>    plt.ylim([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1.05</span>])
<span class="linenr"> 89: </span>    plt.xlabel(<span style="color: #98be65;">'False Positive Rate'</span>)
<span class="linenr"> 90: </span>    plt.ylabel(<span style="color: #98be65;">'True Positive Rate'</span>)
<span class="linenr"> 91: </span>    plt.title(<span style="color: #98be65;">'Receiver operating characteristic: &#26354;&#32218;&#20197;&#19979;&#38754;&#31309;:{0:0.2f}'</span>.<span style="color: #c678dd;">format</span>(areaUnderROC))
<span class="linenr"> 92: </span>    plt.legend(loc=<span style="color: #98be65;">"lower right"</span>)
<span class="linenr"> 93: </span>    plt.savefig(<span style="color: #98be65;">'images/'</span>+imgName+<span style="color: #98be65;">'-2.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>, bbox_inches=<span style="color: #98be65;">'tight'</span>)
<span class="linenr"> 94: </span>
<span class="linenr"> 95: </span>
<span class="linenr"> 96: </span>    <span style="color: #51afef;">if</span> returnPreds==<span style="color: #a9a1e1;">True</span>:
<span class="linenr"> 97: </span>        <span style="color: #51afef;">return</span> preds
<span class="linenr"> 98: </span>
<span class="linenr"> 99: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">View scatterplot</span>
<span class="linenr">100: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">scatterPlot</span>(xDF, yDF, algoName, imgName=<span style="color: #98be65;">''</span>):
<span class="linenr">101: </span>    plt.cla()
<span class="linenr">102: </span>    setPlot()
<span class="linenr">103: </span>    tempDF = pd.DataFrame(data=xDF.loc[:,<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">1</span>], index=xDF.index)
<span class="linenr">104: </span>    tempDF = pd.concat((tempDF,yDF), axis=<span style="color: #da8548; font-weight: bold;">1</span>, join=<span style="color: #98be65;">"inner"</span>)
<span class="linenr">105: </span>    tempDF.columns = [<span style="color: #98be65;">"First Vector"</span>, <span style="color: #98be65;">"Second Vector"</span>, <span style="color: #98be65;">"Label"</span>]
<span class="linenr">106: </span>    sns.lmplot(x=<span style="color: #98be65;">"First Vector"</span>, y=<span style="color: #98be65;">"Second Vector"</span>, hue=<span style="color: #98be65;">"Label"</span>, \
<span class="linenr">107: </span>               data=tempDF, fit_reg=<span style="color: #a9a1e1;">False</span>)
<span class="linenr">108: </span>    ax = plt.gca()
<span class="linenr">109: </span>    ax.set_title(<span style="color: #98be65;">"&#28436;&#31639;&#27861;:"</span>+algoName)
<span class="linenr">110: </span>    plt.savefig(<span style="color: #98be65;">'images/'</span>+imgName+<span style="color: #98be65;">'.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>, bbox_inches=<span style="color: #98be65;">'tight'</span>)
<span class="linenr">111: </span>
<span class="linenr">112: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr">113: </span>
<span class="linenr">114: </span>n_components = <span style="color: #da8548; font-weight: bold;">30</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20445;&#30041;30o&#22266;&#20027;&#25104;&#20221;</span>
<span class="linenr">115: </span>whiten = <span style="color: #a9a1e1;">False</span>
<span class="linenr">116: </span>random_state = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr">117: </span>
<span class="linenr">118: </span>pca = PCA(n_components=n_components, whiten=whiten, \
<span class="linenr">119: </span>          random_state=random_state)
<span class="linenr">120: </span>
<span class="linenr">121: </span>X_train_PCA = pca.fit_transform(X_train)
<span class="linenr">122: </span>X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)
<span class="linenr">123: </span>
<span class="linenr">124: </span>X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)
<span class="linenr">125: </span>X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, index=X_train.index)
<span class="linenr">126: </span>
<span class="linenr">127: </span>scatterPlot(X_train_PCA, y_train, <span style="color: #98be65;">'AD-PCA'</span>, <span style="color: #98be65;">'AD-PCA'</span>)
<span class="linenr">128: </span>anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)
<span class="linenr">129: </span>preds = plotResults(y_train, anomalyScoresPCA, <span style="color: #a9a1e1;">True</span>, <span style="color: #98be65;">'AD-PCA'</span>)
<span class="linenr">130: </span>
<span class="linenr">131: </span>
</pre>
</div>

<pre class="example">
Number of fraudulent transactions: 492
</pre>
</div>
</div>
</div>
<div id="outline-container-org8146501" class="outline-3">
<h3 id="org8146501"><span class="section-number-3">7.5.</span> Sparse PCA異常偵測</h3>
<div class="outline-text-3" id="text-7-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Sparse PCA</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> SparsePCA
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">27</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">alpha</span> = <span style="color: #da8548; font-weight: bold;">0.0001</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">n_jobs</span> = -<span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">sparsePCA</span> = SparsePCA(n_components=n_components, \
<span class="linenr">10: </span>                alpha=alpha, random_state=random_state, n_jobs=n_jobs)
<span class="linenr">11: </span>
<span class="linenr">12: </span>sparsePCA.fit(X_train.loc[:,:])
<span class="linenr">13: </span>X_train_sparsePCA = sparsePCA.transform(X_train)
<span class="linenr">14: </span>X_train_sparsePCA = pd.DataFrame(data=X_train_sparsePCA, index=X_train.index)
<span class="linenr">15: </span>
<span class="linenr">16: </span>scatterPlot(X_train_sparsePCA, y_train, <span style="color: #98be65;">"Sparse PCA"</span>, <span style="color: #98be65;">"AD-SPCA"</span>)
<span class="linenr">17: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">View plot</span>
<span class="linenr">18: </span>X_train_sparsePCA_inverse = np.array(X_train_sparsePCA). \
<span class="linenr">19: </span>    dot(sparsePCA.components_) + np.array(X_train.mean(axis=<span style="color: #da8548; font-weight: bold;">0</span>))
<span class="linenr">20: </span>X_train_sparsePCA_inverse = \
<span class="linenr">21: </span>    pd.DataFrame(data=X_train_sparsePCA_inverse, index=X_train.index)
<span class="linenr">22: </span>
<span class="linenr">23: </span>anomalyScoresSparsePCA = anomalyScores(X_train, X_train_sparsePCA_inverse)
<span class="linenr">24: </span>preds = plotResults(y_train, anomalyScoresSparsePCA, <span style="color: #a9a1e1;">True</span>, <span style="color: #98be65;">'AD-SPCA'</span>)
</pre>
</div>


<div id="org873e30d" class="figure">
<p><img src="images/AD-SPCA.png" alt="AD-SPCA.png" width="500" />
</p>
<p><span class="figure-number">Figure 31: </span>Caption</p>
</div>


<div id="org7e86e59" class="figure">
<p><img src="images/AD-SPCA-1.png" alt="AD-SPCA-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 32: </span>Caption</p>
</div>

<div id="orgc0d0fed" class="figure">
<p><img src="images/AD-SPCA-2.png" alt="AD-SPCA-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 33: </span>Caption</p>
</div>
</div>
<div id="outline-container-org8e1ee41" class="outline-4">
<h4 id="org8e1ee41"><span class="section-number-4">7.5.1.</span> 分析結果</h4>
<div class="outline-text-4" id="text-7-5-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Analyze results</span>
<span class="linenr">2: </span>preds.sort_values(by=<span style="color: #98be65;">"anomalyScore"</span>,ascending=<span style="color: #a9a1e1;">False</span>,inplace=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">3: </span>cutoff = <span style="color: #da8548; font-weight: bold;">350</span>
<span class="linenr">4: </span>predsTop = preds[:cutoff]
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Precision: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr">6: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr">7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Recall: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr">8: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/y_train.<span style="color: #c678dd;">sum</span>(),<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr">9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #c678dd;">sum</span>())
</pre>
</div>

<pre class="example">
Precision:  0.75
Recall:  0.79
Fraud Caught out of 330 Cases: 261
</pre>
</div>
</div>
</div>
<div id="outline-container-org136e498" class="outline-3">
<h3 id="org136e498"><span class="section-number-3">7.6.</span> Kernel PCA異常偵測</h3>
<div class="outline-text-3" id="text-7-6">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Kernel PCA</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> KernelPCA
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">27</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">kernel</span> = <span style="color: #98be65;">'rbf'</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">gamma</span> = <span style="color: #a9a1e1;">None</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">fit_inverse_transform</span> = <span style="color: #a9a1e1;">True</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">n_jobs</span> = <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #dcaeea;">kernelPCA</span> = KernelPCA(n_components=n_components, kernel=kernel, \
<span class="linenr">12: </span>                gamma=gamma, fit_inverse_transform= \
<span class="linenr">13: </span>                fit_inverse_transform, n_jobs=n_jobs, \
<span class="linenr">14: </span>                random_state=random_state)
<span class="linenr">15: </span>
<span class="linenr">16: </span>kernelPCA.fit(X_train.iloc[:<span style="color: #da8548; font-weight: bold;">2000</span>])
<span class="linenr">17: </span>X_train_kernelPCA = kernelPCA.transform(X_train)
<span class="linenr">18: </span>X_train_kernelPCA = pd.DataFrame(data=X_train_kernelPCA, \
<span class="linenr">19: </span>                                 index=X_train.index)
<span class="linenr">20: </span>
<span class="linenr">21: </span>X_train_kernelPCA_inverse = kernelPCA.inverse_transform(X_train_kernelPCA)
<span class="linenr">22: </span>X_train_kernelPCA_inverse = pd.DataFrame(data=X_train_kernelPCA_inverse, \
<span class="linenr">23: </span>                                         index=X_train.index)
<span class="linenr">24: </span>
<span class="linenr">25: </span>scatterPlot(X_train_kernelPCA, y_train, <span style="color: #98be65;">"Kernel PCA"</span>, <span style="color: #98be65;">"AD-KPCA"</span>)
<span class="linenr">26: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">View plot</span>
<span class="linenr">27: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">View plot</span>
<span class="linenr">28: </span>anomalyScoresKernelPCA = anomalyScores(X_train, X_train_kernelPCA_inverse)
<span class="linenr">29: </span>preds = plotResults(y_train, anomalyScoresKernelPCA, <span style="color: #a9a1e1;">True</span>, <span style="color: #98be65;">'AD-KPCA'</span>)
</pre>
</div>


<div id="orga09fadf" class="figure">
<p><img src="images/AD-KPCA.png" alt="AD-KPCA.png" width="500" />
</p>
<p><span class="figure-number">Figure 34: </span>Caption</p>
</div>


<div id="org0e9992c" class="figure">
<p><img src="images/AD-KPCA-1.png" alt="AD-KPCA-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 35: </span>Caption</p>
</div>

<div id="org18380aa" class="figure">
<p><img src="images/AD-KPCA-2.png" alt="AD-KPCA-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 36: </span>Caption</p>
</div>
</div>
<div id="outline-container-org16f4cce" class="outline-4">
<h4 id="org16f4cce"><span class="section-number-4">7.6.1.</span> 分析結果</h4>
<div class="outline-text-4" id="text-7-6-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Analyze results</span>
<span class="linenr"> 2: </span>preds.sort_values(by=<span style="color: #98be65;">"anomalyScore"</span>,ascending=<span style="color: #a9a1e1;">False</span>,inplace=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 3: </span>cutoff = <span style="color: #da8548; font-weight: bold;">350</span>
<span class="linenr"> 4: </span>predsTop = preds[:cutoff]
<span class="linenr"> 5: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Precision: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr"> 6: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Recall: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr"> 8: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/y_train.<span style="color: #c678dd;">sum</span>(),<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #c678dd;">sum</span>())
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Write dimensions to CSV</span>
<span class="linenr">11: </span>X_train_kernelPCA.loc[sample_indices,:].to_csv(<span style="color: #98be65;">'kernel_pca_data.tsv'</span>, sep = <span style="color: #98be65;">'\t'</span>, index=<span style="color: #a9a1e1;">False</span>, header=<span style="color: #a9a1e1;">False</span>)
</pre>
</div>

<pre class="example">
Precision:  0.22
Recall:  0.23
Fraud Caught out of 330 Cases: 77
</pre>

<p>
結果不如普通的PCA與sparse PCA
</p>
</div>
</div>
</div>
<div id="outline-container-org56a1deb" class="outline-3">
<h3 id="org56a1deb"><span class="section-number-3">7.7.</span> 高斯隨機投影異常偵測</h3>
<div class="outline-text-3" id="text-7-7">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Gaussian Random Projection</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> sklearn.random_projection <span style="color: #51afef;">import</span> GaussianRandomProjection
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">27</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">eps</span> = <span style="color: #a9a1e1;">None</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">GRP</span> = GaussianRandomProjection(n_components=n_components, \
<span class="linenr"> 9: </span>                               eps=eps, random_state=random_state)
<span class="linenr">10: </span>
<span class="linenr">11: </span>X_train_GRP = GRP.fit_transform(X_train)
<span class="linenr">12: </span>X_train_GRP = pd.DataFrame(data=X_train_GRP, index=X_train.index)
<span class="linenr">13: </span>
<span class="linenr">14: </span>scatterPlot(X_train_GRP, y_train, <span style="color: #98be65;">"Gaussian Random Projection"</span>, <span style="color: #98be65;">"AD-GRP"</span>)
<span class="linenr">15: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">View plot</span>
<span class="linenr">16: </span>X_train_GRP_inverse = np.array(X_train_GRP).dot(GRP.components_)
<span class="linenr">17: </span>X_train_GRP_inverse = pd.DataFrame(data=X_train_GRP_inverse, \
<span class="linenr">18: </span>                                   index=X_train.index)
<span class="linenr">19: </span>
<span class="linenr">20: </span>anomalyScoresGRP = anomalyScores(X_train, X_train_GRP_inverse)
<span class="linenr">21: </span>preds = plotResults(y_train, anomalyScoresGRP, <span style="color: #a9a1e1;">True</span>, <span style="color: #98be65;">"AD-GRP"</span>)
</pre>
</div>


<div id="orgcd3b9d2" class="figure">
<p><img src="images/AD-GRP.png" alt="AD-GRP.png" width="500" />
</p>
<p><span class="figure-number">Figure 37: </span>Caption</p>
</div>

<div id="org32903e7" class="figure">
<p><img src="images/AD-GRP-1.png" alt="AD-GRP-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 38: </span>Caption</p>
</div>

<div id="orga0d75e8" class="figure">
<p><img src="images/AD-GRP-2.png" alt="AD-GRP-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 39: </span>Caption</p>
</div>
</div>
<div id="outline-container-orgf1759e9" class="outline-4">
<h4 id="orgf1759e9"><span class="section-number-4">7.7.1.</span> 分析結果</h4>
<div class="outline-text-4" id="text-7-7-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Analyze results</span>
<span class="linenr"> 2: </span>preds.sort_values(by=<span style="color: #98be65;">"anomalyScore"</span>,ascending=<span style="color: #a9a1e1;">False</span>,inplace=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 3: </span>cutoff = <span style="color: #da8548; font-weight: bold;">350</span>
<span class="linenr"> 4: </span>predsTop = preds[:cutoff]
<span class="linenr"> 5: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Precision: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr"> 6: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Recall: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr"> 8: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/y_train.<span style="color: #c678dd;">sum</span>(),<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #c678dd;">sum</span>())
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Write dimensions to CSV</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span>X_train_GRP.loc[sample_indices,:].to_csv(<span style="color: #98be65;">'gaussian_random_projection_data.tsv'</span>, sep = <span style="color: #98be65;">'\t'</span>, index=<span style="color: #a9a1e1;">False</span>, header=<span style="color: #a9a1e1;">False</span>)
</pre>
</div>

<pre class="example">
Precision:  0.22
Recall:  0.23
Fraud Caught out of 330 Cases: 76
</pre>
</div>
</div>
</div>
<div id="outline-container-org4bb93d3" class="outline-3">
<h3 id="org4bb93d3"><span class="section-number-3">7.8.</span> 稀疏隨機投影異常偵測</h3>
<div class="outline-text-3" id="text-7-8">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Sparse Random Projection</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> sklearn.random_projection <span style="color: #51afef;">import</span> SparseRandomProjection
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">27</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">density</span> = <span style="color: #98be65;">'auto'</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">eps</span> = .<span style="color: #da8548; font-weight: bold;">01</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">dense_output</span> = <span style="color: #a9a1e1;">True</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #dcaeea;">SRP</span> = SparseRandomProjection(n_components=n_components, \
<span class="linenr">11: </span>        density=density, eps=eps, dense_output=dense_output, \
<span class="linenr">12: </span>                                random_state=random_state)
<span class="linenr">13: </span>
<span class="linenr">14: </span>X_train_SRP = SRP.fit_transform(X_train)
<span class="linenr">15: </span>X_train_SRP = pd.DataFrame(data=X_train_SRP, index=X_train.index)
<span class="linenr">16: </span>
<span class="linenr">17: </span>scatterPlot(X_train_SRP, y_train, <span style="color: #98be65;">"Sparse Random Projection"</span>, <span style="color: #98be65;">"AD-SRP"</span>)
<span class="linenr">18: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">View plot</span>
<span class="linenr">19: </span>X_train_SRP_inverse = np.array(X_train_SRP).dot(SRP.components_.todense())
<span class="linenr">20: </span>X_train_SRP_inverse = pd.DataFrame(data=X_train_SRP_inverse, index=X_train.index)
<span class="linenr">21: </span>
<span class="linenr">22: </span>anomalyScoresSRP = anomalyScores(X_train, X_train_SRP_inverse)
<span class="linenr">23: </span>preds = plotResults(y_train, anomalyScoresSRP, <span style="color: #a9a1e1;">True</span>, <span style="color: #98be65;">"AD-SRP"</span>)
</pre>
</div>


<div id="orga6d30f4" class="figure">
<p><img src="images/AD-SRP.png" alt="AD-SRP.png" width="500" />
</p>
<p><span class="figure-number">Figure 40: </span>Caption</p>
</div>

<div id="orgbc274d5" class="figure">
<p><img src="images/AD-SRP-1.png" alt="AD-SRP-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 41: </span>Caption</p>
</div>

<div id="orgfeae40d" class="figure">
<p><img src="images/AD-SRP-2.png" alt="AD-SRP-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 42: </span>Caption</p>
</div>
</div>
<div id="outline-container-org4b28218" class="outline-4">
<h4 id="org4b28218"><span class="section-number-4">7.8.1.</span> 分析結果</h4>
<div class="outline-text-4" id="text-7-8-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Analyze results</span>
<span class="linenr"> 2: </span>preds.sort_values(by=<span style="color: #98be65;">"anomalyScore"</span>,ascending=<span style="color: #a9a1e1;">False</span>,inplace=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 3: </span>cutoff = <span style="color: #da8548; font-weight: bold;">350</span>
<span class="linenr"> 4: </span>predsTop = preds[:cutoff]
<span class="linenr"> 5: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Precision: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr"> 6: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Recall: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr"> 8: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/y_train.<span style="color: #c678dd;">sum</span>(),<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #c678dd;">sum</span>())
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Write dimensions to CSV</span>
<span class="linenr">11: </span>X_train_SRP.loc[sample_indices,:].to_csv(<span style="color: #98be65;">'sparse_random_projection_data.tsv'</span>, sep = <span style="color: #98be65;">'\t'</span>, index=<span style="color: #a9a1e1;">False</span>, header=<span style="color: #a9a1e1;">False</span>)
</pre>
</div>

<pre class="example">
Precision:  0.21
Recall:  0.22
Fraud Caught out of 330 Cases: 73
</pre>
</div>
</div>
</div>
<div id="outline-container-orgadc6f96" class="outline-3">
<h3 id="orgadc6f96"><span class="section-number-3">7.9.</span> 字典學習異常偵測</h3>
<div class="outline-text-3" id="text-7-9">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Mini-batch dictionary learning</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> MiniBatchDictionaryLearning
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">28</span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">alpha</span> = <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">batch_size</span> = <span style="color: #da8548; font-weight: bold;">200</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">n_iter</span> = <span style="color: #da8548; font-weight: bold;">10</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span><span style="color: #dcaeea;">miniBatchDictLearning</span> = MiniBatchDictionaryLearning( \
<span class="linenr">11: </span>    n_components=n_components, alpha=alpha, batch_size=batch_size, \
<span class="linenr">12: </span>    n_iter=n_iter, random_state=random_state)
<span class="linenr">13: </span>
<span class="linenr">14: </span>miniBatchDictLearning.fit(X_train)
<span class="linenr">15: </span>X_train_miniBatchDictLearning = \
<span class="linenr">16: </span>    miniBatchDictLearning.fit_transform(X_train)
<span class="linenr">17: </span>X_train_miniBatchDictLearning = \
<span class="linenr">18: </span>    pd.DataFrame(data=X_train_miniBatchDictLearning, index=X_train.index)
<span class="linenr">19: </span>
<span class="linenr">20: </span>scatterPlot(X_train_miniBatchDictLearning, y_train, \
<span class="linenr">21: </span>            <span style="color: #98be65;">"Mini-batch Dictionary Learning"</span>, <span style="color: #98be65;">"AD-MBDL"</span>)
<span class="linenr">22: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">View plot</span>
<span class="linenr">23: </span>X_train_miniBatchDictLearning_inverse = \
<span class="linenr">24: </span>    np.array(X_train_miniBatchDictLearning). \
<span class="linenr">25: </span>    dot(miniBatchDictLearning.components_)
<span class="linenr">26: </span>
<span class="linenr">27: </span>X_train_miniBatchDictLearning_inverse = \
<span class="linenr">28: </span>    pd.DataFrame(data=X_train_miniBatchDictLearning_inverse, \
<span class="linenr">29: </span>                 index=X_train.index)
<span class="linenr">30: </span>
<span class="linenr">31: </span>anomalyScoresMiniBatchDictLearning = anomalyScores(X_train, \
<span class="linenr">32: </span>    X_train_miniBatchDictLearning_inverse)
<span class="linenr">33: </span>preds = plotResults(y_train, anomalyScoresMiniBatchDictLearning, <span style="color: #a9a1e1;">True</span>, <span style="color: #98be65;">"AD-MBDL"</span>)
</pre>
</div>


<div id="org7dd8ba6" class="figure">
<p><img src="images/AD-MBDL.png" alt="AD-MBDL.png" width="500" />
</p>
<p><span class="figure-number">Figure 43: </span>Caption</p>
</div>

<div id="orgc6f2265" class="figure">
<p><img src="images/AD-MBDL-1.png" alt="AD-MBDL-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 44: </span>Caption</p>
</div>

<div id="org269194c" class="figure">
<p><img src="images/AD-MBDL-2.png" alt="AD-MBDL-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 45: </span>Caption</p>
</div>
</div>
<div id="outline-container-org0436ada" class="outline-4">
<h4 id="org0436ada"><span class="section-number-4">7.9.1.</span> 分析結果</h4>
<div class="outline-text-4" id="text-7-9-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Analyze results</span>
<span class="linenr"> 2: </span>preds.sort_values(by=<span style="color: #98be65;">"anomalyScore"</span>,ascending=<span style="color: #a9a1e1;">False</span>,inplace=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 3: </span>cutoff = <span style="color: #da8548; font-weight: bold;">350</span>
<span class="linenr"> 4: </span>predsTop = preds[:cutoff]
<span class="linenr"> 5: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Precision: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr"> 6: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Recall: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr"> 8: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/y_train.<span style="color: #c678dd;">sum</span>(),<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #c678dd;">sum</span>())
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Write dimensions to CSV</span>
<span class="linenr">11: </span>X_train_miniBatchDictLearning.loc[sample_indices,:].to_csv(<span style="color: #98be65;">'dictionary_learning_data.tsv'</span>, sep = <span style="color: #98be65;">'\t'</span>, index=<span style="color: #a9a1e1;">False</span>, header=<span style="color: #a9a1e1;">False</span>)
</pre>
</div>

<pre class="example">
Precision:  0.43
Recall:  0.46
Fraud Caught out of 330 Cases: 151
</pre>
</div>
</div>
</div>
<div id="outline-container-org822f120" class="outline-3">
<h3 id="org822f120"><span class="section-number-3">7.10.</span> ICA異常偵測</h3>
<div class="outline-text-3" id="text-7-10">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Independent Component Analysis</span>
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> FastICA
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span><span style="color: #dcaeea;">n_components</span> = <span style="color: #da8548; font-weight: bold;">27</span>
<span class="linenr"> 6: </span><span style="color: #dcaeea;">algorithm</span> = <span style="color: #98be65;">'parallel'</span>
<span class="linenr"> 7: </span><span style="color: #dcaeea;">whiten</span> = <span style="color: #a9a1e1;">True</span>
<span class="linenr"> 8: </span><span style="color: #dcaeea;">max_iter</span> = <span style="color: #da8548; font-weight: bold;">200</span>
<span class="linenr"> 9: </span><span style="color: #dcaeea;">random_state</span> = <span style="color: #da8548; font-weight: bold;">2018</span>
<span class="linenr">10: </span>
<span class="linenr">11: </span><span style="color: #dcaeea;">fastICA</span> = FastICA(n_components=n_components, \
<span class="linenr">12: </span>    algorithm=algorithm, whiten=whiten, max_iter=max_iter, \
<span class="linenr">13: </span>    random_state=random_state)
<span class="linenr">14: </span>
<span class="linenr">15: </span>X_train_fastICA = fastICA.fit_transform(X_train)
<span class="linenr">16: </span>X_train_fastICA = pd.DataFrame(data=X_train_fastICA, index=X_train.index)
<span class="linenr">17: </span>
<span class="linenr">18: </span>X_train_fastICA_inverse = fastICA.inverse_transform(X_train_fastICA)
<span class="linenr">19: </span>X_train_fastICA_inverse = pd.DataFrame(data=X_train_fastICA_inverse, \
<span class="linenr">20: </span>                                       index=X_train.index)
<span class="linenr">21: </span>
<span class="linenr">22: </span>scatterPlot(X_train_fastICA, y_train, <span style="color: #98be65;">"Independent Component Analysis"</span>, <span style="color: #98be65;">"AD-ICA"</span>)
<span class="linenr">23: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">View plot</span>
<span class="linenr">24: </span>anomalyScoresFastICA = anomalyScores(X_train, X_train_fastICA_inverse)
<span class="linenr">25: </span>preds = plotResults(y_train, anomalyScoresFastICA, <span style="color: #a9a1e1;">True</span>, <span style="color: #98be65;">"AD-ICA"</span>)
</pre>
</div>


<div id="org65f8e81" class="figure">
<p><img src="images/AD-ICA.png" alt="AD-ICA.png" width="500" />
</p>
<p><span class="figure-number">Figure 46: </span>Caption</p>
</div>

<div id="org1c6f640" class="figure">
<p><img src="images/AD-ICA-1.png" alt="AD-ICA-1.png" width="500" />
</p>
<p><span class="figure-number">Figure 47: </span>Caption</p>
</div>

<div id="orgbc0a4cd" class="figure">
<p><img src="images/AD-ICA-2.png" alt="AD-ICA-2.png" width="500" />
</p>
<p><span class="figure-number">Figure 48: </span>Caption</p>
</div>
</div>
<div id="outline-container-org8cb375a" class="outline-4">
<h4 id="org8cb375a"><span class="section-number-4">7.10.1.</span> 分析結果</h4>
<div class="outline-text-4" id="text-7-10-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Analyze results</span>
<span class="linenr"> 2: </span>preds.sort_values(by=<span style="color: #98be65;">"anomalyScore"</span>,ascending=<span style="color: #a9a1e1;">False</span>,inplace=<span style="color: #a9a1e1;">True</span>)
<span class="linenr"> 3: </span>cutoff = <span style="color: #da8548; font-weight: bold;">350</span>
<span class="linenr"> 4: </span>predsTop = preds[:cutoff]
<span class="linenr"> 5: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Precision: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr"> 6: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/cutoff,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Recall: "</span>,np.<span style="color: #c678dd;">round</span>(predsTop. \
<span class="linenr"> 8: </span>            anomalyScore[predsTop.trueLabel==<span style="color: #da8548; font-weight: bold;">1</span>].count()/y_train.<span style="color: #c678dd;">sum</span>(),<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Fraud Caught out of 330 Cases:"</span>, predsTop.trueLabel.<span style="color: #c678dd;">sum</span>())
<span class="linenr">10: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Write dimensions to CSV</span>
<span class="linenr">11: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Write dimensions to CSV</span>
<span class="linenr">12: </span>X_train_fastICA.loc[sample_indices,:].to_csv(<span style="color: #98be65;">'independent_component_analysis_data.tsv'</span>, sep = <span style="color: #98be65;">'\t'</span>, index=<span style="color: #a9a1e1;">False</span>, header=<span style="color: #a9a1e1;">False</span>)
</pre>
</div>

<pre class="example">
Precision:  0.75
Recall:  0.79
Fraud Caught out of 330 Cases: 261
</pre>
</div>
</div>
</div>
<div id="outline-container-orgb5ef812" class="outline-3">
<h3 id="orgb5ef812"><span class="section-number-3">7.11.</span> 結論</h3>
<div class="outline-text-3" id="text-7-11">
<p>
普通PCA與ICA能捕捉到超過80%的已知詐欺，並有80%的精確率，較之監督式學習能捕捉到90%，已十分難得。
</p>
</div>
</div>
</div>

<div id="outline-container-org248186a" class="outline-2">
<h2 id="org248186a"><span class="section-number-2">8.</span> 分群</h2>
<div class="outline-text-2" id="text-8">
</div>
<div id="outline-container-org1f2b4d9" class="outline-3">
<h3 id="org1f2b4d9"><span class="section-number-3">8.1.</span> K-Means</h3>
</div>
<div id="outline-container-orga2cab67" class="outline-3">
<h3 id="orga2cab67"><span class="section-number-3">8.2.</span> Hierarchical Clustering</h3>
</div>
<div id="outline-container-org4dc877d" class="outline-3">
<h3 id="org4dc877d"><span class="section-number-3">8.3.</span> DBSCAN</h3>
</div>
</div>

<div id="outline-container-orgd93d89a" class="outline-2">
<h2 id="orgd93d89a"><span class="section-number-2">9.</span> 群組區隔</h2>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Hands-On Machine Learning with Scikit-Learn: Aurelien Geron
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.oreilly.com/library/view/hands-on-unsupervised-learning/9781492035633/">Hands-On Unsupervised Learning Using Python</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2022-06-27 Mon 11:16</p>
</div>
</body>
</html>
