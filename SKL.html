<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-07-02 Sat 20:48 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Scikit Learn</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/white.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Scikit Learn</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org4b130f0">1. 簡介</a></li>
<li><a href="#org9c886e1">2. 常用模組</a></li>
<li><a href="#org5b21781">3. 分類：識別某個對象屬於哪個類別</a></li>
<li><a href="#org1e51cb5">4. 使用 scikit-learn: 感知器演算法</a></li>
<li><a href="#org73e026b">5. 使用 scikit-learn: 邏輯斯迴歸</a></li>
<li><a href="#orgf38a245">6. 使用 SVM</a></li>
<li><a href="#org5b2cf04">7. 使用 SVM 解決非線性問題</a></li>
<li><a href="#org4d4dd71">8. 使用決策樹</a></li>
<li><a href="#org3fd27e9">9. 使用隨機森林結合決策樹</a></li>
<li><a href="#orgad88396">10. KNN</a></li>
<li><a href="#orgfa19954">11. 有母數模型與無母數模型</a></li>
</ul>
</div>
</div>

<div id="outline-container-org4b130f0" class="outline-2">
<h2 id="org4b130f0"><span class="section-number-2">1.</span> 簡介</h2>
<div class="outline-text-2" id="text-1">
<p>
scikit-learn，又寫作 sklearn，是一個開源的基於 python 語言的機器學習工具包。它通過 NumPy, SciPy 和 Matplotlib 等 python 數值計算的庫實現高效的算法應用，並且涵蓋了幾乎所有主流機器學習算法<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。<br />
</p>

<p>
工程應用中，用 python 手寫代碼來從頭實現一個算法的可能性非常低，這樣不僅耗時耗力，還不一定能夠寫出構架清晰，穩定性強的模型。更多情況下，是分析採集到的數據，根據數據特徵選擇適合的算法，在工具包中調用算法，調整算法的參數，獲取需要的信息，從而實現算法效率和效果之間的平衡。而 sklearn，正是這樣一個可以幫助我們高效實現算法應用的工具包。<br />
</p>

<p>
sklearn 有一個完整而豐富的官網，裡面講解了基於 sklearn 對所有算法的實現和簡單應用。<br />
</p>
</div>
</div>

<div id="outline-container-org9c886e1" class="outline-2">
<h2 id="org9c886e1"><span class="section-number-2">2.</span> 常用模組</h2>
<div class="outline-text-2" id="text-2">
<p>
sklearn 中常用的模塊有分類、回歸、聚類、降維、模型選擇、預處理。<br />
</p>
<ul class="org-ul">
<li>分類：識別某個對象屬於哪個類別，常用的算法有：SVM（支持向量機）、nearest neighbors（最近鄰）、random forest（隨機森林），常見的應用有：垃圾郵件識別、圖像識別。<br /></li>
<li>回歸：預測與對象相關聯的連續值屬性，常見的算法有：SVR（支持向量機）、 ridge regression（嶺回歸）、Lasso，常見的應用有：藥物反應，預測股價。<br /></li>
<li>聚類：將相似對象自動分組，常用的算法有：k-Means、 spectral clustering、mean-shift，常見的應用有：客戶細分，分組實驗結果。<br /></li>
<li>降維：減少要考慮的隨機變量的數量，常見的算法有：PCA（主成分分析）、feature selection（特徵選擇）、non-negative matrix factorization（非負矩陣分解），常見的應用有：可視化，提高效率。<br /></li>
<li>模型選擇：比較，驗證，選擇參數和模型，常用的模塊有：grid search（網格搜索）、cross validation（交叉驗證）、 metrics（度量）。它的目標是通過參數調整提高精度。<br /></li>
<li>預處理：特徵提取和歸一化，常用的模塊有：preprocessing，feature extraction，常見的應用有：把輸入數據（如文本）轉換為機器學習算法可用的數據。<br /></li>
</ul>
</div>
</div>

<div id="outline-container-org5b21781" class="outline-2">
<h2 id="org5b21781"><span class="section-number-2">3.</span> 分類：識別某個對象屬於哪個類別</h2>
<div class="outline-text-2" id="text-3">
<p>
解決分類問題要先理解：每個演算法都是基於某些特定前提之下所開發，各有優缺點，故，沒有任何一個分類器可以面對所有情境均能取得最佳解，以下以一資料集(鳶尾花)為例，取出其中兩種屬性，利用不同的分類法進行分類，包括：perception, logistic, SVM, decision tree, k-nearest。<br />
</p>
</div>
</div>

<div id="outline-container-org1e51cb5" class="outline-2">
<h2 id="org1e51cb5"><span class="section-number-2">4.</span> 使用 scikit-learn: 感知器演算法</h2>
<div class="outline-text-2" id="text-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  3: </span>
<span class="linenr">  4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr">  5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr">  6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr">  7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr">  8: </span>
<span class="linenr">  9: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr"> 10: </span>
<span class="linenr"> 11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#34389;&#29702;&#65306;&#23559;&#36039;&#26009;&#20998;&#28858;&#35347;&#32244;&#21644;&#28204;&#35430;</span>
<span class="linenr"> 12: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr"> 13: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 14: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr"> 15: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr"> 16: </span>
<span class="linenr"> 17: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr"> 18: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr"> 19: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr"> 20: </span>
<span class="linenr"> 21: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr"> 22: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 23: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr"> 24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr"> 25: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr"> 26: </span>  sc.fit(X_train)
<span class="linenr"> 27: </span>  X_train_std = sc.transform(X_train)
<span class="linenr"> 28: </span>  X_test_std = sc.transform(X_test)
<span class="linenr"> 29: </span>
<span class="linenr"> 30: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr"> 31: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr"> 32: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 33: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr"> 34: </span>
<span class="linenr"> 35: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr"> 36: </span>      markers = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr"> 37: </span>      colors = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr"> 38: </span>      cmap = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr"> 39: </span>
<span class="linenr"> 40: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr"> 41: </span>      x1_min, x1_max = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 42: </span>      x2_min, x2_max = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 43: </span>      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr"> 44: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr"> 45: </span>      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr"> 46: </span>      Z = Z.reshape(xx1.shape)
<span class="linenr"> 47: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr"> 48: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr"> 49: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr"> 50: </span>
<span class="linenr"> 51: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr"> 52: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr"> 53: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 54: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>,
<span class="linenr"> 55: </span>                      c=colors[idx],
<span class="linenr"> 56: </span>                      marker=markers[idx],
<span class="linenr"> 57: </span>                      label=cl,
<span class="linenr"> 58: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr"> 59: </span>
<span class="linenr"> 60: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr"> 61: </span>      <span style="color: #51afef;">if</span> test_idx:
<span class="linenr"> 62: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr"> 63: </span>          X_test, y_test = X[test_idx, :], y[test_idx]
<span class="linenr"> 64: </span>
<span class="linenr"> 65: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr"> 66: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 67: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr"> 68: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr"> 69: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr"> 70: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr"> 71: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr"> 72: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr"> 73: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr"> 74: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr"> 75: </span>
<span class="linenr"> 76: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;perceptron model</span>
<span class="linenr"> 77: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24478;linear_model&#27169;&#32068;&#20013;&#36617;&#20837;Perceptron&#39006;&#21029;,&#20197;fit&#36914;&#34892;&#35347;&#32244;</span>
<span class="linenr"> 78: </span>  <span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> Perceptron
<span class="linenr"> 79: </span>  ppn = Perceptron(n_iter_no_change=<span style="color: #da8548; font-weight: bold;">40</span>, eta0=<span style="color: #da8548; font-weight: bold;">0.01</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 80: </span>  ppn.fit(X_train_std, y_train)
<span class="linenr"> 81: </span>
<span class="linenr"> 82: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr"> 83: </span>  y_pred = ppn.predict(X_test_std)
<span class="linenr"> 84: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Misclassified samples: %d'</span> % (y_test != y_pred).<span style="color: #c678dd;">sum</span>())
<span class="linenr"> 85: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))</span>
<span class="linenr"> 86: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Accuracy: %.2f'</span> % ppn.score(X_test_std, y_test))
<span class="linenr"> 87: </span>
<span class="linenr"> 88: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr"> 89: </span>  X_combined_std = np.vstack((X_train_std, X_test_std))
<span class="linenr"> 90: </span>  y_combined = np.hstack((y_train, y_test))
<span class="linenr"> 91: </span>
<span class="linenr"> 92: </span>  plot_decision_regions(X=X_combined_std, y=y_combined,
<span class="linenr"> 93: </span>                        classifier=ppn, test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr"> 94: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr"> 95: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr"> 96: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr"> 97: </span>
<span class="linenr"> 98: </span>  plt.tight_layout()
<span class="linenr"> 99: </span>  plt.savefig(<span style="color: #98be65;">'03_01.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">100: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
Misclassified samples: 3
Accuracy: 0.93
</pre>


<p>
要找出一個合適的學習速率，需要一些實務經驗。如果學習速率過大，演算法會衝過「全域最小值」；如果學習速率過小，則演算法就要迭代很多很多次才會收歛。在上例中，學習速率(eta)在設為 0.1 時，分類準確率為 0.82；而當設為 0.01 時，分類準確率為 0.93。<br />
</p>

<p>
感知器演算法遇到無法完美「線性分類」的數據時，會導致無法收歛，如圖<a href="#org6b462f4">1</a>。<br />
</p>


<div id="org6b462f4" class="figure">
<p><img src="images/03_01.png" alt="03_01.png" /><br />
</p>
<p><span class="figure-number">Figure 1: </span>線性模式的限制：無法完美分類三種花</p>
</div>
</div>
</div>

<div id="outline-container-org73e026b" class="outline-2">
<h2 id="org73e026b"><span class="section-number-2">5.</span> 使用 scikit-learn: 邏輯斯迴歸</h2>
<div class="outline-text-2" id="text-5">
<p>
Logistic regression 簡稱 logreg，雖然其中譯為邏輯斯迴歸，但與邏輯無關，也和迴歸沒關係，它本質上是一種分類演算法，常被視為現代機器學習的&ldquo;Hello world.&rdquo;。<br />
</p>

<p>
當要處理的數據並非「線性可分」，則在每一輪的迭代中，總會有一些樣本會被錯誤分類，於是加權就會不斷的被更新，對於這種數據，感知器絕對不會收歛。<br />
</p>

<p>
要理解 logistic regression, 需先理解「勝算比」(odds ratio)，勝算比可以\( \frac{p}{1-p} \)，其中\(p\)為「正事件」(positive event)的機率，此一詞並不一定意味著好，只是指我們想要預測的事件。例如，某個病患且具有某種疾病的機率；我們可以將「正事件」視為類別標籤 y=1，然後，我們可以進一步定義 logit 函數，稱為「對數勝算」(log-odds)：<br />
\[logit(p) = log\frac{p}{1-p}\]<br />
函數\(logit\)的輸入為 0 到 1 間的值，它會將其轉換為分佈於整個實數範圍內的值，我們可以用它來表達「特徵值」和「勝算比」間的線性關係：<br />
\[logit(p(y=1|x))=w_0x_0+w_1x_1+...+w_mx_m=\sum_{i=1}^{m}=w^tx\]<br />
在此\(p(y=1|x)\)的意思是：給定特徵值 x，當某特定樣本屬於類別 1 的條件機率。<br />
</p>

<p>
實際上，我們真正感興趣的是：預測某特定樣本屬於特定類別的機率，也就是 logit 函數的反函數，又稱為 logistic 函數。又因為其特殊的 S 形曲線(如圖<a href="#orgda06c09">2</a>)，故有時也被稱為 sigmoid 函數。<br />
</p>

<div id="orgda06c09" class="figure">
<p><img src="images/sigmoidplot2.png" alt="sigmoidplot2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 2: </span>sigmoid 函數圖</p>
</div>



<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  3: </span>
<span class="linenr">  4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr">  5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr">  6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr">  7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr">  8: </span>
<span class="linenr">  9: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr"> 10: </span>
<span class="linenr"> 11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr"> 12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr"> 14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr"> 15: </span>
<span class="linenr"> 16: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr"> 17: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr"> 18: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr"> 19: </span>
<span class="linenr"> 20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr"> 21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr"> 23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr"> 24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr"> 25: </span>  sc.fit(X_train)
<span class="linenr"> 26: </span>  X_train_std = sc.transform(X_train)
<span class="linenr"> 27: </span>  X_test_std = sc.transform(X_test)
<span class="linenr"> 28: </span>
<span class="linenr"> 29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr"> 30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr"> 31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr"> 33: </span>
<span class="linenr"> 34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr"> 35: </span>      markers = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr"> 36: </span>      colors = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr"> 37: </span>      cmap = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr"> 38: </span>
<span class="linenr"> 39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr"> 40: </span>      x1_min, x1_max = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 41: </span>      x2_min, x2_max = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 42: </span>      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr"> 43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr"> 44: </span>      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr"> 45: </span>      Z = Z.reshape(xx1.shape)
<span class="linenr"> 46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr"> 47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr"> 48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr"> 49: </span>
<span class="linenr"> 50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr"> 51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr"> 52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>,
<span class="linenr"> 54: </span>                      c=colors[idx],
<span class="linenr"> 55: </span>                      marker=markers[idx],
<span class="linenr"> 56: </span>                      label=cl,
<span class="linenr"> 57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr"> 58: </span>
<span class="linenr"> 59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr"> 60: </span>      <span style="color: #51afef;">if</span> test_idx:
<span class="linenr"> 61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr"> 62: </span>          X_test, y_test = X[test_idx, :], y[test_idx]
<span class="linenr"> 63: </span>
<span class="linenr"> 64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr"> 65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr"> 67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr"> 68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr"> 69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr"> 70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr"> 71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr"> 72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr"> 73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr"> 74: </span>
<span class="linenr"> 75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;scikit-learn&#35347;&#32244;&#19968;&#20491;logistic regression model</span>
<span class="linenr"> 76: </span>  <span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr"> 77: </span>  lr = LogisticRegression(C=<span style="color: #da8548; font-weight: bold;">100.0</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 78: </span>  lr.fit(X_train_std, y_train)
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr"> 81: </span>  X_combined_std = np.vstack((X_train_std, X_test_std))
<span class="linenr"> 82: </span>  y_combined = np.hstack((y_train, y_test))
<span class="linenr"> 83: </span>
<span class="linenr"> 84: </span>  plot_decision_regions(X_combined_std, y_combined,
<span class="linenr"> 85: </span>                        classifier=lr,
<span class="linenr"> 86: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr"> 87: </span>
<span class="linenr"> 88: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr"> 89: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr"> 90: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr"> 91: </span>
<span class="linenr"> 92: </span>  plt.tight_layout()
<span class="linenr"> 93: </span>  plt.savefig(<span style="color: #98be65;">'03_06.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr"> 94: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr"> 95: </span>
<span class="linenr"> 96: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr"> 97: </span>  <span style="color: #c678dd;">print</span>(lr.predict_proba(X_test_std[:<span style="color: #da8548; font-weight: bold;">3</span>, :]))
<span class="linenr"> 98: </span>  <span style="color: #c678dd;">print</span>(lr.predict_proba(X_test_std[:<span style="color: #da8548; font-weight: bold;">3</span>, :]).<span style="color: #c678dd;">sum</span>(axis=<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr"> 99: </span>  <span style="color: #c678dd;">print</span>(lr.predict_proba(X_test_std[:<span style="color: #da8548; font-weight: bold;">3</span>, :]).argmax(axis=<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">100: </span>  <span style="color: #c678dd;">print</span>(lr.predict(X_test_std[:<span style="color: #da8548; font-weight: bold;">3</span>, :]))
<span class="linenr">101: </span>  <span style="color: #c678dd;">print</span>(lr.predict(X_test_std[<span style="color: #da8548; font-weight: bold;">0</span>, :].reshape(<span style="color: #da8548; font-weight: bold;">1</span>, -<span style="color: #da8548; font-weight: bold;">1</span>)))
<span class="linenr">102: </span>
</pre>
</div>

<pre class="example" id="org9cdce66">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
[[3.17983737e-08 1.44886616e-01 8.55113353e-01]
 [8.33962295e-01 1.66037705e-01 4.55557009e-12]
 [8.48762934e-01 1.51237066e-01 4.63166788e-13]]
[1. 1. 1.]
[2 0 0]
[2 0 0]
[2]
</pre>


<div id="org8683769" class="figure">
<p><img src="images/03_06.png" alt="03_06.png" /><br />
</p>
<p><span class="figure-number">Figure 3: </span>Logistic Regression: 鳶尾花分類</p>
</div>

<p>
lr.predict_proba(X_test_std[:3, :]))列出一個二維陣列，各列中的最高值均代表預測的答案，故，第一列預測答案為第一類花的機率最高(0.855)；也可以通過找出每列中最大的行值來取得預測的類別標籤：lr.predict_proba(X_test_std[:3, :]).argmax(axis=1))。<br />
</p>
</div>
</div>

<div id="outline-container-orgf38a245" class="outline-2">
<h2 id="orgf38a245"><span class="section-number-2">6.</span> 使用 SVM</h2>
<div class="outline-text-2" id="text-6">
<p>
SVM 的目標是在兩種類別(class)的資料點(data points)間找到最佳決策邊界(decision boundaries)，SVM 透過以下兩個步驟來找到決策邊介：<br />
</p>
<ol class="org-ol">
<li>把資料映射到一個高維表示法，在此空間屬找最佳決策邊界，一般這個邊人田中中為一超曲面。至於為何要將資料映射到高維度空，可由圖<a href="#org0cdcd95">4</a>看出其原理，圖左的二維資料點原本很難找到有效的區分策略，但若將其映射到 3 維空間，這些資料點很有可能會變成圖右的狀況，此時就能透過一個平面將兩類資料點做有效的分割。<br /></li>
</ol>


<div id="org0cdcd95" class="figure">
<p><img src="images/DecisionSurface-1.jpg" alt="DecisionSurface-1.jpg" /><br />
</p>
<p><span class="figure-number">Figure 4: </span>SVM 映射高維空間</p>
</div>

<ol class="org-ol">
<li>在找超曲面時，超曲面要位於兩類倶料點之間的中線。這步驟稱為最大化邊界(maximizing the margin)，也就是讓曲面分別和兩類資料點保有最大距離，如圖<a href="#org1733c2f">5</a>。<br /></li>
</ol>


<div id="org1733c2f" class="figure">
<p><img src="images/svm-max.jpg" alt="svm-max.jpg" /><br />
</p>
<p><span class="figure-number">Figure 5: </span>SVM</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr">14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr">17: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr">18: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr">25: </span>  sc.fit(X_train)
<span class="linenr">26: </span>  X_train_std = sc.transform(X_train)
<span class="linenr">27: </span>  X_test_std = sc.transform(X_test)
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">35: </span>      markers = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">36: </span>      colors = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">37: </span>      cmap = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">38: </span>
<span class="linenr">39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">40: </span>      x1_min, x1_max = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      x2_min, x2_max = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">42: </span>      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">44: </span>      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">45: </span>      Z = Z.reshape(xx1.shape)
<span class="linenr">46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">49: </span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>,
<span class="linenr">54: </span>                      c=colors[idx],
<span class="linenr">55: </span>                      marker=markers[idx],
<span class="linenr">56: </span>                      label=cl,
<span class="linenr">57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">58: </span>
<span class="linenr">59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">60: </span>      <span style="color: #51afef;">if</span> test_idx:
<span class="linenr">61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">62: </span>          X_test, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">63: </span>
<span class="linenr">64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr">76: </span>  X_combined_std = np.vstack((X_train_std, X_test_std))
<span class="linenr">77: </span>  y_combined = np.hstack((y_train, y_test))
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;SVM&#20358;&#38928;&#28204;</span>
<span class="linenr">80: </span>  <span style="color: #51afef;">from</span> sklearn.svm <span style="color: #51afef;">import</span> SVC
<span class="linenr">81: </span>  svm = SVC(kernel=<span style="color: #98be65;">'linear'</span>, C=<span style="color: #da8548; font-weight: bold;">1.0</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">82: </span>  svm.fit(X_train_std, y_train)
<span class="linenr">83: </span>
<span class="linenr">84: </span>  plot_decision_regions(X_combined_std, y_combined,
<span class="linenr">85: </span>                        classifier=svm,
<span class="linenr">86: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">87: </span>
<span class="linenr">88: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr">89: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr">90: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">91: </span>
<span class="linenr">92: </span>  plt.tight_layout()
<span class="linenr">93: </span>  plt.savefig(<span style="color: #98be65;">'03_11.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">94: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">95: </span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
</pre>



<div id="orga095958" class="figure">
<p><img src="images/03_11.png" alt="03_11.png" /><br />
</p>
<p><span class="figure-number">Figure 6: </span>SVM Regression: 鳶尾花分類</p>
</div>

<p>
Logistic regression(圖<a href="#org8683769">3</a>)與 SVM(圖<a href="#orga095958">6</a>)常會產生相似的結果，但 logistic regression 試圖最大化「訓練數據集」的「條件概似」(conditional likelihood)，這會使 logistic regression 比 SVM 更容易傾向「離群值」(outlier)，SVM 主要在意的是那些非常接近「決策邊界」的那些點。Logistic regression 的優點是簡單。<br />
</p>
</div>
</div>

<div id="outline-container-org5b2cf04" class="outline-2">
<h2 id="org5b2cf04"><span class="section-number-2">7.</span> 使用 SVM 解決非線性問題</h2>
<div class="outline-text-2" id="text-7">
<p>
SVM 受到愛用的一個主要原因是它可用「核心化」(kernelized)來解決非線性分類問題(如 XOR)。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">X_xor</span> = np.random.randn(<span style="color: #da8548; font-weight: bold;">200</span>, <span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">y_xor</span> = np.logical_xor(X_xor[:, <span style="color: #da8548; font-weight: bold;">0</span>] &gt; <span style="color: #da8548; font-weight: bold;">0</span>,
<span class="linenr"> 5: </span>                         X_xor[:, <span style="color: #da8548; font-weight: bold;">1</span>] &gt; <span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">y_xor</span> = np.where(y_xor, <span style="color: #da8548; font-weight: bold;">1</span>, -<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">10: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">11: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">polot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">12: </span>
<span class="linenr">13: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">14: </span>      markers = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">15: </span>      colors = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">16: </span>      cmap = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">17: </span>
<span class="linenr">18: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">19: </span>      x1_min, x1_max = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">20: </span>      x2_min, x2_max = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">21: </span>      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">22: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">23: </span>      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">24: </span>      Z = Z.reshape(xx1.shape)
<span class="linenr">25: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">26: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">27: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">28: </span>
<span class="linenr">29: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">30: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">31: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">32: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>,
<span class="linenr">33: </span>                      c=colors[idx],
<span class="linenr">34: </span>                      marker=markers[idx],
<span class="linenr">35: </span>                      label=cl,
<span class="linenr">36: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">37: </span>
<span class="linenr">38: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">39: </span>      <span style="color: #51afef;">if</span> test_idx:
<span class="linenr">40: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">41: </span>          X_test, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">42: </span>
<span class="linenr">43: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">44: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">45: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">46: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">47: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">48: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">49: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">50: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">51: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">52: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">53: </span>
<span class="linenr">54: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Using the kernel trick to find separating hyperplanes in higher dimensional space</span>
<span class="linenr">55: </span>  <span style="color: #51afef;">from</span> sklearn.svm <span style="color: #51afef;">import</span> SVC
<span class="linenr">56: </span>  svm = SVC(kernel=<span style="color: #98be65;">'rbf'</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, gamma=<span style="color: #da8548; font-weight: bold;">0.10</span>, C=<span style="color: #da8548; font-weight: bold;">10.0</span>)
<span class="linenr">57: </span>  svm.fit(X_xor, y_xor)
<span class="linenr">58: </span>  plot_decision_regions(X_xor, y_xor,
<span class="linenr">59: </span>                        classifier=svm)
<span class="linenr">60: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">61: </span>  plt.tight_layout()
<span class="linenr">62: </span>  plt.savefig(<span style="color: #98be65;">'03_14.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">63: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">64: </span>
</pre>
</div>


<div id="orge64bec4" class="figure">
<p><img src="images/03_14.png" alt="03_14.png" /><br />
</p>
<p><span class="figure-number">Figure 7: </span>SVM 解決非線性問題: XOR</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr">14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr">17: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr">18: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr">25: </span>  sc.fit(X_train)
<span class="linenr">26: </span>  X_train_std = sc.transform(X_train)
<span class="linenr">27: </span>  X_test_std = sc.transform(X_test)
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">35: </span>      markers = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">36: </span>      colors = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">37: </span>      cmap = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">38: </span>
<span class="linenr">39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">40: </span>      x1_min, x1_max = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      x2_min, x2_max = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">42: </span>      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">44: </span>      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">45: </span>      Z = Z.reshape(xx1.shape)
<span class="linenr">46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">49: </span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>,
<span class="linenr">54: </span>                      c=colors[idx],
<span class="linenr">55: </span>                      marker=markers[idx],
<span class="linenr">56: </span>                      label=cl,
<span class="linenr">57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">58: </span>
<span class="linenr">59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">60: </span>      <span style="color: #51afef;">if</span> test_idx:
<span class="linenr">61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">62: </span>          X_test, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">63: </span>
<span class="linenr">64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr">76: </span>  X_combined_std = np.vstack((X_train_std, X_test_std))
<span class="linenr">77: </span>  y_combined = np.hstack((y_train, y_test))
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;SVM&#20358;&#38928;&#28204;&#40182;&#23614;&#33457;</span>
<span class="linenr">80: </span>  <span style="color: #51afef;">from</span> sklearn.svm <span style="color: #51afef;">import</span> SVC
<span class="linenr">81: </span>  svm = SVC(kernel=<span style="color: #98be65;">'rbf'</span>, random_state=<span style="color: #da8548; font-weight: bold;">0</span>, gamma=<span style="color: #da8548; font-weight: bold;">0.10</span>, C=<span style="color: #da8548; font-weight: bold;">10.0</span>)
<span class="linenr">82: </span>  svm.fit(X_train_std, y_train)
<span class="linenr">83: </span>
<span class="linenr">84: </span>  plot_decision_regions(X_combined_std, y_combined,
<span class="linenr">85: </span>                        classifier=svm,
<span class="linenr">86: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">87: </span>
<span class="linenr">88: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr">89: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr">90: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">91: </span>
<span class="linenr">92: </span>  plt.tight_layout()
<span class="linenr">93: </span>  plt.savefig(<span style="color: #98be65;">'03_15.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">94: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">95: </span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
</pre>



<div id="org8dd5e87" class="figure">
<p><img src="images/03_15.png" alt="03_15.png" /><br />
</p>
<p><span class="figure-number">Figure 8: </span>SVM: 鳶尾花分類</p>
</div>
</div>
</div>

<div id="outline-container-org4d4dd71" class="outline-2">
<h2 id="org4d4dd71"><span class="section-number-2">8.</span> 使用決策樹</h2>
<div class="outline-text-2" id="text-8">
<p>
「決策樹」可透過劃分特徵空間來建構複雜的矩形「決策邊界」。然而，越深的「決策樹」便會產生越複雜的「決策邊界」，因而導致過度擬知。以下透過 scikit-learn，以「熵」作為「不純度」的標準，訓練一個深度最多為 3 的「決策樹」。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr">14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr">17: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr">18: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr">25: </span>  sc.fit(X_train)
<span class="linenr">26: </span>  X_train_std = sc.transform(X_train)
<span class="linenr">27: </span>  X_test_std = sc.transform(X_test)
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">35: </span>      markers = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">36: </span>      colors = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">37: </span>      cmap = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">38: </span>
<span class="linenr">39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">40: </span>      x1_min, x1_max = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      x2_min, x2_max = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">42: </span>      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">44: </span>      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">45: </span>      Z = Z.reshape(xx1.shape)
<span class="linenr">46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">49: </span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>,
<span class="linenr">54: </span>                      c=colors[idx],
<span class="linenr">55: </span>                      marker=markers[idx],
<span class="linenr">56: </span>                      label=cl,
<span class="linenr">57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">58: </span>
<span class="linenr">59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">60: </span>      <span style="color: #51afef;">if</span> test_idx:
<span class="linenr">61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">62: </span>          X_test, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">63: </span>
<span class="linenr">64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr">76: </span>  X_combined = np.vstack((X_train, X_test))
<span class="linenr">77: </span>  y_combined = np.hstack((y_train, y_test))
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#27770;&#31574;&#27193;&#20358;&#38928;&#28204;&#40182;&#23614;&#33457;</span>
<span class="linenr">80: </span>  <span style="color: #51afef;">from</span> sklearn.tree <span style="color: #51afef;">import</span> DecisionTreeClassifier
<span class="linenr">81: </span>  tree = DecisionTreeClassifier(criterion=<span style="color: #98be65;">'gini'</span>,
<span class="linenr">82: </span>                                max_depth=<span style="color: #da8548; font-weight: bold;">4</span>,
<span class="linenr">83: </span>                                random_state=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">84: </span>  tree.fit(X_train, y_train)
<span class="linenr">85: </span>
<span class="linenr">86: </span>  plot_decision_regions(X_combined, y_combined,
<span class="linenr">87: </span>                        classifier=tree,
<span class="linenr">88: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">89: </span>
<span class="linenr">90: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr">91: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr">92: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">93: </span>
<span class="linenr">94: </span>  plt.tight_layout()
<span class="linenr">95: </span>  plt.savefig(<span style="color: #98be65;">'03_17.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">96: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">97: </span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
</pre>



<div id="org9f0991c" class="figure">
<p><img src="images/03_17.png" alt="03_17.png" /><br />
</p>
<p><span class="figure-number">Figure 9: </span>Decision Tree: 鳶尾花分類</p>
</div>
</div>
</div>

<div id="outline-container-org3fd27e9" class="outline-2">
<h2 id="org3fd27e9"><span class="section-number-2">9.</span> 使用隨機森林結合決策樹</h2>
<div class="outline-text-2" id="text-9">
<p>
「隨機森林」(random forest)可被視為多個「決策樹」結合成的一個整體(ensemble)，隨機森林的想法是結合多個具有高變異的深度決策樹，將它們的結果平均，來建構一個更強固的模型，這種模型的「一般化誤差」較低，也較不會發生過度擬合的問題。<br />
</p>

<p>
隨機森林演算法可以簡單歸納為下列四個步驟：<br />
</p>
<ol class="org-ol">
<li>定義大小為 n 的隨機「自助」(bootstrap)樣本（從「訓練樣本集」中隨機選擇 n 個樣本，採用「取出後放回」方式）。<br /></li>
<li>從 bootstrap 樣本中導出決策樹：<br />
<ul class="org-ul">
<li>隨機選擇 d 個特徵<br /></li>
<li>使用特徵分割該節點，依「目標函數」找出最佳方式<br /></li>
</ul></li>
<li>重複 k 次步驟 1 與 2<br /></li>
<li>匯總所有決策樹的預測，以「多數決」(majority voting)的方式來指定類別標籤。<br /></li>
</ol>

<p>
隨機森林的一大優勢是我們不必擔心如何選擇一個好的「超參數」值，一般而言，我們不需修剪「隨機森林」，因為「整體學習模型」是相當強固的，個別「決策樹」的雜訊不會影響整體結果。我們唯一真正需要關心的參數是上述步驟中的數量 k。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr">14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr">17: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr">18: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr">25: </span>  sc.fit(X_train)
<span class="linenr">26: </span>  X_train_std = sc.transform(X_train)
<span class="linenr">27: </span>  X_test_std = sc.transform(X_test)
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">35: </span>      markers = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">36: </span>      colors = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">37: </span>      cmap = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">38: </span>
<span class="linenr">39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">40: </span>      x1_min, x1_max = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      x2_min, x2_max = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">42: </span>      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">44: </span>      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">45: </span>      Z = Z.reshape(xx1.shape)
<span class="linenr">46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">49: </span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>,
<span class="linenr">54: </span>                      c=colors[idx],
<span class="linenr">55: </span>                      marker=markers[idx],
<span class="linenr">56: </span>                      label=cl,
<span class="linenr">57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">58: </span>
<span class="linenr">59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">60: </span>      <span style="color: #51afef;">if</span> test_idx:
<span class="linenr">61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">62: </span>          X_test, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">63: </span>
<span class="linenr">64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr">76: </span>  X_combined = np.vstack((X_train, X_test))
<span class="linenr">77: </span>  y_combined = np.hstack((y_train, y_test))
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#38568;&#27231;&#26862;&#26519;&#27770;&#31574;&#27193;&#20358;&#38928;&#28204;&#40182;&#23614;&#33457;</span>
<span class="linenr">80: </span>  <span style="color: #51afef;">from</span> sklearn.ensemble <span style="color: #51afef;">import</span> RandomForestClassifier
<span class="linenr">81: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">n_estimators: &#29986;&#29983;&#30340;&#27770;&#31574;&#27193;&#25976;&#37327;</span>
<span class="linenr">82: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;model&#26178;&#20351;&#29992;&#30340;&#38651;&#33126;&#26680;&#24515;&#25976;&#37327;</span>
<span class="linenr">83: </span>  forest = RandomForestClassifier(criterion=<span style="color: #98be65;">'gini'</span>,
<span class="linenr">84: </span>                                  n_estimators=<span style="color: #da8548; font-weight: bold;">25</span>,
<span class="linenr">85: </span>                                  random_state=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">86: </span>                                  n_jobs=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">87: </span>  forest.fit(X_train, y_train)
<span class="linenr">88: </span>  plot_decision_regions(X_combined, y_combined,
<span class="linenr">89: </span>                        classifier=forest,
<span class="linenr">90: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">91: </span>
<span class="linenr">92: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr">93: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr">94: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">95: </span>
<span class="linenr">96: </span>  plt.tight_layout()
<span class="linenr">97: </span>  plt.savefig(<span style="color: #98be65;">'03_18.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">98: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
</pre>


<p>
+ATTR_HTML: :width 500<br />
<img src="images/03_18.png" alt="03_18.png" /><br />
</p>
</div>
</div>

<div id="outline-container-orgad88396" class="outline-2">
<h2 id="orgad88396"><span class="section-number-2">10.</span> KNN</h2>
<div class="outline-text-2" id="text-10">
<p>
KNN (k-nearest neighbor classifier)為 lazy learner(惰性學習器)的典型例子，所謂惰性是指它不會從「訓練數據集」中學習出「判別函數」(discriminative function)，它的作法是把「訓練數據集」記憶起來。其步驟如下：<br />
</p>
<ol class="org-ol">
<li>選定 k 的值和一個「距離度量」(distance metric)。<br /></li>
<li>找出 k 個想要分類的、最相近的鄰近樣本。<br /></li>
<li>以多數決的方式指定類別標籤。<br /></li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">X</span> = iris.data[:, [<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>]]
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">y</span> = iris.target
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Class labels:'</span>, np.unique(y))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">stratify=y, &#34920;&#31034;&#26371;&#23559;&#36039;&#26009;&#20381;test_size&#27604;&#20363;&#22238;&#20659;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">13: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(
<span class="linenr">14: </span>      X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>, stratify=y)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y:'</span>, np.bincount(y))
<span class="linenr">17: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_train:'</span>, np.bincount(y_train))
<span class="linenr">18: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Labels counts in y_test:'</span>, np.bincount(y_test))
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">22: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr">25: </span>  sc.fit(X_train)
<span class="linenr">26: </span>  X_train_std = sc.transform(X_train)
<span class="linenr">27: </span>  X_test_std = sc.transform(X_test)
<span class="linenr">28: </span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">31: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, test_idx=<span style="color: #a9a1e1;">None</span>, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">35: </span>      markers = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">36: </span>      colors = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">37: </span>      cmap = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">38: </span>
<span class="linenr">39: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">40: </span>      x1_min, x1_max = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      x2_min, x2_max = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">42: </span>      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">43: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">44: </span>      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">45: </span>      Z = Z.reshape(xx1.shape)
<span class="linenr">46: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">47: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">49: </span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>,
<span class="linenr">54: </span>                      c=colors[idx],
<span class="linenr">55: </span>                      marker=markers[idx],
<span class="linenr">56: </span>                      label=cl,
<span class="linenr">57: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">58: </span>
<span class="linenr">59: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">highlight test samples</span>
<span class="linenr">60: </span>      <span style="color: #51afef;">if</span> test_idx:
<span class="linenr">61: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot all samples</span>
<span class="linenr">62: </span>          X_test, y_test = X[test_idx, :], y[test_idx]
<span class="linenr">63: </span>
<span class="linenr">64: </span>          plt.scatter(X_test[:, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">65: </span>                      X_test[:, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">66: </span>                      c=<span style="color: #98be65;">''</span>,
<span class="linenr">67: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">68: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">1.0</span>,
<span class="linenr">69: </span>                      linewidth=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr">70: </span>                      marker=<span style="color: #98be65;">'o'</span>,
<span class="linenr">71: </span>                      s=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">72: </span>                      label=<span style="color: #98be65;">'test set'</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=======&#32362;&#22294;&#21103;&#31243;&#24335;=======</span>
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training a perceptron model using the standardized training data:</span>
<span class="linenr">76: </span>  X_combined_std = np.vstack((X_train_std, X_test))
<span class="linenr">77: </span>  y_combined = np.hstack((y_train, y_test))
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;KNN&#20358;&#38928;&#28204;&#40182;&#23614;&#33457;</span>
<span class="linenr">80: </span>  <span style="color: #51afef;">from</span> sklearn.neighbors <span style="color: #51afef;">import</span> KNeighborsClassifier
<span class="linenr">81: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">n_neighbors: &#35201;&#25214;&#20986;&#30340;&#26368;&#36817;&#30456;&#37168;&#27171;&#26412;&#25976;</span>
<span class="linenr">82: </span>  knn = KNeighborsClassifier(n_neighbors=<span style="color: #da8548; font-weight: bold;">5</span>, p=<span style="color: #da8548; font-weight: bold;">2</span>,
<span class="linenr">83: </span>                             metric=<span style="color: #98be65;">'minkowski'</span>)
<span class="linenr">84: </span>  knn.fit(X_train_std, y_train)
<span class="linenr">85: </span>  plot_decision_regions(X_combined_std, y_combined,
<span class="linenr">86: </span>                        classifier=knn,
<span class="linenr">87: </span>                        test_idx=<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">105</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">88: </span>
<span class="linenr">89: </span>  plt.xlabel(<span style="color: #98be65;">'petal length [standardized]'</span>)
<span class="linenr">90: </span>  plt.ylabel(<span style="color: #98be65;">'petal width [standardized]'</span>)
<span class="linenr">91: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">92: </span>
<span class="linenr">93: </span>  plt.tight_layout()
<span class="linenr">94: </span>  plt.savefig(<span style="color: #98be65;">'03_19.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">95: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">96: </span>
</pre>
</div>

<pre class="example">
Class labels: [0 1 2]
Labels counts in y: [50 50 50]
Labels counts in y_train: [35 35 35]
Labels counts in y_test: [15 15 15]
</pre>



<div id="org5d341f4" class="figure">
<p><img src="images/03_19.png" alt="03_19.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 10: </span>KNN: 鳶尾花分類</p>
</div>

<p>
想要在「過度擬合」與「擬合不足」之間取得一個良好的平衡，其中一個關鍵因素是選擇一個「好」的 k 值。我們選擇的「距離度量」必須是對該「訓練數據集」的特徵是有意義的。通常會以簡單的「歐氏距離」來度量實數值的樣本。<br />
</p>
</div>
</div>

<div id="outline-container-orgfa19954" class="outline-2">
<h2 id="orgfa19954"><span class="section-number-2">11.</span> 有母數模型與無母數模型</h2>
<div class="outline-text-2" id="text-11">
<p>
「機器學習演算法」可分為「有母數」(parametric)和「無母數」(nonparametric)模型。使用有母數模型，我們從「訓練數據集」估計參數值，並學習出一個函數，他可以不需要原始的「訓練數據集」而對新數據集進行分類。「感知器模型」、「logistic regression」與「線性 SVM」都是典型的例子。另一方面，「無母數模型」無法用一組固定的特徵來描述，此外，特徵的個數還會因為「訓練數據集」的增大而變多 「決策樹」/「隨機森林」與「核支援向量機」則是目前我們看到的「無母數模型「的例子。KNN 也被歸類為「無母數模型」中的一種，稱為「基於實例學習」(instance-based learning)。「基於實例學習」會把「訓練數據集」記憶起來。而「惰性學習器」則是「基於實例學習」中的一個特例。「惰性學習器」在學習的過程中是沒有成本的。<br />
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.itread01.com/content/1551725786.html">機器學習入門之sklearn介紹</a><br />
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2022-07-02 Sat 20:48</p>
</div>
</body>
</html>
