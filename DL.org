#+TITLE: 深度學習(Deep Learning)
# -*- org-export-babel-evaluate: nil -*-
#+TAGS: AI
#+OPTIONS: toc:2 ^:nil num:5
#+PROPERTY: header-args :eval never-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/white.css" />
#+EXCLUDE_TAGS: noexport
#+latex:\newpage

* 深度學習
神經網絡的基礎模型是感知機(Perceptron)，因此神經網絡也可以叫做多層感知機(Multi-layer Perceptron)，簡稱MLP。單層感知機叫做感知機，多層感知機(MLP)≈人工神經網絡(ANN)[fn:1]。

那麼多層到底是幾層？一般來說有1-2個隱藏層的神經網絡就可以叫做多層，準確的說是(淺層)神經網絡(Shallow Neural Networks)。隨著隱藏層的增多，更深的神經網絡(一般來說超過5層)就都叫做深度學習(DNN)[fn:1]。

然而，「深度」只是一個商業概念，很多時候工業界把3層隱藏層也叫做「深度學習」，所以不要在層數上太較真。在機器學習領域的約定俗成是，名字中有深度(Deep)的網絡僅代表其有超過5-7層的隱藏層[fn:1]。

深度學習是加深層數後的多層神經網路。MNIST 歷年的競賽前幾名都是以 CNN 為基礎，進一步提高辨識準確率的方法還包括整體學習、學習率遞減（learning rate decay）、資料擴增（Data Augmentation, 如利用旋轉、垂直或水平移動輸入影像來小幅改變輸入資料以增加輸入影像張數）。

關於增加層數的重要性，目前還缺乏理論佐證，但從過往的研究或實驗中，有幾點可以說明。
1. 在 ILSVRC 這種大型視覺辨識競賽結果中，加深層數的比例多與辨識效能成正比。
1. 加深層數可以在減少網路參數的狀況下得到相同成效，透過重叠層級，可以讓 ReLU 等活化函數夾在卷積層之間，進一步提高網路的表現力，因為透過活化函數，可以在網路增加「非線性」的能力，重叠非線性性函數，也能達到更複雜的表現力。
1. 學習的效率也是加深層數的優點之一，卷積層的神經元會反應出邊界等單純形狀，隨著層數增加，可以反應出紋理、物體部位等特質，依照階層逐漸變複雜。
1. 以辨識「狗」為例子，如果要以層數較少的網路來解決這個問題，卷積層就要一次「理解」眾多特徵，還要因應不同拍攝環境帶來的變化，一次處理這些龐大的資料會花費許多學習時間； 如果加深層數，就能用階層分解必須學習的問題，每一層可以處理單純的問題，例如，最初的層級可以只學習邊界，利用少量的學習資料來進行效率化的學習。
1. 加深層數可以階層性的傳遞資料，例如，擷取出邊界的下一層會使用邊界資料來學習更高階的問題（如判斷形狀）。

典型的深度學習如圖[[fig:python-deep-learning-1]]，在此例中，輸入為一張手寫數字的影像，經由 4 層的深度學習模型後得知此數字為 4。

#+CAPTION: 典型的深度神經網路-1
#+LABEL:fig:python-deep-learning-1
#+name: fig:python-deep-learning-1
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/img-191107113927.jpg]]

圖[[fig:python-deep-learning-2]]進一步說明網路模型中每一層的作用，可以將每一層網路視為對影像的特殊運算，如此一層一層逐一精煉(purified)，最後得到結果。

#+CAPTION: 典型的深度神經網路-2
#+LABEL:fig:python-deep-learning-2
#+name: fig:python-deep-learning-2
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 500
[[file:images/img-1911071139277.jpg]]

* 建構良好的訓練集：數據預處理

進行數運模式運算之前，需要進行的數據預處理工作大致可分為以下幾點：
1. 數據遺漏值處理
1. 數據分類編碼
1. 數據訓練集與測試集之分割
1. 數據特徵選取

** 處理數據遺漏

現實世界中可能會因各種原因導致數據缺失或遺漏(如問卷被刻意留白)，這些部份通常會以「空白」、「NaN」或「NULL」來取代。

*** 遺漏值的識別

#+BEGIN_SRC python -r -n :results output :exports both
  csv_data = '''A,X,B,C,D
  1.0,,2.0,3.0,4.0
  5.0,,6.0,,8.0
  10.0,,11.0,12.0
  ,,,,'''
  import sys
  import pandas as pd
  # python 2.7需進行unicode轉碼
  if (sys.version_info < (3, 0)):
      csv_data = unicode(csv_data)
  # 讀入程式檔中的csv資料
  from io import StringIO
  df = pd.read_csv(StringIO(csv_data))
  print(df)
  # 列出每行有的null個數
  print(df.isnull().sum())
  # access the underlying NumPy array
  # via the `values` attribute
  df.values

  # 剛除有遺失值的資料列
  print('刪掉有遺失值的列:df.dropna(axis=1)')
  print(df.dropna(axis=0))
  # 剛除有遺失值的資料行
  print('刪掉有遺失值的行:df.dropna(axis=1)')
  print(df.dropna(axis=1))
  # 剛除整列為NaN者
  print('剛除整行為NaN者:df.dropna(how=\'all\')')
  print(df.dropna(how='all') )
  # 刪除有值個數低於thresh的列
  print('刪除有值個數低於thresh的列:df.dropna(thresh=4)')
  print(df.dropna(thresh=4))
  # 刪除特定行(如第C行)中有NaN之列
  print('刪除特定行(如第C行)中有NaN之列:df.dropna(subset=[\'C\'])')
  print(df.dropna(subset=['C']))
#+END_SRC

#+RESULTS:
#+begin_example
      A   X     B     C    D
0   1.0 NaN   2.0   3.0  4.0
1   5.0 NaN   6.0   NaN  8.0
2  10.0 NaN  11.0  12.0  NaN
3   NaN NaN   NaN   NaN  NaN
A    1
X    4
B    1
C    2
D    2
dtype: int64
刪掉有遺失值的列:df.dropna(axis=1)
Empty DataFrame
Columns: [A, X, B, C, D]
Index: []
刪掉有遺失值的行:df.dropna(axis=1)
Empty DataFrame
Columns: []
Index: [0, 1, 2, 3]
剛除整行為NaN者:df.dropna(how='all')
      A   X     B     C    D
0   1.0 NaN   2.0   3.0  4.0
1   5.0 NaN   6.0   NaN  8.0
2  10.0 NaN  11.0  12.0  NaN
刪除有值個數低於thresh的列:df.dropna(thresh=4)
     A   X    B    C    D
0  1.0 NaN  2.0  3.0  4.0
刪除特定行(如第C行)中有NaN之列:df.dropna(subset=['C'])
      A   X     B     C    D
0   1.0 NaN   2.0   3.0  4.0
2  10.0 NaN  11.0  12.0  NaN
#+end_example

雖然刪除包含遺漏值的數據似乎是個方便的方法，但終究可能會刪除過多的樣本，導致分析的結果並不可靠；或是因為刪除了特徵的時候，卻失去了重要的資訊。

** 填補遺遺漏值

最常見的「插補技術」之一為「平均插補」(mean imputation)，即，以整個特徵行的平均值來代替遺漏值。

#+BEGIN_SRC python -r -n :results output :exports both
  csv_data = '''A,X,B,C,D
  1.0,,2.0,3.0,4.0
  5.0,,6.0,,8.0
  10.0,,11.0,12.0
  ,,,,'''
  import sys
  import pandas as pd
  # python 2.7需進行unicode轉碼
  if (sys.version_info < (3, 0)):
      csv_data = unicode(csv_data)
  # 讀入程式檔中的csv資料
  from io import StringIO
  df = pd.read_csv(StringIO(csv_data))

  # impute missing values via the column mean
  from sklearn.preprocessing import Imputer
  # axis=0: 以行的平均值來補
  # axis=1: 以列的平均值來補
  # strategy的選項有: median(中位數)、most_freqent(最頻繁出現者)
  # most_freqent在做為分類特徵時很有用
  imr = Imputer(missing_values='NaN', strategy='mean', axis=0)
  imr = imr.fit(df.values)
  imputed_data = imr.transform(df.values)
  print(df)
  print(imputed_data)
#+END_SRC

#+RESULTS:
:       A   X     B     C    D
: 0   1.0 NaN   2.0   3.0  4.0
: 1   5.0 NaN   6.0   NaN  8.0
: 2  10.0 NaN  11.0  12.0  NaN
: 3   NaN NaN   NaN   NaN  NaN
: [[ 1.          2.          3.          4.        ]
:  [ 5.          6.          7.5         8.        ]
:  [10.         11.         12.          6.        ]
:  [ 5.33333333  6.33333333  7.5         6.        ]]

Imputer 類別在 scikit-learn 中屬於 transformer 類別，主要的工作是做「數據轉換」，這些 estimator 有兩種基本方法：fit 與 transform，fit 方法是用來進行參數學習。

** 處理數據中的分類特徵編碼問題

*** categorical feature

真實世界的數據集往往包含各種「類別特徵」(categorical feature)，類別特徵可再分為
- nominal feature: 名義特徵
- ordinal feature: 次序特徵

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  df = pd.DataFrame([['green', 'M', 10.1, 'class2'],
                     ['red', 'L', 13.5, 'class1'],
                     ['blue', 'XL', 15.3, 'class2']])

  df.columns = ['color', 'size', 'price', 'classlabel']
  print(df)
#+END_SRC

#+RESULTS:
:    color size  price classlabel
: 0  green    M   10.1     class2
: 1    red    L   13.5     class1
: 2   blue   XL   15.3     class2

*** 對應 ordinal feature

自定一個 mapping dictionary，即 size\under{}mapping，然後將 classlabel 對應到 size\under{}mapping 中的鍵值(程式第[[(sizeMapping)]]行)。

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  df = pd.DataFrame([['green', 'M', 10.1, 'class2'],
                     ['red', 'L', 13.5, 'class1'],
                     ['blue', 'XL', 15.3, 'class2']])

  df.columns = ['color', 'size', 'price', 'classlabel']
  ### Mapping ordinal features
  size_mapping = {'XL': 3,
                  'L': 2,
                  'M': 1}
  df['size'] = df['size'].map(size_mapping)   (ref:sizeMapping)
  print(df)
#+END_SRC

#+RESULTS:
:    color  size  price classlabel
: 0  green     1   10.1     class2
: 1    red     2   13.5     class1
: 2   blue     3   15.3     class2

*** 對應 nominal feature

許多機器學習的函式庫需要將「類別標籤」編碼為整數值。方法之一是以列舉方式為這些 nominal features 自 0 開始編號，先以 enumerate 方式建立一個 mapping dictionary: class_mapping(程式第[[(classMapping)]]行)，然後利用這個字典將類別特徵轉換為整數值。

此外，也可以利用已產生的對應字典，藉由借調 key-value 來產生「反轉字典」(第[[(invClassMapping)]]行)，將對調產生的整數還原回原始類別特徵。

scikit-learn 中有一個更為方便的 LabelEncoder 類別則可以直接完成上述工作(第[[(labelEncoder)]]行)。

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  df = pd.DataFrame([['green', 'M', 10.1, 'class2'],
                     ['red', 'L', 13.5, 'class1'],
                     ['blue', 'XL', 15.3, 'class2']])

  df.columns = ['color', 'size', 'price', 'classlabel']
  # 建利對應字典
  import pandas as np
  class_mapping = {
      label: idx for idx, label in enumerate(np.unique(df['classlabel'])) (ref:classMapping)
  }
  print(class_mapping)
  # 將類別特徵轉換為整數值
  df['classlabel'] = df['classlabel'].map(class_mapping)
  print(df)

  # 產生反轉字典，將整數還原至原始的類別標籤
  inv_class_mapping = {v: k for k, v in class_mapping.items()} (ref:invClassMapping)
  df['classlabel'] = df['classlabel'].map(inv_class_mapping)
  print(df)

  # Label encoding with sklearn's LabelEncoder
  from sklearn.preprocessing import LabelEncoder
  class_le = LabelEncoder()
  y = class_le.fit_transform(df['classlabel'].values) (ref:labelEncoder)
  print(y)
  df['classlabel'] = y
  print(df) # 類別與數字的對應不一定與自訂字典一致

#+END_SRC

#+RESULTS:
#+begin_example
{'class2': 0, 'class1': 1}
   color size  price  classlabel
0  green    M   10.1           0
1    red    L   13.5           1
2   blue   XL   15.3           0
   color size  price classlabel
0  green    M   10.1     class2
1    red    L   13.5     class1
2   blue   XL   15.3     class2
[1 0 1]
   color size  price  classlabel
0  green    M   10.1           1
1    red    L   13.5           0
2   blue   XL   15.3           1
#+end_example

*** 對 nominal feature 執行 one-hot encoding

scikit-learn 的 LabelENcoder 類別可以用來將「類別特徵」編碼為整數值，但這樣會引發另一個問題，如果我們將上述資料中的 color 特徵轉換為整數值，如下：

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  df = pd.DataFrame([['green', 'M', 10.1, 'class2'],
                     ['red', 'L', 13.5, 'class1'],
                     ['blue', 'XL', 15.3, 'class2']])

  df.columns = ['color', 'size', 'price', 'classlabel']

  X = df[['color', 'size', 'price', 'classlabel']].values

  # 以LabelEncoder轉換
  from sklearn.preprocessing import LabelEncoder
  color_le = LabelEncoder()
  print(X[:,0])
  X[:,0] = color_le.fit_transform(X[:,0])
  print(X[:,0])

#+END_SRC

#+RESULTS:
: ['green' 'red' 'blue']
: [1 2 0]

由輸出結果可以發現，經過類別編碼後的顏色特徵，由原本不具次序的特徵變成存在大小關係(red>green>blue)，這明顯會影響 model 運算的結果。針對此一問題，常見的解決方案是 one-hot encoding，其原理是：對特徵值中的每個值，建立一個新的「虛擬特徵」(dummy feature)。方法有二：
- 利用 ColumnTransformer 函式庫的 ColumnTransformer 類別，將特徵值轉換 One-Hot Encoding 的對應矩陣，如程式第[[(FitTransform)]]行。
- 利用 Pandas 套件的 get\under{}dummies 類別，一次將矩陣內指定之 column 轉換為 One-Hot encoding，如程式第[[(GetDummies)]]行。這種轉換只有字串數據會被轉換，其他內容則否。

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  df = pd.DataFrame([['green', 'M', 10.1, 'class2'],
                     ['red', 'L', 13.5, 'class1'],
                     ['blue', 'XL', 15.3, 'class2']])

  df.columns = ['color', 'size', 'price', 'classlabel']

  X = df[['color', 'size', 'price', 'classlabel']].values
  print(df)

  # one-hot encoding: ColumnTransformer / fit_transform
  from sklearn.preprocessing import LabelEncoder
  from sklearn.preprocessing import OneHotEncoder
  from sklearn.compose import ColumnTransformer
  import numpy as np

  X = df[['color', 'size', 'price']].values

  ct = ColumnTransformer(
      # The column numbers to be transformed (here is [0] but can be [0, 1, 3])
      # Leave the rest of the columns untouched
      [('OneHot', OneHotEncoder(), [0])], remainder='passthrough'
  )
  print(ct.fit_transform(X)) (ref:FitTransform)

  # on-hot encoding: pandas / get_dummies
  import pandas as pd
  print(pd.get_dummies(df[['price', 'color', 'size']])) (ref:GetDummies)
#+END_SRC

#+RESULTS:
#+begin_example
   color size  price classlabel
0  green    M   10.1     class2
1    red    L   13.5     class1
2   blue   XL   15.3     class2
[[0.0 1.0 0.0 'M' 10.1]
 [0.0 0.0 1.0 'L' 13.5]
 [1.0 0.0 0.0 'XL' 15.3]]
   price  color_blue  color_green  color_red  size_L  size_M  size_XL
0   10.1           0            1          0       0       1        0
1   13.5           0            0          1       1       0        0
2   15.3           1            0          0       0       0        1
#+end_example

應用 one-hot encoding 時，我們必須留意它所引入的「多元共線性」(multicollinearity)問題，這在某些狀況下(如要計算反矩陣)可能會產生一些問題，若特徵間有高度相關，則會難以計算反矩陣，導致數值不穩定的舘計。

** 訓練集與測試集的數據分割

#+BEGIN_SRC python -r -n :results output :exports both  :eval no
  # # Partitioning a dataset into a seperate training and test set
  df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                        'ml/machine-learning-databases/wine/wine.data',
                        header=None)

  # if the Wine dataset is temporarily unavailable from the
  # UCI machine learning repository, un-comment the following line
  # of code to load the dataset from a local path:

  # df_wine = pd.read_csv('wine.data', header=None)


  df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                     'Alcalinity of ash', 'Magnesium', 'Total phenols',
                     'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                     'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                     'Proline']

  print('Class labels', np.unique(df_wine['Class label']))
  df_wine.head()

  X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

  X_train, X_test, y_train, y_test =    train_test_split(X, y,
                       test_size=0.3,
                       random_state=0,
                       stratify=y)

#+END_SRC

** 縮放特徵值、維持特徵值影響比例：正規化(normalization)

「特徵縮放」(Feature scaling)是資料預處理的一個關鍵，「決策樹」和「隨機森林」是極少數無需進行 feature scaling 的分類技術；對多數機器學習演算法而言，若特徵值經過適當的縮放，都能有更佳成效。

Feature scaling 的重要性可以以下例子看出，假設有兩個特徵值(a, b)，其中 a 的測量範圍為 1 到 10，b 的測量值範圍為 1 到 100000，以典型分類演算法的做法，一定是忙於最佳化特徵值 b；若以 KNN 的演算法，也會被特徵值 b 所技配。

正規化有兩種常用的方法，可以將不同規模的特徵轉化為相同的規模：常態化(normalization)和標準化(standardization)：
*** 常態化

將特徵值縮化為 0~1 間，這是「最小最大縮放」(min-max scaling)的一個特例，某一特徵值的常態化做法如下：
$$x_{norm}^i = \frac{x^i-x_{min}}{x_{max}-x_{min}}$$
若以 scikit-learn 套件來完成實作，其程式碼如下：
#+BEGIN_SRC python -r -n :results output :exports both :
  from sklearn.preprocessing import MinMaxScaler
  mms = MinMaxScaler()
  X_train_norm = mms.fit_transform(X_train)
  X_test_norm = mms.fit_transform(X_test)
#+END_SRC

*** 標準化

雖說常態化簡單實用，但對許多機器學習演算法來說(特別是梯度下降法的最佳化)，標準化則更為實際，我們可令標準化後的特徵值其平均數為 0、標準差為 1，這樣一來，特徵值會滿足常態分佈，進而使演算法對於離群值不那麼敏感。標準化的公式如下：
$$x_{std}^i = \frac{x^i-\mu_x}{\sigma_x}$$
若以 scikit-learn 套件來完成實作，其程式碼如下：
#+BEGIN_SRC python -r -n :results output :exports both :
  from sklearn.preprocessing import StandardScaler
  stdsc = StandardScaler()
  X_train_std = stdsc.fit_transform(X_train)
  X_test_std = stdsc.transform(X_test)
#+END_SRC

** 選取有意義的特徵

overfitting 的產生原因是模型過度遷就於訓練數據，導致面對新數據(測試集)時成效不彰，我們稱這種模型具有較高變異性(high variance)，一般的解決策略有：
- 收集更多的訓練數據集
- 經由正規化，對於過度複雜的模型引進一個「懲罰」(penalty)
- 以較少的參數做出較簡單的模型(使用更簡單的模型)
- 減少數據維度

*** L1L2 regularzation

一個典型的解釋[fn:2]如圖[[fig:OverFitting-1]]，"我們知道, 過擬合就是所謂的模型對可見的數據過度自信, 非常完美的擬合上了這些數據, 如果具備過擬合的能力, 那麼這個方程就可能是一個比較複雜的非線性方程 , 正是因為這裡的 x^3 和 x^2 使得這條虛線能夠被彎來彎去, 所以整個模型就會特別努力地去學習作用在 x^3 和 x^2 上的 c, d 參數. 但是我們期望模型要學到的卻是 這條藍色的曲線. 因為它能更有效地概括數據.而且只需要一個 y=a+bx 就能表達出數據的規律. 或者是說, 藍色的線最開始時, 和紅色線同樣也有 c d 兩個參數, 可是最終學出來時, c 和 d 都學成了 0, 雖然藍色方程的誤差要比紅色大, 但是概括起數據來還是藍色好. 那我們如何保證能學出來這樣的參數呢? 這就是 l1 l2 正規化出現的原因啦."

#+CAPTION: 過擬合問題
#+LABEL:fig: OverFitting-1
#+name: fig:OverFitting-1
#+ATTR_LATEX: :width 300
[[file:images/L1l2regularization2.png]]

對於上述訓練出的兩個方程式，我們可以用\((y_{\theta}(x)-y)^2\)來計算模型預測值\(y(x)\)和真實數據\(y\)的誤差，而 L1, L2 就只是在這個誤差公式後加上一些式子來修正這個公式(如圖[[fig:OverFitting-2]])，其目的在於讓誤差的最佳化不僅取決於訓練數據擬合的優劣，同時也取決於參數值(如 c,d)的大小；L2 正規化以參數平方來做為計算方式，L1 正規化則是計算每個參數的絕對值。
#+CAPTION: L1,L2 正規化公式
#+LABEL:fig: OverFitting-2
#+name: fig:OverFitting-2
#+ATTR_LATEX: :width 300
[[file:images/L1l2regularization3.png]]

進一步以 Tensorflow Playground 的圖示來觀察 L1,L2 正規化的差異[fn:3]，如果把正規化(Regularization)設定為 L1，再執行訓練。可以看到很多權重都被設定為 0，特徵輸入與隱藏層的神經元被大大的減少，如圖[[fig:L1l2regularization4]]，整個模型的複雜度簡化很多。L1 正規化確實有助於將我們的複雜模型縮減為更小的泛化模型。添加正規化後，我們看到無用的功能全部變為零，並且連接線變得稀疏並顯示為灰色。倖存下來的唯一特徵是 x_1 平方和 x_2 平方，這是有道理的，因為這 2 個特徵加在一起就構成了一個圓的方程。

#+CAPTION: L1 正規化
#+LABEL:fig: L1l2regularization4
#+name: fig:L1l2regularization4
#+ATTR_LATEX: :width 400
[[file:images/L1l2regularization4.png]]

反觀 L2 正規化，當我們訓練它時，每個權重與神經元都還是處於活動狀態，但是非常虛弱，如圖[[fig:OverFitting-3]]，L1 正規化使用其中一個特徵而將某些拋棄，而 L2 正規化將同時保留特徵並使權重值保持較小。因此，使用 L1，您可以得到一個較小的模型，但預測性可能較低。。所以：

- L1 正規化：有可能導致零權重，因刪除更多特徵而使模型稀疏。
- L2 正規化：會對更大的權重值造成更大的影響，將使權重值保持較小。

#+CAPTION: L2 正規化
#+LABEL:fig: OverFitting-3
#+name: fig:OverFitting-3
#+ATTR_LATEX: :width 400
[[file:images/L1l2regularization5.png]]

** 循序特徵選擇法

另一種降低模型複雜度以避免過度擬合的方式是經由「特徵選擇」(feature selection)來做「降維」(dimensionality reduction)，降維的做法有二：
- 特徵選擇：feature selection, 由原本的特徵中，選出一個子集合
- 特徵提取：feature extraction，由原本的特徵中，導出資訊來建構新的特徵

循序特徵選擇法(sequential feature selection)為貪婪演算法的一種，目標在移除不相關或相關較低的特徵，以提高計算效率，這對於不支援「正規化」的演算法來說是很有用的。「循序向後選擇」(Sequential Backward Selection, SBS)便是一個典型的循序特徵選擇法，其做法是逐一從特徵空間中移除特徵，直到只剩下所要的特徵個數。為了達到這個目的，我們要定義一個最小化的「準則函數」(criterion function), 這個準則可以簡化為「模型在移除某特徵前/後的效能差異。SBS 的 python 實作如下：

#+BEGIN_SRC python -r -n :results output :exports both
  # ## Sequential feature selection algorithms
  from sklearn.base import clone
  from sklearn.metrics import accuracy_score
  from itertools import combinations
  class SBS():
      def __init__(self, estimator, k_features, scoring=accuracy_score,
                   test_size=0.25, random_state=1):
          self.scoring = scoring
          self.estimator = clone(estimator)
          self.k_features = k_features
          self.test_size = test_size
          self.random_state = random_state

      def fit(self, X, y):

          X_train, X_test, y_train, y_test =             train_test_split(X, y, test_size=self.test_size,
                               random_state=self.random_state)

          dim = X_train.shape[1]
          self.indices_ = tuple(range(dim))
          self.subsets_ = [self.indices_]
          score = self._calc_score(X_train, y_train,
                                   X_test, y_test, self.indices_)
          self.scores_ = [score]

          while dim > self.k_features: (ref:fitWhile)
              scores = []
              subsets = []

              for p in combinations(self.indices_, r=dim - 1):
                  score = self._calc_score(X_train, y_train,
                                           X_test, y_test, p) (ref:scoreXtest)
                  scores.append(score)
                  subsets.append(p)

              best = np.argmax(scores)
              self.indices_ = subsets[best]
              self.subsets_.append(self.indices_)
              dim -= 1

              self.scores_.append(scores[best]) (ref:bestScore)
          self.k_score_ = self.scores_[-1]

          return self

      def transform(self, X):
          return X[:, self.indices_]

      def _calc_score(self, X_train, y_train, X_test, y_test, indices):
          self.estimator.fit(X_train[:, indices], y_train)
          y_pred = self.estimator.predict(X_test[:, indices])
          score = self.scoring(y_test, y_pred)
          return score

  import matplotlib.pyplot as plt
  from sklearn.neighbors import KNeighborsClassifier

  knn = KNeighborsClassifier(n_neighbors=5)

  ##========
  # 讀入資料
  from sklearn import datasets
  import numpy as np
  import pandas as pd
  df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)
  df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                     'Alcalinity of ash', 'Magnesium', 'Total phenols',
                     'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                     'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                     'Proline']
  X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test =    train_test_split(X, y,
                       test_size=0.3,
                       random_state=0,
                       stratify=y)

  # 將資料標準化: 利用preprocessing模組裡的StandardScaler類別
  from sklearn.preprocessing import StandardScaler
  sc = StandardScaler() # 實例化一個StandardScaler物件
  # 利用fit方法，對X_train中每個特徵值估平均數和標準差
  # 然後對每個特徵值進行標準化(train和test都要做)
  sc.fit(X_train)
  X_train_std = sc.transform(X_train)
  X_test_std = sc.transform(X_test)

  ##===

  # selecting features
  sbs = SBS(knn, k_features=1) (ref:kFeatures)
  sbs.fit(X_train_std, y_train)

  # plotting performance of feature subsets
  k_feat = [len(k) for k in sbs.subsets_]

  plt.plot(k_feat, sbs.scores_, marker='o')  (ref:accuracyScore)
  plt.ylim([0.7, 1.02])
  plt.ylabel('Accuracy')
  plt.xlabel('Number of features')
  plt.grid()
  plt.tight_layout()
  plt.savefig('04_08.png', dpi=300)
  #plt.show()
  print(sbs.subsets_) # 全部列出，找到3個特徵值是在第幾個位置 (ref:sbsSubsets)
  print(list(sbs.subsets_[10]))
  k3 = list(sbs.subsets_[10])
  print(df_wine.columns[1:][k3])
  ## 比較全部特徵值與三個特徵值的效能
  knn.fit(X_train_std, y_train)
  print('Training accuracy (FULL):', knn.score(X_train_std, y_train))
  print('Test accuracy (FULL):', knn.score(X_test_std, y_test))
  knn.fit(X_train_std[:, k3], y_train)
  print('Training accuracy (K3):', knn.score(X_train_std[:,k3], y_train))
  print('Test accuracy (K3):', knn.score(X_test_std[:,k3], y_test))

#+END_SRC

#+RESULTS:
: [(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), (0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12), (0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11), (0, 1, 2, 3, 4, 5, 6, 7, 9, 11), (0, 1, 2, 3, 4, 5, 7, 9, 11), (0, 1, 2, 3, 5, 7, 9, 11), (0, 1, 2, 3, 5, 7, 11), (0, 1, 2, 3, 5, 11), (0, 1, 2, 3, 11), (0, 1, 2, 11), (0, 1, 11), (0, 11), (0,)]
: [0, 1, 11]
: Index(['Alcohol', 'Malic acid', 'OD280/OD315 of diluted wines'], dtype='object')
: Training accuracy (FULL): 0.967741935483871
: Test accuracy (FULL): 0.9629629629629629
: Training accuracy (K3): 0.9516129032258065
: Test accuracy (K3): 0.9259259259259259

#+CAPTION: SBS
#+LABEL:fig: SBS-1
#+name: fig:SBS-1
#+ATTR_LATEX: :width 400
#+ATTR_HTML: :width 500
[[file:images/04_08.png]]

前述實作中，k\under{}features 參數(程式第[[(kFeatures)]]行)定義了我們希望演算法「最後要保留多少特徵」，在預設情況下，以 accuracy\under{}score(程式第[[(accuracyScore)]]行)來評估模型效能。在 fit 的 while 迴圈中([[(fitWhile)]]行)，由 itertools 模組的 combinations 方法所產生的特徵子集合會被評估並降維，直到只剩下所要的特徵個數。

在每次迭代中，演算法使用內部創建的測試數據集 X\under{}test(第[[(scoreXtest)]]行)來評估特徵子集合，然後留下精確度最佳的特徵子集合所得分數，加入串列 self.scores\under{}中(第[[(bestScore)]]行)，之後再以這些分數來評估結果。最後的特徵子集合「行索引」會被分派到變數 self.indices\under{}中，然後以 transform 將這些所選定的特徵轉為新的數據陣列。

由圖[[fig:SBS-1]]可以看到，當特徵數 k={3, 7, 8, 9, 10, 11, 12}時，KNN 分類器的準確率為 100%。若進一步想確定當 k=3 時，是哪三個特徵，則可以由 sbs.subset\under{}中逐步探索出來(程式第[[(sbsSubsets)]]行)。

進一步比較「全部特徵值」以及「三個特徵值」所得出的模型效能，可以看到即使只留下三個特徵值，模型的效能仍相去不遠，更重要的是，透過降低維度，可以有效的提升運算效能。
** 以隨機森林評估特徵的重要性

隨機森林顧名思義，是用隨機的方式建立一個森林，森林裡面有很多的決策樹組成，隨機森林的每一棵決策樹之間是沒有關聯的。在得到森林之後，當有一個新的輸入樣本進入的時候，就讓森林中的每一棵決策樹分別進行一下判斷，看看這個樣本應該屬於哪一類（對於分類演算法），然後看看哪一類被選擇最多，就預測這個樣本為那一類[fn:4]。上述 SBS 演算法係將低相關的特徵刪除、留下重要的特徵；而隨機森林則是利用許多決策樹來票選最後的決定。


#+BEGIN_SRC python -r -n :results output :exports both
  from sklearn import datasets
  import pandas as pd
  import numpy as np
  import matplotlib.pyplot as plt

  df_wine = pd.read_csv('https://archive.ics.uci.edu/'
                      'ml/machine-learning-databases/wine/wine.data',
                      header=None)
  df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                     'Alcalinity of ash', 'Magnesium', 'Total phenols',
                     'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                     'Color intensity', 'Hue', 'OD280/OD315 of diluted wines',
                     'Proline']
  X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,
                                                      random_state=0,
                                                      stratify=y)


  from sklearn.ensemble import RandomForestClassifier
  feat_labels = df_wine.columns[1:]
  forest = RandomForestClassifier(n_estimators=500,
                                  random_state=1)

  forest.fit(X_train, y_train)
  importances = forest.feature_importances_

  indices = np.argsort(importances)[::-1]

  for f in range(X_train.shape[1]):
      print("%2d) %-*s %f" % (f + 1, 30,
                              feat_labels[indices[f]],
                              importances[indices[f]]))

  plt.title('Feature Importance')
  plt.bar(range(X_train.shape[1]),
          importances[indices],
          align='center')
1
  plt.xticks(range(X_train.shape[1]),
             feat_labels[indices], rotation=90)
  plt.xlim([-1, X_train.shape[1]])
  plt.tight_layout()
  plt.savefig('04_09.png', dpi=300)
  #plt.show()

#+END_SRC

#+RESULTS:
#+begin_example
 1) Proline                        0.185453
 2) Flavanoids                     0.174751
 3) Color intensity                0.143920
 4) OD280/OD315 of diluted wines   0.136162
 5) Alcohol                        0.118529
 6) Hue                            0.058739
 7) Total phenols                  0.050872
 8) Magnesium                      0.031357
 9) Malic acid                     0.025648
10) Proanthocyanins                0.025570
11) Alcalinity of ash              0.022366
12) Nonflavanoid phenols           0.013354
13) Ash                            0.013279
#+end_example

#+CAPTION: FandomForest
#+LABEL:fig: 04_09
#+name: fig:04_09
#+ATTR_LATEX: :width 400
#+ATTR_HTML: :width 500
[[file:images/04_09.png]]


由圖[[fig:04_09]]的特徵排序為從 500 棵「決策樹」的「不純度」中最具「判別性」的特徵排列順序，

#+latex:\newpage

* 深度學習運作原理
** Layer, 損失函數與優化器
前節深度學習中的每一「層」(layer)如何運作，取決於儲存於該層的權重(weight)，而權重是由多個數字組成。從技術層面來看，layer 是由各個權重參數(parameters)來和輸入的資料進行運算以執行資料轉換的工作(如圖[[fig:python-deep-learning-3]])。而所謂的學習，指的就是幫助神經網路的每一層找出適當的權重值，讓神經網路可以將輸入的訓練資料經由與權重的運作推導出接近標準答案的運算結果(即圖[[fig:python-deep-learning-3]]中的預測 Y)。然而，這在實際運作上是十分困難的，因為一個深度神經網路可以包含數千萬個權重，此外，其中一個權重被改變後，往往會影響其他權重的運作。

#+CAPTION: nn 中 layer 的 parameter
#+LABEL:fig:python-deep-learning-3
#+name: fig:python-deep-learning-3
#+ATTR_LATEX: :width 400
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 400
[[file:images/img-191107115233.jpg]]

為了提高神經網路的效能(預測的準確率)，我們要即時的掌握目前的輸出(Y)與真正的標準答案還差多少，這個評估由神經網路的損失函數(loss function, 或稱目標函數, objective function)來負責，如圖[[fig:python-deep-learning-4]]。損失函數會取得神經網路的預測結果與標準答案二者的損失分數(又稱差距分數)，做為每一次學習的表現效能之評估標準。


#+CAPTION: 損失函數
#+LABEL:fig:python-deep-learning-4
#+name: fig:python-deep-learning-4
#+ATTR_LATEX: :width 400
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 400
[[file:images/img-191107115304.jpg]]

而深度學習的基本工作就是使用損失函數做為回饋訊息來一步步微調權重，逐步降低每次學習的損失分數，最終目標在於讓損失函數結果達到最小，而這個微調工作則由優化器(optimizer，也稱最佳化函數)來執行。優化器實作了反向傳播演算法(Backpropagation)，這也是深度學習中的核心演算法，藉此來週整權重。

#+CAPTION: 優化器
#+LABEL:fig:python-deep-learning-5
#+name: fig:python-deep-learning-5
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
[[file:images/img-1911071153041.jpg]]

那麼，在最初一次的學習，權重的值是如何設定的呢？可以先全數設為零，但更常用的做法是隨機指定，隨著多次學習後，權重會逐步往正確的方向調整，損失分數也會慢慢降低。
** 梯度
*** 深度網路誤差曲面的局部極小值

最佳化深度學習模型的挑戰在於我們只能運用局部的訊息去推斷誤差曲面的整體結構，雖然梯度遞減法可以確保我們找到極小值，但若曲面結構非碗型（即，存在不只一個谷地，或稱局部極小值），則即便我們探用隨機誤差曲面演算法，也是無法解決問題。
局部極小值與「模型可區分性(model indentifiability)」的概念有關，在全連接(fully-conntectd)的正向饋送神經網路中，同一層的神經元就算重新排列組合，網路末端還是會出現相同的最終輸出，結果，一層有 n 個神經元的網路就存在\(n!\)種排列方式，對於有 l 層的深度網路而言，則其等效配置方式就有\(n!^l\)種。結果，不論送進什麼輸入值，表現出來的行為也全都相同而無法區分；換言之，無論用的是訓練組、驗證組、測試組的樣本，所有的這些等效配置都會表現出相同的誤差。
局部極小值不是太嚴重的問題，但若找到的是「假的（spurious）」局部極小值則就是個大問題，所謂假的局部極小值指的是它在神經網路中所對應的權重值，會比真正的整體最小值所對應的權重值帶來更大的誤差），從事深度學習的人總是把訓練深度網路時所遇到的問題歸咎於假的局部極小值。想解決這個問題，有個天真的想法：在訓練深度神經網路的過程中，同時畫出誤差函數隨時間而變的值，但是這個策略並不能針對誤差曲面提供足夠的訊息，因為我們很難判斷誤差的變化是來自曲面本身的「顛簸」或是因為遲遲無法找到最佳的前進方向。
Goodfellow 等人[fn:5]（Google 和 Standford 合作的研究小組）在 2014 年發表一篇論文試圖解決上述問題，他們沒有去分析誤差的函數隨時間而變的情況，而是在隨機選取的初始化參數向量和最後真正的最佳點之間，運用線性插值取點，再觀察這些插值點在誤差曲面上呈現什麼樣的變化，也就是說，只要給定一個隨機初始化參數向量\(\theta_i\)，加上隨機梯度遞減法(SBD)最後找到的最佳點\(\theta_f\)，我們就可以沿著線性插值的每個點，計算出相應的誤差函數值\(\theta_\alpha = \alpha \cdot \theta_f + (1-\alpha) \cdot \theta_i \)。

Goodfellow 等人的研究顯示，對於各種具有不同型態神經元的實際網路而言，參數空間中隨機選取的初始點數與隨機梯度遞減最佳解之間直接相連的路經，並不會受到局部極小值的影響；換言之，我們應該把重點放在「尋找合適的前進方向」上。
*** 找出正確的移動軌跡

梯度通常不是尋找最小值時最好的移動軌跡參考指標，最佳應用時機是等高線為完美㘣形，然而多數等高線均為楕圓，此時梯度所指的方向就會與正確方向有所偏差。對參數空間中的每個權向\(w_i\)來說，梯度計算的是\(\frac{\partial{E}}{\partial{w_i}}\)，代表當\(w_i\)被改變時，誤差如何隨之變化的程度。因此，只要綜合考慮參數空間的所有權重，梯度就可以給出遞減最快的方向；然而，當我們朝著這個方向移動一步後，此時的梯度又會隨之改變。

進一步量化我們往某方向移等時腳下梯度變化的程度，我們必須計算二階導函數，即求出\(\farc{\partial{\frac{ \parital{E}}{\partial{w_j} }}}{\partial{w_i}}\)，代表當我們改變\(w_i\)的值時，梯度中的分量\(w_j\)如何隨之而改變。將這些訊息編寫之的矩陣稱之為「海森矩陣 (Hessian matrix)」，在描述誤差曲面時，如果我們往遞減最快方向移動，腳下的梯度也跟著改變，我們就會說這是個病態(ill-conditioned)矩陣。

**** 動量
病態海森矩陣的問題往往會以梯度大幅波動的形式表現出來，因此我們可以考慮如何在訓練期間消除這些波動。想像一顆球滾落至誤差曲面中，最終一定會抵達曲面的最低點，而且不會有大幅波動。球的平滑滾落動作不只受到加速度的影響，也受到「速度」的影響，而球的速度以一種記憶的形式讓球往最低方向更有效的累積移動量，同時抵消正交(orthogonal)方向上的振盪加速度；為了模擬出球體的自然動作，我們可以在最佳化演算法中以某種方式引入速度的概念，也就是追蹤之前梯度的「指數加權衺減量」。換言之，我們用一個「動量超參數」\(m\)，以決定在新的更新值中，前一次速度要保留多少比例，藉此把我們對前一個梯度值的「記憶」添加至目前最新的梯度值中。這種做法所運用到的概念通常就動為「動量(momentum)」。
**** Nesterow 動量
為 Sutskever 等人在 2013 年，基於改進古典動量技術所提出的動量替代方案
**** 共軛梯度遞減(conjugate gradient descent)
這是試圖改進單純最陡遞減法的另一做法，最陡遞減法是計算梯度方向，然後沿此方向搜索最小值，跳到最小值處再重新計算，實際情況則會大幅波動，這是因為每次往最陡方向移動，往往會稍微抵消另一方向的進展，補救方式是不往最陡方向移動，而是相對先前所選擇的方向，往其「共軛方向(conjugate direction)」移動。
**** BFGS(Broyden-Fletcher-Goldfarb-Shanno)
以迭代方式計算海森矩陣的逆矩陣，以有效最佳化參數向量
**** L-BFGS
解決 BFGS 佔用記憶體的問題
*** 學習率自動調整
**** AdaGrad
根據累積歷史梯度，對整體學習率進行自動調整 由 Duchi 等人在 2011 年提出
**** RMSProp
累似以動量抑制梯度波動的做法，改以指數加權移動平均，將很久以前的值也納入考慮。
**** Adam
可視為 RMSProp 與動量的變種組合
**** AdaDelta
** 最佳選擇
對於大多數深度學習實作者，推動深度學習的最佳途徑並不是創造出更高級的最佳化演算法，相反的，過去幾十年來絕大多數深度學習的突破，都是因為發現了更容易訓練的架構，而不是因為與那些討厭的誤差曲面搏鬥所得到的成果。

* 降維來壓縮數據
** 以主成份分析(PCA)對非監督式數據壓縮

「特徵選擇」需要原始的「特徵」；而「特徵提取」則是在於「轉換」數據，或是「投影」(project)數據到一個新的「特徵空間」，特徵提取不僅能改善儲存空間的使用或是提高學習演算法的計算效率，也可以有效地藉由降低「維數災難」來提高預測的正確性，特別是在處理非正規化模型時。

*** 主成分分析 1

「主成份分析」(principal component analysis, PCA)是一種非監督式線性變換技術」，經常應用於「特徵提取」與「降維」，其他應用包括「探索式數據分析」和「股票市場分析」中的雜訊消除、生物資訊學領域中的「基因數據分析」與「基因表現層分析」。

這邊先簡單說維度詛咒，預測/分類能力通常是隨著維度數(變數)增加而上生，但當模型樣本數沒有繼續增加的情況下，預測/分類能力增加到一定程度之後，預測/分類能力會隨著維度的繼續增加而減小[fn:6]。

主成份分析的基本假設是希望資料可以在特徵空間找到一個投影軸(向量)投影後可以得到這組資料的最大變異量。以圖[[fig:pca-1]]為例，PCA 的目的在於找到一個向量可以投影(圖中紅色的線)，讓投影後的資料變異量最大。

#+CAPTION: PCA-1 [fn:31]
#+LABEL:fig: pca-1
#+name: fig:pca-1
#+ATTR_LATEX: :width 500
#+ATTR_HTML: :width 600
[[file:images/pca-1.png]]

**** 投影(projection)

假設有一個點藍色的點對原點的向量為\(\vec{x_i}\)，有一個軸為 v，他的投影(正交為虛線和藍色線為 90 度)向量為紅色那條線，紅色線和黑色線的夾角為\(\theta\)，\(\vec{x_i}\)投影長度為藍色線，其長度公式為\(\left\|{x_i}\right\|cos\theta\)。

#+CAPTION: PCA-2 [fn:31]
#+LABEL:fig: pca-2
#+name: fig:pca-2
#+ATTR_HTML: :width 300
#+ATTR_LATEX: :width 200
[[file:images/pca-2.png]]

假設有一組資料六個點(\(x_1, x_2, x_3, x_4, x_5, x_6\))，有兩個投影向量\(\vec{v}\)和\(\vec{v'}\)(如圖[[fig:pca-3]])，投影下來後，資料在\(\vec{v'}\)上的變異量比\(v\)上的變異量小。

#+CAPTION: PCA-3 [fn:31]
#+LABEL:fig: pca-3
#+name: fig:pca-3
#+ATTR_HTML: :width 600
#+ATTR_LATEX: :width 500
[[file:images/pca-3.png]]

從圖[[fig:pca-4]]也可以看出這些資料在\(v\)向量資料投影后有較大的變異量(較之投影於\(\vec{v'}\))。

#+CAPTION: PCA-4 [fn:31]
#+LABEL:fig: pca-4
#+name: fig:pca-4
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 300
[[file:images/pca-4.png]]

**** 變異量的計算

典型的變異數公式如下：
$\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (X -\mu)^2}$

若要計算前述所有資料點(\(x_1, x_2, x_3, x_4, x_5, x_6\))在\(v\)上的投影\(v^Tx_1, v^Tx_2, v^Tx_3, v^Tx_4, v^Tx_5, v^Tx_6\) ，則其變異數公式為
$\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i -\mu)^2$

又因 PCA 之前提假設是將資 shift 到 0(即，變異數的平均數為 0)以簡化運算，其公式會變為
$\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i -\mu)^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i - 0)^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i)^2$

而機器學習處理的資料點通常為多變量，故上述式子會以矩陣方式呈現

$\Sigma = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i)(v^Tx_i)^T = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_iv^Tx_iv) = v^T(\frac{1}{N}\sum\limits_{i=1}^Nx_iX_i^T)v = v^TCv$

其中 C 為共變異數矩陣(covariance matrix)

$C=\frac{1}{n}\sum\limits_{i=1}^nx_ix_i^T,\cdots x_i = \begin{bmatrix}
x_1^{(1)}     \\
x_2^{(2)}     \\
\vdots  \\
x_i^{(d)}     \\
\end{bmatrix}$

主成份分析的目的則是在找出一個投影向量讓投影後的資料變異量最大化（最佳化問題）：

$v = \mathop{\arg\max}\limits_{x \in \mathcal{R}^d,\left\|v\right\|=1} {v^TCv}$

進一步轉成 Lagrange、透過偏微分求解，其實就是解 C 的特徵值(eigenvalue, \(\lambda\))和特徵向量(eigenvector, \(v\))。

*** 主成份分析 2

回到前述例子(身高和體重)，下左圖，經由 PCA 可以萃取出兩個特徵成分(投影軸，下圖右的兩條垂直的紅線，較長的紅線軸為變異量較大的主成份)。此範例算最大主成份的變異量為 13.26，第二大主成份的變異量為 1.23。

#+CAPTION: PCA-5 [fn:31]
#+LABEL:fig: pca-5
#+name: fig:pca-5
#+ATTR_LATEX: :width 500
#+ATTR_HTML: :width 600

[[file:images/pca-5.png]]

PCA 投影完的資料為下圖，從下圖可知，PC1 的變異足以表示此筆資料資訊。

#+CAPTION: PCA-6 [fn:31]
#+LABEL:fig: pca-6
#+name: fig:pca-6
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 300

[[file:images/pca-6.png]]

此做法可以有效的減少維度數，但整體變異量並沒有減少太多，此例從兩個變成只有一個，但變異量卻可以保留(13.26/(13.26+1.23)= 91.51%)，兩維度的資料做 PCA，對資料進行降維比較沒有感覺，但講解圖例比較容易。

*** 主成份分析的主要步驟

1. 標準化數據集
1. 建立共變數矩陣
1. 從共變數矩陣分解出特徵值與特徵向量
1. 以遞減方式對特徵值進行排序，以便對特徵向量排名

#+BEGIN_SRC python -r -n :results output :exports both
  import pandas as pd
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.decomposition import PCA

  # ## Extracting the principal components step-by-step

  df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'
                        'machine-learning-databases/wine/wine.data',
                        header=None)

  df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
                     'Alcalinity of ash', 'Magnesium', 'Total phenols',
                     'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
                     'Color intensity', 'Hue',
                     'OD280/OD315 of diluted wines', 'Proline']

  print(df_wine.head())

  # Splitting the data into 70% training and 30% test subsets.

  X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values

  X_train, X_test, y_train, y_test = train_test_split(X, y,
                                     test_size=0.3,
                                     stratify=y, random_state=0)

  # 1. Standardizing the data.
  sc = StandardScaler()
  X_train_std = sc.fit_transform(X_train)
  X_test_std = sc.transform(X_test)

  # 2. Eigendecomposition of the covariance matrix.
  cov_mat = np.cov(X_train_std.T)
  eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)

  print('\nEigenvalues \n%s' % eigen_vals)

  # ## Total and explained variance

  tot = sum(eigen_vals)
  var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
  cum_var_exp = np.cumsum(var_exp)

  plt.bar(range(1, 14), var_exp, alpha=0.5, align='center',
          label='individual explained variance')
  plt.step(range(1, 14), cum_var_exp, where='mid',
           label='cumulative explained variance')
  plt.ylabel('Explained variance ratio')
  plt.xlabel('Principal component index')
  plt.legend(loc='best')
  plt.tight_layout()
  plt.savefig('05_02.png', dpi=300)
  #plt.show()

#+END_SRC

#+RESULTS:
#+begin_example
   Class label  Alcohol  ...  OD280/OD315 of diluted wines  Proline
0            1    14.23  ...                          3.92     1065
1            1    13.20  ...                          3.40     1050
2            1    13.16  ...                          3.17     1185
3            1    14.37  ...                          3.45     1480
4            1    13.24  ...                          2.93      735

[5 rows x 14 columns]

Eigenvalues
[4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634
 0.51828472 0.34650377 0.3131368  0.10754642 0.21357215 0.15362835
 0.1808613 ]
#+end_example

#+CAPTION: Principal component index
#+LABEL:fig: 05_02
#+name: fig:05_02
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
[[file:images/05_02.png]]

雖然上圖的「解釋變異數」圖有點類似隨機森林評估特徵值重要性的結果，但二者最大的不同處在於 PCA 為一種非監督式方法，也就是說，關於類別標籤資訊是被忽略的。

*** 特徵轉換

在分解「共變數矩陣」成為「特徵對」後，接下來要將資料集轉換為新的「主成份」，其步驟如下：
1. 選取\(k\)個最大特徵值所對應的 k 個特徵向量，其中\(k\)為新「特徵空間」的維數(\(k \le d\))。
1. 用最前面的\(k\)個特徵向量建立「投影矩陣」(project matrix)\(W\)。
1. 使用投影矩陣\(W\)，輸入值為\(d\)維數據集、輸出值為新的\(k\)維「特徵子空間」。

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import pandas as pd
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.decomposition import PCA

  # ## Extracting the principal components step-by-step

  df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'
                          'machine-learning-databases/wine/wine.data',
                          header=None)

  #  df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
  #                     'Alcalinity of ash', 'Magnesium', 'Total phenols',
  #                     'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
  #                     'Color intensity', 'Hue',
  #                     'OD280/OD315 of diluted wines', 'Proline']

      # Splitting the data into 70% training and 30% test subsets.
  X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
  X_train, X_test, y_train, y_test = train_test_split(X, y,
                                      test_size=0.3,
                                      stratify=y, random_state=0)
  # 1. Standardizing the data.
  sc = StandardScaler()
  X_train_std = sc.fit_transform(X_train)
  X_test_std = sc.transform(X_test)
  # 2. Eigendecomposition of the covariance matrix.
  cov_mat = np.cov(X_train_std.T)
  eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
  # ## Total and explained variance
  #tot = sum(eigen_vals)
  #var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
  #cum_var_exp = np.cumsum(var_exp)
  # ## Feature transformation
  # Make a list of (eigenvalue, eigenvector) tuples
  eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i])
                  for i in range(len(eigen_vals))]
  # Sort the (eigenvalue, eigenvector) tuples from high to low
  eigen_pairs.sort(key=lambda k: k[0], reverse=True)
  w = np.hstack((eigen_pairs[0][1][:, np.newaxis],
                  eigen_pairs[1][1][:, np.newaxis]))
  print('Matrix W:\n', w)
  print(X_train_std[0].dot(w)) (ref:x-train-dot)
  X_train_pca = X_train_std.dot(w) (ref:x-train-pca)
  # plot
  colors = ['r', 'b', 'g']
  markers = ['s', 'x', 'o']

  for l, c, m in zip(np.unique(y_train), colors, markers):
      plt.scatter(X_train_pca[y_train == l, 0],
                  X_train_pca[y_train == l, 1],
                  c=c, label=l, marker=m)

  plt.xlabel('PC 1')
  plt.ylabel('PC 2')
  plt.legend(loc='lower left')
  plt.tight_layout()
  plt.savefig('05_03.png', dpi=300)
  #plt.show()

#+END_SRC

#+RESULTS:
#+begin_example
Matrix W:
 [[-0.13724218  0.50303478]
 [ 0.24724326  0.16487119]
 [-0.02545159  0.24456476]
 [ 0.20694508 -0.11352904]
 [-0.15436582  0.28974518]
 [-0.39376952  0.05080104]
 [-0.41735106 -0.02287338]
 [ 0.30572896  0.09048885]
 [-0.30668347  0.00835233]
 [ 0.07554066  0.54977581]
 [-0.32613263 -0.20716433]
 [-0.36861022 -0.24902536]
 [-0.29669651  0.38022942]]
[2.38299011 0.45458499]
#+end_example

使用上述程式碼產生的 13*2 維的投影矩陣可以轉換一個樣本\(x\)(以\(1 \times 13\)維的列向量表示)到 PCA 子空間(\(x'\))(前兩個主成份)：\(x' = xW\)(程式碼第[[(x-train-dot)]]行)；同樣的，我們也可以將整個\(124 \times 13\)維的訓練數據集轉換到兩個主成份(\(124 \times 2\)維)(程式第[[(x-train-pca)]]行)，最後，將轉換過的\(124 \times 2\)維矩陣以二維散點圖表示：

#+CAPTION: 05_03
#+LABEL:fig:05_03
#+name: fig:05_03
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
[[file:images/05_03.png]]

由圖[[fig:05_03]]中可看出，與第二個主成份(y 軸)相比，數據沿著第一主成份(x 軸)的分散程度更嚴重，而由此圖也可判斷，該數據應可以一個「線性分類器」進行有效分類。

*** 以 Scikit-learn 進行主成份分析
#+BEGIN_SRC python -r -n :results output :exports both
  from matplotlib.colors import ListedColormap
  import pandas as pd
  from sklearn.model_selection import train_test_split
  from sklearn.preprocessing import StandardScaler
  import numpy as np
  import matplotlib.pyplot as plt
  from sklearn.decomposition import PCA
  from sklearn.linear_model import LogisticRegression

  # ## Extracting the principal components step-by-step

  df_wine = pd.read_csv('https://archive.ics.uci.edu/ml/'
                          'machine-learning-databases/wine/wine.data',
                          header=None)

  #  df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',
  #                     'Alcalinity of ash', 'Magnesium', 'Total phenols',
  #                     'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',
  #                     'Color intensity', 'Hue',
  #                     'OD280/OD315 of diluted wines', 'Proline']

      # Splitting the data into 70% training and 30% test subsets.
  X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values
  X_train, X_test, y_train, y_test = train_test_split(X, y,
                                      test_size=0.3,
                                      stratify=y, random_state=0)
  # 1. Standardizing the data.
  sc = StandardScaler()
  X_train_std = sc.fit_transform(X_train)
  X_test_std = sc.transform(X_test)

  def plot_decision_regions(X, y, classifier, resolution=0.02):
      # setup marker generator and color map
      markers = ('s', 'x', 'o', '^', 'v')
      colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
      cmap = ListedColormap(colors[:len(np.unique(y))])

      # plot the decision surface
      x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
      x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                             np.arange(x2_min, x2_max, resolution))
      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
      Z = Z.reshape(xx1.shape)
      plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
      plt.xlim(xx1.min(), xx1.max())
      plt.ylim(xx2.min(), xx2.max())

      # plot class samples
      for idx, cl in enumerate(np.unique(y)):
          plt.scatter(x=X[y == cl, 0],
                      y=X[y == cl, 1],
                      alpha=0.6,
                      c=cmap(idx),
                      edgecolor='black',
                      marker=markers[idx],
                      label=cl)

  # Training logistic regression classifier using the first 2 principal components.
  pca = PCA(n_components=2)
  X_train_pca = pca.fit_transform(X_train_std) (ref:pca-fit)
  X_test_pca = pca.transform(X_test_std)

  lr = LogisticRegression()
  lr = lr.fit(X_train_pca, y_train)

  plot_decision_regions(X_train_pca, y_train, classifier=lr)
  plt.xlabel('PC 1')
  plt.ylabel('PC 2')
  plt.legend(loc='lower left')
  plt.tight_layout()
  plt.savefig('05_04.png', dpi=300)
  #plt.show()
  plot_decision_regions(X_test_pca, y_test, classifier=lr)
  plt.xlabel('PC 1')
  plt.ylabel('PC 2')
  plt.legend(loc='lower left')
  plt.tight_layout()
  plt.savefig('05_05.png', dpi=300)
  #plt.show()
#+END_SRC



PCA 類別是 scikit-learn 中許多轉換類別之一，首先使用訓練數據集來 fit 模型並轉換數據集(程式第[[(pca-fit)]]行)，最後以 Logistic 迴歸對數據進行分類。圖[[fig:05_04]]為訓練集資料的分類結果，圖[[fig:05_05]]測為測試資料集分類結果，可以看出二者差異不大。

#+CAPTION: PCA 訓練數據
#+LABEL:fig:05_04
#+name: fig:05_04
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
[[file:images/05_04.png]]

#+CAPTION: PCA 測試數據
#+LABEL:fig:05_05
#+name: fig:05_05
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
[[file:images/05_05.png]]
** 利用線性判別分析(LDA)做監督式數據壓縮

LDA 的全稱是 Linear Discriminant Analysis（線性判別分析），是一種 supervised learning。因為是由 Fisher 在 1936 年提出的，所以也叫 Fisher's Linear Discriminant。「線性判別分析」(linear discriminant analysis, LDA)為一種用來做「特徵提取」的技術，藉由降維來處理「維數災難」，可提高非正規化模型的計算效率。PCA 在於找出一個在數據集中最大化變異數的正交成分軸； 而 LDA 則是要找出可以最佳化類別分離的特徵子空間。

從主觀的理解上，主成分分析到底是什麼？它其實是對數據在高維空間下的一個投影轉換，通過一定的投影規則將原來從一個角度看到的多個維度映射成較少的維度。到底什麼是映射，下面的圖就可以很好地解釋這個問題——正常角度看是兩個半橢圓形分佈的數據集，但經過旋轉（映射）之後是兩條線性分佈數據集。[fn:7]

#+ATTR_LATEX: :environment longtable :align |p{3.5cm}|p{3.5cm}|p{3.5cm}|p{3.5cm}|
|---------------------------+---------------------------+---------------------------+---------------------------|
| file:images/lda-rot-1.jpg | [[file:images/lda-rot-2.jpg]] | [[file:images/lda-rot-3.jpg]] | [[file:images/lda-rot-4.jpg]] |
|---------------------------+---------------------------+---------------------------+---------------------------|
|                         1 |                         2 |                         3 |                         4 |
|---------------------------+---------------------------+---------------------------+---------------------------|
| [[file:images/lda-rot-5.jpg]] | [[file:images/lda-rot-6.jpg]] | [[file:images/lda-rot-7.jpg]] | [[file:images/lda-rot-8.jpg]] |
|---------------------------+---------------------------+---------------------------+---------------------------|
|                         5 |                         6 |                         7 |                         8 |
|---------------------------+---------------------------+---------------------------+---------------------------|

LDA 與 PCA 都是常用的降維方法，二者的區別在於[fn:7]：
- 出發思想不同。PCA 主要是從特徵的協方差角度，去找到比較好的投影方式，即選擇樣本點投影具有最大方差的方向（ 在信號處理中認為信號具有較大的方差，噪聲有較小的方差，信噪比就是信號與噪聲的方差比，越大越好。）；而 LDA 則更多的是考慮了分類標籤信息，尋求投影后不同類別之間數據點距離更大化以及同一類別數據點距離最小化，即選擇分類性能最好的方向。
- 學習模式不同。PCA 屬於無監督式學習，因此大多場景下只作為數據處理過程的一部分，需要與其他算法結合使用，例如將 PCA 與聚類、判別分析、回歸分析等組合使用；LDA 是一種監督式學習方法，本身除了可以降維外，還可以進行預測應用，因此既可以組合其他模型一起使用，也可以獨立使用。
- 降維後可用維度數量不同。LDA 降維後最多可生成 C-1 維子空間（分類標籤數-1），因此 LDA 與原始維度 N 數量無關，只有數據標籤分類數量有關；而 PCA 最多有 n 維度可用，即最大可以選擇全部可用維度。

圖[[fig:pca-lda]]左側是 PCA 的降維思想，它所作的只是將整組數據整體映射到最方便表示這組數據的坐標軸上，映射時沒有利用任何數據內部的分類信息。因此，雖然 PCA 後的數據在表示上更加方便（降低了維數並能最大限度的保持原有信息），但在分類上也許會變得更加困難；圖[[fig:pca-lda]]右側是 LDA 的降維思想，可以看到 LDA 充分利用了數據的分類信息，將兩組數據映射到了另外一個坐標軸上，使得數據更易區分了（在低維上就可以區分，減少了運算量）。

#+CAPTION: PCA LDA 差異
#+LABEL:fig:pca-lda
#+name: fig:pca-lda
#+ATTR_LATEX: :width 400
[[file:images/pca-lda.png]]

線性判別分析 LDA 算法由於其簡單有效性在多個領域都得到了廣泛地應用，是目前機器學習、數據挖掘領域經典且熱門的一個算法；但是算法本身仍然存在一些侷限性：
- 當樣本數量遠小於樣本的特徵維數，樣本與樣本之間的距離變大使得距離度量失效，使 LDA 算法中的類內、類間離散度矩陣奇異，不能得到最優的投影方向，在人臉識別領域中表現得尤為突出
- LDA 不適合對非高斯分佈的樣本進行降維
- LDA 在樣本分類信息依賴方差而不是均值時，效果不好
- LDA 可能過度擬合數據
** TODO 利用核主成份分析(KPCA)處理非線性對應
** 相關資源
- [[https://blog.csdn.net/kuweicai/article/details/79255270][主成分分析（PCA）和線性判別分析（LDA）原理簡介]]

#+latex:\newpage

* 深度學習應用領域
** 影像辨識
*** 卷積神經網路 CNN

傳統機器學習進行圖片識別，主要是希望能透過原始像數值找出一種適合的分類器(classifier)，但事實證明這麼做不管用，因為信噪比太低。後來的改善方式是由人類挑選出重要特徵，然後由機器學習演算法使用這些「特徵向量(feature vectors)」進行分類判斷。這種特徵提取(feature extraction)的做法確實改善了信噪比，但是如果圖片的重要特點因光線或其他因素難以識別，則精確率會降低很多，而且，事前的人工挑選特徵花去太多人力，以深度學習進行圖片視覺就是設法消除那些既繁瑣又會造成侷限性的特徵選取程序。David Hubel 和 Torsten Wiesel 發現動物視覺皮層有一部份專門負責檢測邊緣，1959 年他們把電極插入貓的大腦中，在螢幕上投射出黑白圖案，發現有些神經元只有在出現垂直線時被激發，有些則只有在出現水平線時被激發，有些則是只有看到某特定角度的線時被激發。進一步的研究確認，視覺皮層是以分層的結構組織起來的，每一層都會根據前一層所偵測到的特徵得出進一步的訊息，從線條、輪廓、形狀，一直到整個物體。由上述研究得來的第一個概念就是「過濾器(filter)」。
典型的過濾器如下：
- blur = [[1./9, 1./9, 1./9], [1./9, 1./9, 1./9], [1./9, 1./9, 1./9]]
#+CAPTION: 模糊過濾器
#+name: fig:blurFilter
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
[[file:images/blur-filter.png]]
- edges = [[1, 1, 1], [1, -8, 1], [1, 1, 1]]
#+CAPTION: 邊緣強調過濾器
#+name: fig:edgesFilter
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 400
[[file:images/edges-filter.png]]
圖[[fig:blurFilter]]為一 3*3 的模楜強過濾器產生的效果，圖[[fig:edgesFilter]]則為邊緣強週器的效果。過濾器可以改變圖形，並顯示可用於「圖形偵測」和「圖形分類」的特徵。例如，為了對數字進行分類，內部的顏色並不重要，此時，邊緣強調過濾器就有助於辨識數字的一般形狀，進而提升數字識別效能。

我們可以用「類神經網路」的方式來理解「過濾器」，將我們定義的「過濾器」視為一組加權，最終的值又做為下一層的啟動值（輸入）。如圖[[fig:filterScanner]]，過濾器會逐次掃過整張圖，然後建立一組新的圖片，
#+CAPTION: 過濾器的掃瞄計算
#+name: fig:filterScanner
#+ATTR_LATEX: :width 260
#+ATTR_ORG: :width 260
[[file:images/filter-scanner.png]]
** 語言模型
*** 遞迴類神經網路(Recurrent Neural Networks, RNNs)

RNN 能夠處理「任意個數的輸入序列」，所以十分適合用在「語言塑模」或「語音辨識」。理論上，RNN 可以用來處理任何問題，因戈大火弓它已被證明具有「圖靈完備性」(Turing-Complete)。以遞迴關係的函數表示 RNN 可將其視為 \(S_t=f(S_{t-1},X_t)\)，這裡的\(S_t\)表示第\(t\)步的狀態，它是由函數\(f\)對上一步(\(t-1\))的狀態(即\(S_{t-1}\))與這一步的輸入\(X_t\)所計算出來的結果，這裡的函數\(f\)可以是任何可微分的函數，如\(S_t=tang(S_{t-1}*W+X_t*U)\)。
正因為每個狀態都會與之前所有的計算有關，其所代表的重要含義為：隨著時間的推移，RNNs 可以說是有記憶力的，因為狀態 S 包含了之前所有步驟的資訊。

語言塑模的目標是計算「字的序列」的機率，這在「語音辨識」、OCR、「機器翻譯」、「拼字校正」上都非常重要。以「字」為基準的「語言模型」是由「字的序列」來定義機率分佈，給定一個長度為\(m\)的字序列，它會為整個字序列給定一個機率\(P(w_1,...,w_m)\)，其「聯合機率」(joint probability)可以由公式[[eqn:JointProbability]]中的連鎖規則(chain rule)計算出來：
#+NAME: eqn:JointProbability
\begin{equation}
P(w_1,...,w_m)=P(w_1)P(w_2|w_1)P(w3|w_2,w_1)...P(w_m|w_1,...,w_{m-1})
\end{equation}

這個聯合機率一般是基於一個「獨立性假設」(independence assumption)，即，第 i 個字只會相依於它之前的 n-1 個字，如果我們的模型是連續 n 個字的聯合機率，就稱為「n元」(n-gram)。例：
- 1-gram / unigram: "The", "quick", "brown" and "fox"
- 2-grams / bigram: "The quick", "quick brown" and "brown fox"
- 3-grams / trigram: "The quick brown" and "quick brown fox"
- 4-grams: "The quick brown fox"

現在，如果我們有一個巨大的語料庫(corpus of text)，我們就可以用一個特定的 n(通常為 2-4)搜尋所有「n元」在「語料庫」中出現的次數，進而在「給定前 n-1 個字的前提下」，估計出每個 n 元中最後一個字出現的機率。
** 棋盤遊戲

大約在 50 年代，研究人員開始建立具有 AI 的遊戲，這些遊戲以「西洋跳棋」(checkers)和「西洋棋」(chess)為主，這兩種遊戲有一些共同之處：
- 它們是所謂的「零和遊戲」(zero-sum games)，即一個玩家所得到的奬勵就來自另一個玩家相對應的損失。另一類相對的遊戲則是指兩位玩家可以選擇合作，如 「囚徒困境」(prisoner's dilemma)。
- 它們都具有「完全資訊」(perfect information)，兩方不同玩家都知道遊戲的整個狀態；另一種相對的遊戲則是撲克。因為得知目前狀態就可以導出最好的行動，所以這種遊戲可以減少 AI 所需處理問題的複雜度。
- 兩種遊戲都有「明確性」(deterministic): 如果一個玩家下了一步，這步就會導致一個明確的下一個狀態；另一種相對的遊戲中，玩家下的一步可能是丟一次骰子或是抽一張牌，這就無法導致一個明確的下一步。
** 電腦遊戲
** 異常偵測
** 物體偵測
從影像中分析出物體位置，進行分類。物體偵測比物體辨識的問題更困難，最著名的方式為 R-CNN，R-CNN 的實際處理流程有點複雜，包括把影像變形成正方形，使用 SVM 分類。
** 影像分割
指針對影像以像素標籤進行類別分類，利用神經網路進行影像分割，最簡單的方法就是以全部的像素為對象，再依照各個像素進行推論處理。典型做法為 FCN(Fully Convolutional Network)，相對於一般 CNN 含有全連接層的情況，FCN 把全連接層更換成「執行相同動作的卷積層」，在物體辨識的網路全連接層中，中間資料的空間大小當作排列成 1 行節點來處理。
** 產生圖說
針對影像自動產生說明該影像的內容，代表性方法為 NIC (Neural Image Caption)模型，NIC 是由處理多層 CNN 與自然語言的 RNN(Recurrent Neural Network)所構成，RNN 指擁有遞迴功能的網路，常用在自然語言、時間序列資料等有連續性的資料上。
** 影像風格轉換
代表論文為 A Neural Algorithm of Artistic Style。
** 產生影像
從零開始產生「臥室」影像，代表性方法為 DCGAN(Deep Convolutional Generative Adversarial Network)。DCGAN 利用大量影像（如大量拍攝臥室影像）來學習，結束學習後，只要利用該模組就能產生新的影像。DCGAN 運用了 Generator(生成器)與 Discriminator(判別器)等兩個神經網路，Generator 產生與本尊相似的影像，Discriminator 判斷是否為本尊，即，確定是由 Generator 產生的影像或是實際拍攝的影像。兩者彼此制䚘學習，Generator 可以學習到更精巧的偽裝影像技術，Discriminator 則學習更高的鑑定技能，二者相互切磋成長，最終，Generator 能學會畫出與本尊一模一樣的影像。
** 自動駕駛
最近在辨識周圍環境的技術中，深度學習的能力頗受期待，例如以 CNN 為基礎的網路 SegNet 即可精確辨識走路的環境。
** Deep Q-Network (強化學習)
人類是透過嚐試錯誤來學習，例如騎腳踏車，在電腦領域中，也有從嚐試錯誤的過程中進行自主學習的例子，稱為強化學習(reinforcement learning)。在強化學習中，代理人(Agent)根據環境狀況來決定要採取的行動，利用該行動讓㼈境變化。隨環境變化，代理人獲得某些報酬。強化學習的目的是決定代理人的行動方針，以獲得更好的報酬。典型的 DQN 可以讓遊戲自動學習，達到超越人類等級的能力，使用 DQN 的 CNN 可以輸入遊戲影像(如連續 4 個畫面)，最後針對遊戲的控制器動作(搖桿的動作與按鈕)分別輸出該動作的「價值」。由於 DQN 的輸入只是影像，所以不用隨著遊戲的不同來改變設定，同一套 DQN 可以學習「小精靈」與「Atari」。DQN 與 AlphaGo 都是 Google Deep Mind 公司的研究。* ex: 入侵偵測系統

* 深度學習的類型
** VGG

VGG 為由卷積層與池化層構成的基本 CNN。特色是含權重層（卷積層及全連接層）共 16-19 層，有時會稱為 VGG16 或 VGG19。VGG 由於結構非常簡單，應用性高，所以多數技術人員喜歡使用以 VGG 為最基礎的網路。

** GoodLeNet

GoogLeLeNet 基本上與 CNN 相同，其特色是不僅會往垂直方向加深網路，也會往水平方向加深。GoogLeNet 往水平方向的做法稱為「Inception 結構」。

** ResNet

ResNet 是由 Microsoft 團隊開發的網路，特色是具有能加深比過去更多層的「結構」，為了解決因加深過多層數無法順利學習的問題，ResNet 導入了「跳躍結構」（也稱為捷徑或分流）。跳躍結構是「直接」傳遞輸入資料，所以在反向傳播時，也會將上層的梯度「直接」傳遞給下層。透過這種跳躍結構，不用擔心梯度變小（或變得太大），可以把「具有意義的梯度」傳遞給上層。因此，跳躍結構能減少之前因為加深層數，使得梯度變小，出現梯度消失的問題。

* 實作範例
** 以 Keras 解決分類問題

*** 二元分類：IMDB

自 IMDB 資料集中取得 50000 個正/負評論，各 25000 個，該資料集已內建於 Keras 中，且資料已先預處理，電影評論內容為由單字構成的 list 結構，例如，若評論內容為"In a Wonderful morning..."，其 list 結構可能為(8, 3, 386, 1969...)，每個單字都會依據其出現頻率給定一個編號，編號越小越常見。(與 IMDb 相關的 paper 參見[[https://paperswithcode.com/sota/sentiment-analysis-on-imdb][Sentiment Analysis on IMDb / paperswithcode]]

#+BEGIN_SRC python -r -n :results output :exports both
  from keras.datasets import imdb
  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
  print(train_data[0])
  print(train_labels[0])
#+END_SRC

#+RESULTS:
: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
: 1

如上為第一筆評論的單字代號與評論結果，若要將原始資料的單字代號還原，其程式碼如下：
#+BEGIN_SRC python -r -n :results output :exports both
  from keras.datasets import imdb
  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) (ref:imdbLoadData)

  # word_index is a dictionary mapping words to an integer index
  word_index = imdb.get_word_index()                                                  (ref:wordIndex)
  print("字典中key為this對應的value:",word_index['this'])
  # We reverse it, mapping integer indices to words
  reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])    (ref:reverseWordIndex)
  print("反轉字典中key為11所對應到的value:",reverse_word_index[11])
  print("反轉字典中key為1所對應到的value:",reverse_word_index[1])
  print("反轉字典中key為2所對應到的value:",reverse_word_index[2])
  # We decode the review; note that our indices were offset by 3
  # because 0, 1 and 2 are reserved indices for "padding", "start of sequence", and "unknown".
  decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]]) (ref:decodedReview)
  print(decoded_review)
#+END_SRC

#+RESULTS:
#+begin_example
字典中key為this對應的value: 11
反轉字典中key為11所對應到的value: this
反轉字典中key為1所對應到的value: the
反轉字典中key為2所對應到的value: and
編號 0的單字: None
編號 1的單字: the
編號 2的單字: and
編號 3的單字: a
編號11的單字: this
? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all
#+end_example

上述程式中第[[(wordIndex)]]行主要負責取得單字(key)的對應數字(value)的字典，再藉由第[[(reverseWordIndex)]]行將(key:value)轉換為(value:key)，最後第[[(decodedReview)]]行將字典中的單字回復至原始評論，程式中(i-3)的原因是第[[(imdbLoadData)]]的 load 已預留了第 0~2 個位置做特殊用途。

**** 準備資料

由於 IMDB 匯入 train\under{}data 及 test\under{}data 均為 list 型態，要先轉換為 tensor 才能輸入至神經網路，方法有二：

1. 填補資料中每個子 list 內容使其具有相同長度，再轉 shapre。
1. 對每個子 list 做 one-hot 編碼，其程式碼如下：

#+BEGIN_SRC python -r -n :results output :exports both
  from keras.datasets import imdb

  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

  import numpy as np

  def vectorize_sequences(sequences, dimension=10000):
      # Create an all-zero matrix of shape (len(sequences), dimension)
      results = np.zeros((len(sequences), dimension))
      for i, sequence in enumerate(sequences):
          results[i, sequence] = 1.  # set specific indices of results[i] to 1s
      return results

  # Our vectorized training data
  x_train = vectorize_sequences(train_data)
  # Our vectorized test data
  x_test = vectorize_sequences(test_data)

  print(x_train[0])

  # 最後再將標籤資料也向量化
  y_train = np.asarray(train_labels).astype('float32')
  y_test = np.asarray(test_labels).astype('float32')

  print(y_train[0])
#+END_SRC


#+RESULTS:
: [0. 1. 1. ... 0. 0. 0.]
: 1.0

**** 建立神經網路

由於輸入資料為向量、標籤為純量(1, 0)，對這樣的問題，適合用 relu 啟動函數的全連接層(Dense)堆疊架構：Dense(16, activation='relu')。其中 16 指該層神經元的數量(也可看成該層的寬度)，典型旳寫法為：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
output = relu(dot(W, input)+b)
#+END_SRC

擁有 16 個神經單元表示權重矩陣 W 的 shape 為(input_dimension, 10)，在 W 和 input 做內積後，input 資料會被映射到 16 維的空間上，最後加上 b、套用 relu 運算來產生輸出值。每一層的神經元數越多，可以讓神經網路學習更複雜的資料表示法，但也使計算成本更高。

要建構一個 Dense 層堆疊架構，要考慮兩個關鍵：

1. 要用多少層？
1. 每一層要有多少神經元？

此處使用兩個中間層、一個輸出層，如圖[[fig:nn3-6]]，一般的神經網路中，介於輸入層和輸出層間的習慣稱為隱藏層(hidden layers)，但 Keras 的輸入層也有隱藏層的特性。圖[[fig:nn3-6]]的 hidden layer 以 relu 為啟動函數，輸出層以 sigmoid 啟動函數輸出機率值。

#+BEGIN_SRC ditaa :file nn3-6.png

        輸入(向量化文字)
            |
            v
  +-------------------+
  |+-----------------+|
  || Dense(units=16) ||-+
  |+--------+--------+| |
  |         |         | +-隱藏層
  |         v         | |
  |+-----------------+| |
  || Dense(units=10) ||-+
  |+--------+--------+|
  |         |         |
  |         v         |
  |+-----------------+|-+
  ||  Dense(units=1) || +-輸出層
  |+-----------------+|-+
  +-------------------+
  #+END_SRC

#+RESULTS:
#+CAPTION: IMDB model 架構
#+name: fig:nn3-6
#+ATTR_LATEX: :width 200
#+ATTR_HTML: :width 200
#+ATTR_ORG: :width 200
[[file:images/nn3-6.png]]

為何要有 relu 等啟動函數？原因之一是這類函數為非線性函數(如圖[[fig:ReLUFunction]])，如果不是線性函數，則 Dense 層的運作就會變成
#+CAPTION: ReLU 函數圖
#+LABEL:fig:ReLUFunction
#+name: fig:ReLUFunction
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
#+RESULTS:
[[file:images/ReLUPlot.png]]

#+BEGIN_SRC python -r -n :results output :exports both :eval no
output = dot(W, input)+b
#+END_SRC

也就是說，該層只能學習輸入資料的線性變換，即使輸入資料的維度再多，也只是這些多維空間的所有可能線性變換，如此一來就算加入再多層的運算，最終仍只是在做線性運算，並無助於複雜學習。

圖[[fig:nn3-6]]的實作程式如下：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras import models
  from keras import layers

  model = models.Sequential()
  model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
  model.add(layers.Dense(16, activation='relu'))
  model.add(layers.Dense(1, activation='sigmoid'))
#+END_SRC

建好 model 後，要選擇一個損失函數和一個優化器，由於要處理的是二元分類問題，所以最好用 binary\under{}crossentropy 損失函數，因為 crossentropy 主要就是用來測量機率分佈之間的距離(差異)。其實作如下：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  model.compile(optimizer='rmsprop',
                loss='binary_crossentropy',
                metrics=['accuracy'])
#+END_SRC

之所以能將 optimizer 和 loss function 以字串方式經由參數傳給 compoile()，這是因為 rmsprop、binary\under{}crossentropy 和 accuracy 均已事先在 Keras 套件中定義好了，若是要進一步自訂參數(如自訂學習率)，做法如下：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  # 調整learning rate
  from keras import optimizers

  model.compile(optimizer=optimizers.RMSprop(lr=0.001),
                loss='binary_crossentropy',
                metrics=['accuracy'])

  # 使用另外的評估函數
  from keras import losses
  from keras import metrics

  model.compile(optimizer=optimizers.RMSprop(lr=0.001),
                loss=losses.binary_crossentropy,
                metrics=[metrics.binary_accuracy])


#+END_SRC

**** 驗證神經網路的 model

為了在訓練期間監控 model 對新資料的準確度，可以從原始訓練資料中分離出 10000 個樣本來建立驗證資料集。

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  x_val = x_train[:10000] # 前10000個資料為驗證集
  partial_x_train = x_train[10000:] # 第10000個以後為訓練集

  y_val = y_train[:10000]
  partial_y_train = y_train[10000:]
#+END_SRC

接下來才是使用 fit()來訓練模型，進行 20 個訓練週期(epoch，即，把 x\under{}train 和 y\under{}train 張量中的所有訓練樣本進行 20 輪的訓練)，以 512 個小樣本的小批量(batch\under{}size)進行訓練，

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  history = model.fit(partial_x_train,
                      partial_y_train,
                      epochs=20,
                      batch_size=512,
                      validation_data=(x_val, y_val))
#+END_SRC

model.fit()會回傳一個 history 物件，這物件本身有一個 history 屬性，為一個包含有關訓練過程中相關數據的字典，這個字期包含有 4 個項目(val\under{}loss, val\under{}acc, loss, acc)，為訓練和驗證時監控的指標。

#+BEGIN_SRC python -r -n :results output :exports both
  # 準備資料
  from keras.datasets import imdb
  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
  import numpy as np
  def vectorize_sequences(sequences, dimension=10000):
      # Create an all-zero matrix of shape (len(sequences), dimension)
      results = np.zeros((len(sequences), dimension))
      for i, sequence in enumerate(sequences):
          results[i, sequence] = 1.  # set specific indices of results[i] to 1s
      return results
  # Our vectorized training data
  x_train = vectorize_sequences(train_data)
  # Our vectorized test data
  x_test = vectorize_sequences(test_data)
  # 最後再將標籤資料也向量化
  y_train = np.asarray(train_labels).astype('float32')
  y_test = np.asarray(test_labels).astype('float32')
  # 建立model
  from keras import models
  from keras import layers
  model = models.Sequential()
  model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
  model.add(layers.Dense(16, activation='relu'))
  model.add(layers.Dense(1, activation='sigmoid'))
  from keras import optimizers
  model.compile(optimizer=optimizers.RMSprop(lr=0.001),
                loss='binary_crossentropy',
                metrics=['accuracy'])
  # 驗證數據集
  x_val = x_train[:10000] # 前10000個資料為驗證集
  partial_x_train = x_train[10000:] # 第10000個以後為訓練集
  y_val = y_train[:10000]
  partial_y_train = y_train[10000:]
  # 訓練model
  history = model.fit(partial_x_train,
                      partial_y_train,
                      epochs=20,
                      batch_size=512,
                      validation_data=(x_val, y_val),
                      verbose=0)
  # 秀出history架構
  history_dict = history.history
  print(history_dict.keys())

  # 畫圖
  import matplotlib.pyplot as plt
  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  epochs = range(1, len(accuracy) + 1)# "bo" is for "blue dot"
  plt.plot(epochs, loss, 'bo', label='Training loss')
  # b is for "solid blue line"
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.plot()
  plt.savefig("imdb-Keras-1.png")
  #plt.show()plt.clf()   # clear figure

  plt.clf()
  acc_values = history_dict['accuracy']
  val_acc_values = history_dict['val_accuracy']
  plt.plot(epochs, accuracy, 'bo', label='Training acc')
  plt.plot(epochs, val_accuracy, 'b', label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.plot()
  plt.savefig("imdb-Keras-2.png")

  #plt.show()

#+END_SRC

#+RESULTS:
: dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])

#+CAPTION: IMDB-Keras-1
#+LABEL:fig: IMDB-Keras-1
#+name: fig:IMDB-Keras-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:images/imdb-Keras-1.png]]

#+CAPTION:IMDB-Keras-2
#+LABEL:fig:IMDB-Keras-2
#+name: fig:IMDB-Keras-2
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:images/imdb-Keras-2.png]]

**** 優化 model

由圖[[fig:IMDB-Keras-1]]、[[fig:IMDB-Keras-2]]可以看出，上述 model 雖然在訓練階段的效能不錯，loss function 隨 epoch 下降、accuracy 也隨 epoch 升高，但在驗證階段的表現卻十分不理想，不僅 accuracy 隨 epoch 的增加呈緩降趨勢，loss function 甚至還往上急升。

第二版的 model 加入了兩層 layer 以及 dropout 層，其架構如下:

#+BEGIN_SRC ditaa :file nn3-6-2.png

        輸入(向量化文字)
            |
            v
  +-------------------+
  |+-----------------+|
  || Dense(units=16) ||-+
  |+--------+--------+| |
  |         |         | |
  |         v         | |
  |+-----------------+| |
  || Dense(units=64) || |
  |+--------+--------+| |
  |         |         | |
  |         v         | |
  |+-----------------+| |
  ||  Dropout(0.25)  || |
  |+--------+--------+| |
  |         |         | +-隱藏層
  |         v         | |
  |+-----------------+| |
  || Dense(units=64) || |
  |+--------+--------+| |
  |         |         | |
  |         v         | |
  |+-----------------+| |
  ||  Dropout(0.25)  || |
  |+--------+--------+| |
  |         |         | |
  |         v         | |
  |+-----------------+| |
  || Dense(units=10) ||-+
  |+--------+--------+|
  |         |         |
  |         v         |
  |+-----------------+|-+
  ||  Dense(units=1) || +-輸出層
  |+-----------------+|-+
  +-------------------+
  #+END_SRC
#+RESULTS:
#+CAPTION: IMDB model 架構#2
#+name: fig:nn3-6-2
#+ATTR_LATEX: :width 200
#+ATTR_HTML: :width 200
#+ATTR_ORG: :width 200
[[file:images/nn3-6-2.png]]

#+BEGIN_SRC python -r -n :results output :exports both
  # 準備資料
  from keras.datasets import imdb
  (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
  import numpy as np
  def vectorize_sequences(sequences, dimension=10000):
      # Create an all-zero matrix of shape (len(sequences), dimension)
      results = np.zeros((len(sequences), dimension))
      for i, sequence in enumerate(sequences):
          results[i, sequence] = 1.  # set specific indices of results[i] to 1s
      return results
  # Our vectorized training data
  x_train = vectorize_sequences(train_data)
  # Our vectorized test data
  x_test = vectorize_sequences(test_data)
  # 最後再將標籤資料也向量化
  y_train = np.asarray(train_labels).astype('float32')
  y_test = np.asarray(test_labels).astype('float32')
  # 建立model
  from keras import models
  from keras import layers

  model = models.Sequential()
  model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dropout(0.25))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dropout(0.25))
  model.add(layers.Dense(1, activation='sigmoid'))
  from keras import optimizers
  from keras import metrics
  model.compile(optimizer=optimizers.RMSprop(lr=0.0001),
                loss='binary_crossentropy',
                metrics=[metrics.binary_accuracy])
  # 驗證數據集
  x_val = x_train[:10000] # 前10000個資料為驗證集
  partial_x_train = x_train[10000:] # 第10000個以後為訓練集
  y_val = y_train[:10000]
  partial_y_train = y_train[10000:]
  # 訓練model
  history = model.fit(partial_x_train,
                      partial_y_train,
                      epochs=20,
                      batch_size=512,
                      validation_data=(x_val, y_val),
                      verbose=0)
  # 秀出history架構
  history_dict = history.history
  print(history_dict.keys())

  # 進行預測
  x = model.predict(x_test)
  print(x)

  # 畫圖
  import matplotlib.pyplot as plt
  plt.clf()
  binary_accuracy = history.history['binary_accuracy']
  val_binary_accuracy = history.history['val_binary_accuracy']
  loss = history.history['loss']
  val_loss = history.history['val_loss']
  epochs = range(1, len(binary_accuracy) + 1)# "bo" is for "blue dot"
  plt.plot(epochs, loss, 'bo', label='Training loss')
  # b is for "solid blue line"
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.plot()
  plt.savefig("imdb-Keras-3.png")
  #plt.show()plt.clf()   # clear figure

  plt.clf()
  acc_values = history_dict['binary_accuracy']
  val_acc_values = history_dict['val_binary_accuracy']
  plt.plot(epochs, binary_accuracy, 'bo', label='Training acc')
  plt.plot(epochs, val_binary_accuracy, 'b', label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.legend()
  plt.plot()
  plt.savefig("imdb-Keras-4.png")
  #plt.show()

#+END_SRC

#+RESULTS:
: dict_keys(['val_loss', 'val_binary_accuracy', 'loss', 'binary_accuracy'])
: [[0.1434195 ]
:  [0.9996901 ]
:  [0.98705375]
:  ...
:  [0.05256996]
:  [0.11039814]
:  [0.7423996 ]]

#+CAPTION: IMDB-Keras-3
#+LABEL:fig: IMDB-Kera-3
#+name: fig:IMDB-Keras-3
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:[[file:images/imdb-Keras-3.png]]images/imdb-Keras-1.png]]

#+CAPTION:IMDB-Keras-4
#+LABEL:fig:IMDB-Keras-4
#+name: fig:IMDB-Keras-4
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:[[file:images/imdb-Keras-4.png]]images/imdb-Keras-2.png]]

比較上述兩組結果，可以發現優化版的 model 在 loss function 以及 accuracy 的表現都有進步。

*** 多類別分類：數位新聞

目標：將路透社(Reuters)的數位新聞專欄分成 46 個主題，這屬於多類別分類(multiclass classification)問題，每個資料點只會被歸入一個類別；如果每個資料點可能屬於多個類別，則屬於多標籤多類別(multilabel multiclass classification)問題。

**** 資料集

和 MNIST、IMDB 一樣，這組由 Reuters 在 1986 年發布的簡短新聞主題資料集也內建在 Keras 中，這個資料集總共分為 46 個不同主題。

#+BEGIN_SRC python -r -n :results output :exports both
  from keras.datasets import reuters
  (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)
  print(train_data[0])
  print(train_labels[0])
#+END_SRC

#+RESULTS:
: [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]
: 3

將資料向量化有幾種方式：將 label list 轉為整數張量，或是用 one-hot 編碼。以下為使用 pythonh 自訂的編碼程式：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import numpy as np

  def vectorize_sequences(sequences, dimension=10000):
      results = np.zeros((len(sequences), dimension))
      for i, sequence in enumerate(sequences):
          results[i, sequence] = 1.
      return results

  # Our vectorized training data
  x_train = vectorize_sequences(train_data)
  # Our vectorized test data
  x_test = vectorize_sequences(test_data)

#+END_SRC

另外，Keras 也有一個內建的函式可用：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras.utils.np_utils import to_categorical

  one_hot_train_labels = to_categorical(train_labels)
  one_hot_test_labels = to_categorical(test_labels)

#+END_SRC

**** 建立神經網路

此次面臨的問題不似 IMDB 只分成兩類，而是共有 46 類，若每個 Dense layer 仍只使用 16 個維度，可能無法學會區分 46 個不同類別，故有需要將維度增加：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras import models
  from keras import layers

  model = models.Sequential()
  model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(46, activation='softmax'))

#+END_SRC

另外，輸出層將啟動函數由 sigmoid 改為 softmax，以機率值來顯示預測的類別結果，配合這種情境，最適合的損失函數為 categorical\under{}crossentropy，它可以測量兩個機率分佈間的差距（即神經網路輸出的預測機率分佈與真實分佈間的距離），透過最小化這兩個分佈間的距離來訓練神經網路，讓結果接近答案。

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  model.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['accuracy'])

#+END_SRC

**** 驗證數據集

由訓練集鵋出 1000 個樣本來驗證：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  x_val = x_train[:1000]
  partial_x_train = x_train[1000:]

  y_val = one_hot_train_labels[:1000]
  partial_y_train = one_hot_train_labels[1000:]
#+END_SRC

**** 完整實作

以下為完整的 model 程式碼

#+BEGIN_SRC python -r -n :results output :exports both
  from keras.datasets import reuters

  (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

  import numpy as np

  def vectorize_sequences(sequences, dimension=10000):
      results = np.zeros((len(sequences), dimension))
      for i, sequence in enumerate(sequences):
          results[i, sequence] = 1.
      return results

  # Our vectorized training data
  x_train = vectorize_sequences(train_data)
  # Our vectorized test data
  x_test = vectorize_sequences(test_data)

  from keras.utils.np_utils import to_categorical

  one_hot_train_labels = to_categorical(train_labels)
  one_hot_test_labels = to_categorical(test_labels)

  # 建構模型
  from keras import models
  from keras import layers

  model = models.Sequential()
  model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
  model.add(layers.Dense(64, activation='relu'))
  model.add(layers.Dense(46, activation='softmax'))

  model.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['accuracy'])  (ref:metricsName)

  # 驗證
  x_val = x_train[:1000]
  partial_x_train = x_train[1000:]

  y_val = one_hot_train_labels[:1000]
  partial_y_train = one_hot_train_labels[1000:]

  # 訓練
  history = model.fit(partial_x_train,
                      partial_y_train,
                      epochs=9,
                      batch_size=512,
                      validation_data=(x_val, y_val),
                      verbose=0)

  history_dict = history.history
  print(history_dict.keys())

  # 評估
  # Returns the loss value & metrics values for the model in test mode.
  results = model.evaluate(x_test, one_hot_test_labels)               (ref:modelEvaluate)
  print("評估資料內容：",results)

  # 預測
  predictions = model.predict(x_test)
  print("預測資料架構：",predictions[0].shape)
  print("預測資料內容：",predictions[0])
  print("預測結果:",np.argmax(predictions[0]))
  print("答案:",one_hot_test_labels[0])
  # 畫圖

  import matplotlib.pyplot as plt

  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(loss) + 1)

  plt.plot(epochs, loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.axis([0, 10, 0, 3])
  plt.legend()
  plt.plot()
  plt.savefig("reuters-1.png")
  #plt.show()

  plt.clf()   # clear figure

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  plt.plot(epochs, accuracy, 'bo', label='Training accuracy')
  plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')
  plt.title('Training and validation accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.axis([0, 10, 0, 1])
  plt.legend()
  plt.plot()
  plt.savefig("reuters-2.png")
  # plt.show()

#+END_SRC

#+RESULTS:
#+begin_example
dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])

  32/2246 [..............................] - ETA: 0s
 672/2246 [=======>......................] - ETA: 0s
1344/2246 [================>.............] - ETA: 0s
2016/2246 [=========================>....] - ETA: 0s
2246/2246 [==============================] - 0s 78us/step
評估資料內容： [0.9810597261783807, 0.7804986834526062]
預測資料架構： (46,)
預測資料內容： [4.7579077e-05 1.2676844e-03 1.5874884e-04 9.6115595e-01 2.2415580e-02
 4.0142340e-06 1.0888425e-04 6.9402384e-05 6.5381191e-04 8.4027524e-05
 1.4560925e-05 1.6368082e-03 9.6688804e-05 4.5832386e-04 2.1395419e-05
 2.1998589e-05 5.2564731e-03 1.6274580e-04 1.8614135e-05 1.5144094e-03
 2.2311162e-03 5.8142754e-04 1.6369991e-05 2.4161035e-04 3.8008704e-05
 1.2996762e-04 9.4583183e-06 1.0127547e-04 1.5613921e-05 2.0752830e-04
 1.2362217e-04 9.5950272e-05 4.5157034e-05 3.6724876e-05 3.9637266e-04
 6.4885942e-05 1.7066645e-04 6.9418798e-05 2.6165835e-05 1.2429565e-04
 1.5218212e-05 7.5062417e-05 1.4183885e-06 6.8154754e-06 2.2027129e-06
 6.0409470e-06]
預測結果: 3
答案: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#+end_example

#+CAPTION:Reuters-1
#+LABEL:fig:Reuters-1
#+name: fig:Reuters-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:images/reuters-1.png]]

#+CAPTION:Reuters-2
#+LABEL:fig:Reuters-2
#+name: fig:Reuters-2
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:images/reuters-2.png]]

程式第[[(modelEvaluate)]]行傳回的值有兩個，一個是 loss value、一個是在建構 model 時(model.compile)所指定的評估標準 metrics（程式第[[(metricsName)]]行），在此處指的是 accuracy。上述程式在經由 9 個 epoch 後精準度已近 80%(0.79)。

**** 優化 model

上例中的中間層若將神經元數(維度)降到 4，則其驗證準確率會降至 71%，主要原因是因為這樣會壓縮大量資訊到一個低維度的中間層表示空間，雖然神經網路能將大部份必要的資訊塞進這 4 維表示法中，但仍顯不足。若再提升維度、增加層數、加入 Dropout，結果似乎沒有顯著改善，為什麼？

#+BEGIN_SRC python -r -n :results output :exports both
  from keras.datasets import reuters

  (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)

  import numpy as np

  def vectorize_sequences(sequences, dimension=10000):
      results = np.zeros((len(sequences), dimension))
      for i, sequence in enumerate(sequences):
          results[i, sequence] = 1.
      return results

  # Our vectorized training data
  x_train = vectorize_sequences(train_data)
  # Our vectorized test data
  x_test = vectorize_sequences(test_data)

  from keras.utils.np_utils import to_categorical

  one_hot_train_labels = to_categorical(train_labels)
  one_hot_test_labels = to_categorical(test_labels)

  # 建構模型
  from keras import models
  from keras import layers

  model = models.Sequential()
  model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))
  model.add(layers.Dense(128, activation='relu'))
  model.add(layers.Dropout(0.25))
  model.add(layers.Dense(256, activation='relu'))
  model.add(layers.Dropout(0.3))
  model.add(layers.Dense(512, activation='relu'))
  model.add(layers.Dropout(0.5))
  model.add(layers.Dense(46, activation='softmax'))

  model.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['accuracy'])  (ref:metricsName)

  # 驗證
  x_val = x_train[:1000]
  partial_x_train = x_train[1000:]

  y_val = one_hot_train_labels[:1000]
  partial_y_train = one_hot_train_labels[1000:]

  # 訓練
  history = model.fit(partial_x_train,
                      partial_y_train,
                      epochs=9,
                      batch_size=512,
                      validation_data=(x_val, y_val),
                      verbose=0)

  history_dict = history.history
  print(history_dict.keys())

  # 評估
  # Returns the loss value & metrics values for the model in test mode.
  results = model.evaluate(x_test, one_hot_test_labels)               (ref:modelEvaluate)
  print("評估資料內容：",results)

  # 預測
  predictions = model.predict(x_test)
  print("預測資料架構：",predictions[0].shape)
  print("預測資料內容：",predictions[0])
  print("預測結果:",np.argmax(predictions[0]))
  print("答案:",one_hot_test_labels[0])
  # 畫圖

  import matplotlib.pyplot as plt

  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(loss) + 1)

  plt.plot(epochs, loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.axis([0, 10, 0, 3])
  plt.legend()
  plt.plot()
  plt.savefig("reuters-3.png")
  #plt.show()

  plt.clf()   # clear figure

  accuracy = history.history['accuracy']
  val_accuracy = history.history['val_accuracy']

  plt.plot(epochs, accuracy, 'bo', label='Training accuracy')
  plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')
  plt.title('Training and validation accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Loss')
  plt.axis([0, 10, 0, 1])
  plt.legend()
  plt.plot()
  plt.savefig("reuters-4.png")
  # plt.show()


#+END_SRC

#+RESULTS:
#+begin_example
dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])

  32/2246 [..............................] - ETA: 0s
 544/2246 [======>.......................] - ETA: 0s
1088/2246 [=============>................] - ETA: 0s
1632/2246 [====================>.........] - ETA: 0s
2208/2246 [============================>.] - ETA: 0s
2246/2246 [==============================] - 0s 96us/step
評估資料內容： [1.3893752790197982, 0.7497773766517639]
預測資料架構： (46,)
預測資料內容： [8.8242497e-11 1.8549613e-07 1.2244985e-12 9.9946600e-01 4.8491010e-04
2.4004774e-11 4.0463274e-08 2.8705716e-09 1.4672127e-06 1.2457425e-12
 2.1608832e-08 1.1945065e-06 1.6438412e-08 6.1330823e-08 5.5952110e-10
 2.0300506e-12 4.4415983e-06 5.0839004e-09 3.8925752e-09 2.0913212e-05
 2.0335670e-05 7.7277225e-09 1.0450782e-12 6.6110601e-08 2.6362378e-11
 2.6260804e-07 2.6264095e-12 4.5255667e-11 4.4987689e-10 2.7449030e-09
 1.0358207e-08 7.0458644e-10 1.4057776e-09 6.6201856e-11 9.8362518e-09
 1.4279193e-11 2.3172060e-08 3.4204664e-10 7.4201589e-10 3.5206096e-08
 2.1588344e-09 2.4565621e-09 1.9249602e-11 5.2338623e-11 4.7235077e-14
 9.4377089e-14]
預測結果: 3
答案: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
#+end_example

#+CAPTION: Rueter-3
#+LABEL:fig:Rueter-3
#+name: fig:Rueter-3
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:[[file:images/reuters-3.png]]images/reuters-1.png]]
#+CAPTION: Rueters-4
#+LABEL:fig:Rueters-4
#+name: fig:Rueters-4
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:[[file:images/reuters-4.png]]images/reuters-2.png]]


#+latex:\newpage

** 以 Keras 解決迴歸問題：預測房價: Boston

迴歸(regression)與邏輯斯迴歸(logistic regression)不同，後者為分類法，與迴歸無關。本例使用資料集為 1970 年中期 Boston 郊區資料，包含犯罪率、當地財產稅等，用以預測某郊區房價中位數，本例有 506 筆資料，分為 404 個訓練樣本和 102 個測試樣本，但每個 feature 的單位不同，故須先進行資料預調整。

*** 準備資料

#+BEGIN_SRC python -r -n :results output :exports both
from keras.datasets import boston_housing

(train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()

print(train_data.shape)
print(test_data.shape)
#+END_SRC

#+RESULTS:
: Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz
:
:  8192/57026 [===>..........................] - ETA: 1s
: 24576/57026 [===========>..................] - ETA: 0s
: 40960/57026 [====================>.........] - ETA: 0s
: 57344/57026 [==============================] - 1s 11us/step
: (404, 13)
: (102, 13)

由於將不同類型不同單位的數值直接輸入神經網路會有問題，故要先資料進行正規化(normalization)處理，即，減去平均值，除以標準差。需留意的是，正規化時要用訓練資料集來計算 mean 和 std，不能使用測試集的資料。

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  mean = train_data.mean(axis=0)
  train_data -= mean
  std = train_data.std(axis=0)
  train_data /= std

  test_data -= mean
  test_data /= std
#+END_SRC

*** 建立神經網路

由於可用的樣本很少，所以使用一個較小的神經網路，一般來說，訓練資料集越少，過度配適的情況會越嚴重。

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras import models
  from keras import layers

  def build_model():
      # Because we will need to instantiate
      # the same model multiple times,
      # we use a function to construct it.
      model = models.Sequential()
      model.add(layers.Dense(64, activation='relu',
                             input_shape=(train_data.shape[1],)))
      model.add(layers.Dense(64, activation='relu'))
      model.add(layers.Dense(1))      (ref:OneUnitLayer)
      model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
      return model

#+END_SRC

這裡以 1 unit 的神經網路結束而且沒有啟動函數(第[[(OneUnitLayer)]]行)，代表為線性轉換，這是純量迴歸的基本設定，會輸出一個浮點數型別的數值(即迴歸值)，如果使用啟動函數，則只會輸出 0~1 間的值。另，mse 也是迴歸常用的損失函數，在評量指標的選擇方面，則採用 mae(mean absolute error，即預測值與目標值間差異的絕對值)。

*** 驗證



本例中由於資料點少，驗證集也只有 100 筆資料，故驗證分數可能會因驗證資料點或訓練資料點的選用而有很大的變化，因而阻礙評估 model 優劣的可靠性。在這種情況下，最好的方式是選用 K-fold corss validation，做法如圖[[fig:K-fold-cross-validation]]，原理是將資料拆分為 K 個區域(通常 K=4 或 5)，每次取一個區域做為驗證資料集，最後求 K 次驗證分數的平均值。

#+CAPTION:K-fold 交叉驗證
#+LABEL:fig:K-fold-cross-validation
#+name: fig:K-fold-cross-validation
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:images/k-fold-validation.png]]

K-fold cross validation 的 python 實作程式碼如下：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import numpy as np

  k = 4
  num_val_samples = len(train_data) // k
  num_epochs = 100
  all_scores = []
  for i in range(k):
      print('processing fold #', i)
      # Prepare the validation data: data from partition # k
      val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
      val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]

      # Prepare the training data: data from all other partitions
      partial_train_data = np.concatenate(
          [train_data[:i * num_val_samples],
           train_data[(i + 1) * num_val_samples:]],
          axis=0)
      partial_train_targets = np.concatenate(
          [train_targets[:i * num_val_samples],
           train_targets[(i + 1) * num_val_samples:]],
          axis=0)

      # Build the Keras model (already compiled)
      model = build_model()
      # Train the model (in silent mode, verbose=0)
      model.fit(partial_train_data, partial_train_targets,
                epochs=num_epochs, batch_size=1, verbose=0)
      # Evaluate the model on the validation data
      val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
      all_scores.append(val_mae)
#+END_SRC



#+BEGIN_SRC python -r -n :results output :exports both
  from keras.datasets import boston_housing
  (train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()
  print(train_data.shape)
  print(test_data.shape)

  mean = train_data.mean(axis=0)
  train_data -= mean
  std = train_data.std(axis=0)
  train_data /= std
  test_data -= mean
  test_data /= std

  from keras import models
  from keras import layers
  def build_model():
      # Because we will need to instantiate
      # the same model multiple times,
      # we use a function to construct it.
      model = models.Sequential()
      model.add(layers.Dense(64, activation='relu',
                             input_shape=(train_data.shape[1],)))
      model.add(layers.Dense(64, activation='relu'))
      model.add(layers.Dense(1))
      model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
      return model

  import numpy as np

  k = 4
  num_val_samples = len(train_data) // k
  num_epochs = 100
  all_scores = []
  for i in range(k):
      print('processing fold #', i)
      # Prepare the validation data: data from partition # k
      val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
      val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
      # Prepare the training data: data from all other partitions
      partial_train_data = np.concatenate(
          [train_data[:i * num_val_samples],
           train_data[(i + 1) * num_val_samples:]],
          axis=0)
      partial_train_targets = np.concatenate(
          [train_targets[:i * num_val_samples],
           train_targets[(i + 1) * num_val_samples:]],
          axis=0)
      # Build the Keras model (already compiled)
      model = build_model()
      # Train the model (in silent mode, verbose=0)
      model.fit(partial_train_data, partial_train_targets,
                epochs=num_epochs, batch_size=1, verbose=0)
      # Evaluate the model on the validation data
      val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
      all_scores.append(val_mae)

  print(all_scores)
  print(np.mean(all_scores))
#+END_SRC

#+RESULTS:
: (404, 13)
: (102, 13)
: processing fold # 0
: processing fold # 1
: processing fold # 2
: processing fold # 3
: [1.8689913749694824, 2.581745147705078, 2.9093284606933594, 2.6838433742523193]
: 2.51097708940506

由上述結果看來，拆成 4 區的驗證分數自 1.87 到 2.91，總平均為 2.51，這個平均值是較為可靠的指標，因為當目標房價的數值很大時，1.87 到 2.91 會變成很大的誤差。

可能是因為 MAC 與 Linux 版本的 Anaconda 相容性問題，或是 Keras 版本差異問題，MAC 版與 Linux 下的 history.history 架構略有差異：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
# Linux with Keras 2.2.5
dict_keys(['val_loss', 'val_mean_absolute_error', 'loss', 'mean_absolute_error'])
# Mac with Keras 2.3.1
dict_keys(['val_loss', 'val_mae', 'loss', 'mae'])
#+END_SRC

#+BEGIN_SRC python -r -n :results output :exports both
  from keras.datasets import boston_housing
  (train_data, train_targets), (test_data, test_targets) =  boston_housing.load_data()
  print(train_data.shape)
  print(test_data.shape)
  mean = train_data.mean(axis=0)
  train_data -= mean
  std = train_data.std(axis=0)
  train_data /= std
  test_data -= mean
  test_data /= std
  from keras import models
  from keras import layers
  def build_model():
      # Because we will need to instantiate
      # the same model multiple times,
      # we use a function to construct it.
      model = models.Sequential()
      model.add(layers.Dense(64, activation='relu',
                             input_shape=(train_data.shape[1],)))
      model.add(layers.Dense(64, activation='relu'))
      model.add(layers.Dense(1))
      model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])
      return model

  import numpy as np
  from keras import backend as K
  # Some memory clean-up
  K.clear_session()
  k = 4
  num_val_samples = len(train_data) // k
  num_epochs = 500
  all_mae_histories = []
  for i in range(k):
      print('processing fold #', i)
      # Prepare the validation data: data from partition # k
      val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
      val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
      # Prepare the training data: data from all other partitions
      partial_train_data = np.concatenate(
          [train_data[:i * num_val_samples],
           train_data[(i + 1) * num_val_samples:]],
          axis=0)
      partial_train_targets = np.concatenate(
          [train_targets[:i * num_val_samples],
           train_targets[(i + 1) * num_val_samples:]],
          axis=0)
      # Build the Keras model (already compiled)
      model = build_model()
      # Train the model (in silent mode, verbose=0)
      history = model.fit(partial_train_data, partial_train_targets,
                          validation_data=(val_data, val_targets),
                          epochs=num_epochs, batch_size=1, verbose=0)
      mae_history = history.history['val_mae']
      all_mae_histories.append(mae_history)

  average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]
  import matplotlib.pyplot as plt
  plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)
  plt.xlabel('Epochs')
  plt.ylabel('Validation MAE')
  plt.plot()
  plt.savefig("Boston-House-Price.png")

  # 排除每週期的前10個資料點
  def smooth_curve(points, factor=0.9):
    smoothed_points = []
    for point in points:
      if smoothed_points:
        previous = smoothed_points[-1]
        smoothed_points.append(previous * factor + point * (1 - factor))
      else:
        smoothed_points.append(point)
    return smoothed_points

  smooth_mae_history = smooth_curve(average_mae_history[10:])
  plt.clf()
  plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)
  plt.xlabel('Epochs')
  plt.ylabel('Validation MAE')
  plt.plot()
  plt.savefig("Boston-House-Price-ex10.png")

#+END_SRC

#+RESULTS:
: (404, 13)
: (102, 13)
: processing fold # 0
: processing fold # 1
: processing fold # 2
: processing fold # 3

#+CAPTION: Boston House Price Training MAE
#+LABEL:fig:BostonHouseMAE
#+name: fig:BostonHouseMAE
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:images/Boston-House-Price.png]]

圖[[fig:BostonHouseMAE]]是由每一訓練週期的平均 MAE 分數所繪出的折線圖，由於單位刻度與 y 軸刻度問題，此圖失去了部份重要細節，經由下列方式進行修正：
- 省略前 10 個資料點，
- 把每個資料點替換成前一點的指數移動平均值(exponential moving average, EMA)，讓誤差變平滑。

EMA 常應用於各領域的資料分析中，其核心概念為：現在的資料會被過去的資料所影響，而時間點越近的資料影響越大，反之越小，如股票的漲幅，前 10 年的漲跌與前 10 日的漲跌，自然是後者對未來的影響更大。

EMA 的數學函式如下：

\( E_t = a \times V_t + (1-a) \times E_{t-1} \)，其中

- \(E_t\)為時間點\(t\)的指數移動平均值
- \(a\)為平滑係數，通常介於 0 到 1 之間
- \(V_t\)為時間點\(t\)的原始數值
- \(E_{t-1}\)為時間點\(t-1\)的指數移動平均值

為什麼前例中前 10 筆數據的與其他數據差異如此巨大？我們以前 10 天的資料(一天一筆)來看，第 10 天的 EMA 為：
\( E_{10} = aV_{10} + (1-a)E_9 \)
展開第 9 天的\(E_9\)後
\( E_{10} = aV_{10} + (1-a)[aV_9 + (1-a)E_8] \)
整理後變成
\( E_{10} = a(V_{10} + (1-a)V_9) + (1-a)^{2}E_8 \)
若繼續展開所有天數，將得到
\( E_{10} = a(V_{10} + (1-a)V_9) + (1-a)^{2}E_8+ \dots + (1-a)^{9}V_{1}) + (1-a)^{9}E_1 \)
通常上式的最後一項會因為時間很長而變太小，故可忽略不計，而由此也可看出，\(E_{10}\)的值會被每天的原始資料\((V_{10} \dots V_{1}\))影響，每多一天，原始數值就會多乘(1-a)倍，成指數關係，故時間越久遠的事件，影響越小。

#+CAPTION: Boston House Price Training MAE (排除前 10 個資料點)
#+LABEL:fig:BostonHouseMAE-ex10
#+name: fig:BostonHouseMAE-ex10
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:images/Boston-House-Price-ex10.png]

由圖[[fig:BostonHouseMAE-ex10]]是可看出 MAE 在 80 個週期後已停止改善，然後開始往上升，即，過了這點就開始發生過度適配的情況。

*** 小結

由此範例可知：
- 進行迴歸分木卜竹一中時，常以 MSE 做為損失函數、以 MAE 做為評估指標(而非 accuracy).
- 當輸入資料的特徵有不同刻度時，應先將每個特徵進行轉換。
- 當可用資料很少時，使用 K-fold 驗證來評估模式。
- 當可用資料很少時，最好使用隠藏層較少(較淺)的小型神經網路，如一個或兩個，以免產生過渡配適。

#+BEGIN_SRC python -r -n :results output :exports both
  import keras
  print(keras.__version__)
#+END_SRC

#+RESULTS:
: 2.3.1

#+BEGIN_SRC python -r -n :results output :exports both :eval no
#+END_SRC

#+BEGIN_SRC python -r -n :results output :exports both :eval no
#+END_SRC

#+BEGIN_SRC python -r -n :results output :exports both :eval no
#+END_SRC

#+BEGIN_SRC python -r -n :results output :exports both :eval no
#+END_SRC


#+latex:\newpage

** 以神經網路重跑鳶尾花問題
*** 鳶尾花分類問題
**** DataSet
收集了3種鳶尾花的四個特徵，分別是花萼(sepal)長寬、花瓣(petal)長寬度，以及對應的鳶尾花種類。
#+CAPTION: 鳶尾花的花萼與花瓣
#+LABEL:fig:iris-1
#+name: fig:iris-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 400
[[file:images/iris-1.png]]
**** Mission
輸入花萼和花瓣數據後，推測所屬的鳶尾花類型。
#+CAPTION: 三種鳶尾花
#+LABEL:fig:Labl
#+name: fig:Name
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 600
[[file:images/iris-2.png]]
*** 實作
1. 讀取資料集
   #+begin_src python -r :results output :exports no :session iris :async
from sklearn import datasets

# 讀入資料
iris = datasets.load_iris()
print(iris.DESCR)
   #+end_src

2. 取出特徵與標籤
   #+begin_src python -r :results output :exports no
x = iris.data
y = iris.target
print(x[:5])
print(y[:5])
   #+end_src
3. 資料觀察
   #+begin_src python -r :results output :exports both
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
#把nupmy ndarray轉為pandas dataFrame,加上columns title
npx = pd.DataFrame(x, columns=['fac1','fac2','fac3','fac4'])
npy = pd.DataFrame(y.astype(int), columns=['category'])
#合併
dataPD = pd.concat([npx, npy], axis=1)
print(dataPD)
# 畫圖
sns.lmplot('fac1', 'fac2', data=dataPD, hue='category', fit_reg=False)
plt.show()
   #+end_src

4. 分割資料集
   #+begin_src python -r :results output :exports both
from sklearn.model_selection import train_test_split
# 劃分資料集
x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=6)
   #+end_src
   - train_test_split()
     所接受的變數其實非常單純，基本上為 3 項：『原始的資料』、『Seed』、『比例』
     1. 原始的資料：就如同上方的 data 一般，是我們打算切成 Training data 以及 Test data 的原始資料
     2. Seed： 亂數種子，可以固定我們切割資料的結果
     3. 比例：可以設定 train_size 或 test_size，只要設定一邊即可，範圍在 [0-1] 之間
   - scikit-learn.org: sklearn.model_selection.train_test_split

     Split arrays or matrices into random train and test subsets

     Quick utility that wraps input validation and next(ShuffleSplit().split(X, y)) and application to input data into a single call for splitting (and optionally subsampling) data in a oneliner.
     #+begin_src python -r :results output :exports both
 sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)[source]
     #+end_src
     - [[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html][online docs]]

5. 資料標準化
   #+begin_src python -r :results output :exports both
# 將資料標準化: 利用preprocessing模組裡的StandardScaler類別
from sklearn.preprocessing import StandardScaler
# 利用fit方法，對X_train中每個特徵值估平均數和標準差
# 然後對每個特徵值進行標準化(train和test都要做)
# 特徵工程：標準化
transfer = StandardScaler()
x_train = transfer.fit_transform(x_train)
x_test = transfer.fit_transform(x_test)
   #+end_src

6. 分類
   #+begin_src python -r :results output :exports both
from sklearn.neighbors import KNeighborsClassifier
# KNN 分類器
estimator = KNeighborsClassifier(n_neighbors=1)
estimator.fit(x_train, y_train)

# 模型評估
# 方法一：直接對比真實值和預測值
y_predict = estimator.predict(x_test)
print('y_predict：\n', y_predict)
print('直接對比真實值和預測值:\n', y_test == y_predict)

# 方法二：計算準確率
score = estimator.score(x_test, y_test)
print('準確率:\n', score)
  #+end_src

#+latex:\newpage

* 以少量資料集實做 CNN

使用少量資料訓練影像分類在實務的電腦視覺應用上十分常見，此處所謂少量樣本從幾百到幾萬張都算在內。此處以 4000 張為例(2000 cats v.s. 2000 dogs)，過程中使用 2000 張來訓練、1000 張用來驗證、1000 張用來測試。接下來導入以下技術來克服 overfitting:

- 資料擴增法(data augmentation):這是常用於減輕電腦視覺 overfitting 的強大技術，可以改善神經網路的成效，提升到 82%的準確率。
- 預先訓練神經網路的特徵萃取法(feature extraction with a pretrained network):應用於少量資料集的基本技術，可使神經網路成效達到 90%~96%的準確度。
- 微調預先訓練神經網路法(fine-tuning a pretrained network):也是常用於深度學習少量資料集的技術，將使神經網路準確率提升到 97%。

** 深度學習與少量資料的相關性

深度學習的基本特色是在它能自行在訓練資料中找到有趣的特徵，而不需要人為介入，但這只有在具備大量訓練樣本時才成立，特別是對於像圖片這類高維度(high-dimensional)的輸入樣本。所以也有人說深度學習一定要有大量資料才能進行。

然而樣本數與神經網路的大小與深度息息相關。只用幾十個樣本不可能訓練出可以解決複雜問題的卷積神經網路；相反的，如果只是要用來解決簡單任務，而且已經做好了 well-regularized 的小 model，那麼幾百個樣本或許就足夠了。因為卷積神經網路可以學習局部 pattern 且具平移不變性，所以在感知問題上具有高度的資料效率性。

此外，本質上，深度學習 model 是可高度再利用的。例如，使用大規模資料集訓練的影像 model 或語音轉文字的 model，只要進行小小的更改，便可以重新用於其他不同問題上。以電腦視覺的應用而言，許多預先訓練好的 model(通常是使用 Image-Net 資料集進行訓練)都是可公開下載的，以這些預先訓練好的 model 為基礎，再加以少量資料的訓練，就能產出更強大的 model。

** 實作

*** 下載資料

2013 年的 Kaggle 貓狗辨識大賽，最佳 model 即是使用 CNN，當時準確率達 95%，2013 年後的準確率已提高至 98%。本案例之資料來源：[[https://www.kaggle.com/c/dogs-vs-cats/data]]，由於原始圖片尺寸未做修改，大小各異，故需先額外處理，複製圖片到訓練、驗證和測試目錄的程式碼如下：
#+BEGIN_SRC python -r -n :results output :exports both
  import os, shutil

  # 解壓縮資料夾所在的目錄路徑
  original_dataset_dir = r'/Volumes/Vanessa/dogs-vs-cats/train/train'
  # 用來儲存少量資料集的目錄位置
  base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small'
  if not os.path.isdir(base_dir): os.mkdir(base_dir)  # 如果目錄不存在, 才建立目錄

  # 分拆成訓練、驗證與測試目錄位置
  train_dir = os.path.join(base_dir, 'train')
  if not os.path.isdir(train_dir): os.mkdir(train_dir)

  validation_dir = os.path.join(base_dir, 'validation')
  if not os.path.isdir(validation_dir): os.mkdir(validation_dir)

  test_dir = os.path.join(base_dir, 'test')
  if not os.path.isdir(test_dir): os.mkdir(test_dir)


  train_cats_dir = os.path.join(train_dir, 'cats')
  if not os.path.isdir(train_cats_dir):
      os.mkdir(train_cats_dir) # 用來訓練貓圖片的目錄位置

  train_dogs_dir = os.path.join(train_dir, 'dogs')
  if not os.path.isdir(train_dogs_dir):
      os.mkdir(train_dogs_dir) # 用來訓練狗圖片的目錄位置

  validation_cats_dir = os.path.join(validation_dir, 'cats')
  if not os.path.isdir(validation_cats_dir):
      os.mkdir(validation_cats_dir) # 用來驗證貓圖片的目錄位置

  validation_dogs_dir = os.path.join(validation_dir, 'dogs')
  if not os.path.isdir(validation_dogs_dir):
      os.mkdir(validation_dogs_dir) # 用來驗證狗圖片的目錄位置

  test_cats_dir = os.path.join(test_dir, 'cats')
  if not os.path.isdir(test_cats_dir):
      os.mkdir(test_cats_dir) # 用來測試貓圖片的目錄位置

  test_dogs_dir = os.path.join(test_dir, 'dogs')
  if not os.path.isdir(test_dogs_dir):
      os.mkdir(test_dogs_dir) # 用來測試狗圖片的目錄位置

  # 複製前面 1000 張貓圖片到 train_cats_dir 訓練目錄
  fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]
  for fname in fnames:
      src = os.path.join(original_dataset_dir, fname)
      dst = os.path.join(train_cats_dir, fname)
      shutil.copyfile(src, dst)

  # 複製下 500 張貓圖片到 validation_cats_dir 驗證目錄
  fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]
  for fname in fnames:
      src = os.path.join(original_dataset_dir, fname)
      dst = os.path.join(validation_cats_dir, fname)
      shutil.copyfile(src, dst)

  # 複製下 500 張貓圖片到 test_cats_dir 測試目錄
  fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]
  for fname in fnames:
      src = os.path.join(original_dataset_dir, fname)
      dst = os.path.join(test_cats_dir, fname)
      shutil.copyfile(src, dst)

  # 複製前面 1000 張狗圖片到 train_dogs_dir 訓練目錄
  fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]
  for fname in fnames:
      src = os.path.join(original_dataset_dir, fname)
      dst = os.path.join(train_dogs_dir, fname)
      shutil.copyfile(src, dst)

  # 複製下 500 張狗圖片到 validation_dogs_dir 驗證目錄
  fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]
  for fname in fnames:
      src = os.path.join(original_dataset_dir, fname)
      dst = os.path.join(validation_dogs_dir, fname)
      shutil.copyfile(src, dst)

  # 複製下 500 張狗圖片到 test_dogs_dir 測試目錄
  fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]
  for fname in fnames:
      src = os.path.join(original_dataset_dir, fname)
      dst = os.path.join(test_dogs_dir, fname)
      shutil.copyfile(src, dst)

      print('複製完成')
#+END_SRC

#+RESULTS:
: 複製完成

上述程式會產生三組資料集：訓練集狗貓各 1000、驗證集各 500、測試集各 500，可再以下列程式驗證：

#+BEGIN_SRC python -r -n :results output :exports both
  import os, shutil

  # 解壓縮資料夾所在的目錄路徑
  original_dataset_dir = r'/Volumes/Vanessa/dogs-vs-cats/train/train'
  # 用來儲存少量資料集的目錄位置
  base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small'

  # 分拆成訓練、驗證與測試目錄位置
  train_dir = os.path.join(base_dir, 'train')
  validation_dir = os.path.join(base_dir, 'validation')
  test_dir = os.path.join(base_dir, 'test')
  train_cats_dir = os.path.join(train_dir, 'cats')
  train_dogs_dir = os.path.join(train_dir, 'dogs')
  validation_cats_dir = os.path.join(validation_dir, 'cats')
  validation_dogs_dir = os.path.join(validation_dir, 'dogs')
  test_cats_dir = os.path.join(test_dir, 'cats')
  test_dogs_dir = os.path.join(test_dir, 'dogs')

  print('訓練用的貓照片張數:', len(os.listdir(train_cats_dir)))
  print('訓練用的狗照片張數:', len(os.listdir(train_dogs_dir)))
  print('驗證用的貓照片張數:', len(os.listdir(validation_cats_dir)))
  print('驗證用的狗照片張數:', len(os.listdir(validation_dogs_dir)))
  print('測試用的貓照片張數:', len(os.listdir(test_cats_dir)))
  print('測試用的狗照片張數:', len(os.listdir(test_dogs_dir)))
#+END_SRC

#+RESULTS:
: 訓練用的貓照片張數: 1000
: 訓練用的狗照片張數: 1000
: 驗證用的貓照片張數: 500
: 驗證用的狗照片張數: 500
: 測試用的貓照片張數: 500
: 測試用的狗照片張數: 500

*** 建立神經網路

#+BEGIN_SRC python -r -n :results output :exports both
  from keras import layers
  from keras import models

  model = models.Sequential()
  model.add(layers.Conv2D(32, (3, 3), activation='relu',
                          input_shape=(150, 150, 3)))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(64, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(128, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(128, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Flatten())
  model.add(layers.Dense(512, activation='relu'))
  model.add(layers.Dense(1, activation='sigmoid'))
  model.summary()  # 查看模型摘要

#+END_SRC

#+RESULTS:
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
#+end_example

在編譯時，以 RMSProp 優化器，由於使用 sigmoid 單元結束神經網路，所以配合使用 binary\under{}crossentropy 二元交叉熵作為損失基準。

#+NAME: 配置 model 以進行訓練
#+BEGIN_SRC python -r -n :results output :exports both
  from keras import optimizers

  model.compile(loss='binary_crossentropy',
                optimizer=optimizers.RMSprop(lr=1e-4),
                metrics=['acc'])
#+END_SRC

*** 資料預處理

資料在送入神經網路前應先將 JPEG 檔案格式化成適當的浮點數張量，其步驟如下：
1. 讀取影像檔
1. 將 JPEG 內容解碼為 RGB 的像素
1. 將 RGB 像素轉為浮點數張量
1. 將像素值(0~255)壓縮到[0,1]區間

上述過程可以用 Keras 的 keras.preprocessing.image 模組來處理，它包含 ImageDataGenerator 類別，過程如下：

#+NAME: 使用 ImageDataGenerator 產生器從目錄中讀取影像
#+BEGIN_SRC python -r -n :results output :exports both
  import os, shutil

  # 解壓縮資料夾所在的目錄路徑
  original_dataset_dir = r'/Volumes/Vanessa/dogs-vs-cats/train/train'
  # 用來儲存少量資料集的目錄位置
  base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small'

  # 分拆成訓練、驗證與測試目錄位置
  train_dir = os.path.join(base_dir, 'train')
  validation_dir = os.path.join(base_dir, 'validation')

  from keras.preprocessing.image import ImageDataGenerator

  train_datagen = ImageDataGenerator(rescale=1./255) #設定訓練、測試資料的 Python 產生器，並將圖片像素值依 1/255 比例重新壓縮到 [0, 1] (ref:ImageDataGenerator)
  test_datagen = ImageDataGenerator(rescale=1./255)

  train_generator = train_datagen.flow_from_directory(
      train_dir,              # 目標目錄
      target_size=(150, 150),  # 調整所有影像大小成 150x150
      batch_size=20,
      class_mode='binary')    # 因為使用二元交叉熵 binary_crossentropy 作為損失值，所以需要二位元標籤


  validation_generator = test_datagen.flow_from_directory(
      validation_dir,
      target_size=(150, 150),
      batch_size=20,
      class_mode='binary')

  # 觀察產生器的結果
  for data_batch, labels_batch in train_generator:
      print('data batch shape:', data_batch.shape)
      print('labels batch shape:', labels_batch.shape)
      break      (ref:DataGeneratorBreak)

#+END_SRC

#+RESULTS: 使用 ImageDataGenerator 產生器從目錄中讀取影像
: Found 2000 images belonging to 2 classes.
: Found 1000 images belonging to 2 classes.
: data batch shape: (20, 150, 150, 3)
: labels batch shape: (20,)

結果顯示每批次產生出的資料為 20 張 150\times150 的 RGB 影像以及 20 個 label(即答案)，需留意的是此處的 generator 會無 止盡的生成批次量樣本，也就會不停的持續循環產生影像到目標目錄中，所以要放 break。而上述程式中的 ImageDataGenerator(第[[(ImageDataGenerator)]]行)是一種產生器(Generator)，在 Python 中是一個持續迭代運作的物件，是一個可以與 for...in 一起使用的物件，產生器是使用 yield 建構的。典型的產生器範例如下：

#+NAME: python generator的DEMO
#+BEGIN_SRC python -r -n :results output
  def generator():
      i = 0
      while True:
          i += 1
          yield i

  for item in generator():
      print(item)
      if item > 3:
          break
#+END_SRC

#+RESULTS:
: 1
: 2
: 3
: 4

建構好 model、整理完資料，接下來就可以調整 model 來搭配產生器所產生的資料，我們可以應用 model 的 fit\under{}generator 方法，這個方法的第 1 個參數即是一個 Python 的產生器，然而由於資料是無止盡地產生，所以在宣告訓練時期之前，Keras model 需要知道從產生器抽取多少樣本，這就是 steps\under{}per\under{}epoch 參數的功能，它指定了從產生器取得的批次量，也就是說，model 在運行了 steps\under{}per\under{}epoch 次的梯度下降步驟後，訓練過程將進入下一個訓練週期(epochs)。在以下的例子中，每個批次量包含 20 個樣本，而目標樣本有 2000 個，所以就需要有 100 個批次量。

#+NAME: 調整 model 以使用批次量產生器
#+BEGIN_SRC python -r -n :results output :exports both
  history = model.fit_generator(
      train_generator,   #設定產生器
      steps_per_epoch=100,   #設定從產生器抽取100個批次量
      epochs=30,
      validation_data=validation_generator,
      validation_steps=50)

  model.save('cats_and_dogs_small_i.h5')
#+END_SRC

使用上述 fit\under{}generator 時，還可以傳遞 validation\under{}data 參數，此參數可以接收一個資料產生器，也可以接收 Numpy 陣列，如果接收的資料來自產生器，則還要指定 validation\under{}steps 參數，告訴程式要從產生器中抽取多少次批量進行評估。在完成訓練後把 model 存起來，並繪製訓練週期與驗證週期的 model 損失值與準確度。

*** 完整程式

#+NAME: 完整的初步cats and dogs model
#+BEGIN_SRC python -r -n :results output :exports both
  import os, shutil

  # 解壓縮資料夾所在的目錄路徑
  original_dataset_dir = r'/Volumes/Vanessa/dogs-vs-cats/train/train'
  # 用來儲存少量資料集的目錄位置
  base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small'

  # 分拆成訓練、驗證與測試目錄位置
  train_dir = os.path.join(base_dir, 'train')
  validation_dir = os.path.join(base_dir, 'validation')
  test_dir = os.path.join(base_dir, 'test')
  train_cats_dir = os.path.join(train_dir, 'cats')
  train_dogs_dir = os.path.join(train_dir, 'dogs')
  validation_cats_dir = os.path.join(validation_dir, 'cats')
  validation_dogs_dir = os.path.join(validation_dir, 'dogs')
  test_cats_dir = os.path.join(test_dir, 'cats')
  test_dogs_dir = os.path.join(test_dir, 'dogs')

  # 建立模組
  from keras import layers
  from keras import models

  model = models.Sequential()
  model.add(layers.Conv2D(32, (3, 3), activation='relu',
                          input_shape=(150, 150, 3)))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(64, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(128, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(128, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Flatten())
  model.add(layers.Dense(512, activation='relu'))
  model.add(layers.Dense(1, activation='sigmoid'))
  model.summary()  # 查看模型摘要

  # 配置 model 以進行訓練
  from keras import optimizers

  model.compile(loss='binary_crossentropy',
                optimizer=optimizers.RMSprop(lr=1e-4),
                metrics=['acc'])

  # 使用 ImageDataGenerator 產生器從目錄中讀取影像
  from keras.preprocessing.image import ImageDataGenerator

  train_datagen = ImageDataGenerator(rescale=1./255) #設定訓練、測試資料的 Python 產生器，並將圖片像素值依 1/255 比例重新壓縮到 [0, 1] (ref:ImageDataGenerator)
  test_datagen = ImageDataGenerator(rescale=1./255)

  train_generator = train_datagen.flow_from_directory(
      train_dir,              # 目標目錄
      target_size=(150, 150),  # 調整所有影像大小成 150x150
      batch_size=20,
      class_mode='binary')    # 因為使用二元交叉熵 binary_crossentropy 作為損失值，所以需要二位元標籤


  validation_generator = test_datagen.flow_from_directory(
      validation_dir,
      target_size=(150, 150),
      batch_size=20,
      class_mode='binary')

  # 訓練model
  from keras import models
  from keras import layers

  history = model.fit_generator(
      train_generator,   #設定產生器
      steps_per_epoch=100,   #設定從產生器抽取100個批次量
      epochs=30, verbose=1, #verbose=1, 不顯示訓練過程
      validation_data=validation_generator,
      validation_steps=50)

  model.save('cats_and_dogs_small_i.h5')

  # 繪製model的損失率與精確率
  import matplotlib.pyplot as plt

  acc = history.history['acc']
  val_acc = history.history['val_acc']
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(acc) + 1)

  plt.clf()
  plt.plot(epochs, acc, 'bo', label='Training acc')
  plt.plot(epochs, val_acc, 'b', label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.legend()
  plt.plot()
  plt.savefig("cats-and-dogs-accuracy-v1.png")
  plt.figure()

  plt.clf()
  plt.plot(epochs, loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()
  plt.plot()
  plt.savefig("cats-and-dogs-loss-v1.png")

#+END_SRC

#+RESULTS: 完整的初步cats and dogs model
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
#+end_example

#+CAPTION: Cats and Dogs Accuracy V1
#+LABEL:fig:Cat-Dog-Acc-V1
#+name: fig:Cat-Dog-Acc-V1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:images/cats-and-dogs-accuracy-v1.png]]

#+CAPTION: Cats and Dogs Loss V1
#+LABEL:fig:Cat-Dog-Loss-V1
#+name: fig:Cat-Dog-Loss-V1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:images/cats-and-dogs-loss-v1.png]]

由圖[[fig:Cat-Dog-Acc-V1]]看出訓練準確度成線性成長直到逼近 100%，但驗證準確度則在第三個訓練週期後就停留在 70%；訓練損失分數也呈線性下降，但驗證損失分數則約在第 12 週期後達到最低點。這些都是明顯的 overfitting 訊號。

由於訓練樣本數(2000)相對較少，overfitting 將成為訓練 model 的首要顧慮因素，幾種緩解 overfitting 的技術有：
- dropout
- 權重調整(L2 regularization)
- 資料擴增法(data augmentation)

** 改善#1: 使用資料擴增法(data augmentation)

Overfitting 的部份成因是由於樣本太少導致無法訓練出具備普適性、可套用到新資料的 model，想像一下如果有無限量的資料，則 model 將會因應用手邊資料的各種可能面向，也就不致於 overfitting。資料擴增就是由現有訓練樣本生成更多訓練資料的方法，主要是透過隨機變換原始資料，以產生相似的影像，進而增加訓練樣本數。最終目標是在訓練時，model 不會看到兩次完全相同的影像。

在 Keras 中，我們可以藉由設定 ImageDataGenerator，在讀取影像時執行隨機變換(random transformation)來達到資料擴增，至於變換的方向則可以在 ImageDataGenerator 的參數中進一步指定。以下例來看：

#+BEGIN_SRC python -r -n :results output :exports both
  datagen = ImageDataGenerator(
      rotation_range=40,       #旋轉角度值(0~180)
      width_shift_range=0.2,   #水平隨機平移(圖片寬度之百分比)
      height_shift_range=0.2,  #垂直隨機平移(圖片高度之百分比)
      shear_range=0.2,         #隨機傾斜(順時鐘傾斜角度)
      zoom_range=0.2,          #隨機縮放(縮放百分比)
      horizontal_flip=True,    #隨機水平翻轉(影像非左右對稱才有效)
      fill_mode='nearest')     #新建影像填補像素方法
#+END_SRC

上述程式之 fill_\under{}mode 共提供四種像素填補方法：
- constant: 依照輸入的 cval(浮點數或整數)將影像邊界之外都以該值填補，例如 cval=k，則影像填補為 kkkkkkkk|abcd|kkkkkkkk
- nearest: 以最接近的像素值填補，如：aaaaaaaa|abcd|dddddddd
- reflect: 以影像重複填補(影像以一正一反方向)，如 abcddcba|abcd|dcbaabcd
- wrap: 以影像重複填補，如：abcdabcd|abcd|abcdabcd

以下為實際運作的示範：

#+BEGIN_SRC python -r -n :results output :exports both
  import matplotlib
  import platform
  if platform.system() == 'Darwin':
      matplotlib.use('MacOSX')
  else:
      matplotlib.use('TkAgg')

  from keras.preprocessing.image import ImageDataGenerator

  datagen = ImageDataGenerator(
      rotation_range=40,       #旋轉角度值(0~180)
      width_shift_range=0.2,   #水平隨機平移(圖片寬度之百分比)
      height_shift_range=0.2,  #垂直隨機平移(圖片高度之百分比)
      shear_range=0.2,         #隨機傾斜(順時鐘傾斜角度)
      zoom_range=0.2,          #隨機縮放(縮放百分比)
      horizontal_flip=True,    #隨機水平翻轉(影像非左右對稱才有效)
      fill_mode='nearest')     #新建影像填補像素方法

  import os, shutil

  # 解壓縮資料夾所在的目錄路徑
  original_dataset_dir = r'/Volumes/Vanessa/dogs-vs-cats/train/train'
  base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small'

  train_dir = os.path.join(base_dir, 'train')
  train_cats_dir = os.path.join(train_dir, 'cats')

  from keras.preprocessing import image
  import numpy as np

  fnames = [os.path.join(train_cats_dir, fname) for
      fname in os.listdir(train_cats_dir)]

  img_path = fnames[3] #選一張影像來擴充
  print(img_path)

  #讀取影像、調整大小
  img = image.load_img(img_path, target_size=(150, 150))
  #將其調整為shape=(150, 150, 3)
  x = image.img_to_array(img)
  #調整shape為(1, 150, 150, 3)
  x = x.reshape((1, ) + x.shape)
  print(x.shape)

  i = 0
  # 繪製model的損失率與精確率
  import matplotlib.pyplot as plt

  for batch in datagen.flow(x, batch_size=1):
      plt.figure(i)
      #imgplot = plt.imshow(image.array_to_img(batch[0]))
      plt.imshow(image.array_to_img(batch[0]))
      #plt.clf()
      plt.plot()
      plt.savefig("CatsAugmentation"+str(i)+".png")
      i += 1
      if i % 4 == 0:
          break
  #plt.show()
  #plt.savefig("CatsAugmentation.png")
#+END_SRC

#+RESULTS:
: /Volumes/Vanessa/dogs-vs-cats/small/train/cats/cat.100.jpg
: (1, 150, 150, 3)

#+CAPTION: Cats image augmentation
#+LABEL:fig:CatAugmentation
#+name: fig:CatAugmentation
#+ATTR_LATEX: :width 200
#+ATTR_ORG: :width 200
[[file:[[file:images/CatsAugmentation0.png]]images/CatsAugmentation1.png]]
[[file:[[file:images/CatsAugmentation2.png]]images/CatsAugmentation3.png]]

雖然資料擴增能擴充來自少量的原始圖片，但終究無法自行產生資訊，只能重新混合現有資訊，影像間仍是高度相關，仍不足以完全擺脫 overfitting 問題，所以進一步在密集連接的分類器前，在 model 中增加 Dropout 層(Fatten 層後)。


#+BEGIN_SRC python -r -n :results output :exports both
  import matplotlib
  import platform
  if platform.system() == 'Darwin':
      matplotlib.use('MacOSX')
  else:
      matplotlib.use('TkAgg')

  from keras.preprocessing.image import ImageDataGenerator

  import os, shutil

  # 解壓縮資料夾所在的目錄路徑
  original_dataset_dir = r'/Volumes/Vanessa/dogs-vs-cats/train/train'
  # 用來儲存少量資料集的目錄位置
  base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small'

  # 分拆成訓練、驗證與測試目錄位置
  train_dir = os.path.join(base_dir, 'train')
  validation_dir = os.path.join(base_dir, 'validation')
  test_dir = os.path.join(base_dir, 'test')
  train_cats_dir = os.path.join(train_dir, 'cats')
  train_dogs_dir = os.path.join(train_dir, 'dogs')
  validation_cats_dir = os.path.join(validation_dir, 'cats')
  validation_dogs_dir = os.path.join(validation_dir, 'dogs')
  test_cats_dir = os.path.join(test_dir, 'cats')
  test_dogs_dir = os.path.join(test_dir, 'dogs')

  # 建立模組
  from keras import layers
  from keras import models
  from keras import regularizers
  model = models.Sequential()
  model.add(layers.Conv2D(32, (3, 3), activation='relu',
                          input_shape=(150, 150, 3)))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(64, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(128, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Conv2D(128, (3, 3), activation='relu'))
  model.add(layers.MaxPooling2D((2, 2)))
  model.add(layers.Flatten())
  model.add(layers.Dropout(0.5))
  model.add(layers.Dense(512, activation='relu'))
  model.add(layers.Dense(1, activation='sigmoid'))
  model.summary()  # 查看模型摘要

  # 配置 model 以進行訓練
  from keras import optimizers

  model.compile(loss='binary_crossentropy',
                optimizer=optimizers.RMSprop(lr=1e-4),
                metrics=['acc'])

  #資料擴增
  train_datagen = ImageDataGenerator(
      rescale=1./255,
      rotation_range=40,
      width_shift_range=0.2,
      height_shift_range=0.2,
      shear_range=0.2,
      zoom_range=0.2,
      horizontal_flip=True, )

  test_datagen = ImageDataGenerator(rescale=1./255) # 請注意！驗證資料不應該擴充!!!

  train_generator = train_datagen.flow_from_directory(
    train_dir,    # 目標目錄
    target_size=(150, 150), # 所有圖像大小調整成 150×150
    batch_size=32,
    class_mode='binary') # 因為使用二元交叉熵 binary_crossentropy 作為損失，所以需要二元標籤


  validation_generator = test_datagen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=32,
    class_mode='binary')

  # 訓練
  history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=100,
      verbose=1,
    validation_data=validation_generator,
    validation_steps=50)

  model.save('cats_and_dogs_small_data_augmentation.h5')

  # 繪製model的損失率與精確率
  import matplotlib.pyplot as plt

  acc = history.history['acc']
  val_acc = history.history['val_acc']
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(acc) + 1)
  plt.clf()
  plt.plot(epochs, acc, 'bo', label='Training acc')
  plt.plot(epochs, val_acc, 'b', label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.legend()
  plt.plot()
  plt.savefig("CatsDogsDataAugmentation-acc.png")
  plt.figure()

  plt.clf()
  plt.plot(epochs, loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()
  plt.plot()
  plt.savefig("CatsDogsDataAugmentation-loss.png")

#+END_SRC

#+RESULTS:
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
100/100 [==============================] - 101s 1s/step - loss: 0.3521 - acc: 0.8438 - val_loss: 0.4726 - val_acc: 0.8061
#+end_example

#+CAPTION: Cats and Dogs Data Augmentation - Accuracy
#+LABEL:fig:Cat-Dog-Data-Augmentation-Acc
#+name: fig:Cat-Dog-Data-Augmentation-Acc
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/CatsDogsDataAugmentation-acc.png]]

#+CAPTION: Cats and Dogs Data Augmentation - Loss
#+LABEL:fig:Cat-Dog-Data-Augmentation-Loss
#+name: fig:Cat-Dog-Data-Augmentation-Loss
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/CatsDogsDataAugmentation-loss.png]]


由圖[[fig:Cat-Dog-Data-Augmentation-Acc]]和[[fig:Cat-Dog-Data-Augmentation-Loss]]可以發現，在加入了 data augmentation 和 dropout 後，訓練曲線與驗證曲線漸趨一致，不再 overfitting，model 的準確度也達到 84%。但值的一題的是，同樣的資料集與演算法，在 Google colab 上以 GPU 執行的結果(下圖)與在本機執行(上圖)時並不相同。

#+CAPTION: Cats and Dogs Data Augmentation on Google colab - Accuracy
#+LABEL:fig:Cat-Dog-Data-Augmentation-Acc-colab
#+name: fig:Cat-Dog-Data-Augmentation-Acc-colab
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/Cat-Dog-Data-Augmentation-Acc-colab.png]]

#+Caption: Cats and Dogs Data Augmentation on Google colab - Loss
#+LABEL:fig:Cat-Dog-Data-Augmentation-Loss-colab
#+name: fig:Cat-Dog-Data-Augmentation-Loss-colab
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/Cat-Dog-Data-Augmentation-loss-colab.png]]

在透過進一步 regularization 技術的使用，以及調整神經網路參數(如每個卷積層的過濾器數量、神經網路中的層數)，我們就能獲得更高的準確度(86%或 87%)，但在資料不及的情況下(如本例)，我們仍很難進一步提升準確度，此時，就要使用預先訓練 model。

** 改善 2: 使用 pretrained network

Pretrained network，以簡單的話來說，就是「站在巨人的肩膀」[fn:8]，所謂「巨人」，就是別人已經用 ImageNet 訓練好的模型，例如 Google 的 Inception Model、Microsoft 的 Resnet Model 等等，把它當作 Pre-trained Model，幫助我們提取出照片的特徵(feature)。順帶一提，所謂的 Transfer Learning 就是把 Pre-trained Model 最後一層拔掉 (註：最後一層是用來分類的)，加入新的一層，然後用新資料訓練新層的參數。

能夠用來被當成 pretrained netwrok 的 model 通常是擁有大量資料集的大規模圖片分類模型，如果這個原始資料集足夠大量且具通用性，那麼 pretrained network 學習的空間層次特徵(spartial hierarchy features)就足以充當視覺世界的通用 model，其特徵對於許多不同的電腦視覺問題都同樣有效，即便是要辨識與原始任務完全不同的類別也能通用。

例如，以 ImageNet 先訓練出一個神經網路(其辨識項目為日常生活用品)，然後重新訓練這個已訓練完成的神經網路，去識別和原始樣本天差地別的家具產品等。和許多淺層的神經網路相較，深度學習的關鍵優勢在於學習到的特徵可移植到不同問題上。

以下由 Karen Simonyan 和 Andrew Zisserman 於 2014 年開發的 VGG16 架構。使用 pretrain network 有兩種方式：特徵萃取(feature extraction)和徵調(fine-tuning)。

*** 特徵萃取

Feature extraction 是使用 pretrained network 學習到的表示法，以這些表示法從新樣本中萃取有趣的特徵，然後將這些特徵輸入到從頭訓練的新分類器中進行處理。用於影像分類的 CNN 分為以下兩部份：以一系列的卷積層和池化層開始，以密集連接的分類器結束。第一部分稱為 model 的 convolutional base (卷積基底)，在 CNN 的情況下，特徵萃取以一個 pretrained network 做為 convolutional base，透過 convolutional base 處理新資料，

#+CAPTION: 套用同樣的 convolutional base，交換分類器
#+LABEL:fig:ConvolutionalBaseChangeDense
#+name: fig:ConvolutionalBaseChangeDense
#+ATTR_LATEX: :width 360
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 360
[[file:images/img-191126103936.jpg]]

為何不連分類器也預先訓練？原因是 CNN 的特徵圖是來自影像上通用 pattern 的概念，因此無論面臨何種電腦視覺問題，都能通用；而分類器學習到的表示法可能只適用於 model 所訓練的類別，僅關於整個影像中該類別相關的機率。此外，卷積特徵圖仍會描述物件出現的位置，但密集層並沒有空間的概念，密集層學習到的表示法不再包含物件在輸入影像中位罝的任何訊息，所以只要是和物件出現位置相關的問題，密集層產生的特徵絕大多數是沒有用的。

特定卷積層所萃取出來的表示法，其普適程度取於該層的深度，model 中較早出現的層會萃取局部、高度通用的特徵圖（例如可視邊緣、顏色或紋理），而較深入的層則會萃取更抽象的概念（如貓耳朵、狗眼），如果新的資料集與訓練原始 model 的資料集有很大的差別，最好使用 model 的前幾層來進行特徵萃取，而不是使用整個 convolutional base。以下以 ImageNet 訓練的 VGG16 所產生的 convolutional base 來實作，類似 pretrained 的影像分類 model 還有 Xception、Inception V3、ResNet50、VGG19、MobileNet，均已收錄於 keras.applications。

**** 1. 初始化 model

要使用這個 pretrained model，還需要傳三個參數給 VGG16 建構式：
- weights: 用於初始化 model 的權重檢查點
- include\under{}top: 指在神經網路頂部有沒有包含密集連接的分類器。預設情況下，密集連接分類器對應於 ImageNet 的 1000 個類別。然而，我們實際想分類的可能沒這麼多層，所以這裡不一定要包含預設分類器。
- input\under{}shape: qpaqamo 供給神經網路的影像張量 shape。這個參數為 optional，如果不傳，則神經網路能處理任何 shape 的輸入張量。

#+NAME: VGG16 pretrained model架構細節
#+BEGIN_SRC python -r -n :results output :exports both
  from keras.applications import VGG16

  conv_base = VGG16(weights='imagenet',
                    include_top=False,
                    input_shape=(150, 150, 3))
  conv_base.summary()
#+END_SRC

#+RESULTS: VGG16 pretrained model架構細節
#+begin_example
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
#+end_example

由上述輸出觀察，最終特徵圖的 shape 為(4, 4, 512)，這算是神經網路的 top 層特徵，這個預訓練的 model 共有 13 層 Conv2D 層，最後要再接上密集連接分類器。做法有二：


1. 在資料集上執行 convolutional base，將輸出記錄到硬碟上的 Numpy 陣列，然後再輸入到獨立的密集分類層。這種解決方案只需要為每個輪入影像執行一次 convolutional base，而 convolutional base 是處理過程中成本最高的部份，所以這種做法速度快成本低。但也因如此，這種做法不允許使用資料擴增法。
1. 在頂部(最後端)增加 Dnese 層來擴展 model (conv\under{}base)，並從輸入資料開始，從頭到尾執行整個處理過程。這種方式允許資料擴增技術，因為每次輸入影像在執行 convolutional base 時都會在 model 處理到。但這種方式的成本較高。

**** 2. 快速特徵萃取

先執行 ImageDataGenerator，將影像轉換為 Numpy 陣列及其 label 向量，然後呼叫 conv\under{}base model 的 predict 方法從這些影像中萃取特徵。


#+BEGIN_SRC python -r -n :results output :exports both
  from keras.applications import VGG16

  conv_base = VGG16(weights='imagenet',
                    include_top=False,
                    input_shape=(150, 150, 3))

  import os
  import numpy as np
  from keras.preprocessing.image import ImageDataGenerator

  base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small'
  train_dir = os.path.join(base_dir, 'train')
  validation_dir = os.path.join(base_dir, 'validation')
  test_dir = os.path.join(base_dir, 'test')

  datagen = ImageDataGenerator(rescale=1./255)
  batch_size = 20

  def extract_features(directory, sample_count):
      features = np.zeros(shape=(sample_count, 4, 4, 512))
      labels = np.zeros(shape=(sample_count))
      generator = datagen.flow_from_directory(directory,
                                              target_size=(150, 150),
                                              batch_size=batch_size,
                                              class_mode='binary')
      i = 0
      for inputs_batch, labels_batch in generator:
          features_batch = conv_base.predict(inputs_batch)
          features[i * batch_size : (i + 1) * batch_size] = features_batch
          labels[i * batch_size : (i + 1) * batch_size] = labels_batch
          i += 1
          print(i, end=' ') # 由於萃取需要較長的時間，我們印出 i 來檢視進度
          if i * batch_size >= sample_count:
              break
      return features, labels

  train_features, train_labels = extract_features(train_dir, 2000)
  validation_features, validation_labels = extract_features(validation_dir, 1000)
  test_features, test_labels = extract_features(test_dir, 1000)
#+END_SRC

#+RESULTS:
: Found 2000 images belonging to 2 classes.
: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 Found 1000 images belonging to 2 classes.
: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Found 1000 images belonging to 2 classes.
: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50

**** 3. 展平資料

由於目前的萃取特徵 shape = (樣本數, 4, 4, 512)，為了要提供給密集層分類器，必須將資料展平為(樣本數, 8192)。

#+BEGIN_SRC python -r -n :results output :exports both
  train_features = np.reshape(train_features, (2000, 4 * 4 * 512))
  validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))
  test_features = np.reshape(test_features, (1000, 4 * 4 * 512))
#+END_SRC

**** 4. 訓練

接下來就可以建立我們的密集分類層（使用 dropout 和 regularization)在剛剛萃取的資料和標籤上進行訓練。因為只有兩個密集層，所以訓練的速度會很快。

#+BEGIN_SRC python -r -n :results output :exports both
  from keras import models
  from keras import layers
  from keras import optimizers

  model = models.Sequential()
  model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))
  model.add(layers.Dropout(0.5))  # 丟棄法
  model.add(layers.Dense(1, activation='sigmoid'))

  model.compile(optimizer=optimizers.RMSprop(lr=2e-5),
                loss='binary_crossentropy',
                metrics=['acc'])

  history = model.fit(train_features,
                      train_labels,epochs=30,
                      batch_size=20,
                      validation_data=(validation_features, validation_labels))
#+END_SRC

**** 5. 繪圖

#+BEGIN_SRC python -r -n :results output :exports both
  import matplotlib.pyplot as plt

  acc = history.history['acc']
  val_acc = history.history['val_acc']
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(acc) + 1)

  plt.plot(epochs, acc, 'bo', label='Training acc')
  plt.plot(epochs, val_acc, 'b', label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.legend()

  plt.figure()

  plt.plot(epochs, loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()

  plt.show()
#+END_SRC

**** 6. 實際執行結果

#+BEGIN_SRC python -r -n :results output :exports both
  #####
  from keras.applications import VGG16

  conv_base = VGG16(weights='imagenet',
                    include_top=False,
                    input_shape=(150, 150, 3))

  import os
  import numpy as np
  from keras.preprocessing.image import ImageDataGenerator

  base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small'
  train_dir = os.path.join(base_dir, 'train')
  validation_dir = os.path.join(base_dir, 'validation')
  test_dir = os.path.join(base_dir, 'test')

  datagen = ImageDataGenerator(rescale=1./255)
  batch_size = 20

  def extract_features(directory, sample_count):
      features = np.zeros(shape=(sample_count, 4, 4, 512))
      labels = np.zeros(shape=(sample_count))
      generator = datagen.flow_from_directory(directory,
                                              target_size=(150, 150),
                                              batch_size=batch_size,
                                              class_mode='binary')
      i = 0
      for inputs_batch, labels_batch in generator:
          features_batch = conv_base.predict(inputs_batch)
          features[i * batch_size : (i + 1) * batch_size] = features_batch
          labels[i * batch_size : (i + 1) * batch_size] = labels_batch
          i += 1
          print(i, end=' ') # 由於萃取需要較長的時間，我們印出 i 來檢視進度
          if i * batch_size >= sample_count:
              break
      return features, labels

  train_features, train_labels = extract_features(train_dir, 2000)
  validation_features, validation_labels = extract_features(validation_dir, 1000)
  test_features, test_labels = extract_features(test_dir, 1000)

  #####
  train_features = np.reshape(train_features, (2000, 4 * 4 * 512))
  validation_features = np.reshape(validation_features, (1000, 4 * 4 * 512))
  test_features = np.reshape(test_features, (1000, 4 * 4 * 512))


  #####
  from keras import models
  from keras import layers
  from keras import optimizers

  model = models.Sequential()
  model.add(layers.Dense(256, activation='relu', input_dim=4 * 4 * 512))
  model.add(layers.Dropout(0.5))  # 丟棄法
  model.add(layers.Dense(1, activation='sigmoid'))

  model.compile(optimizer=optimizers.RMSprop(lr=2e-5),
                loss='binary_crossentropy',
                metrics=['acc'])

  history = model.fit(train_features,
                      train_labels,epochs=30,
                      batch_size=20,
                      validation_data=(validation_features, validation_labels))


  #####
  import matplotlib.pyplot as plt

  acc = history.history['acc']
  val_acc = history.history['val_acc']
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(acc) + 1)
  plt.clf()
  plt.plot(epochs, acc, 'bo', label='Training acc')
  plt.plot(epochs, val_acc, 'b', label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.legend()
  plt.plot()
  plt.savefig("Pretrained-VGG16-1-acc.png")
  plt.figure()

  plt.clf()
  plt.plot(epochs, loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()
  plt.plot()
  plt.savefig("Pretrained-VGG16-1-loss.png")

#+END_SRC

#+RESULTS:
#+begin_example
Found 2000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Train on 2000 samples, validate on 1000 samples
Epoch 30/30
1980/2000 [============================>.] - ETA: 0s - loss: 0.0909 - acc: 0.9722
2000/2000 [==============================] - 3s 2ms/step - loss: 0.0905 - acc: 0.9725 - val_loss: 0.2450 - val_acc: 0.9020
#+end_example

#+CAPTION: 簡單特徵萃取的訓練和驗證準確度
#+LABEL:fig:Pretrained-VGG16-1-Acc
#+name: fig:Pretrained-VGG16-1-Acc
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/Pretrained-VGG16-1-acc.png]]

#+CAPTION: 簡單特徵萃取的訓練和驗證損失
#+LABEL:fig:Pretrained-VGG16-1-loss
#+name: fig:Pretrained-VGG16-1-loss
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/Pretrained-VGG16-1-loss.png]]

圖中顯示可以達到 90%的驗證準確度，比較原來的 model 成效，雖然準確度有提高，但仍可看到 overfitting 的情況，即便 model 裡已套用了 dropout，也許是因為無法使用資料擴增法，對 overfitting 的防治仍然有限。

**** 7. 加入資料擴增的特徵萃取

將資料擴增加入特徵萃取的作法是擴展 conv\under{}base model 並從輸入資料開始，從頭到尾執行整個處理過程，這種做法的運算成本非常昂貴，只能在 GPU 上執行，在 CPU 上絕對難以處理。由於 model 的行為與 layer 類似，因此可以將 model(如 conv\uunder{}base)視為 layer，增加到 Sequential model 中，就如同增加神經網路的 layer 一樣。其作法如下：

#+NAME: 在 convolutional base 卷積基底上增加密集層分類器
#+BEGIN_SRC python -r -n :results output :exports both
  from keras import models
  from keras import layers
  from keras.applications import VGG16

  conv_base = VGG16(weights='imagenet',   # 卷積基底
                    include_top=False,
                    input_shape=(150, 150, 3))

  model = models.Sequential()
  model.add(conv_base)        # 將卷積基底視為層加入 Sequential 模型中
  model.add(layers.Flatten()) # 攤平
  model.add(layers.Dense(256, activation='relu'))
  model.add(layers.Dense(1, activation='sigmoid')) # 增加密集層分類器
  model.summary() # 查看模型摘要
#+END_SRC

#+RESULTS: 在 convolutional base 卷積基底上增加密集層分類器
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
vgg16 (Model)                (None, 4, 4, 512)         14714688
_________________________________________________________________
flatten_1 (Flatten)          (None, 8192)              0
_________________________________________________________________
dense_1 (Dense)              (None, 256)               2097408
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 257
=================================================================
Total params: 16,812,353
Trainable params: 16,812,353
Non-trainable params: 0
_________________________________________________________________
#+end_example

如上圖，VGG16 的 convolutional base 有 14714688 個參數，在頂部(後端)增加的分類器有 200 多萬個參數。在加入資料擴增之前，凍結 convolutional base 是非常重要的，凍結(freeze)表示在訓練期間禁止更新權重，如果不這樣做，則 convolutional base 先前學習到的表示法就會在訓練期間被修改掉，因為頂部的 Dense 層是隨機初始化的，所以非常大量的權重更新將透過神經網路傳播，會導致先前學習到的表示法被破壞掉。

在 Keras 中，可以透過設定模型的 trainable 屬性為 False 來凍結 convolutional base 神經網路：

#+NAME: 凍結卷積基底神經網路
#+BEGIN_SRC python -r -n :results output :exports both
  from keras import models
  from keras import layers
  from keras.applications import VGG16

  conv_base = VGG16(weights='imagenet',   # 卷積基底
                    include_top=False,
                    input_shape=(150, 150, 3))

  model = models.Sequential()
  model.add(conv_base)        # 將卷積基底視為層加入 Sequential 模型中
  model.add(layers.Flatten()) # 攤平
  model.add(layers.Dense(256, activation='relu'))
  model.add(layers.Dense(1, activation='sigmoid')) # 增加密集層分類器

  # freeze convolutional base
  print('This is the number of trainable weights '
  'before freezing the conv base:', len(model.trainable_weights))
  conv_base.trainable = False  # 凍結權重
  print('This is the number of trainable weights '
  'after freezing the conv base:', len(model.trainable_weights))
#+END_SRC

#+RESULTS: 凍結卷積基底神經網路
: This is the number of trainable weights before freezing the conv base: 30
: This is the number of trainable weights after freezing the conv base: 4

由於 conv\under{}base 被凍結更新權重，所以 model 只會訓練增力的兩個 Dense 層權重，每層有兩個參數要更新(主要權重矩陣和偏差向量)，所以一共剩 4 個 trainable weights，原本的 pretrained model 有 13 層 Conv2D，共 26 個 trainable weights。

接下來就可以使用資料擴增來訓練 model:

#+NAME: 以凍結的 convolutional base 卷積基底進行從頭到尾完整的 model 訓練
#+BEGIN_SRC python -r -n :results output :exports both
  from keras import models
  from keras import layers
  from keras.applications import VGG16

  conv_base = VGG16(weights='imagenet',   # 卷積基底
                    include_top=False,
                    input_shape=(150, 150, 3))

  import os
  import numpy as np
  from keras.preprocessing.image import ImageDataGenerator

  base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small'
  train_dir = os.path.join(base_dir, 'train')
  validation_dir = os.path.join(base_dir, 'validation')
  test_dir = os.path.join(base_dir, 'test')

  model = models.Sequential()
  model.add(conv_base)        # 將卷積基底視為層加入 Sequential 模型中
  model.add(layers.Flatten()) # 攤平
  model.add(layers.Dense(256, activation='relu'))
  model.add(layers.Dense(1, activation='sigmoid')) # 增加密集層分類器

  conv_base.trainable = False  # 凍結權重

  # data augmentation
  from keras.preprocessing.image import ImageDataGenerator
  from keras import optimizers

  train_datagen = ImageDataGenerator( # 擴充訓練資料
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')

  test_datagen = ImageDataGenerator(rescale=1./255) # 請注意驗證資料不應該擴充


  train_generator = train_datagen.flow_from_directory(
    train_dir, # 目標目錄路徑
    target_size=(150, 150), # 調整所有圖像大小成 150×150
    batch_size=20,
    class_mode='binary') # 因為使用二元交叉熵 binary_crossentropy 作為損失分數，所						以需要二元標籤

  validation_generator = test_datagen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')

  model.compile( loss='binary_crossentropy',
         optimizer=optimizers.RMSprop(lr=2e-5),
         metrics=['acc'])

  history = model.fit_generator(
    train_generator,
    steps_per_epoch=100,
    epochs=30,
    validation_data=validation_generator,
    validation_steps=50)

  # 繪製model的損失率與精確率
  import matplotlib.pyplot as plt

  acc = history.history['acc']
  val_acc = history.history['val_acc']
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(acc) + 1)
  plt.clf()
  plt.plot(epochs, acc, 'bo', label='Training acc')
  plt.plot(epochs, val_acc, 'b', label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.legend()
  plt.plot()
  plt.savefig("CatsDogsDataAugmentationPretrained-acc.png")
  plt.figure()

  plt.clf()
  plt.plot(epochs, loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()
  plt.plot()
  plt.savefig("CatsDogsDataAugmentationPretrained-loss.png")

#+END_SRC

#+RESULTS: 以凍結的 convolutional base 卷積基底進行從頭到尾完整的 model 訓練
#+begin_example
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Epoch 1/30
2900  1/100 [..............................] - ETA: 8:14 - loss: 0.6634 - acc: 0.5500
100/100 [==============================] - 575s 6s/step - loss: 0.2791 - acc: 0.8820 - val_loss: 0.4186 - val_acc: 0.8990
#+end_example

#+CAPTION: Cats and Dogs Data Augmentation / Pretrained- Accuracy
#+LABEL:fig:Cat-Dog-Data-AugmentationPretrained-Acc
#+name: fig:Cat-Dog-Data-AugmentationPretrained-Acc
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/CatsDogsDataAugmentationPretrained-acc.png]]

#+CAPTION: Cats and Dogs Data Augmentation - Loss
#+LABEL:fig:Cat-Dog-Data-AugmentationPretrained-Loss
#+name: fig:Cat-Dog-Data-AugmentationPretrained-Loss
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/CatsDogsDataAugmentationPretrained-loss.png]]

實作結果，驗證準確率達 90%，優於從頭訓練小型神經網路（結果與原書中達 96%有所出入）。

*** 微調

微調(fine-tuning)為另一種廣泛使用的 model reuse 技術，本質上是特徵萃取的變化版，其做法是在特徵萃取的過程中不凍結整個 convolutional base，而是解凍 convolutional base 頂部的某些層以用於特徵萃取，並對於新增加於 model 的部份(如密集層分類器)與被解凍的部份層一起進行聯合訓練。

微調神經網路的步驟如下：
1. 在已訓練過的基礎神經網路(即 convolutional base)上增加自定義神經網路
1. 凍結 convolutional base
1. 訓練步驟 1 增加的部份(即最頂端的分類器)
1. 解凍 convolutional base 的某幾層
1. 共同訓練解凍層和分類器

以 VGG16 的模組架構為例，其分層架構如下：

#+NAME: VGG16-arch
#+BEGIN_SRC python -r -n :results output :exports both
  from keras.applications import VGG16

  conv_base = VGG16(weights='imagenet',
                    include_top=False,
                    input_shape=(150, 150, 3))
  conv_base.summary()
#+END_SRC

#+RESULTS: VGG16-arch
#+begin_example
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
#+end_example

我們可以調整這個 convolutional base 的最頂層(block5)三層的卷積層，即 block5\under{}conv1、block5\under{}conv2、block5\under{}conv3 三層，然後凍結 block4\under{}pool 以下的所有層。之所以選擇只解凍 convolutional base 的最頂層，幾個考量原因如下：

 - 相對於 convolutional base 中的低層主要是對更通用、可重複使用的特徵進行編碼；更高層則是對更特定的特徵進行編碼，所以這些特徵需要重新調整才能適用於新的問題。如果是對低層進行微調，則會出現反效果。
- 訓練的參數越多，就越可能 overfitting。convolutional base 有近 1500 萬個參數，因此在少量資料集上訓練會有風險。

解凍部份 convolutional base 的方式如下：

#+NAME: 解凍部份convolutional base
#+BEGIN_SRC python -r -n :results output :exports both
  from keras.applications import VGG16

  conv_base = VGG16(weights='imagenet',
                    include_top=False,
                    input_shape=(150, 150, 3))
  conv_base.summary()

  conv_base.trainable = True #先設定所有layer都可訓練?
  set_trainable = False #預設為凍結

  for layer in conv_base.layers: #由低到高
      if layer.name == 'block5_conv1': #直到出現block5_conv1這層後開始解凍
          set_trainable = True
      if set_trainable:
          layer.trainable = True
      else:
          layer.trainable = False
#+END_SRC

解凍完部份 layer 後即可開始徵調神經網路，這裡使用 RMSProp 優化器以非常低的學習率來微調，降低學習率的目的在減小 3 個解凍層的修改幅度，以免因為過大的修改損害到這些表示法。

#+NAME: 微調
#+Begin_src python -r -n :results output :exports both
  # 編譯模型
  model.compile(
      loss='binary_crossentropy',
      optimizer=optimizers.RMSprop(lr=1e-5),
      metrics=['acc'])

  # 訓練模型
  history = model.fit_generator(
      train_generator,
      steps_per_epoch=100,
      epochs=100,
      validation_data=validation_generator,
      validation_steps=50)
#+END_SRC





#+NAME: 完整徵調過程
#+Begin_src python -r -n :results output :exports both
  import os
  import numpy as np
  from keras.preprocessing.image import ImageDataGenerator

  base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small'
  train_dir = os.path.join(base_dir, 'train')
  validation_dir = os.path.join(base_dir, 'validation')
  test_dir = os.path.join(base_dir, 'test')

  # 部份凍結

  from keras.applications import VGG16

  conv_base = VGG16(weights='imagenet',
                    include_top=False,
                    input_shape=(150, 150, 3))
  conv_base.summary()

  conv_base.trainable = True #先設定所有layer都可訓練?
  set_trainable = False #預設為凍結

  for layer in conv_base.layers: #由低到高
      if layer.name == 'block5_conv1': #直到出現block5_conv1這層後開始解凍
          set_trainable = True
      if set_trainable:
          layer.trainable = True
      else:
          layer.trainable = False

  # data augmentation
  from keras.preprocessing.image import ImageDataGenerator
  from keras import optimizers

  train_datagen = ImageDataGenerator( # 擴充訓練資料
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest')

  test_datagen = ImageDataGenerator(rescale=1./255) # 請注意驗證資料不應該擴充


  train_generator = train_datagen.flow_from_directory(
    train_dir, # 目標目錄路徑
    target_size=(150, 150), # 調整所有圖像大小成 150×150
    batch_size=20,
    class_mode='binary') # 因為使用二元交叉熵 binary_crossentropy 作為損失分數，所						以需要二元標籤

  validation_generator = test_datagen.flow_from_directory(
    validation_dir,
    target_size=(150, 150),
    batch_size=20,
    class_mode='binary')


  from keras import models
  from keras import layers
  from keras import optimizers

  # model還是要加後面的layer?
  model = models.Sequential()
  model.add(conv_base)        # 將卷積基底視為層加入 Sequential 模型中
  model.add(layers.Flatten()) # 攤平
  model.add(layers.Dense(256, activation='relu'))
  model.add(layers.Dense(1, activation='sigmoid')) # 增加密集層分類器


  # 微調
  # 編譯模型
  model.compile(
      loss='binary_crossentropy',
      optimizer=optimizers.RMSprop(lr=1e-5),
      metrics=['acc'])

  # 訓練模型
  history = model.fit_generator(
      train_generator,
      steps_per_epoch=100,
      epochs=100,
      validation_data=validation_generator,
      validation_steps=50)

  # 繪製model的損失率與精確率
  import matplotlib.pyplot as plt

  acc = history.history['acc']
  val_acc = history.history['val_acc']
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(acc) + 1)
  plt.clf()
  plt.plot(epochs, acc, 'bo', label='Training acc')
  plt.plot(epochs, val_acc, 'b', label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.legend()
  plt.plot()
  plt.savefig("FineTune-acc-1.png")
  plt.figure()

  plt.clf()
  plt.plot(epochs, loss, 'bo', label='Training loss')
  plt.plot(epochs, val_loss, 'b', label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()
  plt.plot()
  plt.savefig("FineTune-loss-1.png")
#+END_SRC

#+RESULTS: 完整徵調過程
#+begin_example
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
100/100 [==============================] - 602s 6s/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0559 - val_acc: 0.9380
#+end_example

#+CAPTION: VGG16 Fine Tune Acc
#+LABEL:fig:VGG16-Fine-Tune-Acc
#+name: fig:VGG16-Fine-Tune-Acc
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/FineTune-acc-1.png]]

#+CAPTION: VGG16 Fine Tune Loss
#+LABEL:fig:VGG16-Fine-Tune Loss
#+name: fig:VGG16-Fine-Tune-Loss
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/FineTune-loss-1.png]]

微調的訓練準確率來到 99%，驗證準確率也有 94%，這是使用 2000 個訓練樣本就達到的結果。

#+latex: \newpage

* 視覺化呈現 CNN 的學習內容

CNN 學習的表示法非常適合以視覺化呈現，因為它們大部份就是視覺概念的表示法(represnetations of visual concepts)，幾種常用的視得化技術如下：
- 視覺化中間層 convnet 的輸出(中間啟動函數)：有助於理解 convnet 是如何一層一層的轉化資料，以及對過濾器(filter)的含義。
- 視覺化 CNN 過濾器：用於準確理解 CNN 中每個過濾器所要接受的視覺 patter 或概念中
- 視覺化類別激活熱圖(heatmaps of class activation): 有助於了解影像的哪些部份被識別為某個類別，藉以定位影像中的物件。

** 中間層輸出視覺化

這部份工作主要是在給定輸入影像後，顯示 convnet 各個卷積層和池化層輸出的特徵層。主要是讓我們能看到在 convnet 的學習過程中，輸入資料是如何經由逐層分解到不同的過濾器，雖然輸入資料為三個維度(width, height, channel)，但其實每個 channel 會針對相對獨立的 feature 進行編碼，所以此處是將每個 channel 的內容獨立繪製成 2D 圖形秀出，此即響應圖(比喻為將吐司切片。

以下先載入之前儲存好的 model，取一張測試集中的照片(未經訓練過)，秀出原始內容，然後萃取出特徵圖，因為我們只要看一張圖，所以要建新一個新的 Keras model。

#+NAME: 使用既有model
#+Begin_src python -r -n :results output :exports both
  #讀出之前存的model
  from keras.models import load_model
  model = load_model('cats_and_dogs_small_data_augmentation.h5')
  model.summary()

  #預處理單張照片
  img_path = base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small/test/cats/cat.1556.jpg'
  from keras.preprocessing import image
  import numpy as np

  img = image.load_img(img_path, target_size=(150, 150))
  #以下將這張照片預處理成4D張量，將值限制到0~1間
  img_tensor = image.img_to_array(img)
  img_tensor = np.expand_dims(img_tensor, axis = 0)
  img_tensor /= 255.
  print(img_tensor.shape)

  #顯示原圖
  import matplotlib.pyplot as plt
  plt.imshow(img_tensor[0])
  plt.plot()
  plt.savefig("origin.cat.png")

  #用一個輸入張量和一個輸出張量list來建一個新model
  from keras import models
  #萃取model的前8層輸出張量
  layer_outputs = [layer.output for layer in model.layers[:8]]
  for op in layer_outputs:
      print(op)
  #在給定輸入張量的條件下，建立會產生這些輸出的model
  activation_model = models.Model(inputs=model.input, outputs=layer_outputs)
  #當餵入一張影像後，此model會輸出原model中前8層啟動函數的值
  #故此處會傳回一個含8個輸出張量的list，即8個layer的啟動函數輸出值
  activations = activation_model.predict(img_tensor)
  print(len(activations))

  #例如，在餵入影像後，第一個卷積層(index=0)的啟動函數張量為
  first_layer_activation = activations[0]
  print(first_layer_activation.shape)

  #繪出第4個channel的響應圖
  plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')
  plt.plot()
  plt.savefig("origin_channel_4_map.png")

  #繪出第4個channel的響應圖
  plt.matshow(first_layer_activation[0, :, :, 7], cmap='viridis')
  plt.plot()
  plt.savefig("origin_channel_7_map.png")
#+END_SRC

#+RESULTS: 使用既有model
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
(1, 150, 150, 3)
Tensor("conv2d_1/Relu:0", shape=(?, 148, 148, 32), dtype=float32)
Tensor("max_pooling2d_1/MaxPool:0", shape=(?, 74, 74, 32), dtype=float32)
Tensor("conv2d_2/Relu:0", shape=(?, 72, 72, 64), dtype=float32)
Tensor("max_pooling2d_2/MaxPool:0", shape=(?, 36, 36, 64), dtype=float32)
Tensor("conv2d_3/Relu:0", shape=(?, 34, 34, 128), dtype=float32)
Tensor("max_pooling2d_3/MaxPool:0", shape=(?, 17, 17, 128), dtype=float32)
Tensor("conv2d_4/Relu:0", shape=(?, 15, 15, 128), dtype=float32)
Tensor("max_pooling2d_4/MaxPool:0", shape=(?, 7, 7, 128), dtype=float32)
8
(1, 148, 148, 32)
#+end_example

由上述結果可看出 firtst\under{}layer\under{}activation 的輸出為一個 148\times148 的特徵圖，共有 32 個 channel，即原 model 中第一層的 conv2d 的輸出內容。其中第 1556 張原圖如下：

#+CAPTION: Origin cat
#+LABEL:fig:origin_cat
#+name: fig:origin_cat
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/origin.cat.png]]

其中第 4 個 channel 似乎是編碼成一個邊緣特徵偵測器，如圖[[fig:origin_channel_4]]。

#+CAPTION: Effect of channel 4
#+LABEL:fig:origin_channel_4
#+name: fig:origin_channel_4
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/origin_channel_4_map.png]]

而第 7 個 channel 看起來則像個亮綠點偵測器，可以用來編碼貓眼特徵。

#+CAPTION: Effect of channel 7
#+LABEL:fig:origin_channel_7
#+name: fig:origin_channel_7
#+ATTR_LATEX: :width 300
#+ATTR_HTML: width 400
#+ATTR_ORG: :width 300
[[file:images/origin_channel_7_map.png]]

如果想看出神經網路中所有啟動函數輸出的完整圖形、每個 channel 的響應圖，其相對應程式碼如下：

#+Begin_src python -r -n :results output :exports both
  #讀出之前存的model
  from keras.models import load_model
  model = load_model('cats_and_dogs_small_data_augmentation.h5')
  model.summary()

  #預處理單張照片
  img_path = base_dir = r'/Volumes/Vanessa/dogs-vs-cats/small/test/cats/cat.1556.jpg'
  from keras.preprocessing import image
  import numpy as np

  img = image.load_img(img_path, target_size=(150, 150))
  #以下將這張照片預處理成4D張量，將值限制到0~1間
  img_tensor = image.img_to_array(img)
  img_tensor = np.expand_dims(img_tensor, axis = 0)
  img_tensor /= 255.
  print(img_tensor.shape)

  #顯示原圖
  import matplotlib.pyplot as plt
  plt.imshow(img_tensor[0])
  plt.plot()
  plt.savefig("origin.cat.png")

  #用一個輸入張量和一個輸出張量list來建一個新model
  from keras import models
  #萃取model的前8層輸出張量
  layer_outputs = [layer.output for layer in model.layers[:8]]
  #在給定輸入張量的條件下，建立會產生這些輸出的model
  activation_model = models.Model(inputs=model.input, outputs=layer_outputs)
  #當餵入一張影像後，此model會輸出原model中前8層啟動函數的值
  #故此處會傳回一個含8個輸出張量的list，即8個layer的啟動函數輸出值
  activations = activation_model.predict(img_tensor)
  #例如，在餵入影像後，第一個卷積層(index=0)的啟動函數張量為
  first_layer_activation = activations[0]

  ###=========================
  layer_names = []

  # 取得各層的名字，這樣才可以成為圖表的一部分
  for layer in model.layers[:8]:
      layer_names.append(layer.name)

  images_per_row = 16

  for layer_name, layer_activation in zip(layer_names, activations):
      n_features = layer_activation.shape[-1]
      size = layer_activation.shape[1]

      n_cols = n_features // images_per_row
      display_grid = np.zeros((size * n_cols, images_per_row * size))

      for col in range(n_cols):
          for row in range(images_per_row):
              channel_image = layer_activation[0, :, :, col * images_per_row + row]
              channel_image -= channel_image.mean()
              channel_image /= channel_image.std()
              channel_image *= 64
              channel_image += 128
              channel_image = np.clip(channel_image, 0, 255).astype('uint8')
              display_grid[col * size : (col + 1) * size,
                           row * size : (row + 1) * size] = channel_image

      scale = 1. / size
      plt.figure(figsize=(scale * display_grid.shape[1],
      scale * display_grid.shape[0]))
      plt.title(layer_name)
      plt.grid(False)
      plt.imshow(display_grid, aspect='auto', cmap='viridis')
      plt.plot()
      plt.savefig(layer_name+".png")
      plt.clf()
#+END_SRC

#+RESULTS:
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
(1, 150, 150, 3)
#+end_example

#+CAPTION: conv2d_1.png
#+LABEL:fig:conv2d_1.png
#+name: fig:conv2d_1.png
#+ATTR_LATEX: :width 500
#+ATTR_HTML: width 500
#+ATTR_ORG: :width 500
[[file:images/conv2d_1.png]]


#+CAPTION: max_pooling2d_1.png
#+LABEL:fig:max_pooling2d_1.png
#+name: fig:max_pooling2d_1.png
#+ATTR_LATEX: :width 500
#+ATTR_HTML: width 500
#+ATTR_ORG: :width 500
[[file:images/max_pooling2d_1.png]]


#+CAPTION: conv2d_2.png
#+LABEL:fig:conv2d_2.png
#+name: fig:conv2d_2.png
#+ATTR_LATEX: :width 500
#+ATTR_HTML: width 500
#+ATTR_ORG: :width 500
[[file:images/conv2d_2.png]]


#+CAPTION: max_pooling2d_2.png
#+LABEL:fig:max_pooling2d_2.png
#+name: fig:max_pooling2d_2.png
#+ATTR_LATEX: :width 500
#+ATTR_HTML: width 600
#+ATTR_ORG: :width 500
[[file:images/max_pooling2d_2.png]]

#+CAPTION: conv2d_3.png
#+LABEL:fig:conv2d_3.png
#+name: fig:conv2d_3.png
#+ATTR_LATEX: :width 500
#+ATTR_HTML: width 600
#+ATTR_ORG: :width 500
[[file:images/conv2d_3.png]]

#+CAPTION: max_pooling2d_3.png
#+LABEL:fig:max_pooling2d_3.png
#+name: fig:max_pooling2d_3.png
#+ATTR_LATEX: :width 500
#+ATTR_HTML: width 600
#+ATTR_ORG: :width 500
[[file:images/max_pooling2d_3.png]]

#+CAPTION: conv2d_4.png
#+LABEL:fig:conv2d_4.png
#+name: fig:conv2d_4.png
#+ATTR_LATEX: :width 500
#+ATTR_HTML: width 600
#+ATTR_ORG: :width 500
[[file:images/conv2d_4.png]]

#+CAPTION: max_pooling2d_4.png
#+LABEL:fig:max_pooling2d_4.png
#+name: fig:max_pooling2d_4.png
#+ATTR_LATEX: :width 500
#+ATTR_HTML: width 600
#+ATTR_ORG: :width 500
[[file:images/max_pooling2d_4.png]]

由上圖可知，隨層數越來越高，啟動函數的輸出變得越來越抽象，視覺上也越來越難解釋，model 開始編碼出更高階的概念。此外，啟動函數輸出的稀疏性也隨著層數的深度而增加，在第一層中，所有的過濾器都被輸入影響所驅動(都有值)，但接下來就有越來越多的 filter 的值是空的(全黑)，這表示在這些層的輸入影像中已經找不到過濾器要編碼的圖案 pattern 了。

上述示例也證明了深度神經網路所學習到的表示法有一個重要特性：各層萃取的特徵隨著層的𣶶度而變的越來越抽象，越高階的啟動函數越不會帶有關於特定輸入的資訊，卻具備更多關於目標的資訊（此例中指的是貓或狗）。這和人或動物感知世界的方式很像：在觀察一個場景幾秒中後閉眼，我們可以記得場景中有哪些抽象事物，但不會記得每個物體的特殊外觀，因為大腦也會將事物抽象化。

** 視覺化 convnet 的 filter

另一種視覺化的方法：convnet 是去看各 filter 要過濾的視覺化圖案(visual pattern)。我們可以先餵給 convnet 一張空白的影像，然後將梯度下降法套用到 convnet 上，一直到所指定的層、所指定的 filter 對輸入影像的響應達到最大化，如此所得到的輸入影像就是讓該 filter 產生最大化響的影像，也就是 filter 的長相。方法如下：

1. 先建立一個損失函數，讓 convnet 指定層中指定的濾波器的啟動函數輸出最大化
2. 使用隨機梯度下降(SGD)來調整輸入影像像素值，以便最大化這個啟動函數輸出值

#+Begin_src python -r -n :results output :exports both
  from keras.applications import VGG16
  from keras import backend as K #載入Keras的後端，用來操作張量(取平均)

  model = VGG16(weights='imagenet',
                include_top=False)

  layer_name = 'block3_conv1'
  filter_index = 0

  layer_output = model.get_layer(layer_name).output
  loss = K.mean(layer_output[:, :, :, filter_index]) # 定義損失函數張量, 其為層輸出張量數值取平均

  #得到損失函數張量後，接著使用backend模組gradients()函數來取得損失函數張量相對於model輸入的梯度
  # gradients() 會傳回一個由張量組成的list, 在本例中, list 的大小為 1, 因此, 只取出其第 0 個元素, 即 grads 是 1 個梯度張量
  grads = K.gradients(loss, model.input)[0]

  #為確保梯度下降過程順利，此處把梯度張量除以其L2 norm，這是為了讓更新幅度限制在規範的範圍內
  # 在做除法之前先加上 1e-5 以避免意外地除以 0
  grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)

  #定義一個Keras後端程式來計算損失張量和梯度張量的數值:iterate
  iterate = K.function([model.input], [loss, grads]) # 定義一個 Keras 後端函式

  import numpy as np
  loss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])
  #輸出損失張量與梯度張量

  #定義一個迴圈來進行SGD
  # 從帶有雜訊的灰階圖像開始
  input_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128. # 1...

  step = 1. # 每個梯度更新的大小
  for i in range(40): # 執行梯度上升 40 步
    loss_value, grads_value = iterate([input_img_data]) # 計算損失值和梯度值
      # 2. 以朝向最大化損失調整輸入圖像 (以前SGD 是用 -= 算符, 現在反過來是用 += 算符)
    input_img_data += grads_value * step

  #得到的圖像張量是shape為(1, 150, 150, 3)的浮點張量，對之進行後處理轉為可顯示格式
  def deprocess_image(x):
      x -= x.mean()
      x /= (x.std() + 1e-5)				# 1. 張量正規化：以 0 為中心, 確保 std 為 0.1
      x *= 0.1
      x += 0.5
      x = np.clip(x, 0, 1) # 修正成 [0, 1], 即 0-1 之間
      x *= 255
      x = np.clip(x, 0, 255).astype('uint8')		# 2.轉換成 RGB 陣列
      return x
  import matplotlib.pyplot as plt
  #將層的名稱和過濾器索引作為輸入參數，回傳有效影像張量
  def generate_pattern(layer_name, filter_index, size=150):
    layer_output = model.get_layer(layer_name).output # 取得指定層的輸出張量
    loss = K.mean(layer_output[:, :, :, filter_index]) # 1. 取得指定過濾器的輸出張量, 並以最大化此張量的均值做為損失
    grads = K.gradients(loss, model.input)[0] # 根據此損失計算輸入影像的梯度
    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5) # 標準化技巧：梯度標準化
    iterate = K.function([model.input], [loss, grads]) # 2.建立 Keras function 來針對給定的輸入影像回傳損失和梯度
    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128. # 3. 從帶有雜訊的灰階影像開始

    step = 1.
    for i in range(40): # 執行梯度上升 40 步
      loss_value, grads_value = iterate([input_img_data]) # 4. 針對給定的輸入影像回傳損失和梯度
      input_img_data += grads_value * step
    img = input_img_data[0]
    return deprocess_image(img)	  # 進行圖像後處理後回傳

  plt.imshow(generate_pattern('block3_conv1', 0)) # 我們來看看 block3_conv1 層中的過濾器 0 的特徵圖
  plt.plot()
  plt.savefig("block3_conv1_filter_feature.png")

  # 產生一層中所有過濾器響應圖
  for layer_name in ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1']:
      size = 64
      margin = 5

      # 1. 用於儲存結果的空(黑色)影像
      results = np.zeros((8 * size + 7 * margin, 8 * size + 7 * margin, 3))

      for i in range(8):  # ← 迭代產生網格的行
          for j in range(8):  # ←迭代產生網格的列
              # 在 layer_name 中產生過濾器 i +(j * 8) 的 pattern
              filter_img = generate_pattern(layer_name, i + (j * 8), size=size)

              # 將結果放在結果網格的方形(i, j)中
              horizontal_start = i * size + i * margin
              horizontal_end = horizontal_start + size
              vertical_start = j * size + j * margin
              vertical_end = vertical_start + size
              results[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img

      # 顯示網格結果
      plt.figure(figsize=(20, 20))
      plt.imshow(results)
      plt.plot()
      plt.savefig(layer_name+"_pattern.png")
#+END_SRC

#+RESULTS:

#+CAPTION: block3\under{}conv1 層中的第 0 個 channel 的最大回應 pattern
#+LABEL:fig:pattern_block3_conv1_channel_0
#+name: fig:pattern_block3_conv1_channel_0
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
[[file:images/block3_conv1_filter_feature.png]]

block3\under{}conv1 層中的過濾器 0 似乎對波爾卡圓點圖案有回應，再來就是開始對每一層中的每個 filter/channel，為簡化起見，只每每一層中的前 64 個過濾器，將結果以 8\times8 網格方式呈現。

結果和課本的圖差異很大......

#+CAPTION: pattern of block1\under{}conv1
#+LABEL:fig:block1_conv1_pattern
#+name: fig:block1_conv1_pattern
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
[[file:images/block1_conv1_pattern.png]]

#+CAPTION: pattern of block\under{}conv1
#+LABEL:fig:block2_conv1_pattern
#+name: fig:block2_conv1_pattern
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
[[file:images/block1_conv2_pattern.png]]

#+CAPTION: pattern of block3\under{}conv1
#+LABEL:fig:block3_conv1_pattern
#+name: fig:block3_conv1_pattern
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
[[file:images/block3_conv1_pattern.png]]

#+CAPTION: pattern of block4\under{}conv1
#+LABEL:fig:block4_conv1_pattern
#+name: fig:block4_conv1_pattern
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
[[file:images/block4_conv1_pattern.png]]

** 視覺化類別激活熱圖 heatmap of class activation

CMA(class activation map)可以用來理解影像中的哪些部份會讓 convnet 做出最終分類的決策，這有助於 convnet 決策過程的偵錯。CAM 主要是針對輸入影像產生類別激活熱圖(heatmap of class)，這是一個 2D 的分數網格圖，針對輸入影像的每個位置(網格)進行計算，然後指出每個位置相對於目前類別的重要性。我們可以使用"Graid-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"這篇論文提到的方法，即，給定影像，取得卷積層的輸出特徵，以"這個類別對每個 channel 的梯度值"對這個特徵圖中的每個 channel 做加權。進而產生「某張圖片激活某類別的強度」的 2D 分數網格圖。做法如下：

#+Begin_src python -r -n :results output :exports both
  from keras.applications.vgg16 import VGG16

  model = VGG16(weights='imagenet')  # 請注意, 在頂部包含了密集連接的分類器 (預設 include_top=True)

  # 預先處理 VGG16 的輸入影像
  from keras.preprocessing import image
  from keras.applications.vgg16 import preprocess_input, decode_predictions
  import numpy as np

  img_path = r'/Volumes/Vanessa/dogs-vs-cats/african_elephants.jpg'

  img = image.load_img(img_path, target_size=(224, 224))
  print(type(img))  # 目前圖片為 <class 'PIL.Image.Image'> 物件
  print(img.size)  # 可以用 size 屬性查看尺寸 -> (224, 224)

  x = image.img_to_array(img) 	# 將 PIL 物件轉為 float32 的 Numpy 陣列
  print(x.shape) 				# shape=(224, 224, 3)


  # 將 x 陣列 (可視為張量) 增加一個批次軸, shape=(1, 224, 224, 3)
  x = np.expand_dims(x, axis=0)
  print(x.shape)

  x = preprocess_input(x) # 預處理批次量 (這會對每一 channel 做顏色值正規化)


  #使用 VGG 神經網路預測圖片類別
  preds = model.predict(x)
  print('預測結果:', decode_predictions(preds, top=3)[0])

  np.argmax(preds[0])

  #設定 Gard-CAM 演算法
  from keras import backend as K

  african_elephant_output = model.output[:, 386] # ← 預測向量中的 "非洲象" 項目

  last_conv_layer = model.get_layer('block5_conv3') # block5_conv3 層的輸出特徵圖, 其為 VGG16 中的最後一個卷積層

  grads = K.gradients(african_elephant_output, last_conv_layer.output)[0] #  block5_conv3 的輸出特徵圖中關於 "非洲象" 類別的梯度

  pooled_grads = K.mean(grads, axis=(0, 1, 2)) #  轉換成向量 shape = (512, ), 其中每個項目是特定特徵圖 channel 的梯度平均強度(值)

  #  給定輸入影像的條件下, 讓我們可以存取剛剛定義的數值：pooled_grads 和 block5_conv3 的輸出特徵圖
  iterate = K.function([model.input],
      [pooled_grads, last_conv_layer.output[0]])

  #  對於給定的兩隻大象樣本影像, 產生這兩個量值, 以 Numpy 陣列呈現
  pooled_grads_value, conv_layer_output_value = iterate([x])

  # 將特徵圖陣列中的每個 channel 與 "大象" 類別相關的 "此 channel 的重要程度" 相乘
  for i in range(512):
      conv_layer_output_value[:, :, i] *= pooled_grads_value[i]

  # 特徵圖的跨 channel 平均值是類別激活函數輸出的熱圖
  heatmap = np.mean(conv_layer_output_value, axis=-1)

  #熱圖後期處理
  import matplotlib.pyplot as plt
  import numpy as np

  heatmap = np.maximum(heatmap, 0)
  heatmap /= np.max(heatmap)
  plt.matshow(heatmap)
  plt.plot()
  plot.savefig("heapmapOfClassActivation.png")

  # 將熱圖與原始影像疊加在一起
  import cv2

  img = cv2.imread(img_path)

  heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))

  heatmap = np.uint8(255 * heatmap)

  heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

  superimposed_img = heatmap * 0.4 + img # 這裡 0.4 是熱圖強度因子

  print('是否儲存成功:', cv2.imwrite('elephant_cam.jpg', superimposed_img))

#+END_SRC

#+CAPTION: Heapmap of class activation
#+LABEL:fig:heapmapOfClassActivation
#+name: fig:heapmapOfClassActivation
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
[[file:images/heapmapOfClassActivation.png]]

#+CAPTION: 將激活熱圖與影像叠加
#+LABEL:fig:heapmapOfClassActivationCombination
#+name: fig:heapmapOfClassActivationCombination
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
[[file:images/elephant_cam-1.jpg]]

上圖的視覺化技術回答了兩個重要問題：
- 為什麼神經網路認為這個影像裡有非洲象?
- 非洲象位於影像中的哪個位置?

#+latex: \newpage

* MLP 神經網路模型實作：以 Keras 為實作工具
** 簡介

Keras 是 Python 的深度學習框架，提供一種便利的方式來定義和訓練幾秬所有類型的深度學習模型。

*** 優點
- 相同的程式碼可在 CPU 或 GPU 上執行
- 內建程式庫支擾了卷積神經網路(用於電腦視覺)、循環神經網路(用於序列資料處理)，以及二者的任何組合。
- Keras 可以使用最少的程式碼，花最少的時間，就能建立深度學習模型，並進行培訓、評做準確率；相對的，如果使用 TensorFlow，則需要更多程式碼，花費更多時間。
- 採用寬鬆的 MIT 授權條款，所以可以自由使用在商業專案上。

Keras 是一個 model-level 模型級的深度學習程式庫，Keras 只處理模型的建立、訓練、預測等功能。深度學習程式庫的運作（如張量運算），Keras 必須配合使用「後端引擎」(backend Engine)進行運算。目前 Keras 提供了兩種 backend engine：Theano 與 TensorFlow。其基本架構如下圖所示：

#+BEGIN_SRC ditaa :file KerasArch.png
  +----------------------------+
  |            Keras           |
  +----------------------------+
  | TensorFlow/Theano/CNTK/... |
  +------------+---------------+
  | CUDA/cuDNN |  BLAS, Eigen  |
  +------------+---------------+
  |     GPU    |     CPU       |
  +------------+---------------+
#+END_SRC
#+CAPTION: 深度學習軟硬體架構
#+name: fig:KerasArch
#+ATTR_LATEX: :width 360
#+ATTR_HTML: :width 360
#+ATTR_ORG: :width 360
[[file:images/KerasArch.png]]

由圖[[fig:KerasArch]]可看出，Keras 並未被綁定在特定張量程式庫中，而是改以模組方式處理，目前可用的後端引擎有 Montreal 大學 MILA 實驗室的 Theano、Google 的 TensorFlow、Microsoft 的 CNTK...等，這些後端引擎在應用不同硬體(CPU/GPU)時則會採用不同的低階程式庫(CUDA/Eigen)。

** Keras 程式設計模式

Keras 的開發流程大致如下：
1. 定義問題並建立資料集
1. 選擇一種評量成功的準則(metrics)
1. 決定驗證(validation)程序
1. 準備資料：定義訓練資料：即 input tensor 和 target tensors(label tensors)
1. 開發出優於基準(baseline)的模型：定義神經網路模型的 layers，以便將 input tensor 對應到預測值
1. 選擇 loss function, optimizer 和監控的評量準則(metrics)來建立學習過程
1. 呼叫模型中的 fit()方法來迭代訓練資料
1. 擴大規模：開發一個過度適配的模型
1. 常規化模型並調整參數

*** 定義問題並建立資料集

進行模型建構之初，我們首先要評估的是：
- 輸入資料是什麼？想要預測什麼？有什麼樣的訓練資料，就只能學習預測該類問題。例如，手上只有電影評論和情緒標註資料，就只能學習對電影評論的情緒分類。
- 面臨什麼樣的問題？是二元分類？多類別分類？純量迴歸？向量迴歸？多類別多標籤？分群？生成式學習？增強式學習？不同的問題類型會引導我們如何選擇模型架構與損失函數。

在確認上述兩項問題後，我們是基於以下兩個假設來進行模型的建立：

- 假設機器可以根據給定的輸入資料預測結果
- 假設手上的資料能提供足夠的資訊，讓機器能學習到輸入與輸出間的關係

在真正建構出一個可用模型之前，上述兩個假設依然只是假設，必須經過驗證後才能確定成立與否。重點是：並非所有的問題都能透過模型來解決，例如：試圖以最近的歷史價格來預測股票市場的走勢就很難成功，因為光是參考歷史價格並不足以提供預測股價所需資訊。

另一種要特別留意的問題類型為非平穩問題(nonstationary problems)，例如分析服裝的消售/推薦，這當中存在的最大問題在於人們購買的衣服種類會隨季節而變化，所以服裝購買在幾個月內是非平穩現象，建立的模型內容會隨著時間而變化。在這種情況下，解決方法有：

- 不斷以最近的資料重新訓練模型，或
- 在相對平穩的時間區間(具有規律的週期間)收集資料，以購買衣服為例，應該以年為單位進行資料收集才足以補捉到季節變化的規律。

最後，切記：機器學習只能用於學習訓練資料中已存在的模式，也就是只能認出以前見過的模式。通常我們所謂以過去的資料預測未來，是假設未來的行為在過去曾發生過，但實際情況則未必如此。

*** 選擇一種評量成功的準則(metrics)

選好評量成功的準則，才有選擇損失函數的依據。在 Keras 中，所謂選擇評量準則，就是在 compile 時選擇適當的 metrics 參數。大概的選擇原則如下：

- 二元分類問題：accuracy 和 ROC AUC(area under the receiver operating characteristic curve)為兩種常用的度量。
- 類別不均(class-imbalanced)問題：使用 precision 和 recall 來做度量。
- 排名問題或多標籤問題：使用平均精度
- 少問的問題：自行定義指標

*** 決定驗證(validation)程序

一旦決定目標，就要決定驗證學習進度的方法，三種常見的驗證方法如下所述，但在大多數情況下，第一個方法就有不錯的效能。

- Simple hold-out: 資料量大時適用
- K-fold cross validation: 樣本資料不夠多時用
- Iterated K-fold validation with shuffling: 資料量非常少時用

*** 準備資料

一旦知道要訓練什麼、要優化什麼、以及如評估效能，就可以著手準備建構模型，但首先要把資料整理成可以輸入神經網路的格式（張量），以監督式學習而言，其輸入的訓練資料會有以下兩類：即 input tensor 和 target tensors(label tensors)

*** 開發優於基準(baseline)的模型

此階段的目標在於實現統計功效(statistical power)，以 MNIST 資料集為例，任何準確度大於 0.1 的模型都可以說具有統計功效的（因為一共有 10 個答案類鞏）；而在 IMDB 範例中，只要準確度大於 0.5 即算。雖然 baseline 是一個很低的標準，但我們不見得都能實現這個目標，如果在嚐試過多個合理架構後模型表現仍無法優於隨機基準能力，則很有可能問題出在輸入資料，也許輸入資料沒有所需答案。

如果一切順利，則接下來我們要做出三個關鍵選擇來建構第一個模型：

- 選擇最後一層的啟動函數：這將為神經網路建立輸出的形式。例如，IMDB 分類最後使用 sigmoid 分成兩個值、MNIST 則以 softmax 分為 10 類。
- 損失函數：要配合問題類型，如 IMDB 使用 binary\under{}crossentropy、迴歸則使用 mse。
- 優化器設定：大多數情況下，rmsprop 可做為預設選項搭配預設學習率

下表為選擇啟動函數與損失函數的參考

| 問題類型                                | 輸出層啟動函數 | 損失函數                          |
|-----------------------------------------+----------------+-----------------------------------|
| Binary classification                   | Sigmoid        | binary\under{}crossentropy        |
| Multiclass, single-label classification | softmax        | categorical\under{}crossentropy   |
| Multiclass, multi-label classification  | sigmoid        | binary\under{}crossentropy        |
| Regression to arbitrary values          | None           | mse                               |
| Regression to values between 0 and 1    | sigmoid        | mse or binary\under{}crossentropy |

*** 擴大規模：開發一個過度適配的模型

一旦成功建構了一個超越 baseline 的模型，問題就變成：這個模型夠不夠強大？有沒有足夠的 layer 和參數來正確模擬手上的問題？例如，只有兩個 units 的單隱藏層也許有辨識 MNIST 的統計功效，但不足以很好的解決該問題。而機器學習就是在最佳化和普適性之間做取捨，理想的模型是介於 underfitting 和 overfitting 的交界、介於模型太小(undercapacity)和模型太大(overcapacity)之間，要找出這個位置，勢必要先越過它再回來。所以，要搞清楚需要多大的模型，就要開發一個太大的模型，有幾種方法可以達到這點：

- 加入更多的 layer
- 增加每一層的 capacity
- 訓練更多的週期

*** 常規化模型並調整參數

這裡會花掉最多時間：要反覆修改模型、訓練模型、評估驗證資料，然後再次修改，以下有幾種做法：

- 加入 dropout
- 嘗試不同架構：新增或刪除 layer
- 添加 L1 或 L2 regularization，或同時使用
- 嘗試使用不同的超參數，如每一曾的 units 數或優化器的學習率
- 著重於特徵工程，如加入新特徵、刪除似乎沒用的特徵

一旦開發出令人滿意的模型配置，就可以在所有可用資料(訓練和驗證)上訓練最終產出的模型，並在測試集上最後一次評估它。

** 基本流程

在 Keras 定義 model 有兩種方法：

- Sequential class: 適用於線性堆叠的模型
- functional API: 適用任何有向無環的神經網路架構

以下為建立 sequential model 的例子：

1. 建立模型
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras import models
  from keras import layers

  model = models.Sequential()
  # 新增一個輸入為874維、輸出為32維的Dense layer
  model.add(layers.Dense(32, activation='relu', input_shape=(784,)))
  # 接數來自上層32維的輸入，輸出一個10維的資料
  model.add(layers.Dense(10, activation='softmax'))
#+END_SRC

若使用 API 來定義相同的模型，其語法如下：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras import models
  from keras import layers

  input_tensor = layers.Input(shape=(784,))
  x = layers.Dense(32, activation='relu')(input_tensor)
  output_tensor = layers.Dense(10, activation='softmax')(x)
  model = models.Model(inputs=input_tensor, outputs=output_tensor)

#+END_SRC

2. 一旦建立好模型架構，則無論是使用 Sequential 或 API，其餘步驟均相同。神經網路是在編譯(model.compile)時建立的，我們可以在其中指定使用的 optimizer 和 loss function，以及訓練期間監看的評量準則(metrics)，典型的範例如下：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras import optimizers
  model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='mse', metrics=['accuracy'])
#+END_SRC

3. 最後，整個學習程序經由 fit()將輸入資料以 Numpy 陣列的形式傳給模型：

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  model.fit(input_tensor, target_tensor, batch_size=128, ephchs=10)
#+END_SRC

** 以 Keras 實作 MNist 手寫數字辨識資料集

*** 讀入資料與預處理

MNist 手寫數字辨識資料集是由 Yann LeCun 所蒐集，他也是 CNN 的創始人。MNist 資料集共有訓練資料集 60000 筆、測試資料集 10000 筆，每筆資料都由一 28*28 的 image 以及相對應的 label 組成。

#+BEGIN_SRC python -r -n :results output :exports both
  '''###1. 下載MNist資料###'''
  '''1.1 滙入Keras及相關所需資源'''
  import numpy as np # 支援維度陣列之矩陣運算
  import pandas as pf
  from keras.utils import np_utils # 要將table轉為one-hot encoding

  np.random.seed(10) #讓每次產生的亂數一致

  '''1.2 匯入Keras模組以下載MNist資料集'''
  from keras.datasets import mnist
  '''1.3 讀取MNist資料集'''
  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()

  '''1.4 查看MNist資料集筆數'''
  print('4. 查看MNist資料集筆數')
  print('train data=', len(x_train_image))
  print(' test data=', len(x_test_image))

  '''###2.  查看訓練資料'''
  '''2.1 輸出訓練資料格式'''
  print('2.1 查看訓練資料格式')
  print('train image=', x_train_image.shape)
  print(' test image=', y_train_label.shape)

  '''2.2 定義plot_image函數顯示數字影像'''
  import matplotlib.pyplot as plt
  def plot_image(imgname, image):
      fig = plt.gcf() #定設圖形大小
      fig.set_size_inches(2,2)
      plt.imshow(image, cmap='binary') #cmap參數設定為binary以黑白灰階顯示
      # plt.show() #for jupyter or colab
      plt.plot()
      plt.savefig(imgname+".png")

  '''2.3 執行plot_image函數查看第0筆數字影像及對應label'''
  plot_image("Keras-mnist-0", x_train_image[0])
  print(y_train_label[0])

  '''###3. 查看多筆資料'''
  '''3.1 建立plot_images_labels_prediction()函式'''
  def plot_images_labels_prediction(imgname, images, labels, prediction, idx, num = 10):
      fig = plt.gcf()
      fig.set_size_inches(8,4) # 設定顯示圖形大小
      if num>25: num=25 # 顯示資料筆數上限
      for i in range(0, num):
          ax=plt.subplot(2,5,1+i) # 此處顯示10筆圖形，2*5個
          ax.imshow(images[idx], cmap='binary') # 畫圖
          title= "label=" +str(labels[idx]) # 加入子圖形title
          if len(prediction)>0:
              title+=",predict="+str(prediction[idx]) # 標題title加入預測結果
          ax.set_title(title,fontsize=10)
          ax.set_xticks([]);ax.set_yticks([]) #不顯示刻度
          idx+=1
      #plt.show()
      #plt.plot()
      plt.savefig(imgname+".png")

  '''3.2 執行plot_images_labels_prediction函數查看多筆images及labels'''
  plot_images_labels_prediction("Keras-mnist-1",x_train_image,y_train_label,[],0,10)

  '''###4. 多層感知器(Multilayer perception, MLP)模型資料預處理'''
  '''4.1 以reshape轉換image矩陣'''
  x_Train = x_train_image.reshape(60000, 784).astype('float32')
  x_Test = x_test_image.reshape(10000, 784).astype('float32')
  print(x_Train.shape)
  print(x_Test.shape)
  '''4.2 將影像之數字矩陣正規化'''
  x_Train_normalize = x_Train/ 255
  x_Test_normalize = x_Test/ 255

  '''4.3 原始label欄位'''
  print(y_train_label[:5]) # 輸出前5筆
  '''4.4 進行One-hot encoding'''
  y_TrainOneHot = np_utils.to_categorical(y_train_label)
  y_TestOneHot = np_utils.to_categorical(y_test_label)
  '''4.5 轉換後之label欄位'''
  print(y_TrainOneHot[:5]) # 輸出前5筆

  '''###5. Keras MLP'''
#+END_SRC

#+RESULTS:
#+begin_example
4. 查看MNist資料集筆數
train data= 60000
 test data= 10000
2.1 查看訓練資料格式
train image= (60000, 28, 28)
 test image= (60000,)
5
(60000, 784)
(10000, 784)
[5 0 4 1 9]
[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
#+end_example

#+CAPTION: MNist 第一筆資料影像
#+LABEL:fig: Keras-mnist-0
#+name: fig:Keras-mnist-0
#+ATTR_LATEX: :width 50
#+ATTR_ORG: :width 150
#+RESULTS:
[[file:images/Keras-mnist-0.png]]

#+CAPTION: MNist 前十筆資料影像
#+LABEL:fig: Keras-mnist-1
#+name: fig:Keras-mnist-1
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+RESULTS:
[[file:images/Keras-mnist-1.png]]

*** Keras MLP 辨識 MNist
**** 多層感知器模型

MNist 的初始模型分為輸入、隠藏及輸出三層, 輸入層有 784 個輸入神經元(\(x_1,x_2,...,x_{784}\))，接收被 reshape 為一維矩陣的手寫圖片(28*28)；隠藏層內部有 256 個神經元，隱藏層的層數與每層的神經元各數在神經網路的建構中主要取決於設計者；輸出層共有 10 個神經元，代表預測的結果(0~9)。
#+CAPTION: MNist MLP 模型
#+LABEL:fig: MNist-MLP-1
#+name: fig:MNist-MLP-1
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+RESULTS:
[[file:images/MLP-2.jpg]]

**** 多層感知器的訓練與預測

多層感知器模型建立後，必須先訓練模型才能夠進行預測(辨識)手寫數字。而這裡所謂的訓練模型，對於神經網路而言，就是學習。
***** 訓練(Traning)
MNist 的資料訓練集共 60000 筆，經資料預處理後會產生 features(數字特徵集)與 label(數字的真實值)，然後將這些資料輸入 MLP 模型進行訓練，訓練完成後的模型才能進行預測。
***** 預測(Predict)
將測試資料集匯入訓練完成的 MLP 模型，最後產生預測結果(此例中為 0~9 的數字)。

*** MLP 模型旳建立步驟
**** 進行資料預處理(preprocess)
***** 匯入所需模組
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras.utils import np_utils
  import numpy as np # 支援維度陣列之矩陣運算
  np.random.seed(10) #讓每次產生的亂數一致
#+END_SRC
***** 讀取 mnist 資料
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras.datasets import mnist
  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
#+END_SRC
***** 利用 reshape 轉換影像特徵值(features)
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  x_Train = x_train_image.reshape(60000, 784).astype('float32')
  x_Test = x_test_image.reshape(10000, 784).astype('float32')
  #+END_SRC***** 建立模型
***** 將 feature 標準化
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  x_Train_normalize = x_Train/ 255
  x_Test_normalize = x_Test/ 255
#+END_SRC
***** 以 one-hot encoding 轉換數字真實值(label)
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  y_TrainOneHot = np_utils.to_categorical(y_train_label)
  y_TestOneHot = np_utils.to_categorical(y_test_label)
#+END_SRC
**** 建立模型
***** 匯入所需模組
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras.models import Sequential
  from keras.layers import Dense
#+END_SRC
在 Keras 在 Keras 中有兩類主要的模型：Sequential 順序模型 和 使用函數式 API 的 Model 類模型。
***** 建立 Sequential 模型

建立一個線性堆叠模型，後續再使用 model.add()方法將各神經網路層加入模型中即可。
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  model = Sequential()
  #+END_SRC
***** 建立「輸入層」與「隠藏層」

Dense 神經網路層的特色：所有的上一層與下一層的神經元都完全連接。
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  # 以常態分佈的亂數初始化weight和bias
  model.add(Dense(units=256, input_dim=784, kernel_initializer='normal', activation='relu'))
#+END_SRC
***** 建立「輸出層」

10 個神經元分別對應 0~9 的答案，softmax 可以將神經元的輸出結果轉換為預測每一個數字的機率。建立這裡的 Dense 網路層時無需設定 input_data，因為 Keras 會自動依照上一層的 units 神經元個數(256)來設定這一層的 input_dim 神經元個數。
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  # 以常態分佈的亂數初始化weight和bias
  model.add(Dense(units=10, kernel_initializer='normal', activation='softmax'))
#+END_SRC
***** 查看模型摘要
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  print(model.summary())
#+END_SRC
***** 執行結果
#+BEGIN_SRC python -r -n :results output :exports both
  from keras.utils import np_utils
  import numpy as np # 支援維度陣列之矩陣運算
  np.random.seed(10) #讓每次產生的亂數一致
  from keras.datasets import mnist
  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
  x_Train = x_train_image.reshape(60000, 784).astype('float32')
  x_Test = x_test_image.reshape(10000, 784).astype('float32')
  x_Train_normalize = x_Train/ 255
  x_Test_normalize = x_Test/ 255
  y_TrainOneHot = np_utils.to_categorical(y_train_label)
  y_TestOneHot = np_utils.to_categorical(y_test_label)
  from keras.models import Sequential
  from keras.layers import Dense
  model = Sequential()
  model.add(Dense(units=256, input_dim=784, kernel_initializer='normal', activation='relu'))
  model.add(Dense(units=10, kernel_initializer='normal', activation='softmax'))
  print(model.summary())
#+END_SRC

#+RESULTS:
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 256)               200960
_________________________________________________________________
dense_2 (Dense)              (None, 10)                2570
=================================================================
Total params: 203,530
Trainable params: 203,530
Non-trainable params: 0
_________________________________________________________________
None
#+end_example

以上每一層 Param 稱為超參數(Hyper-Parameters)，計算方式為：Param=(上一層神經元數量)\(\times\)(本層的神經元數量)\(+\)(本層的神經元數量)。其中：
- 隠藏層的 Param 為 200960，即 784(輸入層神經元數量)\(\times\)256(隠藏層神經元數量)+256(隠藏層神經元數量)=200960
- 輸出層的 Param 為 2570，即 256(隠藏層神經元數量)\(\times\)10(輸出層神經元數量)+10(輸出層神經元數量)=2570

**** 進行訓練

模型建立後，即可利用 Back Propagation 來進行訓練，其步驟如下：

***** 定義訓練方式[fn:9]

在訓練模型前，我們必須使用 compile 方式，設定訓練模式
#+BEGIN_SRC python -r -n :results output :exports both :eval no
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
#+END_SRC
model.compile()接收三個參數：

- 優化器 optimizer。它可以是現有優化器的字符串標識符，如 rmsprop 或 adagrad，也可以是 Optimizer 類的實例。詳見：optimizers。
- 損失函數 loss，模型試圖最小化的目標函數。它可以是現有損失函數的字符串標識符，如 categorical_crossentropy 或 mse，也可以是一個目標函數。詳見：losses。
- 評估標準 metrics。對於任何分類問題，你都希望將其設置為 metrics = ['accuracy']。評估標準可以是現有的標準的字符串標識符，也可以是自定義的評估標準函數。
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  # 多分類問題
  model.compile(optimizer='rmsprop',
                loss='categorical_crossentropy',
                metrics=['accuracy'])

  # 二分類問題
  model.compile(optimizer='rmsprop',
                loss='binary_crossentropy',
                metrics=['accuracy'])

  # 均方誤差回歸問題
  model.compile(optimizer='rmsprop',
                loss='mse')

  # 自定義評估標準函數
  import keras.backend as K

  def mean_pred(y_true, y_pred):
      return K.mean(y_pred)

  model.compile(optimizer='rmsprop',
                loss='binary_crossentropy',
                metrics=['accuracy', mean_pred])

#+END_SRC

***** 開始訓練

x,y 分別為輸入之訓練參數資料，split=0.2 表示該批資料的 80%作為訓練用、20%作為驗證用，共執行 10 次訓練週期、verbose=2 則表示要顯示訓練過程。
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  train_history = model.fit(x=x_Train_normalize, y=y_Train_OneHot,
                            validation_split=0.2, epochs=10, verbose=2)
#+END_SRC

fit 完整語法如下：
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  fit(self, x=None, y=None, batch_size=None,
      epochs=1, verbose=1, callbacks=None,
      validation_split=0.0, validation_data=None,
      shuffle=True, class_weight=None, sample_weight=None,
      initial_epoch=0, steps_per_epoch=None, validation_steps=None)
#+END_SRC
對應參數分別為：[fn:10]
- x：輸入數據。如果模型只有一個輸入，那麼 x 的類型是 numpy array，如果模型有多個輸入，那麼 x 的類型應當為 list，list 的元素是對應於各個輸入的 numpy array。如果模型的每個輸入都有名字，則可以傳入一個字典，將輸入名與其輸入數據對應起來。
- y：標籤，numpy array。如果模型有多個輸出，可以傳入一個 numpy array 的 list。如果模型的輸出擁有名字，則可以傳入一個字典，將輸出名與其標籤對應起來。
- batch_size：整數，指定進行梯度下降時每個 batch 包含的樣本數。訓練時一個 batch 的樣本會被計算一次梯度下降，使目標函數優化一步。
- epochs：整數，訓練終止時的 epoch 值，訓練將在達到該 epoch 值時停止，當沒有設置 initial_epoch 時，它就是訓練的總輪數，否則訓練的總輪數為 epochs - inital_epoch
- verbose：日誌顯示，0為不在標準輸出流輸出日誌信息，1為輸出進度條記錄，2為每個 epoch 輸出一行記錄
- callbacks：list，其中的元素是 keras.callbacks.Callback 的對象。這個 list 中的回調函數將會在訓練過程中的適當時機被調用，參考回調函數
- validation_split：0~1 之間的浮點數，用來指定訓練集的一定比例數據作為驗證集。驗證集將不參與訓練，並在每個 epoch 結束後測試的模型的指標，如損失函數、精確度等。注意，validation_split 的劃分在 shuffle 之後，因此如果你的數據本身是有序的，需要先手工打亂再指定 validation_split，否則可能會出現驗證集樣本不均勻。
- validation_data：形式為（X，y）或（X，y，sample_weights）的 tuple，是指定的驗證集。此參數將覆蓋 validation_spilt。
- shuffle：布爾值，表示是否在訓練過程中每個 epoch 前隨機打亂輸入樣本的順序。
- class_weight：字典，將不同的類別映射為不同的權值，該參數用來在訓練過程中調整損失函數（只能用於訓練）。該參數在處理非平衡的訓練數據（某些類的訓練樣本數很少）時，可以使得損失函數對樣本數不足的數據更加關注。
- sample_weight：權值的 numpy array，用於在訓練時調整損失函數（僅用於訓練）。可以傳遞一個 1D 的與樣本等長的向量用於對樣本進行 1 對 1 的加權，或者在面對時序數據時，傳遞一個的形式為（samples，sequence_length）的矩陣來為每個時間步上的樣本賦不同的權。這種情況下請確定在編譯模型時添加了 sample_weight_mode='temporal'。
- initial_epoch: 從該參數指定的 epoch 開始訓練，在繼續之前的訓練時有用。
- steps_per_epoch: 一個 epoch 包含的步數（每一步是一個 batch 的數據送入），當使用如 TensorFlow 數據 Tensor 之類的輸入張量進行訓練時，預設的 None 代表自動分割，即數據集樣本數/batch 樣本數。
- validation_steps: 僅當 steps_per_epoch 被指定時有用，在驗證集上的 step 總數。

***** 建立、顯示訓練過程：show_train_history

上述過程包括 accuracy 及 loss 均儲存於 train_history 變數中，可以下列程式碼將其轉變為圖表：
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import matplotlib.pyplot as plt
  def show_train_history(train_history, train, validation):
      plt.plot(train_history.history[train])
      plt.plot(train_history.history[validation])
      plt.title('Train History')
      plt.ylabel(train)
      plt.xlabel('Epoch')
      plt.legend(['train','validation'], toc='upper left')
      plt.show()
#+END_SRC

***** 畫出 accuracy 執行結果
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  show_train_history(train_history,'train_acc','val_acc')
#+END_SRC

***** 完整執行結果
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import warnings
  import tensorflow as tf

  warnings.filterwarnings('ignore')
  from keras.utils import np_utils
  import numpy as np # 支援維度陣列之矩陣運算
  np.random.seed(10) #讓每次產生的亂數一致
  from keras.datasets import mnist
  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
  x_Train = x_train_image.reshape(60000, 784).astype('float32')
  x_Test = x_test_image.reshape(10000, 784).astype('float32')
  x_Train_normalize = x_Train/ 255
  x_Test_normalize = x_Test/ 255
  y_TrainOneHot = np_utils.to_categorical(y_train_label)
  y_TestOneHot = np_utils.to_categorical(y_test_label)
  from keras.models import Sequential
  from keras.layers import Dense
  model = Sequential()
  model.add(Dense(units=256, input_dim=784, kernel_initializer='normal', activation='relu'))
  model.add(Dense(units=10, kernel_initializer='normal', activation='softmax'))
  #===進行訓練===
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  train_history = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
                                validation_split=0.2, epochs=20, verbose=2)
  import matplotlib.pyplot as plt
  def show_train_history(train_history, train, validation):
      plt.plot(train_history.history[train])
      plt.plot(train_history.history[validation])
      plt.title('Train History')
      plt.ylabel(train)
      plt.xlabel('Epoch')
      plt.legend(['train','validation'], loc='upper left')
      #plt.show() # for jupyter notebook
      # 以下修改for console run
      img = plt.plot()
      return img

  # 以下的accuracy在Linux/Windows下要改為acc
  # 以下的val_accuracy在Linux/Windows下要改為val_acc
  # show_train_history(train_history, 'accuracy', 'val_accuracy')
  img = show_train_history(train_history, 'accuracy', 'val_accuracy')
  plt.savefig("Keras-MNist-Train-1.png")
  img = show_train_history(train_history, 'loss', 'val_loss')
  plt.savefig("Keras-MNist-Train-2.png") #
  # 以測試資料評估精確率
  # save
  print('test before save model: ', model.predict(x_Test[0:5]))

  scores = model.evaluate(x_Test_normalize, y_TestOneHot)
  print('accuracy',scores)
  print('accuracy',scores[1])

  model.save('Keras_MNist_model.h5')   # HDF5 file, you have to pip3 install h5py if don't have it
  del model  # deletes the existing model
#+END_SRC

#+RESULTS:
#+begin_example
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 9s - loss: 0.2697 - accuracy: 0.9225 - val_loss: 0.1326 - val_accuracy: 0.9611
Epoch 2/20
 - 8s - loss: 0.1075 - accuracy: 0.9683 - val_loss: 0.1038 - val_accuracy: 0.9694
Epoch 3/20
 - 8s - loss: 0.0710 - accuracy: 0.9777 - val_loss: 0.0929 - val_accuracy: 0.9726
Epoch 4/20
 - 8s - loss: 0.0513 - accuracy: 0.9843 - val_loss: 0.0826 - val_accuracy: 0.9762
Epoch 5/20
 - 8s - loss: 0.0377 - accuracy: 0.9882 - val_loss: 0.0786 - val_accuracy: 0.9757
Epoch 6/20
 - 8s - loss: 0.0262 - accuracy: 0.9920 - val_loss: 0.0790 - val_accuracy: 0.9787
Epoch 7/20
 - 8s - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.0863 - val_accuracy: 0.9770
Epoch 8/20
 - 8s - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.0873 - val_accuracy: 0.9779
Epoch 9/20
 - 8s - loss: 0.0138 - accuracy: 0.9957 - val_loss: 0.0984 - val_accuracy: 0.9753
Epoch 10/20
 - 8s - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.1103 - val_accuracy: 0.9750
Epoch 11/20
 - 8s - loss: 0.0114 - accuracy: 0.9960 - val_loss: 0.0920 - val_accuracy: 0.9787
Epoch 12/20
 - 8s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.1012 - val_accuracy: 0.9777
Epoch 13/20
 - 8s - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.1182 - val_accuracy: 0.9758
Epoch 14/20
 - 8s - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0993 - val_accuracy: 0.9799
Epoch 15/20
 - 8s - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.1104 - val_accuracy: 0.9786
Epoch 16/20
 - 8s - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.1036 - val_accuracy: 0.9797
Epoch 17/20
 - 8s - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.1288 - val_accuracy: 0.9767
Epoch 18/20
 - 8s - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.1279 - val_accuracy: 0.9773
Epoch 19/20
 - 9s - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.1226 - val_accuracy: 0.9763
Epoch 20/20
 - 9s - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.1365 - val_accuracy: 0.9772
test before save model:  [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]

   32/10000 [..............................] - ETA: 0s
 1664/10000 [===>..........................] - ETA: 0s
 3168/10000 [========>.....................] - ETA: 0s
 4832/10000 [=============>................] - ETA: 0s
 6432/10000 [==================>...........] - ETA: 0s
 8032/10000 [=======================>......] - ETA: 0s
 9760/10000 [============================>.] - ETA: 0s
10000/10000 [==============================] - 0s 31us/step
accuracy [0.11840327789002267, 0.9767000079154968]
accuracy 0.9767000079154968
#+end_example

#+CAPTION: Keras Mnist Model 訓練#1: accuracy
#+LABEL:fig:Keras-MNist-Train-1
#+name: fig:Keras-MNist-Train-1
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
#+RESULTS:
[[file:images/Keras-MNist-Train-1.png]]

圖[[fig:Keras-MNist-Train-1]]為執行 10 次週期後的預測精確度變化，可以看出以下現象：
1. 訓練與驗證的精確率均隨訓練週期增加而提高
1. 訓練精確度較驗證精確度高，原因是用來評估訓練精確率的資料已在訓練階段用過了；而用來評做驗證精確率的資料則否；這就類似，考試時考學過的練習題，學生得分較高。
1. 驗證精確率雖然低，但較符合現實情況，即，考試時考學生沒有做過的題目。
1. 如果訓練精確率持續增高，但驗證精率卻無法提升，可能是出現過度擬合(overfitting)的現象。,
#+CAPTION: Keras Mnist Model 訓練#1: lost function
#+LABEL:fig:Keras-MNist-Train-2
#+name: fig:Keras-MNist-Train-2
#+ATTR_LATEX: :width 300
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 300
#+RESULTS:
[[file:images/Keras-MNist-Train-2.png]]

由圖[[fig:Keras-MNist-Train-2]]亦可看出，訓練誤差與驗證誤差亦隨週期增加而降低，且訓練襄差最終低於驗證誤差。

訓練完成後，再以測試資料來評估模型準確率。

**** 進行預測

模型在訓練、驗證、測試後，即可以此訓練完之模型進行預測，預測方式如下：
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras.utils import np_utils
  from keras.datasets import mnist
  import matplotlib.pyplot as plt
  from keras.models import load_model

  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
  x_Train = x_train_image.reshape(60000, 784).astype('float32')
  x_Test = x_test_image.reshape(10000, 784).astype('float32')
  # Define func
  def plot_images_labels_prediction(imgname, images, labels, prediction, idx, num = 10):
        fig = plt.gcf()
        fig.set_size_inches(8,4) # 設定顯示圖形大小
        if num>25: num=25 # 顯示資料筆數上限
        for i in range(0, num):
            ax=plt.subplot(2,5,1+i) # 此處顯示10筆圖形，2*5個
            ax.imshow(images[idx], cmap='binary') # 畫圖
            title= "label=" +str(labels[idx]) # 加入子圖形title
            if len(prediction)>0:
                title+=",predict="+str(prediction[idx]) # 標題title加入預測結果
            ax.set_title(title,fontsize=10)
            ax.set_xticks([]);ax.set_yticks([]) #不顯示刻度
            idx+=1
        #plt.show()
        #plt.plot()
        plt.savefig(imgname+".png")
  # 載入儲存之模型
  model = load_model('Keras_MNist_model.h5')
  # 顯示340-349共10筆資料
  prediction = model.predict_classes(x_Test)
  plot_images_labels_prediction('Keras-MNist-Train-3',x_test_image, y_test_label,
                                prediction, idx=340)

#+END_SRC
#+CAPTION: Keras Mnist Model 訓練#1: prediction
#+LABEL:fig:Keras-MNist-Train-3
#+name: fig:Keras-Mnist-Train-3
#+ATTR_LATEX: :width 400
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 400
#+RESULTS:
[[file:images/Keras-MNist-Train-3.png]]
上述程式碼將訓練好後儲存的模型取出，拿 10 筆記錄去預測，發現第一筆有誤（真實值 label 為 5、預測值為 3）。

**** 顯示混淆矩陣(confusion matrix)

若想進一步得知哪些數字容易被混淆，可以使用混淆矩陣(confustion matrix, 也稱為誤差矩陣 error matrix)。實務上可以利用 pandas crosstab 來建立，程式碼如下：
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from keras.utils import np_utils
  from keras.datasets import mnist
  import matplotlib.pyplot as plt
  from keras.models import load_model

  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()

  x_Train = x_train_image.reshape(60000, 784).astype('float32')
  x_Test = x_test_image.reshape(10000, 784).astype('float32')

  model = load_model('Keras_MNist_model.h5')
  prediction = model.predict_classes(x_Test)

  import pandas as pd
  confuse = pd.crosstab(y_test_label, prediction, rownames=['label'], colnames=['predict'])
  print(confuse)
#+END_SRC

#+RESULTS:
#+begin_example
  predict    0     1     2    3    4    5    6     7    8    9
  label
  0        972     1     2    0    0    0    2     1    2    0
  1          0  1126     3    1    0    0    2     1    2    0
  2          7     1  1005    4    2    0    2     7    4    0
  3          1     0     1  999    0    3    0     3    2    1
  4          2     0     3    1  958    1    3     3    1   10
  5          5     0     1   29    1  842    5     3    5    1
  6          7     3     1    1    3    2  941     0    0    0
  7          2     3     5    3    1    0    0  1009    1    4
  8         10     0     4   10    2    1    3     5  938    1
  9          5     5     0    4    9    0    0    11    0  975
#+end_example

由輸出結果可以得知，5很容易被誤判為 3(共 29 次)。若想進一步得知到底有那幾張圖為上述狀況，則可以加入限制條件，程式碼如下：
#+BEGIN_SRC python -r -n :results output :exports both
  from keras.utils import np_utils
  from keras.datasets import mnist
  import matplotlib.pyplot as plt
  from keras.models import load_model
  import pandas as pd

  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
  x_Test = x_test_image.reshape(10000, 784).astype('float32')

  model = load_model('Keras_MNist_model.h5')
  prediction = model.predict_classes(x_Test)
  df = pd.DataFrame({'label': y_test_label, 'predict':prediction})
  print(df[(df.label==5)&(df.predict==3)])

  def plot_image(imgname, image):
      fig = plt.gcf() #定設圖形大小
      fig.set_size_inches(2,2)
      plt.imshow(image, cmap='binary') #cmap參數設定為binary以黑白灰階顯示
      # plt.show() #for jupyter or colab
      plt.plot()
      plt.savefig(imgname+".png")

  # 顯示第340筆資料
  plot_image('Keras-MNist-Train-4', x_test_image[340])

#+END_SRC

#+RESULTS:
#+begin_example
      label  predict
340       5        3
1003      5        3
1082      5        3
1393      5        3
2035      5        3
2291      5        3
2369      5        3
2526      5        3
2597      5        3
2604      5        3
2810      5        3
2970      5        3
3117      5        3
3414      5        3
4255      5        3
4271      5        3
4360      5        3
5874      5        3
5891      5        3
5913      5        3
5937      5        3
5972      5        3
5982      5        3
5985      5        3
6028      5        3
6042      5        3
6043      5        3
6598      5        3
9982      5        3
#+end_example
#+CAPTION: Keras Mnist Model 訓練#1: error sample
#+LABEL:fig:Keras-MNist-Train-4
#+name: fig:Keras-Mnist-Train-4
#+ATTR_LATEX: :width 50
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 50
[[file:images/Keras-MNist-Train-4.png]]

** 強化 MLP 辨識 solution #1: 增加隠藏層神經元數

為了增加 MLP 的準確率，其中一種方法可以增加隠藏層的神經元數至 1000 個，程式碼如下：
#+BEGIN_SRC python -r -n :results output :exports both eval: no
  from keras.utils import np_utils
  import numpy as np # 支援維度陣列之矩陣運算
  np.random.seed(10) #讓每次產生的亂數一致
  from keras.datasets import mnist
  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
  x_Train = x_train_image.reshape(60000, 784).astype('float32')
  x_Test = x_test_image.reshape(10000, 784).astype('float32')
  x_Train_normalize = x_Train/ 255
  x_Test_normalize = x_Test/ 255
  y_TrainOneHot = np_utils.to_categorical(y_train_label)
  y_TestOneHot = np_utils.to_categorical(y_test_label)
  from keras.models import Sequential
  from keras.layers import Dense
  model = Sequential()
  #=====增加神經元數
  model.add(Dense(units=1000, input_dim=784, kernel_initializer='normal', activation='relu'))
  model.add(Dense(units=10, kernel_initializer='normal', activation='softmax'))
  print(model.summary())
  #=====查看訓練過程及結果
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  train_history = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
                                validation_split=0.2, epochs=10, verbose=2)
  import matplotlib.pyplot as plt
  def show_train_history(train_history, train, validation):
      plt.plot(train_history.history[train])
      plt.plot(train_history.history[validation])
      plt.title('Train History')
      plt.ylabel(train)
      plt.xlabel('Epoch')
      plt.legend(['train','validation'], loc='upper left')
      # plt.show() # for jupyter notebook
      # 以下修改for console run
      img = plt.plot()
      return img

  # 以下的accuracy在Linux/Windows下要改為acc
  # 以下的val_accuracy在Linux/Windows下要改為val_acc
  # show_train_history(train_history, 'accuracy', 'val_accuracy')
  img = show_train_history(train_history, 'accuracy', 'val_accuracy')
  plt.savefig("Keras-MNist-Train-5.png")
  img = show_train_history(train_history, 'loss', 'val_loss')
  plt.savefig("Keras-MNist-Train-6.png") #
  # 以測試資料評估精確率
  scores = model.evaluate(x_Test_normalize, y_TestOneHot)
  print('accuracy',scores[1])

  model.save('Keras_MNist_model-1.h5')
  del model  # deletes the existing model
#+END_SRC

#+RESULTS:
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 1000)              785000
_________________________________________________________________
dense_2 (Dense)              (None, 10)                10010
=================================================================
Total params: 795,010
Trainable params: 795,010
Non-trainable params: 0
_________________________________________________________________
None
Train on 48000 samples, validate on 12000 samples
Epoch 1/10
 - 22s - loss: 0.2047 - accuracy: 0.9385 - val_loss: 0.1026 - val_accuracy: 0.9691
Epoch 2/10
 - 22s - loss: 0.0797 - accuracy: 0.9754 - val_loss: 0.0866 - val_accuracy: 0.9734
Epoch 3/10
 - 23s - loss: 0.0504 - accuracy: 0.9833 - val_loss: 0.0917 - val_accuracy: 0.9737
Epoch 4/10
 - 22s - loss: 0.0356 - accuracy: 0.9888 - val_loss: 0.0820 - val_accuracy: 0.9772
Epoch 5/10
 - 23s - loss: 0.0259 - accuracy: 0.9914 - val_loss: 0.0763 - val_accuracy: 0.9785
Epoch 6/10
 - 22s - loss: 0.0201 - accuracy: 0.9931 - val_loss: 0.0898 - val_accuracy: 0.9792
Epoch 7/10
 - 23s - loss: 0.0171 - accuracy: 0.9942 - val_loss: 0.0934 - val_accuracy: 0.9789
Epoch 8/10
 - 22s - loss: 0.0148 - accuracy: 0.9950 - val_loss: 0.0965 - val_accuracy: 0.9790
Epoch 9/10
 - 22s - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.1181 - val_accuracy: 0.9771
Epoch 10/10
 - 22s - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.1280 - val_accuracy: 0.9741

   32/10000 [..............................] - ETA: 0s
 1120/10000 [==>...........................] - ETA: 0s
 2112/10000 [=====>........................] - ETA: 0s
 3008/10000 [========>.....................] - ETA: 0s
 4000/10000 [===========>..................] - ETA: 0s
 5152/10000 [==============>...............] - ETA: 0s
 6208/10000 [=================>............] - ETA: 0s
 7296/10000 [====================>.........] - ETA: 0s
 8448/10000 [========================>.....] - ETA: 0s
 9568/10000 [===========================>..] - ETA: 0s
10000/10000 [==============================] - 0s 48us/step
accuracy 0.9760000109672546
#+end_example

#+CAPTION: Keras Mnist Model 訓練#2: accuracy
#+LABEL:fig:Keras-MNist-Train-5
#+name: fig:Keras-Mnist-Train-5
#+ATTR_LATEX: :width 400
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 400
[[file:images/Keras-MNist-Train-5.png]]
#+CAPTION: Keras Mnist Model 訓練#2: loss information
#+LABEL:fig:Keras-MNist-Train-6
#+name: fig:Keras-Mnist-Train-6
#+ATTR_LATEX: :width 400
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 400
[[file:images/Keras-MNist-Train-6.png]]

將神經元數增加至 1000 後，精確率由 0.97 提升至 0.98。但測驗準確率仍未提升，可見 overfitting 仍然嚴重。

** 強化 MLP 辨識 solution #2: 加入 DropOut 以避免 overfitting

為解決 Overfitting 問題，此處再加入 Dropout(0.5)指令，其功能為在每次訓練迭代時，會隨機地在隱藏層中放棄 50%的神經元，以避免 overfitting，程式碼如下：
#+BEGIN_SRC python -r -n :results output :exports both
  from keras.utils import np_utils
  import numpy as np # 支援維度陣列之矩陣運算

  np.random.seed(10) #讓每次產生的亂數一致
  from keras.datasets import mnist
  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
  x_Train = x_train_image.reshape(60000, 784).astype('float32')
  x_Test = x_test_image.reshape(10000, 784).astype('float32')
  x_Train_normalize = x_Train/ 255
  x_Test_normalize = x_Test/ 255
  y_TrainOneHot = np_utils.to_categorical(y_train_label)
  y_TestOneHot = np_utils.to_categorical(y_test_label)

  from keras.models import Sequential
  from keras.layers import Dense
  model = Sequential()
  #=====增加神經元數
  model.add(Dense(units=1000, input_dim=784, kernel_initializer='normal', activation='relu'))
  #=====加入Dropout
  from keras.layers import Dropout
  model.add(Dropout(0.5))
  #=====加入輸出層
  model.add(Dense(units=10, kernel_initializer='normal', activation='softmax'))
  print(model.summary())
  #=====訓練模型、查看結果
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  train_history = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
                                validation_split=0.2, epochs=10, verbose=2)
  import matplotlib.pyplot as plt
  def show_train_history(train_history, train, validation):
      plt.plot(train_history.history[train])
      plt.plot(train_history.history[validation])
      plt.title('Train History')
      plt.ylabel(train)
      plt.xlabel('Epoch')
      plt.legend(['train','validation'], loc='upper left')
      # plt.show() # for jupyter notebook
      # 以下修改for console run
      img = plt.plot()
      return img

  # 以下的accuracy在Linux/Windows下要改為acc
  # 以下的val_accuracy在Linux/Windows下要改為val_acc
  # show_train_history(train_history, 'accuracy', 'val_accuracy')
  img = show_train_history(train_history, 'accuracy', 'val_accuracy')
  plt.savefig("Keras-MNist-Train-7.png")
  img = show_train_history(train_history, 'loss', 'val_loss')
  plt.savefig("Keras-MNist-Train-8.png") #
  # 以測試資料評估精確率
  scores = model.evaluate(x_Test_normalize, y_TestOneHot)
  print('accuracy=',scores[1])
  # 進行預測
  prediction=model.predict_classes(x_Test)
  # 輸出confuse mqtrix
  import pandas as pd
  pd.crosstab(y_test_label,prediction,
              rownames=['label'],colnames=['predict'])
  # 儲存訓練好的模式
  model.save('Keras_MNist_model-2.h5')
  del model  # deletes the existing model
#+END_SRC

#+RESULTS:
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 1000)              785000
_________________________________________________________________
dropout_1 (Dropout)          (None, 1000)              0
_________________________________________________________________
dense_2 (Dense)              (None, 10)                10010
=================================================================
Total params: 795,010
Trainable params: 795,010
Non-trainable params: 0
_________________________________________________________________
None
Train on 48000 samples, validate on 12000 samples
Epoch 1/10
 - 25s - loss: 0.2673 - accuracy: 0.9171 - val_loss: 0.1145 - val_accuracy: 0.9659
Epoch 2/10
 - 25s - loss: 0.1319 - accuracy: 0.9581 - val_loss: 0.1046 - val_accuracy: 0.9689
Epoch 3/10
 - 26s - loss: 0.1008 - accuracy: 0.9686 - val_loss: 0.1004 - val_accuracy: 0.9690
Epoch 4/10
 - 26s - loss: 0.0884 - accuracy: 0.9719 - val_loss: 0.0826 - val_accuracy: 0.9763
Epoch 5/10
 - 26s - loss: 0.0764 - accuracy: 0.9752 - val_loss: 0.0858 - val_accuracy: 0.9763
Epoch 6/10
 - 25s - loss: 0.0703 - accuracy: 0.9779 - val_loss: 0.0826 - val_accuracy: 0.9779
Epoch 7/10
 - 25s - loss: 0.0617 - accuracy: 0.9811 - val_loss: 0.0849 - val_accuracy: 0.9772
Epoch 8/10
 - 25s - loss: 0.0597 - accuracy: 0.9817 - val_loss: 0.0816 - val_accuracy: 0.9795
Epoch 9/10
 - 26s - loss: 0.0529 - accuracy: 0.9829 - val_loss: 0.0878 - val_accuracy: 0.9787
Epoch 10/10
 - 28s - loss: 0.0516 - accuracy: 0.9838 - val_loss: 0.0833 - val_accuracy: 0.9805

   32/10000 [..............................] - ETA: 0s
 1088/10000 [==>...........................] - ETA: 0s
 2016/10000 [=====>........................] - ETA: 0s
 2848/10000 [=======>......................] - ETA: 0s
 3904/10000 [==========>...................] - ETA: 0s
 5024/10000 [==============>...............] - ETA: 0s
 6144/10000 [=================>............] - ETA: 0s
 7264/10000 [====================>.........] - ETA: 0s
 8384/10000 [========================>.....] - ETA: 0s
 9472/10000 [===========================>..] - ETA: 0s
10000/10000 [==============================] - 0s 48us/step
accuracy= 0.9815000295639038
#+end_example

由訓練過程可以看出，驗證精確率(0.9805)已接近訓練精確率(0.9838)，可見已改善了 overfitting 問題。

#+CAPTION: Keras Mnist Model 訓練#3: accuracy
#+LABEL:fig:Keras-MNist-Train-7
#+name: fig:Keras-MNist-Train-7
#+ATTR_LATEX: :width 400
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 400
[[file:images/Keras-MNist-Train-7.png]]
#+CAPTION: Keras Mnist Model 訓練#3: loss information
#+LABEL:fig:Keras-MNist-Train-8
#+name: fig:Keras-Mnist-Train-8
#+ATTR_LATEX: :width 400
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 400
[[file:images/Keras-MNist-Train-8.png]]

由圖[[fig:Keras-MNist-Train-7]]也可看出，驗證精確率已隨訓練週期提高，驗證誤差也隨訓練週期降低。

** 強化 MLP 辨識 solution #3: 增加隱藏層層數

本例 MLP 模型的預測能力仍有改善空間：增加隠藏層層數。以下程式碼將隠藏層數提高至 2 層：

#+BEGIN_SRC python -r -n :results output :exports both
  from keras.utils import np_utils
  import numpy as np # 支援維度陣列之矩陣運算

  np.random.seed(10) #讓每次產生的亂數一致
  from keras.datasets import mnist
  (x_train_image, y_train_label), (x_test_image, y_test_label) = mnist.load_data()
  x_Train = x_train_image.reshape(60000, 784).astype('float32')
  x_Test = x_test_image.reshape(10000, 784).astype('float32')
  x_Train_normalize = x_Train/ 255
  x_Test_normalize = x_Test/ 255
  y_TrainOneHot = np_utils.to_categorical(y_train_label)
  y_TestOneHot = np_utils.to_categorical(y_test_label)

  from keras.models import Sequential
  from keras.layers import Dense
  from keras.layers import Dropout

  model = Sequential()
  #=====增加第1層隱藏層神經元數
  model.add(Dense(units=1000, input_dim=784, kernel_initializer='normal', activation='relu'))
  #=====加入Dropout
  model.add(Dropout(0.5))
  #=====增加第2層隱藏層
  model.add(Dense(units=1000, kernel_initializer='normal', activation='relu'))
  #=====加入第2層Dropout
  model.add(Dropout(0.5))
  #=====加入輸出層
  model.add(Dense(units=10, kernel_initializer='normal', activation='softmax'))
  print(model.summary())
  #=====訓練模型、查看結果
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  train_history = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
                                validation_split=0.2, epochs=10, verbose=2)
  import matplotlib.pyplot as plt
  def show_train_history(train_history, train, validation):
      plt.plot(train_history.history[train])
      plt.plot(train_history.history[validation])
      plt.title('Train History')
      plt.ylabel(train)
      plt.xlabel('Epoch')
      plt.legend(['train','validation'], loc='upper left')
      # plt.show() # for jupyter notebook
      # 以下修改for console run
      img = plt.plot()
      return img

  # 以下的accuracy在Linux/Windows下要改為acc
  # 以下的val_accuracy在Linux/Windows下要改為val_acc
  # show_train_history(train_history, 'accuracy', 'val_accuracy')
  img = show_train_history(train_history, 'accuracy', 'val_accuracy')
  plt.savefig("Keras-MNist-Train-9.png")
  img = show_train_history(train_history, 'loss', 'val_loss')
  plt.savefig("Keras-MNist-Train-a.png") #
  # 以測試資料評估精確率
  scores = model.evaluate(x_Test_normalize, y_TestOneHot)
  print('accuracy=',scores[1])
  # 進行預測
  prediction=model.predict_classes(x_Test)
  # 輸出confuse mqtrix
  import pandas as pd
  pd.crosstab(y_test_label,prediction,
              rownames=['label'],colnames=['predict'])
  # 儲存訓練好的模式
  model.save('Keras_MNist_model-3.h5')
  del model  # deletes the existing model
#+END_SRC

#+RESULTS:
#+begin_example
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 1000)              785000
_________________________________________________________________
dropout_1 (Dropout)          (None, 1000)              0
_________________________________________________________________
dense_2 (Dense)              (None, 1000)              1001000
_________________________________________________________________
dropout_2 (Dropout)          (None, 1000)              0
_________________________________________________________________
dense_3 (Dense)              (None, 10)                10010
=================================================================
Total params: 1,796,010
Trainable params: 1,796,010
Non-trainable params: 0
_________________________________________________________________
None
Train on 48000 samples, validate on 12000 samples
Epoch 1/10
 - 51s - loss: 0.3197 - accuracy: 0.9040 - val_loss: 0.1234 - val_accuracy: 0.9613
Epoch 2/10
 - 53s - loss: 0.1753 - accuracy: 0.9463 - val_loss: 0.1277 - val_accuracy: 0.9635
Epoch 3/10
 - 50s - loss: 0.1493 - accuracy: 0.9557 - val_loss: 0.0965 - val_accuracy: 0.9725
Epoch 4/10
 - 49s - loss: 0.1321 - accuracy: 0.9616 - val_loss: 0.1069 - val_accuracy: 0.9693
Epoch 5/10
 - 50s - loss: 0.1265 - accuracy: 0.9632 - val_loss: 0.1008 - val_accuracy: 0.9714
Epoch 6/10
 - 50s - loss: 0.1204 - accuracy: 0.9658 - val_loss: 0.0802 - val_accuracy: 0.9772
Epoch 7/10
 - 50s - loss: 0.1086 - accuracy: 0.9681 - val_loss: 0.0860 - val_accuracy: 0.9768
Epoch 8/10
 - 50s - loss: 0.1018 - accuracy: 0.9709 - val_loss: 0.0904 - val_accuracy: 0.9755
Epoch 9/10
 - 50s - loss: 0.1055 - accuracy: 0.9708 - val_loss: 0.0878 - val_accuracy: 0.9768
Epoch 10/10
 - 50s - loss: 0.0973 - accuracy: 0.9739 - val_loss: 0.0886 - val_accuracy: 0.9771

   32/10000 [..............................] - ETA: 0s
  768/10000 [=>............................] - ETA: 0s
 1472/10000 [===>..........................] - ETA: 0s
 2016/10000 [=====>........................] - ETA: 0s
 2656/10000 [======>.......................] - ETA: 0s
 3360/10000 [=========>....................] - ETA: 0s
 4032/10000 [===========>..................] - ETA: 0s
 4736/10000 [=============>................] - ETA: 0s
 5472/10000 [===============>..............] - ETA: 0s
 6176/10000 [=================>............] - ETA: 0s
 6912/10000 [===================>..........] - ETA: 0s
 7648/10000 [=====================>........] - ETA: 0s
 8384/10000 [========================>.....] - ETA: 0s
 9120/10000 [==========================>...] - ETA: 0s
 9856/10000 [============================>.] - ETA: 0s
10000/10000 [==============================] - 1s 73us/step
accuracy= 0.9793999791145325
#+end_example

#+CAPTION: Keras Mnist Model 訓練#3: accuracy
#+LABEL:fig:Keras-MNist-Train-9
#+name: fig:Keras-MNist-Train-9
#+ATTR_LATEX: :width 400
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 400
[[file:images/Keras-MNist-Train-9.png]]
#+CAPTION: Keras Mnist Model 訓練#3: loss information
#+LABEL:fig:Keras-MNist-Train-a
#+name: fig:Keras-MNist-Train-a
#+ATTR_LATEX: :width 400
#+ATTR_HTML: :width 400
#+ATTR_ORG: :width 400
[[file:images/Keras-MNist-Train-a.png]]

由訓練成果判斷，雖然精確率並未提升，但是由圖[[fig:Keras-MNist-Train-9]]可以看出驗證精確率已經比訓練精確率高，已確實可以解決 overfitting 的問題。

隨著 MLP 模型的改進，雖然精確率可逐步提升，也可藉由加入 Dropout 解決 overfitting 的問題，但 MLP 仍有其極限，如果要進一步提升準確率，就要使用卷積神經網路 CNN (convolutional neural network)。

#+latex:\newpage

* 深度學習的高速化
由於大數據與大型網路的關係，使得深度學習必須進行大量運算，過去我們使用 CPU 來進行運算，如今多數深度學習的框架多支援 GPU，甚至支援以多個 GPU 與多台裝置進行分散式學習。GPU 原本是圖形專用處理器，可以快速處理平行運算，GPU 運算的目標是把其強大的效能運用在各種用途。比較 CPU 與 GPU 在 AlexNet 的學習，CPU 需花費 40 天以上，GPU 則可以在 6 天內完成。

利用 GPU 除了可以大幅提升深度學習的運算速度，但是一旦變成多層網路時，就需要花費數天或數週的時間來學習，Google 的 TensorFlow、Microsoft 的 CNTK 便是針對分散式學習來開發的，100 個分散式的 GPU 可以提升比單一 GPU 高到 56 倍的速度，意味著原本要有天才能完成的學習，只要 3 小時就可以結束。

在深度學習的高速化過程中，包含運算量在內，記憶體容量、匯流排頻寬等，都會造成瓶頸，就記憶體容量來說，必須考慮到大量權重參數及中間資料會儲存在記憶體的情況。至於匯流排頻寛，一旦通過 GPU(或 CPU)的匯流排資料超過一定的限制，該處就會形成瓶頸，所以，最好能儘量減少通過網路的資料位元數。

** GPU v.s. CPU

- CPU 是由幾個每次可處理數個獨立「執行緒」(threads)的核心(core)所組成；GPU 則有數百個這樣的核心，同時可以處理上千個執行緒
- CPU 主要是線性執行； GPU 則是個高度平行化的單元
- CPU 的發展主要致力於最佳化系統的遲滯時間，讓系統能有迅速流暢的反應；GPU 的發展則是朝頻寬最佳化努力。在深度神經網路中，頻寬為主要的系統瓶頸
- GPU 的 Level 1 cache 比 CPU 快且大，在深度神經網路中，大部份的資料都會再次被使用到

* 深度學習的未來方向
對於大多數深度學習實作者，推動深度學習的最佳途徑並不是創造出更高級的最佳化演算法，相反的，過去幾十年來絕大多數深度學習的突破，都是因為發現了更容易訓練的架構，而不是因為與那些討厭的誤差曲面搏鬥所得到的成果。

* Footnotes

[fn:1] [[https://kknews.cc/zh-tw/tech/b4zkbom.html][主流的深度學習模型有哪些？]]

[fn:2][[https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-09-l1l2regularization/][L1 / L2 正規化]]

[fn:3][[https://ithelp.ithome.com.tw/articles/10219648?sc=rss.iron][Google ML課程筆記 - Overfitting 與 L1 /L2 Regularization ]]

[fn:4][[https://www.itread01.com/content/1549579879.html][機器學習十大演算法---8. 隨機森林演算法]]

[fn:5] Goodfello, Ian J., Oriol Vinyals & Andrew M. Saxe, Qualitatively characterizing neural ntwork optimization problems, arXiv preprint arXiv: 1412.6544 (2014).

[fn:6][[https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E7%B5%B1%E8%A8%88%E5%AD%B8%E7%BF%92-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-principle-component-analysis-pca-58229cd26e71][機器/統計學習:主成分分析(Principal Component Analysis, PCA)]]

[fn:7][[https://blog.csdn.net/dongyanwen6036/article/details/78311071][LDA與PCA都是常用的降維方法，二者的區別]]

[fn:8][[https://medium.com/yiyi-network/transfer-learning-1f87d4f1886f][Kaggle Learn | Deep Learning 深度學習 | 學習資源介紹 (Part 2)]]

[fn:9][[https://keras.io/zh/getting-started/sequential-model-guide/][Sequential 順序模型指引]]

[fn:10][[https://keras-cn.readthedocs.io/en/latest/models/model/][函數式模型接口]]
