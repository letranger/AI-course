<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-07-05 Tue 09:42 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>深度學習(Deep Learning)</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/white.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">深度學習(Deep Learning)</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgc08e38e">1. 深度學習</a></li>
<li><a href="#org04e2dcf">2. 建構良好的訓練集：數據預處理</a>
<ul>
<li><a href="#orgb958782">2.1. 處理數據遺漏</a></li>
<li><a href="#org344d18d">2.2. 填補遺遺漏值</a></li>
<li><a href="#org7d27101">2.3. 處理數據中的分類特徵編碼問題</a></li>
<li><a href="#org21e9247">2.4. 訓練集與測試集的數據分割</a></li>
<li><a href="#orgc22e003">2.5. 縮放特徵值、維持特徵值影響比例：正規化(normalization)</a></li>
<li><a href="#org3812063">2.6. 選取有意義的特徵</a></li>
<li><a href="#org63dee7d">2.7. 循序特徵選擇法</a></li>
<li><a href="#orgfaf9c81">2.8. 以隨機森林評估特徵的重要性</a></li>
</ul>
</li>
<li><a href="#org4265b8a">3. 深度學習運作原理</a>
<ul>
<li><a href="#orge0ba937">3.1. Layer, 損失函數與優化器</a></li>
<li><a href="#org4b9a524">3.2. 梯度</a></li>
<li><a href="#org369d332">3.3. 最佳選擇</a></li>
</ul>
</li>
<li><a href="#org02f698e">4. 降維來壓縮數據</a>
<ul>
<li><a href="#orgb538917">4.1. 以主成份分析(PCA)對非監督式數據壓縮</a></li>
<li><a href="#org2365a00">4.2. 利用線性判別分析(LDA)做監督式數據壓縮</a></li>
<li><a href="#orgb37e821">4.3. <span class="todo TODO">TODO</span> 利用核主成份分析(KPCA)處理非線性對應</a></li>
<li><a href="#org2703aa5">4.4. 相關資源</a></li>
</ul>
</li>
<li><a href="#orgacf3768">5. 深度學習應用領域</a>
<ul>
<li><a href="#org7b701a5">5.1. 影像辨識</a></li>
<li><a href="#orgc61f295">5.2. 語言模型</a></li>
<li><a href="#org21f54f8">5.3. 棋盤遊戲</a></li>
<li><a href="#orgd802e2d">5.4. 電腦遊戲</a></li>
<li><a href="#org2079bb0">5.5. 異常偵測</a></li>
<li><a href="#org6f83984">5.6. 物體偵測</a></li>
<li><a href="#orgf3b190c">5.7. 影像分割</a></li>
<li><a href="#org976b46c">5.8. 產生圖說</a></li>
<li><a href="#orga0d90c8">5.9. 影像風格轉換</a></li>
<li><a href="#org5b9061d">5.10. 產生影像</a></li>
<li><a href="#org2ef3c85">5.11. 自動駕駛</a></li>
<li><a href="#org1f83acc">5.12. Deep Q-Network (強化學習)</a></li>
</ul>
</li>
<li><a href="#orgb5d2713">6. 深度學習的類型</a>
<ul>
<li><a href="#orgbd21b5f">6.1. VGG</a></li>
<li><a href="#org53b6cc0">6.2. GoodLeNet</a></li>
<li><a href="#org5f0bca3">6.3. ResNet</a></li>
</ul>
</li>
<li><a href="#orgc03fb24">7. 實作範例</a>
<ul>
<li><a href="#org8bba0c6">7.1. 以 Keras 解決分類問題</a></li>
<li><a href="#org109c4c7">7.2. 以 Keras 解決迴歸問題：預測房價: Boston</a></li>
<li><a href="#orgdaf7752">7.3. 以神經網路重跑鳶尾花問題</a></li>
</ul>
</li>
<li><a href="#org3c206fb">8. 以少量資料集實做 CNN</a>
<ul>
<li><a href="#org853f458">8.1. 深度學習與少量資料的相關性</a></li>
<li><a href="#orgddd0eda">8.2. 實作</a></li>
<li><a href="#org615c1dd">8.3. 改善#1: 使用資料擴增法(data augmentation)</a></li>
<li><a href="#orgfee191f">8.4. 改善 2: 使用 pretrained network</a></li>
</ul>
</li>
<li><a href="#org05a6222">9. 視覺化呈現 CNN 的學習內容</a>
<ul>
<li><a href="#orgeb7f78b">9.1. 中間層輸出視覺化</a></li>
<li><a href="#orgc8d1763">9.2. 視覺化 convnet 的 filter</a></li>
<li><a href="#orga8353a6">9.3. 視覺化類別激活熱圖 heatmap of class activation</a></li>
</ul>
</li>
<li><a href="#orgac6f2a7">10. MLP 神經網路模型實作：以 Keras 為實作工具</a>
<ul>
<li><a href="#orgd38e85c">10.1. 簡介</a></li>
<li><a href="#org252149f">10.2. Keras 程式設計模式</a></li>
<li><a href="#org3b1fe28">10.3. 基本流程</a></li>
<li><a href="#org35c6c58">10.4. 以 Keras 實作 MNist 手寫數字辨識資料集</a></li>
<li><a href="#orge1c4529">10.5. 強化 MLP 辨識 solution #1: 增加隠藏層神經元數</a></li>
<li><a href="#org15b9c2c">10.6. 強化 MLP 辨識 solution #2: 加入 DropOut 以避免 overfitting</a></li>
<li><a href="#org64f7ca3">10.7. 強化 MLP 辨識 solution #3: 增加隱藏層層數</a></li>
</ul>
</li>
<li><a href="#orgbf30f89">11. 深度學習的高速化</a>
<ul>
<li><a href="#org039b8cb">11.1. GPU v.s. CPU</a></li>
</ul>
</li>
<li><a href="#org4725c69">12. 深度學習的未來方向</a></li>
</ul>
</div>
</div>

<div id="outline-container-orgc08e38e" class="outline-2">
<h2 id="orgc08e38e"><span class="section-number-2">1.</span> 深度學習</h2>
<div class="outline-text-2" id="text-1">
<p>
神經網絡的基礎模型是感知機(Perceptron)，因此神經網絡也可以叫做多層感知機(Multi-layer Perceptron)，簡稱MLP。單層感知機叫做感知機，多層感知機(MLP)≈人工神經網絡(ANN)<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。
</p>

<p>
那麼多層到底是幾層？一般來說有1-2個隱藏層的神經網絡就可以叫做多層，準確的說是(淺層)神經網絡(Shallow Neural Networks)。隨著隱藏層的增多，更深的神經網絡(一般來說超過5層)就都叫做深度學習(DNN)<sup><a id="fnr.1.100" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。
</p>

<p>
然而，「深度」只是一個商業概念，很多時候工業界把3層隱藏層也叫做「深度學習」，所以不要在層數上太較真。在機器學習領域的約定俗成是，名字中有深度(Deep)的網絡僅代表其有超過5-7層的隱藏層<sup><a id="fnr.1.100" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。
</p>

<p>
深度學習是加深層數後的多層神經網路。MNIST 歷年的競賽前幾名都是以 CNN 為基礎，進一步提高辨識準確率的方法還包括整體學習、學習率遞減（learning rate decay）、資料擴增（Data Augmentation, 如利用旋轉、垂直或水平移動輸入影像來小幅改變輸入資料以增加輸入影像張數）。
</p>

<p>
關於增加層數的重要性，目前還缺乏理論佐證，但從過往的研究或實驗中，有幾點可以說明。
</p>
<ol class="org-ol">
<li>在 ILSVRC 這種大型視覺辨識競賽結果中，加深層數的比例多與辨識效能成正比。</li>
<li>加深層數可以在減少網路參數的狀況下得到相同成效，透過重叠層級，可以讓 ReLU 等活化函數夾在卷積層之間，進一步提高網路的表現力，因為透過活化函數，可以在網路增加「非線性」的能力，重叠非線性性函數，也能達到更複雜的表現力。</li>
<li>學習的效率也是加深層數的優點之一，卷積層的神經元會反應出邊界等單純形狀，隨著層數增加，可以反應出紋理、物體部位等特質，依照階層逐漸變複雜。</li>
<li>以辨識「狗」為例子，如果要以層數較少的網路來解決這個問題，卷積層就要一次「理解」眾多特徵，還要因應不同拍攝環境帶來的變化，一次處理這些龐大的資料會花費許多學習時間； 如果加深層數，就能用階層分解必須學習的問題，每一層可以處理單純的問題，例如，最初的層級可以只學習邊界，利用少量的學習資料來進行效率化的學習。</li>
<li>加深層數可以階層性的傳遞資料，例如，擷取出邊界的下一層會使用邊界資料來學習更高階的問題（如判斷形狀）。</li>
</ol>

<p>
典型的深度學習如圖<a href="#orgec75a23">1</a>，在此例中，輸入為一張手寫數字的影像，經由 4 層的深度學習模型後得知此數字為 4。
</p>


<div id="orgec75a23" class="figure">
<p><img src="images/img-191107113927.jpg" alt="img-191107113927.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 1: </span>典型的深度神經網路-1</p>
</div>

<p>
圖<a href="#org1622680">2</a>進一步說明網路模型中每一層的作用，可以將每一層網路視為對影像的特殊運算，如此一層一層逐一精煉(purified)，最後得到結果。
</p>


<div id="org1622680" class="figure">
<p><img src="images/img-1911071139277.jpg" alt="img-1911071139277.jpg" width="500" />
</p>
<p><span class="figure-number">Figure 2: </span>典型的深度神經網路-2</p>
</div>
</div>
</div>

<div id="outline-container-org04e2dcf" class="outline-2">
<h2 id="org04e2dcf"><span class="section-number-2">2.</span> 建構良好的訓練集：數據預處理</h2>
<div class="outline-text-2" id="text-2">
<p>
進行數運模式運算之前，需要進行的數據預處理工作大致可分為以下幾點：
</p>
<ol class="org-ol">
<li>數據遺漏值處理</li>
<li>數據分類編碼</li>
<li>數據訓練集與測試集之分割</li>
<li>數據特徵選取</li>
</ol>
</div>

<div id="outline-container-orgb958782" class="outline-3">
<h3 id="orgb958782"><span class="section-number-3">2.1.</span> 處理數據遺漏</h3>
<div class="outline-text-3" id="text-2-1">
<p>
現實世界中可能會因各種原因導致數據缺失或遺漏(如問卷被刻意留白)，這些部份通常會以「空白」、「NaN」或「NULL」來取代。
</p>
</div>

<div id="outline-container-orgd95be72" class="outline-4">
<h4 id="orgd95be72"><span class="section-number-4">2.1.1.</span> 遺漏值的識別</h4>
<div class="outline-text-4" id="text-2-1-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #dcaeea;">csv_data</span> = <span style="color: #98be65;">'''A,X,B,C,D</span>
<span class="linenr"> 2: </span><span style="color: #98be65;">  1.0,,2.0,3.0,4.0</span>
<span class="linenr"> 3: </span><span style="color: #98be65;">  5.0,,6.0,,8.0</span>
<span class="linenr"> 4: </span><span style="color: #98be65;">  10.0,,11.0,12.0</span>
<span class="linenr"> 5: </span><span style="color: #98be65;">  ,,,,'''</span>
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> sys
<span class="linenr"> 7: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">python 2.7&#38656;&#36914;&#34892;unicode&#36681;&#30908;</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">if</span> (sys.version_info &lt; (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">0</span>)):
<span class="linenr">10: </span>      <span style="color: #dcaeea;">csv_data</span> = <span style="color: #c678dd;">unicode</span>(csv_data)
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#31243;&#24335;&#27284;&#20013;&#30340;csv&#36039;&#26009;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> io <span style="color: #51afef;">import</span> StringIO
<span class="linenr">13: </span>  <span style="color: #dcaeea;">df</span> = pd.read_csv(StringIO(csv_data))
<span class="linenr">14: </span>  <span style="color: #c678dd;">print</span>(df)
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21015;&#20986;&#27599;&#34892;&#26377;&#30340;null&#20491;&#25976;</span>
<span class="linenr">16: </span>  <span style="color: #c678dd;">print</span>(df.isnull().<span style="color: #c678dd;">sum</span>())
<span class="linenr">17: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">access the underlying NumPy array</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">via the `values` attribute</span>
<span class="linenr">19: </span>  df.values
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21083;&#38500;&#26377;&#36986;&#22833;&#20540;&#30340;&#36039;&#26009;&#21015;</span>
<span class="linenr">22: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#21034;&#25481;&#26377;&#36986;&#22833;&#20540;&#30340;&#21015;:df.dropna(axis=1)'</span>)
<span class="linenr">23: </span>  <span style="color: #c678dd;">print</span>(df.dropna(axis=<span style="color: #da8548; font-weight: bold;">0</span>))
<span class="linenr">24: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21083;&#38500;&#26377;&#36986;&#22833;&#20540;&#30340;&#36039;&#26009;&#34892;</span>
<span class="linenr">25: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#21034;&#25481;&#26377;&#36986;&#22833;&#20540;&#30340;&#34892;:df.dropna(axis=1)'</span>)
<span class="linenr">26: </span>  <span style="color: #c678dd;">print</span>(df.dropna(axis=<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">27: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21083;&#38500;&#25972;&#21015;&#28858;NaN&#32773;</span>
<span class="linenr">28: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#21083;&#38500;&#25972;&#34892;&#28858;NaN&#32773;:df.dropna(how=\'all\')'</span>)
<span class="linenr">29: </span>  <span style="color: #c678dd;">print</span>(df.dropna(how=<span style="color: #98be65;">'all'</span>) )
<span class="linenr">30: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21034;&#38500;&#26377;&#20540;&#20491;&#25976;&#20302;&#26044;thresh&#30340;&#21015;</span>
<span class="linenr">31: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#21034;&#38500;&#26377;&#20540;&#20491;&#25976;&#20302;&#26044;thresh&#30340;&#21015;:df.dropna(thresh=4)'</span>)
<span class="linenr">32: </span>  <span style="color: #c678dd;">print</span>(df.dropna(thresh=<span style="color: #da8548; font-weight: bold;">4</span>))
<span class="linenr">33: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21034;&#38500;&#29305;&#23450;&#34892;(&#22914;&#31532;C&#34892;)&#20013;&#26377;NaN&#20043;&#21015;</span>
<span class="linenr">34: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#21034;&#38500;&#29305;&#23450;&#34892;(&#22914;&#31532;C&#34892;)&#20013;&#26377;NaN&#20043;&#21015;:df.dropna(subset=[\'C\'])'</span>)
<span class="linenr">35: </span>  <span style="color: #c678dd;">print</span>(df.dropna(subset=[<span style="color: #98be65;">'C'</span>]))
</pre>
</div>

<pre class="example" id="orgf0656da">
      A   X     B     C    D
0   1.0 NaN   2.0   3.0  4.0
1   5.0 NaN   6.0   NaN  8.0
2  10.0 NaN  11.0  12.0  NaN
3   NaN NaN   NaN   NaN  NaN
A    1
X    4
B    1
C    2
D    2
dtype: int64
刪掉有遺失值的列:df.dropna(axis=1)
Empty DataFrame
Columns: [A, X, B, C, D]
Index: []
刪掉有遺失值的行:df.dropna(axis=1)
Empty DataFrame
Columns: []
Index: [0, 1, 2, 3]
剛除整行為NaN者:df.dropna(how='all')
      A   X     B     C    D
0   1.0 NaN   2.0   3.0  4.0
1   5.0 NaN   6.0   NaN  8.0
2  10.0 NaN  11.0  12.0  NaN
刪除有值個數低於thresh的列:df.dropna(thresh=4)
     A   X    B    C    D
0  1.0 NaN  2.0  3.0  4.0
刪除特定行(如第C行)中有NaN之列:df.dropna(subset=['C'])
      A   X     B     C    D
0   1.0 NaN   2.0   3.0  4.0
2  10.0 NaN  11.0  12.0  NaN
</pre>

<p>
雖然刪除包含遺漏值的數據似乎是個方便的方法，但終究可能會刪除過多的樣本，導致分析的結果並不可靠；或是因為刪除了特徵的時候，卻失去了重要的資訊。
</p>
</div>
</div>
</div>

<div id="outline-container-org344d18d" class="outline-3">
<h3 id="org344d18d"><span class="section-number-3">2.2.</span> 填補遺遺漏值</h3>
<div class="outline-text-3" id="text-2-2">
<p>
最常見的「插補技術」之一為「平均插補」(mean imputation)，即，以整個特徵行的平均值來代替遺漏值。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #dcaeea;">csv_data</span> = <span style="color: #98be65;">'''A,X,B,C,D</span>
<span class="linenr"> 2: </span><span style="color: #98be65;">  1.0,,2.0,3.0,4.0</span>
<span class="linenr"> 3: </span><span style="color: #98be65;">  5.0,,6.0,,8.0</span>
<span class="linenr"> 4: </span><span style="color: #98be65;">  10.0,,11.0,12.0</span>
<span class="linenr"> 5: </span><span style="color: #98be65;">  ,,,,'''</span>
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> sys
<span class="linenr"> 7: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">python 2.7&#38656;&#36914;&#34892;unicode&#36681;&#30908;</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">if</span> (sys.version_info &lt; (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">0</span>)):
<span class="linenr">10: </span>      <span style="color: #dcaeea;">csv_data</span> = <span style="color: #c678dd;">unicode</span>(csv_data)
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#31243;&#24335;&#27284;&#20013;&#30340;csv&#36039;&#26009;</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> io <span style="color: #51afef;">import</span> StringIO
<span class="linenr">13: </span>  <span style="color: #dcaeea;">df</span> = pd.read_csv(StringIO(csv_data))
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">impute missing values via the column mean</span>
<span class="linenr">16: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> Imputer
<span class="linenr">17: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">axis=0: &#20197;&#34892;&#30340;&#24179;&#22343;&#20540;&#20358;&#35036;</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">axis=1: &#20197;&#21015;&#30340;&#24179;&#22343;&#20540;&#20358;&#35036;</span>
<span class="linenr">19: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">strategy&#30340;&#36984;&#38917;&#26377;: median(&#20013;&#20301;&#25976;)&#12289;most_freqent(&#26368;&#38971;&#32321;&#20986;&#29694;&#32773;)</span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">most_freqent&#22312;&#20570;&#28858;&#20998;&#39006;&#29305;&#24501;&#26178;&#24456;&#26377;&#29992;</span>
<span class="linenr">21: </span>  imr = Imputer(missing_values=<span style="color: #98be65;">'NaN'</span>, strategy=<span style="color: #98be65;">'mean'</span>, axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">22: </span>  imr = imr.fit(df.values)
<span class="linenr">23: </span>  imputed_data = imr.transform(df.values)
<span class="linenr">24: </span>  <span style="color: #c678dd;">print</span>(df)
<span class="linenr">25: </span>  <span style="color: #c678dd;">print</span>(imputed_data)
</pre>
</div>

<pre class="example">
      A   X     B     C    D
0   1.0 NaN   2.0   3.0  4.0
1   5.0 NaN   6.0   NaN  8.0
2  10.0 NaN  11.0  12.0  NaN
3   NaN NaN   NaN   NaN  NaN
[[ 1.          2.          3.          4.        ]
 [ 5.          6.          7.5         8.        ]
 [10.         11.         12.          6.        ]
 [ 5.33333333  6.33333333  7.5         6.        ]]
</pre>


<p>
Imputer 類別在 scikit-learn 中屬於 transformer 類別，主要的工作是做「數據轉換」，這些 estimator 有兩種基本方法：fit 與 transform，fit 方法是用來進行參數學習。
</p>
</div>
</div>

<div id="outline-container-org7d27101" class="outline-3">
<h3 id="org7d27101"><span class="section-number-3">2.3.</span> 處理數據中的分類特徵編碼問題</h3>
<div class="outline-text-3" id="text-2-3">
</div>
<div id="outline-container-orgcf7a7a5" class="outline-4">
<h4 id="orgcf7a7a5"><span class="section-number-4">2.3.1.</span> categorical feature</h4>
<div class="outline-text-4" id="text-2-3-1">
<p>
真實世界的數據集往往包含各種「類別特徵」(categorical feature)，類別特徵可再分為
</p>
<ul class="org-ul">
<li>nominal feature: 名義特徵</li>
<li>ordinal feature: 次序特徵</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">2: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame([[<span style="color: #98be65;">'green'</span>, <span style="color: #98be65;">'M'</span>, <span style="color: #da8548; font-weight: bold;">10.1</span>, <span style="color: #98be65;">'class2'</span>],
<span class="linenr">3: </span>                     [<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'L'</span>, <span style="color: #da8548; font-weight: bold;">13.5</span>, <span style="color: #98be65;">'class1'</span>],
<span class="linenr">4: </span>                     [<span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'XL'</span>, <span style="color: #da8548; font-weight: bold;">15.3</span>, <span style="color: #98be65;">'class2'</span>]])
<span class="linenr">5: </span>
<span class="linenr">6: </span>  df.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]
<span class="linenr">7: </span>  <span style="color: #c678dd;">print</span>(df)
</pre>
</div>

<pre class="example">
   color size  price classlabel
0  green    M   10.1     class2
1    red    L   13.5     class1
2   blue   XL   15.3     class2
</pre>
</div>
</div>

<div id="outline-container-org2022ed4" class="outline-4">
<h4 id="org2022ed4"><span class="section-number-4">2.3.2.</span> 對應 ordinal feature</h4>
<div class="outline-text-4" id="text-2-3-2">
<p>
自定一個 mapping dictionary，即 size_mapping，然後將 classlabel 對應到 size_mapping 中的鍵值(程式第<a href="#coderef-sizeMapping" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-sizeMapping');" onmouseout="CodeHighlightOff(this, 'coderef-sizeMapping');">11</a>行)。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame([[<span style="color: #98be65;">'green'</span>, <span style="color: #98be65;">'M'</span>, <span style="color: #da8548; font-weight: bold;">10.1</span>, <span style="color: #98be65;">'class2'</span>],
<span class="linenr"> 3: </span>                     [<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'L'</span>, <span style="color: #da8548; font-weight: bold;">13.5</span>, <span style="color: #98be65;">'class1'</span>],
<span class="linenr"> 4: </span>                     [<span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'XL'</span>, <span style="color: #da8548; font-weight: bold;">15.3</span>, <span style="color: #98be65;">'class2'</span>]])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  df.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]
<span class="linenr"> 7: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">Mapping ordinal features</span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">size_mapping</span> = {<span style="color: #98be65;">'XL'</span>: <span style="color: #da8548; font-weight: bold;">3</span>,
<span class="linenr"> 9: </span>                  <span style="color: #98be65;">'L'</span>: <span style="color: #da8548; font-weight: bold;">2</span>,
<span class="linenr">10: </span>                  <span style="color: #98be65;">'M'</span>: <span style="color: #da8548; font-weight: bold;">1</span>}
<span id="coderef-sizeMapping" class="coderef-off"><span class="linenr">11: </span>  <span style="color: #dcaeea;">df</span>[<span style="color: #98be65;">'size'</span>] = df[<span style="color: #98be65;">'size'</span>].<span style="color: #c678dd;">map</span>(size_mapping)</span>
<span class="linenr">12: </span>  <span style="color: #c678dd;">print</span>(df)
</pre>
</div>

<pre class="example">
   color  size  price classlabel
0  green     1   10.1     class2
1    red     2   13.5     class1
2   blue     3   15.3     class2
</pre>
</div>
</div>

<div id="outline-container-org53ec58a" class="outline-4">
<h4 id="org53ec58a"><span class="section-number-4">2.3.3.</span> 對應 nominal feature</h4>
<div class="outline-text-4" id="text-2-3-3">
<p>
許多機器學習的函式庫需要將「類別標籤」編碼為整數值。方法之一是以列舉方式為這些 nominal features 自 0 開始編號，先以 enumerate 方式建立一個 mapping dictionary: class_mapping(程式第<a href="#coderef-classMapping" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-classMapping');" onmouseout="CodeHighlightOff(this, 'coderef-classMapping');">10</a>行)，然後利用這個字典將類別特徵轉換為整數值。
</p>

<p>
此外，也可以利用已產生的對應字典，藉由借調 key-value 來產生「反轉字典」(第<a href="#coderef-invClassMapping" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-invClassMapping');" onmouseout="CodeHighlightOff(this, 'coderef-invClassMapping');">18</a>行)，將對調產生的整數還原回原始類別特徵。
</p>

<p>
scikit-learn 中有一個更為方便的 LabelEncoder 類別則可以直接完成上述工作(第<a href="#coderef-labelEncoder" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-labelEncoder');" onmouseout="CodeHighlightOff(this, 'coderef-labelEncoder');">25</a>行)。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame([[<span style="color: #98be65;">'green'</span>, <span style="color: #98be65;">'M'</span>, <span style="color: #da8548; font-weight: bold;">10.1</span>, <span style="color: #98be65;">'class2'</span>],
<span class="linenr"> 3: </span>                     [<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'L'</span>, <span style="color: #da8548; font-weight: bold;">13.5</span>, <span style="color: #98be65;">'class1'</span>],
<span class="linenr"> 4: </span>                     [<span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'XL'</span>, <span style="color: #da8548; font-weight: bold;">15.3</span>, <span style="color: #98be65;">'class2'</span>]])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  df.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#21033;&#23565;&#25033;&#23383;&#20856;</span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> np
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">class_mapping</span> = {
<span id="coderef-classMapping" class="coderef-off"><span class="linenr">10: </span>      <span style="color: #dcaeea;">label</span>: idx <span style="color: #51afef;">for</span> idx, label <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(df[<span style="color: #98be65;">'classlabel'</span>]))</span>
<span class="linenr">11: </span>  }
<span class="linenr">12: </span>  <span style="color: #c678dd;">print</span>(class_mapping)
<span class="linenr">13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#39006;&#21029;&#29305;&#24501;&#36681;&#25563;&#28858;&#25972;&#25976;&#20540;</span>
<span class="linenr">14: </span>  df[<span style="color: #98be65;">'classlabel'</span>] = df[<span style="color: #98be65;">'classlabel'</span>].<span style="color: #c678dd;">map</span>(class_mapping)
<span class="linenr">15: </span>  <span style="color: #c678dd;">print</span>(df)
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;&#21453;&#36681;&#23383;&#20856;&#65292;&#23559;&#25972;&#25976;&#36996;&#21407;&#33267;&#21407;&#22987;&#30340;&#39006;&#21029;&#27161;&#31844;</span>
<span id="coderef-invClassMapping" class="coderef-off"><span class="linenr">18: </span>  <span style="color: #dcaeea;">inv_class_mapping</span> = {<span style="color: #dcaeea;">v</span>: k <span style="color: #51afef;">for</span> k, v <span style="color: #51afef;">in</span> class_mapping.items()}</span>
<span class="linenr">19: </span>  df[<span style="color: #98be65;">'classlabel'</span>] = df[<span style="color: #98be65;">'classlabel'</span>].<span style="color: #c678dd;">map</span>(inv_class_mapping)
<span class="linenr">20: </span>  <span style="color: #c678dd;">print</span>(df)
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Label encoding with sklearn's LabelEncoder</span>
<span class="linenr">23: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> LabelEncoder
<span class="linenr">24: </span>  <span style="color: #dcaeea;">class_le</span> = LabelEncoder()
<span id="coderef-labelEncoder" class="coderef-off"><span class="linenr">25: </span>  <span style="color: #dcaeea;">y</span> = class_le.fit_transform(df[<span style="color: #98be65;">'classlabel'</span>].values)</span>
<span class="linenr">26: </span>  <span style="color: #c678dd;">print</span>(y)
<span class="linenr">27: </span>  <span style="color: #dcaeea;">df</span>[<span style="color: #98be65;">'classlabel'</span>] = y
<span class="linenr">28: </span>  <span style="color: #c678dd;">print</span>(df) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39006;&#21029;&#33287;&#25976;&#23383;&#30340;&#23565;&#25033;&#19981;&#19968;&#23450;&#33287;&#33258;&#35330;&#23383;&#20856;&#19968;&#33268;</span>
<span class="linenr">29: </span>
</pre>
</div>

<pre class="example" id="orga702e42">
{'class2': 0, 'class1': 1}
   color size  price  classlabel
0  green    M   10.1           0
1    red    L   13.5           1
2   blue   XL   15.3           0
   color size  price classlabel
0  green    M   10.1     class2
1    red    L   13.5     class1
2   blue   XL   15.3     class2
[1 0 1]
   color size  price  classlabel
0  green    M   10.1           1
1    red    L   13.5           0
2   blue   XL   15.3           1
</pre>
</div>
</div>

<div id="outline-container-orgf5d44e6" class="outline-4">
<h4 id="orgf5d44e6"><span class="section-number-4">2.3.4.</span> 對 nominal feature 執行 one-hot encoding</h4>
<div class="outline-text-4" id="text-2-3-4">
<p>
scikit-learn 的 LabelENcoder 類別可以用來將「類別特徵」編碼為整數值，但這樣會引發另一個問題，如果我們將上述資料中的 color 特徵轉換為整數值，如下：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame([[<span style="color: #98be65;">'green'</span>, <span style="color: #98be65;">'M'</span>, <span style="color: #da8548; font-weight: bold;">10.1</span>, <span style="color: #98be65;">'class2'</span>],
<span class="linenr"> 3: </span>                     [<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'L'</span>, <span style="color: #da8548; font-weight: bold;">13.5</span>, <span style="color: #98be65;">'class1'</span>],
<span class="linenr"> 4: </span>                     [<span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'XL'</span>, <span style="color: #da8548; font-weight: bold;">15.3</span>, <span style="color: #98be65;">'class2'</span>]])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  df.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">X</span> = df[[<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]].values
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;LabelEncoder&#36681;&#25563;</span>
<span class="linenr">11: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> LabelEncoder
<span class="linenr">12: </span>  <span style="color: #dcaeea;">color_le</span> = LabelEncoder()
<span class="linenr">13: </span>  <span style="color: #c678dd;">print</span>(X[:,<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">14: </span>  <span style="color: #dcaeea;">X</span>[:,<span style="color: #da8548; font-weight: bold;">0</span>] = color_le.fit_transform(X[:,<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">15: </span>  <span style="color: #c678dd;">print</span>(X[:,<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">16: </span>
</pre>
</div>

<pre class="example">
['green' 'red' 'blue']
[1 2 0]
</pre>


<p>
由輸出結果可以發現，經過類別編碼後的顏色特徵，由原本不具次序的特徵變成存在大小關係(red&gt;green&gt;blue)，這明顯會影響 model 運算的結果。針對此一問題，常見的解決方案是 one-hot encoding，其原理是：對特徵值中的每個值，建立一個新的「虛擬特徵」(dummy feature)。方法有二：
</p>
<ul class="org-ul">
<li>利用 ColumnTransformer 函式庫的 ColumnTransformer 類別，將特徵值轉換 One-Hot Encoding 的對應矩陣，如程式第<a href="#coderef-FitTransform" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-FitTransform');" onmouseout="CodeHighlightOff(this, 'coderef-FitTransform');">24</a>行。</li>
<li>利用 Pandas 套件的 get_dummies 類別，一次將矩陣內指定之 column 轉換為 One-Hot encoding，如程式第<a href="#coderef-GetDummies" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-GetDummies');" onmouseout="CodeHighlightOff(this, 'coderef-GetDummies');">28</a>行。這種轉換只有字串數據會被轉換，其他內容則否。</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame([[<span style="color: #98be65;">'green'</span>, <span style="color: #98be65;">'M'</span>, <span style="color: #da8548; font-weight: bold;">10.1</span>, <span style="color: #98be65;">'class2'</span>],
<span class="linenr"> 3: </span>                     [<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'L'</span>, <span style="color: #da8548; font-weight: bold;">13.5</span>, <span style="color: #98be65;">'class1'</span>],
<span class="linenr"> 4: </span>                     [<span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'XL'</span>, <span style="color: #da8548; font-weight: bold;">15.3</span>, <span style="color: #98be65;">'class2'</span>]])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  df.<span style="color: #dcaeea;">columns</span> = [<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">X</span> = df[[<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'classlabel'</span>]].values
<span class="linenr"> 9: </span>  <span style="color: #c678dd;">print</span>(df)
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">one-hot encoding: ColumnTransformer / fit_transform</span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> LabelEncoder
<span class="linenr">13: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> OneHotEncoder
<span class="linenr">14: </span>  <span style="color: #51afef;">from</span> sklearn.compose <span style="color: #51afef;">import</span> ColumnTransformer
<span class="linenr">15: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #dcaeea;">X</span> = df[[<span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>, <span style="color: #98be65;">'price'</span>]].values
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #dcaeea;">ct</span> = ColumnTransformer(
<span class="linenr">20: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">The column numbers to be transformed (here is [0] but can be [0, 1, 3])</span>
<span class="linenr">21: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Leave the rest of the columns untouched</span>
<span class="linenr">22: </span>      [(<span style="color: #98be65;">'OneHot'</span>, OneHotEncoder(), [<span style="color: #da8548; font-weight: bold;">0</span>])], remainder=<span style="color: #98be65;">'passthrough'</span>
<span class="linenr">23: </span>  )
<span id="coderef-FitTransform" class="coderef-off"><span class="linenr">24: </span>  <span style="color: #c678dd;">print</span>(ct.fit_transform(X))</span>
<span class="linenr">25: </span>
<span class="linenr">26: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">on-hot encoding: pandas / get_dummies</span>
<span class="linenr">27: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span id="coderef-GetDummies" class="coderef-off"><span class="linenr">28: </span>  <span style="color: #c678dd;">print</span>(pd.get_dummies(df[[<span style="color: #98be65;">'price'</span>, <span style="color: #98be65;">'color'</span>, <span style="color: #98be65;">'size'</span>]]))</span>
</pre>
</div>

<pre class="example" id="org704095d">
   color size  price classlabel
0  green    M   10.1     class2
1    red    L   13.5     class1
2   blue   XL   15.3     class2
[[0.0 1.0 0.0 'M' 10.1]
 [0.0 0.0 1.0 'L' 13.5]
 [1.0 0.0 0.0 'XL' 15.3]]
   price  color_blue  color_green  color_red  size_L  size_M  size_XL
0   10.1           0            1          0       0       1        0
1   13.5           0            0          1       1       0        0
2   15.3           1            0          0       0       0        1
</pre>

<p>
應用 one-hot encoding 時，我們必須留意它所引入的「多元共線性」(multicollinearity)問題，這在某些狀況下(如要計算反矩陣)可能會產生一些問題，若特徵間有高度相關，則會難以計算反矩陣，導致數值不穩定的舘計。
</p>
</div>
</div>
</div>

<div id="outline-container-org21e9247" class="outline-3">
<h3 id="org21e9247"><span class="section-number-3">2.4.</span> 訓練集與測試集的數據分割</h3>
<div class="outline-text-3" id="text-2-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;"># Partitioning a dataset into a seperate training and test set</span>
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/'</span>
<span class="linenr"> 3: </span>                        <span style="color: #98be65;">'ml/machine-learning-databases/wine/wine.data'</span>,
<span class="linenr"> 4: </span>                        header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">if the Wine dataset is temporarily unavailable from the</span>
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">UCI machine learning repository, un-comment the following line</span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">of code to load the dataset from a local path:</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">df_wine = pd.read_csv('wine.data', header=None)</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span>
<span class="linenr">13: </span>  df_wine.columns = [<span style="color: #98be65;">'Class label'</span>, <span style="color: #98be65;">'Alcohol'</span>, <span style="color: #98be65;">'Malic acid'</span>, <span style="color: #98be65;">'Ash'</span>,
<span class="linenr">14: </span>                     <span style="color: #98be65;">'Alcalinity of ash'</span>, <span style="color: #98be65;">'Magnesium'</span>, <span style="color: #98be65;">'Total phenols'</span>,
<span class="linenr">15: </span>                     <span style="color: #98be65;">'Flavanoids'</span>, <span style="color: #98be65;">'Nonflavanoid phenols'</span>, <span style="color: #98be65;">'Proanthocyanins'</span>,
<span class="linenr">16: </span>                     <span style="color: #98be65;">'Color intensity'</span>, <span style="color: #98be65;">'Hue'</span>, <span style="color: #98be65;">'OD280/OD315 of diluted wines'</span>,
<span class="linenr">17: </span>                     <span style="color: #98be65;">'Proline'</span>]
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Class labels'</span>, np.unique(df_wine[<span style="color: #98be65;">'Class label'</span>]))
<span class="linenr">20: </span>  df_wine.head()
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> =    train_test_split(X, y,
<span class="linenr">25: </span>                       test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">26: </span>                       random_state=<span style="color: #da8548; font-weight: bold;">0</span>,
<span class="linenr">27: </span>                       stratify=y)
<span class="linenr">28: </span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgc22e003" class="outline-3">
<h3 id="orgc22e003"><span class="section-number-3">2.5.</span> 縮放特徵值、維持特徵值影響比例：正規化(normalization)</h3>
<div class="outline-text-3" id="text-2-5">
<p>
「特徵縮放」(Feature scaling)是資料預處理的一個關鍵，「決策樹」和「隨機森林」是極少數無需進行 feature scaling 的分類技術；對多數機器學習演算法而言，若特徵值經過適當的縮放，都能有更佳成效。
</p>

<p>
Feature scaling 的重要性可以以下例子看出，假設有兩個特徵值(a, b)，其中 a 的測量範圍為 1 到 10，b 的測量值範圍為 1 到 100000，以典型分類演算法的做法，一定是忙於最佳化特徵值 b；若以 KNN 的演算法，也會被特徵值 b 所技配。
</p>

<p>
正規化有兩種常用的方法，可以將不同規模的特徵轉化為相同的規模：常態化(normalization)和標準化(standardization)：
</p>
</div>
<div id="outline-container-org76cfc91" class="outline-4">
<h4 id="org76cfc91"><span class="section-number-4">2.5.1.</span> 常態化</h4>
<div class="outline-text-4" id="text-2-5-1">
<p>
將特徵值縮化為 0~1 間，這是「最小最大縮放」(min-max scaling)的一個特例，某一特徵值的常態化做法如下：
\[x_{norm}^i = \frac{x^i-x_{min}}{x_{max}-x_{min}}\]
若以 scikit-learn 套件來完成實作，其程式碼如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> MinMaxScaler
<span class="linenr">2: </span>  <span style="color: #dcaeea;">mms</span> = MinMaxScaler()
<span class="linenr">3: </span>  <span style="color: #dcaeea;">X_train_norm</span> = mms.fit_transform(X_train)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">X_test_norm</span> = mms.fit_transform(X_test)
</pre>
</div>
</div>
</div>

<div id="outline-container-orgf0ec1ec" class="outline-4">
<h4 id="orgf0ec1ec"><span class="section-number-4">2.5.2.</span> 標準化</h4>
<div class="outline-text-4" id="text-2-5-2">
<p>
雖說常態化簡單實用，但對許多機器學習演算法來說(特別是梯度下降法的最佳化)，標準化則更為實際，我們可令標準化後的特徵值其平均數為 0、標準差為 1，這樣一來，特徵值會滿足常態分佈，進而使演算法對於離群值不那麼敏感。標準化的公式如下：
\[x_{std}^i = \frac{x^i-\mu_x}{\sigma_x}\]
若以 scikit-learn 套件來完成實作，其程式碼如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr">2: </span>  <span style="color: #dcaeea;">stdsc</span> = StandardScaler()
<span class="linenr">3: </span>  <span style="color: #dcaeea;">X_train_std</span> = stdsc.fit_transform(X_train)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">X_test_std</span> = stdsc.transform(X_test)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org3812063" class="outline-3">
<h3 id="org3812063"><span class="section-number-3">2.6.</span> 選取有意義的特徵</h3>
<div class="outline-text-3" id="text-2-6">
<p>
overfitting 的產生原因是模型過度遷就於訓練數據，導致面對新數據(測試集)時成效不彰，我們稱這種模型具有較高變異性(high variance)，一般的解決策略有：
</p>
<ul class="org-ul">
<li>收集更多的訓練數據集</li>
<li>經由正規化，對於過度複雜的模型引進一個「懲罰」(penalty)</li>
<li>以較少的參數做出較簡單的模型(使用更簡單的模型)</li>
<li>減少數據維度</li>
</ul>
</div>

<div id="outline-container-org28fc01a" class="outline-4">
<h4 id="org28fc01a"><span class="section-number-4">2.6.1.</span> L1L2 regularzation</h4>
<div class="outline-text-4" id="text-2-6-1">
<p>
一個典型的解釋<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>如圖<a href="#orgd67f34a">3</a>，&ldquo;我們知道, 過擬合就是所謂的模型對可見的數據過度自信, 非常完美的擬合上了這些數據, 如果具備過擬合的能力, 那麼這個方程就可能是一個比較複雜的非線性方程 , 正是因為這裡的 x^3 和 x^2 使得這條虛線能夠被彎來彎去, 所以整個模型就會特別努力地去學習作用在 x^3 和 x^2 上的 c, d 參數. 但是我們期望模型要學到的卻是 這條藍色的曲線. 因為它能更有效地概括數據.而且只需要一個 y=a+bx 就能表達出數據的規律. 或者是說, 藍色的線最開始時, 和紅色線同樣也有 c d 兩個參數, 可是最終學出來時, c 和 d 都學成了 0, 雖然藍色方程的誤差要比紅色大, 但是概括起數據來還是藍色好. 那我們如何保證能學出來這樣的參數呢? 這就是 l1 l2 正規化出現的原因啦.&rdquo;
</p>


<div id="orgd67f34a" class="figure">
<p><img src="images/L1l2regularization2.png" alt="L1l2regularization2.png" />
</p>
<p><span class="figure-number">Figure 3: </span>過擬合問題</p>
</div>

<p>
對於上述訓練出的兩個方程式，我們可以用\((y_{\theta}(x)-y)^2\)來計算模型預測值\(y(x)\)和真實數據\(y\)的誤差，而 L1, L2 就只是在這個誤差公式後加上一些式子來修正這個公式(如圖<a href="#org36c33da">4</a>)，其目的在於讓誤差的最佳化不僅取決於訓練數據擬合的優劣，同時也取決於參數值(如 c,d)的大小；L2 正規化以參數平方來做為計算方式，L1 正規化則是計算每個參數的絕對值。
</p>

<div id="org36c33da" class="figure">
<p><img src="images/L1l2regularization3.png" alt="L1l2regularization3.png" />
</p>
<p><span class="figure-number">Figure 4: </span>L1,L2 正規化公式</p>
</div>

<p>
進一步以 Tensorflow Playground 的圖示來觀察 L1,L2 正規化的差異<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>，如果把正規化(Regularization)設定為 L1，再執行訓練。可以看到很多權重都被設定為 0，特徵輸入與隱藏層的神經元被大大的減少，如圖<a href="#orgfd5a082">5</a>，整個模型的複雜度簡化很多。L1 正規化確實有助於將我們的複雜模型縮減為更小的泛化模型。添加正規化後，我們看到無用的功能全部變為零，並且連接線變得稀疏並顯示為灰色。倖存下來的唯一特徵是 x_1 平方和 x_2 平方，這是有道理的，因為這 2 個特徵加在一起就構成了一個圓的方程。
</p>


<div id="orgfd5a082" class="figure">
<p><img src="images/L1l2regularization4.png" alt="L1l2regularization4.png" />
</p>
<p><span class="figure-number">Figure 5: </span>L1 正規化</p>
</div>

<p>
反觀 L2 正規化，當我們訓練它時，每個權重與神經元都還是處於活動狀態，但是非常虛弱，如圖<a href="#orge8081d3">6</a>，L1 正規化使用其中一個特徵而將某些拋棄，而 L2 正規化將同時保留特徵並使權重值保持較小。因此，使用 L1，您可以得到一個較小的模型，但預測性可能較低。。所以：
</p>

<ul class="org-ul">
<li>L1 正規化：有可能導致零權重，因刪除更多特徵而使模型稀疏。</li>
<li>L2 正規化：會對更大的權重值造成更大的影響，將使權重值保持較小。</li>
</ul>


<div id="orge8081d3" class="figure">
<p><img src="images/L1l2regularization5.png" alt="L1l2regularization5.png" />
</p>
<p><span class="figure-number">Figure 6: </span>L2 正規化</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org63dee7d" class="outline-3">
<h3 id="org63dee7d"><span class="section-number-3">2.7.</span> 循序特徵選擇法</h3>
<div class="outline-text-3" id="text-2-7">
<p>
另一種降低模型複雜度以避免過度擬合的方式是經由「特徵選擇」(feature selection)來做「降維」(dimensionality reduction)，降維的做法有二：
</p>
<ul class="org-ul">
<li>特徵選擇：feature selection, 由原本的特徵中，選出一個子集合</li>
<li>特徵提取：feature extraction，由原本的特徵中，導出資訊來建構新的特徵</li>
</ul>

<p>
循序特徵選擇法(sequential feature selection)為貪婪演算法的一種，目標在移除不相關或相關較低的特徵，以提高計算效率，這對於不支援「正規化」的演算法來說是很有用的。「循序向後選擇」(Sequential Backward Selection, SBS)便是一個典型的循序特徵選擇法，其做法是逐一從特徵空間中移除特徵，直到只剩下所要的特徵個數。為了達到這個目的，我們要定義一個最小化的「準則函數」(criterion function), 這個準則可以簡化為「模型在移除某特徵前/後的效能差異。SBS 的 python 實作如下：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Sequential feature selection algorithms</span>
<span class="linenr">  2: </span>  <span style="color: #51afef;">from</span> sklearn.base <span style="color: #51afef;">import</span> clone
<span class="linenr">  3: </span>  <span style="color: #51afef;">from</span> sklearn.metrics <span style="color: #51afef;">import</span> accuracy_score
<span class="linenr">  4: </span>  <span style="color: #51afef;">from</span> itertools <span style="color: #51afef;">import</span> combinations
<span class="linenr">  5: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">SBS</span>():
<span class="linenr">  6: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, estimator, k_features, scoring=accuracy_score,
<span class="linenr">  7: </span>                   test_size=<span style="color: #da8548; font-weight: bold;">0.25</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr">  8: </span>          <span style="color: #51afef;">self</span>.scoring = scoring
<span class="linenr">  9: </span>          <span style="color: #51afef;">self</span>.estimator = clone(estimator)
<span class="linenr"> 10: </span>          <span style="color: #51afef;">self</span>.k_features = k_features
<span class="linenr"> 11: </span>          <span style="color: #51afef;">self</span>.test_size = test_size
<span class="linenr"> 12: </span>          <span style="color: #51afef;">self</span>.random_state = random_state
<span class="linenr"> 13: </span>
<span class="linenr"> 14: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">fit</span>(<span style="color: #51afef;">self</span>, X, y):
<span class="linenr"> 15: </span>
<span class="linenr"> 16: </span>          X_train, X_test, y_train, y_test =             train_test_split(X, y, test_size=<span style="color: #51afef;">self</span>.test_size,
<span class="linenr"> 17: </span>                               random_state=<span style="color: #51afef;">self</span>.random_state)
<span class="linenr"> 18: </span>
<span class="linenr"> 19: </span>          dim = X_train.shape[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 20: </span>          <span style="color: #51afef;">self</span>.indices_ = <span style="color: #c678dd;">tuple</span>(<span style="color: #c678dd;">range</span>(dim))
<span class="linenr"> 21: </span>          <span style="color: #51afef;">self</span>.subsets_ = [<span style="color: #51afef;">self</span>.indices_]
<span class="linenr"> 22: </span>          score = <span style="color: #51afef;">self</span>._calc_score(X_train, y_train,
<span class="linenr"> 23: </span>                                   X_test, y_test, <span style="color: #51afef;">self</span>.indices_)
<span class="linenr"> 24: </span>          <span style="color: #51afef;">self</span>.scores_ = [score]
<span class="linenr"> 25: </span>
<span id="coderef-fitWhile" class="coderef-off"><span class="linenr"> 26: </span>          <span style="color: #51afef;">while</span> dim &gt; <span style="color: #51afef;">self</span>.k_features:</span>
<span class="linenr"> 27: </span>              scores = []
<span class="linenr"> 28: </span>              subsets = []
<span class="linenr"> 29: </span>
<span class="linenr"> 30: </span>              <span style="color: #51afef;">for</span> p <span style="color: #51afef;">in</span> combinations(<span style="color: #51afef;">self</span>.indices_, r=dim - <span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr"> 31: </span>                  score = <span style="color: #51afef;">self</span>._calc_score(X_train, y_train,
<span id="coderef-scoreXtest" class="coderef-off"><span class="linenr"> 32: </span>                                           X_test, y_test, p)</span>
<span class="linenr"> 33: </span>                  scores.append(score)
<span class="linenr"> 34: </span>                  subsets.append(p)
<span class="linenr"> 35: </span>
<span class="linenr"> 36: </span>              best = np.argmax(scores)
<span class="linenr"> 37: </span>              <span style="color: #51afef;">self</span>.indices_ = subsets[best]
<span class="linenr"> 38: </span>              <span style="color: #51afef;">self</span>.subsets_.append(<span style="color: #51afef;">self</span>.indices_)
<span class="linenr"> 39: </span>              dim -= <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 40: </span>
<span id="coderef-bestScore" class="coderef-off"><span class="linenr"> 41: </span>              <span style="color: #51afef;">self</span>.scores_.append(scores[best])</span>
<span class="linenr"> 42: </span>          <span style="color: #51afef;">self</span>.k_score_ = <span style="color: #51afef;">self</span>.scores_[-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 43: </span>
<span class="linenr"> 44: </span>          <span style="color: #51afef;">return</span> <span style="color: #51afef;">self</span>
<span class="linenr"> 45: </span>
<span class="linenr"> 46: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">transform</span>(<span style="color: #51afef;">self</span>, X):
<span class="linenr"> 47: </span>          <span style="color: #51afef;">return</span> X[:, <span style="color: #51afef;">self</span>.indices_]
<span class="linenr"> 48: </span>
<span class="linenr"> 49: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">_calc_score</span>(<span style="color: #51afef;">self</span>, X_train, y_train, X_test, y_test, indices):
<span class="linenr"> 50: </span>          <span style="color: #51afef;">self</span>.estimator.fit(X_train[:, indices], y_train)
<span class="linenr"> 51: </span>          y_pred = <span style="color: #51afef;">self</span>.estimator.predict(X_test[:, indices])
<span class="linenr"> 52: </span>          score = <span style="color: #51afef;">self</span>.scoring(y_test, y_pred)
<span class="linenr"> 53: </span>          <span style="color: #51afef;">return</span> score
<span class="linenr"> 54: </span>
<span class="linenr"> 55: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 56: </span>  <span style="color: #51afef;">from</span> sklearn.neighbors <span style="color: #51afef;">import</span> KNeighborsClassifier
<span class="linenr"> 57: </span>
<span class="linenr"> 58: </span>  knn = KNeighborsClassifier(n_neighbors=<span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr"> 59: </span>
<span class="linenr"> 60: </span>  <span style="color: #5B6268;">##</span><span style="color: #5B6268;">========</span>
<span class="linenr"> 61: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 62: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 63: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 64: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 65: </span>  df_wine = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/'</span>
<span class="linenr"> 66: </span>                      <span style="color: #98be65;">'ml/machine-learning-databases/wine/wine.data'</span>,
<span class="linenr"> 67: </span>                      header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr"> 68: </span>  df_wine.columns = [<span style="color: #98be65;">'Class label'</span>, <span style="color: #98be65;">'Alcohol'</span>, <span style="color: #98be65;">'Malic acid'</span>, <span style="color: #98be65;">'Ash'</span>,
<span class="linenr"> 69: </span>                     <span style="color: #98be65;">'Alcalinity of ash'</span>, <span style="color: #98be65;">'Magnesium'</span>, <span style="color: #98be65;">'Total phenols'</span>,
<span class="linenr"> 70: </span>                     <span style="color: #98be65;">'Flavanoids'</span>, <span style="color: #98be65;">'Nonflavanoid phenols'</span>, <span style="color: #98be65;">'Proanthocyanins'</span>,
<span class="linenr"> 71: </span>                     <span style="color: #98be65;">'Color intensity'</span>, <span style="color: #98be65;">'Hue'</span>, <span style="color: #98be65;">'OD280/OD315 of diluted wines'</span>,
<span class="linenr"> 72: </span>                     <span style="color: #98be65;">'Proline'</span>]
<span class="linenr"> 73: </span>  X, y = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr"> 74: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 75: </span>  X_train, X_test, y_train, y_test =    train_test_split(X, y,
<span class="linenr"> 76: </span>                       test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr"> 77: </span>                       random_state=<span style="color: #da8548; font-weight: bold;">0</span>,
<span class="linenr"> 78: </span>                       stratify=y)
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span class="linenr"> 81: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 82: </span>  sc = StandardScaler() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23526;&#20363;&#21270;&#19968;&#20491;StandardScaler&#29289;&#20214;</span>
<span class="linenr"> 83: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span class="linenr"> 84: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span class="linenr"> 85: </span>  sc.fit(X_train)
<span class="linenr"> 86: </span>  X_train_std = sc.transform(X_train)
<span class="linenr"> 87: </span>  X_test_std = sc.transform(X_test)
<span class="linenr"> 88: </span>
<span class="linenr"> 89: </span>  <span style="color: #5B6268;">##</span><span style="color: #5B6268;">===</span>
<span class="linenr"> 90: </span>
<span class="linenr"> 91: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">selecting features</span>
<span id="coderef-kFeatures" class="coderef-off"><span class="linenr"> 92: </span>  sbs = SBS(knn, k_features=<span style="color: #da8548; font-weight: bold;">1</span>)</span>
<span class="linenr"> 93: </span>  sbs.fit(X_train_std, y_train)
<span class="linenr"> 94: </span>
<span class="linenr"> 95: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plotting performance of feature subsets</span>
<span class="linenr"> 96: </span>  k_feat = [<span style="color: #c678dd;">len</span>(k) <span style="color: #51afef;">for</span> k <span style="color: #51afef;">in</span> sbs.subsets_]
<span class="linenr"> 97: </span>
<span id="coderef-accuracyScore" class="coderef-off"><span class="linenr"> 98: </span>  plt.plot(k_feat, sbs.scores_, marker=<span style="color: #98be65;">'o'</span>)</span>
<span class="linenr"> 99: </span>  plt.ylim([<span style="color: #da8548; font-weight: bold;">0.7</span>, <span style="color: #da8548; font-weight: bold;">1.02</span>])
<span class="linenr">100: </span>  plt.ylabel(<span style="color: #98be65;">'Accuracy'</span>)
<span class="linenr">101: </span>  plt.xlabel(<span style="color: #98be65;">'Number of features'</span>)
<span class="linenr">102: </span>  plt.grid()
<span class="linenr">103: </span>  plt.tight_layout()
<span class="linenr">104: </span>  plt.savefig(<span style="color: #98be65;">'04_08.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">105: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span id="coderef-sbsSubsets" class="coderef-off"><span class="linenr">106: </span>  <span style="color: #c678dd;">print</span>(sbs.subsets_) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20840;&#37096;&#21015;&#20986;&#65292;&#25214;&#21040;3&#20491;&#29305;&#24501;&#20540;&#26159;&#22312;&#31532;&#24190;&#20491;&#20301;&#32622;</span></span>
<span class="linenr">107: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">list</span>(sbs.subsets_[<span style="color: #da8548; font-weight: bold;">10</span>]))
<span class="linenr">108: </span>  k3 = <span style="color: #c678dd;">list</span>(sbs.subsets_[<span style="color: #da8548; font-weight: bold;">10</span>])
<span class="linenr">109: </span>  <span style="color: #c678dd;">print</span>(df_wine.columns[<span style="color: #da8548; font-weight: bold;">1</span>:][k3])
<span class="linenr">110: </span>  <span style="color: #5B6268;">## </span><span style="color: #5B6268;">&#27604;&#36611;&#20840;&#37096;&#29305;&#24501;&#20540;&#33287;&#19977;&#20491;&#29305;&#24501;&#20540;&#30340;&#25928;&#33021;</span>
<span class="linenr">111: </span>  knn.fit(X_train_std, y_train)
<span class="linenr">112: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Training accuracy (FULL):'</span>, knn.score(X_train_std, y_train))
<span class="linenr">113: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Test accuracy (FULL):'</span>, knn.score(X_test_std, y_test))
<span class="linenr">114: </span>  knn.fit(X_train_std[:, k3], y_train)
<span class="linenr">115: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Training accuracy (K3):'</span>, knn.score(X_train_std[:,k3], y_train))
<span class="linenr">116: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Test accuracy (K3):'</span>, knn.score(X_test_std[:,k3], y_test))
<span class="linenr">117: </span>
</pre>
</div>

<pre class="example">
[(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12), (0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12), (0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11), (0, 1, 2, 3, 4, 5, 6, 7, 9, 11), (0, 1, 2, 3, 4, 5, 7, 9, 11), (0, 1, 2, 3, 5, 7, 9, 11), (0, 1, 2, 3, 5, 7, 11), (0, 1, 2, 3, 5, 11), (0, 1, 2, 3, 11), (0, 1, 2, 11), (0, 1, 11), (0, 11), (0,)]
[0, 1, 11]
Index(['Alcohol', 'Malic acid', 'OD280/OD315 of diluted wines'], dtype='object')
Training accuracy (FULL): 0.967741935483871
Test accuracy (FULL): 0.9629629629629629
Training accuracy (K3): 0.9516129032258065
Test accuracy (K3): 0.9259259259259259
</pre>



<div id="org76101fd" class="figure">
<p><img src="images/04_08.png" alt="04_08.png" width="500" />
</p>
<p><span class="figure-number">Figure 7: </span>SBS</p>
</div>

<p>
前述實作中，k_features 參數(程式第<a href="#coderef-kFeatures" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-kFeatures');" onmouseout="CodeHighlightOff(this, 'coderef-kFeatures');">92</a>行)定義了我們希望演算法「最後要保留多少特徵」，在預設情況下，以 accuracy_score(程式第<a href="#coderef-accuracyScore" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-accuracyScore');" onmouseout="CodeHighlightOff(this, 'coderef-accuracyScore');">98</a>行)來評估模型效能。在 fit 的 while 迴圈中(<a href="#coderef-fitWhile" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-fitWhile');" onmouseout="CodeHighlightOff(this, 'coderef-fitWhile');">26</a>行)，由 itertools 模組的 combinations 方法所產生的特徵子集合會被評估並降維，直到只剩下所要的特徵個數。
</p>

<p>
在每次迭代中，演算法使用內部創建的測試數據集 X_test(第<a href="#coderef-scoreXtest" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-scoreXtest');" onmouseout="CodeHighlightOff(this, 'coderef-scoreXtest');">32</a>行)來評估特徵子集合，然後留下精確度最佳的特徵子集合所得分數，加入串列 self.scores_中(第<a href="#coderef-bestScore" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-bestScore');" onmouseout="CodeHighlightOff(this, 'coderef-bestScore');">41</a>行)，之後再以這些分數來評估結果。最後的特徵子集合「行索引」會被分派到變數 self.indices_中，然後以 transform 將這些所選定的特徵轉為新的數據陣列。
</p>

<p>
由圖<a href="#org76101fd">7</a>可以看到，當特徵數 k={3, 7, 8, 9, 10, 11, 12}時，KNN 分類器的準確率為 100%。若進一步想確定當 k=3 時，是哪三個特徵，則可以由 sbs.subset_中逐步探索出來(程式第<a href="#coderef-sbsSubsets" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-sbsSubsets');" onmouseout="CodeHighlightOff(this, 'coderef-sbsSubsets');">106</a>行)。
</p>

<p>
進一步比較「全部特徵值」以及「三個特徵值」所得出的模型效能，可以看到即使只留下三個特徵值，模型的效能仍相去不遠，更重要的是，透過降低維度，可以有效的提升運算效能。
</p>
</div>
</div>
<div id="outline-container-orgfaf9c81" class="outline-3">
<h3 id="orgfaf9c81"><span class="section-number-3">2.8.</span> 以隨機森林評估特徵的重要性</h3>
<div class="outline-text-3" id="text-2-8">
<p>
隨機森林顧名思義，是用隨機的方式建立一個森林，森林裡面有很多的決策樹組成，隨機森林的每一棵決策樹之間是沒有關聯的。在得到森林之後，當有一個新的輸入樣本進入的時候，就讓森林中的每一棵決策樹分別進行一下判斷，看看這個樣本應該屬於哪一類（對於分類演算法），然後看看哪一類被選擇最多，就預測這個樣本為那一類<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>。上述 SBS 演算法係將低相關的特徵刪除、留下重要的特徵；而隨機森林則是利用許多決策樹來票選最後的決定。
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/'</span>
<span class="linenr"> 7: </span>                      <span style="color: #98be65;">'ml/machine-learning-databases/wine/wine.data'</span>,
<span class="linenr"> 8: </span>                      header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr"> 9: </span>  df_wine.columns = [<span style="color: #98be65;">'Class label'</span>, <span style="color: #98be65;">'Alcohol'</span>, <span style="color: #98be65;">'Malic acid'</span>, <span style="color: #98be65;">'Ash'</span>,
<span class="linenr">10: </span>                     <span style="color: #98be65;">'Alcalinity of ash'</span>, <span style="color: #98be65;">'Magnesium'</span>, <span style="color: #98be65;">'Total phenols'</span>,
<span class="linenr">11: </span>                     <span style="color: #98be65;">'Flavanoids'</span>, <span style="color: #98be65;">'Nonflavanoid phenols'</span>, <span style="color: #98be65;">'Proanthocyanins'</span>,
<span class="linenr">12: </span>                     <span style="color: #98be65;">'Color intensity'</span>, <span style="color: #98be65;">'Hue'</span>, <span style="color: #98be65;">'OD280/OD315 of diluted wines'</span>,
<span class="linenr">13: </span>                     <span style="color: #98be65;">'Proline'</span>]
<span class="linenr">14: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">15: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr">16: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(X, y, test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">17: </span>                                                      random_state=<span style="color: #da8548; font-weight: bold;">0</span>,
<span class="linenr">18: </span>                                                      stratify=y)
<span class="linenr">19: </span>
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #51afef;">from</span> sklearn.ensemble <span style="color: #51afef;">import</span> RandomForestClassifier
<span class="linenr">22: </span>  feat_labels = df_wine.columns[<span style="color: #da8548; font-weight: bold;">1</span>:]
<span class="linenr">23: </span>  forest = RandomForestClassifier(n_estimators=<span style="color: #da8548; font-weight: bold;">500</span>,
<span class="linenr">24: </span>                                  random_state=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">25: </span>
<span class="linenr">26: </span>  forest.fit(X_train, y_train)
<span class="linenr">27: </span>  importances = forest.feature_importances_
<span class="linenr">28: </span>
<span class="linenr">29: </span>  indices = np.argsort(importances)[::-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">30: </span>
<span class="linenr">31: </span>  <span style="color: #51afef;">for</span> f <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(X_train.shape[<span style="color: #da8548; font-weight: bold;">1</span>]):
<span class="linenr">32: </span>      <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"%2d) %-*s %f"</span> % (f + <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">30</span>,
<span class="linenr">33: </span>                              feat_labels[indices[f]],
<span class="linenr">34: </span>                              importances[indices[f]]))
<span class="linenr">35: </span>
<span class="linenr">36: </span>  plt.title(<span style="color: #98be65;">'Feature Importance'</span>)
<span class="linenr">37: </span>  plt.bar(<span style="color: #c678dd;">range</span>(X_train.shape[<span style="color: #da8548; font-weight: bold;">1</span>]),
<span class="linenr">38: </span>          importances[indices],
<span class="linenr">39: </span>          align=<span style="color: #98be65;">'center'</span>)
<span class="linenr">40: </span><span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>  plt.xticks(<span style="color: #c678dd;">range</span>(X_train.shape[<span style="color: #da8548; font-weight: bold;">1</span>]),
<span class="linenr">42: </span>             feat_labels[indices], rotation=<span style="color: #da8548; font-weight: bold;">90</span>)
<span class="linenr">43: </span>  plt.xlim([-<span style="color: #da8548; font-weight: bold;">1</span>, X_train.shape[<span style="color: #da8548; font-weight: bold;">1</span>]])
<span class="linenr">44: </span>  plt.tight_layout()
<span class="linenr">45: </span>  plt.savefig(<span style="color: #98be65;">'04_09.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">46: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">47: </span>
</pre>
</div>

<pre class="example" id="orgcc3c9cb">
 1) Proline                        0.185453
 2) Flavanoids                     0.174751
 3) Color intensity                0.143920
 4) OD280/OD315 of diluted wines   0.136162
 5) Alcohol                        0.118529
 6) Hue                            0.058739
 7) Total phenols                  0.050872
 8) Magnesium                      0.031357
 9) Malic acid                     0.025648
10) Proanthocyanins                0.025570
11) Alcalinity of ash              0.022366
12) Nonflavanoid phenols           0.013354
13) Ash                            0.013279
</pre>


<div id="orgc22b43b" class="figure">
<p><img src="images/04_09.png" alt="04_09.png" width="500" />
</p>
<p><span class="figure-number">Figure 8: </span>FandomForest</p>
</div>


<p>
由圖<a href="#orgc22b43b">8</a>的特徵排序為從 500 棵「決策樹」的「不純度」中最具「判別性」的特徵排列順序，
</p>
</div>
</div>
</div>

<div id="outline-container-org4265b8a" class="outline-2">
<h2 id="org4265b8a"><span class="section-number-2">3.</span> 深度學習運作原理</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-orge0ba937" class="outline-3">
<h3 id="orge0ba937"><span class="section-number-3">3.1.</span> Layer, 損失函數與優化器</h3>
<div class="outline-text-3" id="text-3-1">
<p>
前節深度學習中的每一「層」(layer)如何運作，取決於儲存於該層的權重(weight)，而權重是由多個數字組成。從技術層面來看，layer 是由各個權重參數(parameters)來和輸入的資料進行運算以執行資料轉換的工作(如圖<a href="#org4fe815f">9</a>)。而所謂的學習，指的就是幫助神經網路的每一層找出適當的權重值，讓神經網路可以將輸入的訓練資料經由與權重的運作推導出接近標準答案的運算結果(即圖<a href="#org4fe815f">9</a>中的預測 Y)。然而，這在實際運作上是十分困難的，因為一個深度神經網路可以包含數千萬個權重，此外，其中一個權重被改變後，往往會影響其他權重的運作。
</p>


<div id="org4fe815f" class="figure">
<p><img src="images/img-191107115233.jpg" alt="img-191107115233.jpg" width="400" />
</p>
<p><span class="figure-number">Figure 9: </span>nn 中 layer 的 parameter</p>
</div>

<p>
為了提高神經網路的效能(預測的準確率)，我們要即時的掌握目前的輸出(Y)與真正的標準答案還差多少，這個評估由神經網路的損失函數(loss function, 或稱目標函數, objective function)來負責，如圖<a href="#orgccc2395">10</a>。損失函數會取得神經網路的預測結果與標準答案二者的損失分數(又稱差距分數)，做為每一次學習的表現效能之評估標準。
</p>



<div id="orgccc2395" class="figure">
<p><img src="images/img-191107115304.jpg" alt="img-191107115304.jpg" width="400" />
</p>
<p><span class="figure-number">Figure 10: </span>損失函數</p>
</div>

<p>
而深度學習的基本工作就是使用損失函數做為回饋訊息來一步步微調權重，逐步降低每次學習的損失分數，最終目標在於讓損失函數結果達到最小，而這個微調工作則由優化器(optimizer，也稱最佳化函數)來執行。優化器實作了反向傳播演算法(Backpropagation)，這也是深度學習中的核心演算法，藉此來週整權重。
</p>


<div id="org2db306d" class="figure">
<p><img src="images/img-1911071153041.jpg" alt="img-1911071153041.jpg" width="400" />
</p>
<p><span class="figure-number">Figure 11: </span>優化器</p>
</div>

<p>
那麼，在最初一次的學習，權重的值是如何設定的呢？可以先全數設為零，但更常用的做法是隨機指定，隨著多次學習後，權重會逐步往正確的方向調整，損失分數也會慢慢降低。
</p>
</div>
</div>
<div id="outline-container-org4b9a524" class="outline-3">
<h3 id="org4b9a524"><span class="section-number-3">3.2.</span> 梯度</h3>
<div class="outline-text-3" id="text-3-2">
</div>
<div id="outline-container-org60b1b3f" class="outline-4">
<h4 id="org60b1b3f"><span class="section-number-4">3.2.1.</span> 深度網路誤差曲面的局部極小值</h4>
<div class="outline-text-4" id="text-3-2-1">
<p>
最佳化深度學習模型的挑戰在於我們只能運用局部的訊息去推斷誤差曲面的整體結構，雖然梯度遞減法可以確保我們找到極小值，但若曲面結構非碗型（即，存在不只一個谷地，或稱局部極小值），則即便我們探用隨機誤差曲面演算法，也是無法解決問題。
局部極小值與「模型可區分性(model indentifiability)」的概念有關，在全連接(fully-conntectd)的正向饋送神經網路中，同一層的神經元就算重新排列組合，網路末端還是會出現相同的最終輸出，結果，一層有 n 個神經元的網路就存在\(n!\)種排列方式，對於有 l 層的深度網路而言，則其等效配置方式就有\(n!^l\)種。結果，不論送進什麼輸入值，表現出來的行為也全都相同而無法區分；換言之，無論用的是訓練組、驗證組、測試組的樣本，所有的這些等效配置都會表現出相同的誤差。
局部極小值不是太嚴重的問題，但若找到的是「假的（spurious）」局部極小值則就是個大問題，所謂假的局部極小值指的是它在神經網路中所對應的權重值，會比真正的整體最小值所對應的權重值帶來更大的誤差），從事深度學習的人總是把訓練深度網路時所遇到的問題歸咎於假的局部極小值。想解決這個問題，有個天真的想法：在訓練深度神經網路的過程中，同時畫出誤差函數隨時間而變的值，但是這個策略並不能針對誤差曲面提供足夠的訊息，因為我們很難判斷誤差的變化是來自曲面本身的「顛簸」或是因為遲遲無法找到最佳的前進方向。
Goodfellow 等人<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>（Google 和 Standford 合作的研究小組）在 2014 年發表一篇論文試圖解決上述問題，他們沒有去分析誤差的函數隨時間而變的情況，而是在隨機選取的初始化參數向量和最後真正的最佳點之間，運用線性插值取點，再觀察這些插值點在誤差曲面上呈現什麼樣的變化，也就是說，只要給定一個隨機初始化參數向量\(\theta_i\)，加上隨機梯度遞減法(SBD)最後找到的最佳點\(\theta_f\)，我們就可以沿著線性插值的每個點，計算出相應的誤差函數值\(\theta_\alpha = \alpha \cdot \theta_f + (1-\alpha) \cdot \theta_i \)。
</p>

<p>
Goodfellow 等人的研究顯示，對於各種具有不同型態神經元的實際網路而言，參數空間中隨機選取的初始點數與隨機梯度遞減最佳解之間直接相連的路經，並不會受到局部極小值的影響；換言之，我們應該把重點放在「尋找合適的前進方向」上。
</p>
</div>
</div>
<div id="outline-container-org7a02edc" class="outline-4">
<h4 id="org7a02edc"><span class="section-number-4">3.2.2.</span> 找出正確的移動軌跡</h4>
<div class="outline-text-4" id="text-3-2-2">
<p>
梯度通常不是尋找最小值時最好的移動軌跡參考指標，最佳應用時機是等高線為完美㘣形，然而多數等高線均為楕圓，此時梯度所指的方向就會與正確方向有所偏差。對參數空間中的每個權向\(w_i\)來說，梯度計算的是\(\frac{\partial{E}}{\partial{w_i}}\)，代表當\(w_i\)被改變時，誤差如何隨之變化的程度。因此，只要綜合考慮參數空間的所有權重，梯度就可以給出遞減最快的方向；然而，當我們朝著這個方向移動一步後，此時的梯度又會隨之改變。
</p>

<p>
進一步量化我們往某方向移等時腳下梯度變化的程度，我們必須計算二階導函數，即求出\(\farc{\partial{\frac{ \parital{E}}{\partial{w_j} }}}{\partial{w_i}}\)，代表當我們改變\(w_i\)的值時，梯度中的分量\(w_j\)如何隨之而改變。將這些訊息編寫之的矩陣稱之為「海森矩陣 (Hessian matrix)」，在描述誤差曲面時，如果我們往遞減最快方向移動，腳下的梯度也跟著改變，我們就會說這是個病態(ill-conditioned)矩陣。
</p>
</div>

<ol class="org-ol">
<li><a id="org5bfea4b"></a>動量<br />
<div class="outline-text-5" id="text-3-2-2-1">
<p>
病態海森矩陣的問題往往會以梯度大幅波動的形式表現出來，因此我們可以考慮如何在訓練期間消除這些波動。想像一顆球滾落至誤差曲面中，最終一定會抵達曲面的最低點，而且不會有大幅波動。球的平滑滾落動作不只受到加速度的影響，也受到「速度」的影響，而球的速度以一種記憶的形式讓球往最低方向更有效的累積移動量，同時抵消正交(orthogonal)方向上的振盪加速度；為了模擬出球體的自然動作，我們可以在最佳化演算法中以某種方式引入速度的概念，也就是追蹤之前梯度的「指數加權衺減量」。換言之，我們用一個「動量超參數」\(m\)，以決定在新的更新值中，前一次速度要保留多少比例，藉此把我們對前一個梯度值的「記憶」添加至目前最新的梯度值中。這種做法所運用到的概念通常就動為「動量(momentum)」。
</p>
</div>
</li>
<li><a id="orged60d54"></a>Nesterow 動量<br />
<div class="outline-text-5" id="text-3-2-2-2">
<p>
為 Sutskever 等人在 2013 年，基於改進古典動量技術所提出的動量替代方案
</p>
</div>
</li>
<li><a id="org3214c55"></a>共軛梯度遞減(conjugate gradient descent)<br />
<div class="outline-text-5" id="text-3-2-2-3">
<p>
這是試圖改進單純最陡遞減法的另一做法，最陡遞減法是計算梯度方向，然後沿此方向搜索最小值，跳到最小值處再重新計算，實際情況則會大幅波動，這是因為每次往最陡方向移動，往往會稍微抵消另一方向的進展，補救方式是不往最陡方向移動，而是相對先前所選擇的方向，往其「共軛方向(conjugate direction)」移動。
</p>
</div>
</li>
<li><a id="org4ab9827"></a>BFGS(Broyden-Fletcher-Goldfarb-Shanno)<br />
<div class="outline-text-5" id="text-3-2-2-4">
<p>
以迭代方式計算海森矩陣的逆矩陣，以有效最佳化參數向量
</p>
</div>
</li>
<li><a id="orgcfbde50"></a>L-BFGS<br />
<div class="outline-text-5" id="text-3-2-2-5">
<p>
解決 BFGS 佔用記憶體的問題
</p>
</div>
</li>
</ol>
</div>
<div id="outline-container-orge278ede" class="outline-4">
<h4 id="orge278ede"><span class="section-number-4">3.2.3.</span> 學習率自動調整</h4>
<div class="outline-text-4" id="text-3-2-3">
</div>
<ol class="org-ol">
<li><a id="orge0070cc"></a>AdaGrad<br />
<div class="outline-text-5" id="text-3-2-3-1">
<p>
根據累積歷史梯度，對整體學習率進行自動調整 由 Duchi 等人在 2011 年提出
</p>
</div>
</li>
<li><a id="org39ea1e5"></a>RMSProp<br />
<div class="outline-text-5" id="text-3-2-3-2">
<p>
累似以動量抑制梯度波動的做法，改以指數加權移動平均，將很久以前的值也納入考慮。
</p>
</div>
</li>
<li><a id="orgf9c7ac1"></a>Adam<br />
<div class="outline-text-5" id="text-3-2-3-3">
<p>
可視為 RMSProp 與動量的變種組合
</p>
</div>
</li>
<li><a id="org54abb0d"></a>AdaDelta<br /></li>
</ol>
</div>
</div>
<div id="outline-container-org369d332" class="outline-3">
<h3 id="org369d332"><span class="section-number-3">3.3.</span> 最佳選擇</h3>
<div class="outline-text-3" id="text-3-3">
<p>
對於大多數深度學習實作者，推動深度學習的最佳途徑並不是創造出更高級的最佳化演算法，相反的，過去幾十年來絕大多數深度學習的突破，都是因為發現了更容易訓練的架構，而不是因為與那些討厭的誤差曲面搏鬥所得到的成果。
</p>
</div>
</div>
</div>

<div id="outline-container-org02f698e" class="outline-2">
<h2 id="org02f698e"><span class="section-number-2">4.</span> 降維來壓縮數據</h2>
<div class="outline-text-2" id="text-4">
</div>
<div id="outline-container-orgb538917" class="outline-3">
<h3 id="orgb538917"><span class="section-number-3">4.1.</span> 以主成份分析(PCA)對非監督式數據壓縮</h3>
<div class="outline-text-3" id="text-4-1">
<p>
「特徵選擇」需要原始的「特徵」；而「特徵提取」則是在於「轉換」數據，或是「投影」(project)數據到一個新的「特徵空間」，特徵提取不僅能改善儲存空間的使用或是提高學習演算法的計算效率，也可以有效地藉由降低「維數災難」來提高預測的正確性，特別是在處理非正規化模型時。
</p>
</div>

<div id="outline-container-org95cebef" class="outline-4">
<h4 id="org95cebef"><span class="section-number-4">4.1.1.</span> 主成分分析 1</h4>
<div class="outline-text-4" id="text-4-1-1">
<p>
「主成份分析」(principal component analysis, PCA)是一種非監督式線性變換技術」，經常應用於「特徵提取」與「降維」，其他應用包括「探索式數據分析」和「股票市場分析」中的雜訊消除、生物資訊學領域中的「基因數據分析」與「基因表現層分析」。
</p>

<p>
這邊先簡單說維度詛咒，預測/分類能力通常是隨著維度數(變數)增加而上生，但當模型樣本數沒有繼續增加的情況下，預測/分類能力增加到一定程度之後，預測/分類能力會隨著維度的繼續增加而減小<sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup>。
</p>

<p>
主成份分析的基本假設是希望資料可以在特徵空間找到一個投影軸(向量)投影後可以得到這組資料的最大變異量。以圖<a href="#org5a083ad">12</a>為例，PCA 的目的在於找到一個向量可以投影(圖中紅色的線)，讓投影後的資料變異量最大。
</p>


<div id="org5a083ad" class="figure">
<p><img src="images/pca-1.png" alt="pca-1.png" width="600" />
</p>
<p><span class="figure-number">Figure 12: </span>PCA-1 [fn:31]</p>
</div>
</div>

<ol class="org-ol">
<li><a id="orgc11acf2"></a>投影(projection)<br />
<div class="outline-text-5" id="text-4-1-1-1">
<p>
假設有一個點藍色的點對原點的向量為\(\vec{x_i}\)，有一個軸為 v，他的投影(正交為虛線和藍色線為 90 度)向量為紅色那條線，紅色線和黑色線的夾角為\(\theta\)，\(\vec{x_i}\)投影長度為藍色線，其長度公式為\(\left\|{x_i}\right\|cos\theta\)。
</p>


<div id="org45e9c19" class="figure">
<p><img src="images/pca-2.png" alt="pca-2.png" width="300" />
</p>
<p><span class="figure-number">Figure 13: </span>PCA-2 [fn:31]</p>
</div>

<p>
假設有一組資料六個點(\(x_1, x_2, x_3, x_4, x_5, x_6\))，有兩個投影向量\(\vec{v}\)和\(\vec{v'}\)(如圖<a href="#org6d37388">14</a>)，投影下來後，資料在\(\vec{v'}\)上的變異量比\(v\)上的變異量小。
</p>


<div id="org6d37388" class="figure">
<p><img src="images/pca-3.png" alt="pca-3.png" width="600" />
</p>
<p><span class="figure-number">Figure 14: </span>PCA-3 [fn:31]</p>
</div>

<p>
從圖<a href="#org3d5f5af">15</a>也可以看出這些資料在\(v\)向量資料投影后有較大的變異量(較之投影於\(\vec{v'}\))。
</p>


<div id="org3d5f5af" class="figure">
<p><img src="images/pca-4.png" alt="pca-4.png" width="300" />
</p>
<p><span class="figure-number">Figure 15: </span>PCA-4 [fn:31]</p>
</div>
</div>
</li>

<li><a id="org0614226"></a>變異量的計算<br />
<div class="outline-text-5" id="text-4-1-1-2">
<p>
典型的變異數公式如下：
\(\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (X -\mu)^2}\)
</p>

<p>
若要計算前述所有資料點(\(x_1, x_2, x_3, x_4, x_5, x_6\))在\(v\)上的投影\(v^Tx_1, v^Tx_2, v^Tx_3, v^Tx_4, v^Tx_5, v^Tx_6\) ，則其變異數公式為
\(\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i -\mu)^2\)
</p>

<p>
又因 PCA 之前提假設是將資 shift 到 0(即，變異數的平均數為 0)以簡化運算，其公式會變為
\(\sigma^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i -\mu)^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i - 0)^2 = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i)^2\)
</p>

<p>
而機器學習處理的資料點通常為多變量，故上述式子會以矩陣方式呈現
</p>

<p>
\(\Sigma = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_i)(v^Tx_i)^T = \frac{1}{N}\sum\limits_{i=1}^N (v^Tx_iv^Tx_iv) = v^T(\frac{1}{N}\sum\limits_{i=1}^Nx_iX_i^T)v = v^TCv\)
</p>

<p>
其中 C 為共變異數矩陣(covariance matrix)
</p>

<p>
\(C=\frac{1}{n}\sum\limits_{i=1}^nx_ix_i^T,\cdots x_i = \begin{bmatrix}
x_1^{(1)}     \\
x_2^{(2)}     \\
\vdots  \\
x_i^{(d)}     \\
\end{bmatrix}\)
</p>

<p>
主成份分析的目的則是在找出一個投影向量讓投影後的資料變異量最大化（最佳化問題）：
</p>

<p>
\(v = \mathop{\arg\max}\limits_{x \in \mathcal{R}^d,\left\|v\right\|=1} {v^TCv}\)
</p>

<p>
進一步轉成 Lagrange、透過偏微分求解，其實就是解 C 的特徵值(eigenvalue, \(\lambda\))和特徵向量(eigenvector, \(v\))。
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org9655554" class="outline-4">
<h4 id="org9655554"><span class="section-number-4">4.1.2.</span> 主成份分析 2</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
回到前述例子(身高和體重)，下左圖，經由 PCA 可以萃取出兩個特徵成分(投影軸，下圖右的兩條垂直的紅線，較長的紅線軸為變異量較大的主成份)。此範例算最大主成份的變異量為 13.26，第二大主成份的變異量為 1.23。
</p>


<div id="org0a119cd" class="figure">
<p><img src="images/pca-5.png" alt="pca-5.png" />
</p>
</div>

<p>
PCA 投影完的資料為下圖，從下圖可知，PC1 的變異足以表示此筆資料資訊。
</p>


<div id="orgcb2d750" class="figure">
<p><img src="images/pca-6.png" alt="pca-6.png" />
</p>
</div>

<p>
此做法可以有效的減少維度數，但整體變異量並沒有減少太多，此例從兩個變成只有一個，但變異量卻可以保留(13.26/(13.26+1.23)= 91.51%)，兩維度的資料做 PCA，對資料進行降維比較沒有感覺，但講解圖例比較容易。
</p>
</div>
</div>

<div id="outline-container-org9fcb7b6" class="outline-4">
<h4 id="org9fcb7b6"><span class="section-number-4">4.1.3.</span> 主成份分析的主要步驟</h4>
<div class="outline-text-4" id="text-4-1-3">
<ol class="org-ol">
<li>標準化數據集</li>
<li>建立共變數矩陣</li>
<li>從共變數矩陣分解出特徵值與特徵向量</li>
<li>以遞減方式對特徵值進行排序，以便對特徵向量排名</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 6: </span>  <span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Extracting the principal components step-by-step</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/ml/'</span>
<span class="linenr">11: </span>                        <span style="color: #98be65;">'machine-learning-databases/wine/wine.data'</span>,
<span class="linenr">12: </span>                        header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  df_wine.columns = [<span style="color: #98be65;">'Class label'</span>, <span style="color: #98be65;">'Alcohol'</span>, <span style="color: #98be65;">'Malic acid'</span>, <span style="color: #98be65;">'Ash'</span>,
<span class="linenr">15: </span>                     <span style="color: #98be65;">'Alcalinity of ash'</span>, <span style="color: #98be65;">'Magnesium'</span>, <span style="color: #98be65;">'Total phenols'</span>,
<span class="linenr">16: </span>                     <span style="color: #98be65;">'Flavanoids'</span>, <span style="color: #98be65;">'Nonflavanoid phenols'</span>, <span style="color: #98be65;">'Proanthocyanins'</span>,
<span class="linenr">17: </span>                     <span style="color: #98be65;">'Color intensity'</span>, <span style="color: #98be65;">'Hue'</span>,
<span class="linenr">18: </span>                     <span style="color: #98be65;">'OD280/OD315 of diluted wines'</span>, <span style="color: #98be65;">'Proline'</span>]
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #c678dd;">print</span>(df_wine.head())
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Splitting the data into 70% training and 30% test subsets.</span>
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">25: </span>
<span class="linenr">26: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(X, y,
<span class="linenr">27: </span>                                     test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">28: </span>                                     stratify=y, random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">29: </span>
<span class="linenr">30: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. Standardizing the data.</span>
<span class="linenr">31: </span>  sc = StandardScaler()
<span class="linenr">32: </span>  X_train_std = sc.fit_transform(X_train)
<span class="linenr">33: </span>  X_test_std = sc.transform(X_test)
<span class="linenr">34: </span>
<span class="linenr">35: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2. Eigendecomposition of the covariance matrix.</span>
<span class="linenr">36: </span>  cov_mat = np.cov(X_train_std.T)
<span class="linenr">37: </span>  eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
<span class="linenr">38: </span>
<span class="linenr">39: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'\nEigenvalues \n%s'</span> % eigen_vals)
<span class="linenr">40: </span>
<span class="linenr">41: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Total and explained variance</span>
<span class="linenr">42: </span>
<span class="linenr">43: </span>  tot = <span style="color: #c678dd;">sum</span>(eigen_vals)
<span class="linenr">44: </span>  var_exp = [(i / tot) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">sorted</span>(eigen_vals, reverse=<span style="color: #a9a1e1;">True</span>)]
<span class="linenr">45: </span>  cum_var_exp = np.cumsum(var_exp)
<span class="linenr">46: </span>
<span class="linenr">47: </span>  plt.bar(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">14</span>), var_exp, alpha=<span style="color: #da8548; font-weight: bold;">0.5</span>, align=<span style="color: #98be65;">'center'</span>,
<span class="linenr">48: </span>          label=<span style="color: #98be65;">'individual explained variance'</span>)
<span class="linenr">49: </span>  plt.step(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">14</span>), cum_var_exp, where=<span style="color: #98be65;">'mid'</span>,
<span class="linenr">50: </span>           label=<span style="color: #98be65;">'cumulative explained variance'</span>)
<span class="linenr">51: </span>  plt.ylabel(<span style="color: #98be65;">'Explained variance ratio'</span>)
<span class="linenr">52: </span>  plt.xlabel(<span style="color: #98be65;">'Principal component index'</span>)
<span class="linenr">53: </span>  plt.legend(loc=<span style="color: #98be65;">'best'</span>)
<span class="linenr">54: </span>  plt.tight_layout()
<span class="linenr">55: </span>  plt.savefig(<span style="color: #98be65;">'05_02.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">56: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">57: </span>
</pre>
</div>

<pre class="example" id="org0da521a">
   Class label  Alcohol  ...  OD280/OD315 of diluted wines  Proline
0            1    14.23  ...                          3.92     1065
1            1    13.20  ...                          3.40     1050
2            1    13.16  ...                          3.17     1185
3            1    14.37  ...                          3.45     1480
4            1    13.24  ...                          2.93      735

[5 rows x 14 columns]

Eigenvalues
[4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634
 0.51828472 0.34650377 0.3131368  0.10754642 0.21357215 0.15362835
 0.1808613 ]
</pre>


<div id="org0cb17ef" class="figure">
<p><img src="images/05_02.png" alt="05_02.png" width="400" />
</p>
<p><span class="figure-number">Figure 16: </span>Principal component index</p>
</div>

<p>
雖然上圖的「解釋變異數」圖有點類似隨機森林評估特徵值重要性的結果，但二者最大的不同處在於 PCA 為一種非監督式方法，也就是說，關於類別標籤資訊是被忽略的。
</p>
</div>
</div>

<div id="outline-container-org5855771" class="outline-4">
<h4 id="org5855771"><span class="section-number-4">4.1.4.</span> 特徵轉換</h4>
<div class="outline-text-4" id="text-4-1-4">
<p>
在分解「共變數矩陣」成為「特徵對」後，接下來要將資料集轉換為新的「主成份」，其步驟如下：
</p>
<ol class="org-ol">
<li>選取\(k\)個最大特徵值所對應的 k 個特徵向量，其中\(k\)為新「特徵空間」的維數(\(k \le d\))。</li>
<li>用最前面的\(k\)個特徵向量建立「投影矩陣」(project matrix)\(W\)。</li>
<li>使用投影矩陣\(W\)，輸入值為\(d\)維數據集、輸出值為新的\(k\)維「特徵子空間」。</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 6: </span>  <span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Extracting the principal components step-by-step</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/ml/'</span>
<span class="linenr">11: </span>                          <span style="color: #98be65;">'machine-learning-databases/wine/wine.data'</span>,
<span class="linenr">12: </span>                          header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',</span>
<span class="linenr">15: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Alcalinity of ash', 'Magnesium', 'Total phenols',</span>
<span class="linenr">16: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',</span>
<span class="linenr">17: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Color intensity', 'Hue',</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'OD280/OD315 of diluted wines', 'Proline']</span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Splitting the data into 70% training and 30% test subsets.</span>
<span class="linenr">21: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">22: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(X, y,
<span class="linenr">23: </span>                                      test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">24: </span>                                      stratify=y, random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">25: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. Standardizing the data.</span>
<span class="linenr">26: </span>  sc = StandardScaler()
<span class="linenr">27: </span>  X_train_std = sc.fit_transform(X_train)
<span class="linenr">28: </span>  X_test_std = sc.transform(X_test)
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2. Eigendecomposition of the covariance matrix.</span>
<span class="linenr">30: </span>  cov_mat = np.cov(X_train_std.T)
<span class="linenr">31: </span>  eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
<span class="linenr">32: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Total and explained variance</span>
<span class="linenr">33: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">tot = sum(eigen_vals)</span>
<span class="linenr">34: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]</span>
<span class="linenr">35: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">cum_var_exp = np.cumsum(var_exp)</span>
<span class="linenr">36: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Feature transformation</span>
<span class="linenr">37: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Make a list of (eigenvalue, eigenvector) tuples</span>
<span class="linenr">38: </span>  eigen_pairs = [(np.<span style="color: #c678dd;">abs</span>(eigen_vals[i]), eigen_vecs[:, i])
<span class="linenr">39: </span>                  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(eigen_vals))]
<span class="linenr">40: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Sort the (eigenvalue, eigenvector) tuples from high to low</span>
<span class="linenr">41: </span>  eigen_pairs.sort(key=<span style="color: #51afef;">lambda</span> k: k[<span style="color: #da8548; font-weight: bold;">0</span>], reverse=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">42: </span>  w = np.hstack((eigen_pairs[<span style="color: #da8548; font-weight: bold;">0</span>][<span style="color: #da8548; font-weight: bold;">1</span>][:, np.newaxis],
<span class="linenr">43: </span>                  eigen_pairs[<span style="color: #da8548; font-weight: bold;">1</span>][<span style="color: #da8548; font-weight: bold;">1</span>][:, np.newaxis]))
<span class="linenr">44: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'Matrix W:\n'</span>, w)
<span id="coderef-x-train-dot" class="coderef-off"><span class="linenr">45: </span>  <span style="color: #c678dd;">print</span>(X_train_std[<span style="color: #da8548; font-weight: bold;">0</span>].dot(w))</span>
<span id="coderef-x-train-pca" class="coderef-off"><span class="linenr">46: </span>  X_train_pca = X_train_std.dot(w)</span>
<span class="linenr">47: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot</span>
<span class="linenr">48: </span>  colors = [<span style="color: #98be65;">'r'</span>, <span style="color: #98be65;">'b'</span>, <span style="color: #98be65;">'g'</span>]
<span class="linenr">49: </span>  markers = [<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>]
<span class="linenr">50: </span>
<span class="linenr">51: </span>  <span style="color: #51afef;">for</span> l, c, m <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(np.unique(y_train), colors, markers):
<span class="linenr">52: </span>      plt.scatter(X_train_pca[y_train == l, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">53: </span>                  X_train_pca[y_train == l, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">54: </span>                  c=c, label=l, marker=m)
<span class="linenr">55: </span>
<span class="linenr">56: </span>  plt.xlabel(<span style="color: #98be65;">'PC 1'</span>)
<span class="linenr">57: </span>  plt.ylabel(<span style="color: #98be65;">'PC 2'</span>)
<span class="linenr">58: </span>  plt.legend(loc=<span style="color: #98be65;">'lower left'</span>)
<span class="linenr">59: </span>  plt.tight_layout()
<span class="linenr">60: </span>  plt.savefig(<span style="color: #98be65;">'05_03.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">61: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">62: </span>
</pre>
</div>

<pre class="example" id="org3647a3f">
Matrix W:
 [[-0.13724218  0.50303478]
 [ 0.24724326  0.16487119]
 [-0.02545159  0.24456476]
 [ 0.20694508 -0.11352904]
 [-0.15436582  0.28974518]
 [-0.39376952  0.05080104]
 [-0.41735106 -0.02287338]
 [ 0.30572896  0.09048885]
 [-0.30668347  0.00835233]
 [ 0.07554066  0.54977581]
 [-0.32613263 -0.20716433]
 [-0.36861022 -0.24902536]
 [-0.29669651  0.38022942]]
[2.38299011 0.45458499]
</pre>

<p>
使用上述程式碼產生的 13*2 維的投影矩陣可以轉換一個樣本\(x\)(以\(1 \times 13\)維的列向量表示)到 PCA 子空間(\(x'\))(前兩個主成份)：\(x' = xW\)(程式碼第<a href="#coderef-x-train-dot" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-x-train-dot');" onmouseout="CodeHighlightOff(this, 'coderef-x-train-dot');">45</a>行)；同樣的，我們也可以將整個\(124 \times 13\)維的訓練數據集轉換到兩個主成份(\(124 \times 2\)維)(程式第<a href="#coderef-x-train-pca" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-x-train-pca');" onmouseout="CodeHighlightOff(this, 'coderef-x-train-pca');">46</a>行)，最後，將轉換過的\(124 \times 2\)維矩陣以二維散點圖表示：
</p>


<div id="orge261734" class="figure">
<p><img src="images/05_03.png" alt="05_03.png" width="400" />
</p>
<p><span class="figure-number">Figure 17: </span>05_03</p>
</div>

<p>
由圖<a href="#orge261734">17</a>中可看出，與第二個主成份(y 軸)相比，數據沿著第一主成份(x 軸)的分散程度更嚴重，而由此圖也可判斷，該數據應可以一個「線性分類器」進行有效分類。
</p>
</div>
</div>

<div id="outline-container-org0614cc3" class="outline-4">
<h4 id="org0614cc3"><span class="section-number-4">4.1.5.</span> 以 Scikit-learn 進行主成份分析</h4>
<div class="outline-text-4" id="text-4-1-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 7: </span>  <span style="color: #51afef;">from</span> sklearn.decomposition <span style="color: #51afef;">import</span> PCA
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> sklearn.linear_model <span style="color: #51afef;">import</span> LogisticRegression
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">## Extracting the principal components step-by-step</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">df_wine</span> = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/ml/'</span>
<span class="linenr">13: </span>                          <span style="color: #98be65;">'machine-learning-databases/wine/wine.data'</span>,
<span class="linenr">14: </span>                          header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">df_wine.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',</span>
<span class="linenr">17: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Alcalinity of ash', 'Magnesium', 'Total phenols',</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',</span>
<span class="linenr">19: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'Color intensity', 'Hue',</span>
<span class="linenr">20: </span>  <span style="color: #5B6268;">#                     </span><span style="color: #5B6268;">'OD280/OD315 of diluted wines', 'Proline']</span>
<span class="linenr">21: </span>
<span class="linenr">22: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Splitting the data into 70% training and 30% test subsets.</span>
<span class="linenr">23: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">y</span> = df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">1</span>:].values, df_wine.iloc[:, <span style="color: #da8548; font-weight: bold;">0</span>].values
<span class="linenr">24: </span>  <span style="color: #dcaeea;">X_train</span>, <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(X, y,
<span class="linenr">25: </span>                                      test_size=<span style="color: #da8548; font-weight: bold;">0.3</span>,
<span class="linenr">26: </span>                                      stratify=y, random_state=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">27: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. Standardizing the data.</span>
<span class="linenr">28: </span>  sc = StandardScaler()
<span class="linenr">29: </span>  X_train_std = sc.fit_transform(X_train)
<span class="linenr">30: </span>  X_test_std = sc.transform(X_test)
<span class="linenr">31: </span>
<span class="linenr">32: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">33: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">34: </span>      markers = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">35: </span>      colors = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">36: </span>      cmap = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">37: </span>
<span class="linenr">38: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">39: </span>      x1_min, x1_max = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">40: </span>      x2_min, x2_max = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">41: </span>      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">42: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">43: </span>      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">44: </span>      Z = Z.reshape(xx1.shape)
<span class="linenr">45: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.4</span>, cmap=cmap)
<span class="linenr">46: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">47: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">48: </span>
<span class="linenr">49: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot class samples</span>
<span class="linenr">50: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">51: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">52: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">53: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.6</span>,
<span class="linenr">54: </span>                      c=cmap(idx),
<span class="linenr">55: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>,
<span class="linenr">56: </span>                      marker=markers[idx],
<span class="linenr">57: </span>                      label=cl)
<span class="linenr">58: </span>
<span class="linenr">59: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Training logistic regression classifier using the first 2 principal components.</span>
<span class="linenr">60: </span>  pca = PCA(n_components=<span style="color: #da8548; font-weight: bold;">2</span>)
<span id="coderef-pca-fit" class="coderef-off"><span class="linenr">61: </span>  X_train_pca = pca.fit_transform(X_train_std)</span>
<span class="linenr">62: </span>  X_test_pca = pca.transform(X_test_std)
<span class="linenr">63: </span>
<span class="linenr">64: </span>  lr = LogisticRegression()
<span class="linenr">65: </span>  lr = lr.fit(X_train_pca, y_train)
<span class="linenr">66: </span>
<span class="linenr">67: </span>  plot_decision_regions(X_train_pca, y_train, classifier=lr)
<span class="linenr">68: </span>  plt.xlabel(<span style="color: #98be65;">'PC 1'</span>)
<span class="linenr">69: </span>  plt.ylabel(<span style="color: #98be65;">'PC 2'</span>)
<span class="linenr">70: </span>  plt.legend(loc=<span style="color: #98be65;">'lower left'</span>)
<span class="linenr">71: </span>  plt.tight_layout()
<span class="linenr">72: </span>  plt.savefig(<span style="color: #98be65;">'05_04.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">73: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">74: </span>  plot_decision_regions(X_test_pca, y_test, classifier=lr)
<span class="linenr">75: </span>  plt.xlabel(<span style="color: #98be65;">'PC 1'</span>)
<span class="linenr">76: </span>  plt.ylabel(<span style="color: #98be65;">'PC 2'</span>)
<span class="linenr">77: </span>  plt.legend(loc=<span style="color: #98be65;">'lower left'</span>)
<span class="linenr">78: </span>  plt.tight_layout()
<span class="linenr">79: </span>  plt.savefig(<span style="color: #98be65;">'05_05.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">80: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
</pre>
</div>



<p>
PCA 類別是 scikit-learn 中許多轉換類別之一，首先使用訓練數據集來 fit 模型並轉換數據集(程式第<a href="#coderef-pca-fit" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-pca-fit');" onmouseout="CodeHighlightOff(this, 'coderef-pca-fit');">61</a>行)，最後以 Logistic 迴歸對數據進行分類。圖<a href="#org9dc427b">18</a>為訓練集資料的分類結果，圖<a href="#org4bfa520">19</a>測為測試資料集分類結果，可以看出二者差異不大。
</p>


<div id="org9dc427b" class="figure">
<p><img src="images/05_04.png" alt="05_04.png" width="400" />
</p>
<p><span class="figure-number">Figure 18: </span>PCA 訓練數據</p>
</div>


<div id="org4bfa520" class="figure">
<p><img src="images/05_05.png" alt="05_05.png" width="400" />
</p>
<p><span class="figure-number">Figure 19: </span>PCA 測試數據</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org2365a00" class="outline-3">
<h3 id="org2365a00"><span class="section-number-3">4.2.</span> 利用線性判別分析(LDA)做監督式數據壓縮</h3>
<div class="outline-text-3" id="text-4-2">
<p>
LDA 的全稱是 Linear Discriminant Analysis（線性判別分析），是一種 supervised learning。因為是由 Fisher 在 1936 年提出的，所以也叫 Fisher&rsquo;s Linear Discriminant。「線性判別分析」(linear discriminant analysis, LDA)為一種用來做「特徵提取」的技術，藉由降維來處理「維數災難」，可提高非正規化模型的計算效率。PCA 在於找出一個在數據集中最大化變異數的正交成分軸； 而 LDA 則是要找出可以最佳化類別分離的特徵子空間。
</p>

<p>
從主觀的理解上，主成分分析到底是什麼？它其實是對數據在高維空間下的一個投影轉換，通過一定的投影規則將原來從一個角度看到的多個維度映射成較少的維度。到底什麼是映射，下面的圖就可以很好地解釋這個問題——正常角度看是兩個半橢圓形分佈的數據集，但經過旋轉（映射）之後是兩條線性分佈數據集。<sup><a id="fnr.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup>
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right"><img src="images/lda-rot-1.jpg" alt="lda-rot-1.jpg" /></th>
<th scope="col" class="org-right"><img src="images/lda-rot-2.jpg" alt="lda-rot-2.jpg" /></th>
<th scope="col" class="org-right"><img src="images/lda-rot-3.jpg" alt="lda-rot-3.jpg" /></th>
<th scope="col" class="org-right"><img src="images/lda-rot-4.jpg" alt="lda-rot-4.jpg" /></th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-right">2</td>
<td class="org-right">3</td>
<td class="org-right">4</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-right"><img src="images/lda-rot-5.jpg" alt="lda-rot-5.jpg" /></td>
<td class="org-right"><img src="images/lda-rot-6.jpg" alt="lda-rot-6.jpg" /></td>
<td class="org-right"><img src="images/lda-rot-7.jpg" alt="lda-rot-7.jpg" /></td>
<td class="org-right"><img src="images/lda-rot-8.jpg" alt="lda-rot-8.jpg" /></td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-right">5</td>
<td class="org-right">6</td>
<td class="org-right">7</td>
<td class="org-right">8</td>
</tr>
</tbody>
</table>

<p>
LDA 與 PCA 都是常用的降維方法，二者的區別在於<sup><a id="fnr.7.100" class="footref" href="#fn.7" role="doc-backlink">7</a></sup>：
</p>
<ul class="org-ul">
<li>出發思想不同。PCA 主要是從特徵的協方差角度，去找到比較好的投影方式，即選擇樣本點投影具有最大方差的方向（ 在信號處理中認為信號具有較大的方差，噪聲有較小的方差，信噪比就是信號與噪聲的方差比，越大越好。）；而 LDA 則更多的是考慮了分類標籤信息，尋求投影后不同類別之間數據點距離更大化以及同一類別數據點距離最小化，即選擇分類性能最好的方向。</li>
<li>學習模式不同。PCA 屬於無監督式學習，因此大多場景下只作為數據處理過程的一部分，需要與其他算法結合使用，例如將 PCA 與聚類、判別分析、回歸分析等組合使用；LDA 是一種監督式學習方法，本身除了可以降維外，還可以進行預測應用，因此既可以組合其他模型一起使用，也可以獨立使用。</li>
<li>降維後可用維度數量不同。LDA 降維後最多可生成 C-1 維子空間（分類標籤數-1），因此 LDA 與原始維度 N 數量無關，只有數據標籤分類數量有關；而 PCA 最多有 n 維度可用，即最大可以選擇全部可用維度。</li>
</ul>

<p>
圖<a href="#orgee3d4e0">20</a>左側是 PCA 的降維思想，它所作的只是將整組數據整體映射到最方便表示這組數據的坐標軸上，映射時沒有利用任何數據內部的分類信息。因此，雖然 PCA 後的數據在表示上更加方便（降低了維數並能最大限度的保持原有信息），但在分類上也許會變得更加困難；圖<a href="#orgee3d4e0">20</a>右側是 LDA 的降維思想，可以看到 LDA 充分利用了數據的分類信息，將兩組數據映射到了另外一個坐標軸上，使得數據更易區分了（在低維上就可以區分，減少了運算量）。
</p>


<div id="orgee3d4e0" class="figure">
<p><img src="images/pca-lda.png" alt="pca-lda.png" />
</p>
<p><span class="figure-number">Figure 20: </span>PCA LDA 差異</p>
</div>

<p>
線性判別分析 LDA 算法由於其簡單有效性在多個領域都得到了廣泛地應用，是目前機器學習、數據挖掘領域經典且熱門的一個算法；但是算法本身仍然存在一些侷限性：
</p>
<ul class="org-ul">
<li>當樣本數量遠小於樣本的特徵維數，樣本與樣本之間的距離變大使得距離度量失效，使 LDA 算法中的類內、類間離散度矩陣奇異，不能得到最優的投影方向，在人臉識別領域中表現得尤為突出</li>
<li>LDA 不適合對非高斯分佈的樣本進行降維</li>
<li>LDA 在樣本分類信息依賴方差而不是均值時，效果不好</li>
<li>LDA 可能過度擬合數據</li>
</ul>
</div>
</div>
<div id="outline-container-orgb37e821" class="outline-3">
<h3 id="orgb37e821"><span class="section-number-3">4.3.</span> <span class="todo TODO">TODO</span> 利用核主成份分析(KPCA)處理非線性對應</h3>
</div>
<div id="outline-container-org2703aa5" class="outline-3">
<h3 id="org2703aa5"><span class="section-number-3">4.4.</span> 相關資源</h3>
<div class="outline-text-3" id="text-4-4">
<ul class="org-ul">
<li><a href="https://blog.csdn.net/kuweicai/article/details/79255270">主成分分析（PCA）和線性判別分析（LDA）原理簡介</a></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgacf3768" class="outline-2">
<h2 id="orgacf3768"><span class="section-number-2">5.</span> 深度學習應用領域</h2>
<div class="outline-text-2" id="text-5">
</div>
<div id="outline-container-org7b701a5" class="outline-3">
<h3 id="org7b701a5"><span class="section-number-3">5.1.</span> 影像辨識</h3>
<div class="outline-text-3" id="text-5-1">
</div>
<div id="outline-container-orga61a7c2" class="outline-4">
<h4 id="orga61a7c2"><span class="section-number-4">5.1.1.</span> 卷積神經網路 CNN</h4>
<div class="outline-text-4" id="text-5-1-1">
<p>
傳統機器學習進行圖片識別，主要是希望能透過原始像數值找出一種適合的分類器(classifier)，但事實證明這麼做不管用，因為信噪比太低。後來的改善方式是由人類挑選出重要特徵，然後由機器學習演算法使用這些「特徵向量(feature vectors)」進行分類判斷。這種特徵提取(feature extraction)的做法確實改善了信噪比，但是如果圖片的重要特點因光線或其他因素難以識別，則精確率會降低很多，而且，事前的人工挑選特徵花去太多人力，以深度學習進行圖片視覺就是設法消除那些既繁瑣又會造成侷限性的特徵選取程序。David Hubel 和 Torsten Wiesel 發現動物視覺皮層有一部份專門負責檢測邊緣，1959 年他們把電極插入貓的大腦中，在螢幕上投射出黑白圖案，發現有些神經元只有在出現垂直線時被激發，有些則只有在出現水平線時被激發，有些則是只有看到某特定角度的線時被激發。進一步的研究確認，視覺皮層是以分層的結構組織起來的，每一層都會根據前一層所偵測到的特徵得出進一步的訊息，從線條、輪廓、形狀，一直到整個物體。由上述研究得來的第一個概念就是「過濾器(filter)」。
典型的過濾器如下：
</p>
<ul class="org-ul">
<li>blur = [[1./9, 1./9, 1./9], [1./9, 1./9, 1./9], [1./9, 1./9, 1./9]]</li>
</ul>

<div id="org4ed3bae" class="figure">
<p><img src="images/blur-filter.png" alt="blur-filter.png" width="400" />
</p>
<p><span class="figure-number">Figure 21: </span>模糊過濾器</p>
</div>
<ul class="org-ul">
<li>edges = [[1, 1, 1], [1, -8, 1], [1, 1, 1]]</li>
</ul>
<p width="400">
<img src="images/edges-filter.png" alt="edges-filter.png" width="400" />
圖<a href="#org4ed3bae">21</a>為一 3*3 的模楜強過濾器產生的效果，圖<a href="#org9f11459">22</a>則為邊緣強週器的效果。過濾器可以改變圖形，並顯示可用於「圖形偵測」和「圖形分類」的特徵。例如，為了對數字進行分類，內部的顏色並不重要，此時，邊緣強調過濾器就有助於辨識數字的一般形狀，進而提升數字識別效能。
</p>

<p>
我們可以用「類神經網路」的方式來理解「過濾器」，將我們定義的「過濾器」視為一組加權，最終的值又做為下一層的啟動值（輸入）。如圖<a href="#org3ca1dcc">22</a>，過濾器會逐次掃過整張圖，然後建立一組新的圖片，
</p>

<div id="org3ca1dcc" class="figure">
<p><img src="images/filter-scanner.png" alt="filter-scanner.png" />
</p>
<p><span class="figure-number">Figure 22: </span>過濾器的掃瞄計算</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgc61f295" class="outline-3">
<h3 id="orgc61f295"><span class="section-number-3">5.2.</span> 語言模型</h3>
<div class="outline-text-3" id="text-5-2">
</div>
<div id="outline-container-orgef25e4e" class="outline-4">
<h4 id="orgef25e4e"><span class="section-number-4">5.2.1.</span> 遞迴類神經網路(Recurrent Neural Networks, RNNs)</h4>
<div class="outline-text-4" id="text-5-2-1">
<p>
RNN 能夠處理「任意個數的輸入序列」，所以十分適合用在「語言塑模」或「語音辨識」。理論上，RNN 可以用來處理任何問題，因戈大火弓它已被證明具有「圖靈完備性」(Turing-Complete)。以遞迴關係的函數表示 RNN 可將其視為 \(S_t=f(S_{t-1},X_t)\)，這裡的\(S_t\)表示第\(t\)步的狀態，它是由函數\(f\)對上一步(\(t-1\))的狀態(即\(S_{t-1}\))與這一步的輸入\(X_t\)所計算出來的結果，這裡的函數\(f\)可以是任何可微分的函數，如\(S_t=tang(S_{t-1}*W+X_t*U)\)。
正因為每個狀態都會與之前所有的計算有關，其所代表的重要含義為：隨著時間的推移，RNNs 可以說是有記憶力的，因為狀態 S 包含了之前所有步驟的資訊。
</p>

<p>
語言塑模的目標是計算「字的序列」的機率，這在「語音辨識」、OCR、「機器翻譯」、「拼字校正」上都非常重要。以「字」為基準的「語言模型」是由「字的序列」來定義機率分佈，給定一個長度為\(m\)的字序列，它會為整個字序列給定一個機率\(P(w_1,...,w_m)\)，其「聯合機率」(joint probability)可以由公式\eqref{org6296720}中的連鎖規則(chain rule)計算出來：
</p>
\begin{equation}
\label{org6296720}
P(w_1,...,w_m)=P(w_1)P(w_2|w_1)P(w3|w_2,w_1)...P(w_m|w_1,...,w_{m-1})
\end{equation}

<p>
這個聯合機率一般是基於一個「獨立性假設」(independence assumption)，即，第 i 個字只會相依於它之前的 n-1 個字，如果我們的模型是連續 n 個字的聯合機率，就稱為「n元」(n-gram)。例：
</p>
<ul class="org-ul">
<li>1-gram / unigram: &ldquo;The&rdquo;, &ldquo;quick&rdquo;, &ldquo;brown&rdquo; and &ldquo;fox&rdquo;</li>
<li>2-grams / bigram: &ldquo;The quick&rdquo;, &ldquo;quick brown&rdquo; and &ldquo;brown fox&rdquo;</li>
<li>3-grams / trigram: &ldquo;The quick brown&rdquo; and &ldquo;quick brown fox&rdquo;</li>
<li>4-grams: &ldquo;The quick brown fox&rdquo;</li>
</ul>

<p>
現在，如果我們有一個巨大的語料庫(corpus of text)，我們就可以用一個特定的 n(通常為 2-4)搜尋所有「n元」在「語料庫」中出現的次數，進而在「給定前 n-1 個字的前提下」，估計出每個 n 元中最後一個字出現的機率。
</p>
</div>
</div>
</div>
<div id="outline-container-org21f54f8" class="outline-3">
<h3 id="org21f54f8"><span class="section-number-3">5.3.</span> 棋盤遊戲</h3>
<div class="outline-text-3" id="text-5-3">
<p>
大約在 50 年代，研究人員開始建立具有 AI 的遊戲，這些遊戲以「西洋跳棋」(checkers)和「西洋棋」(chess)為主，這兩種遊戲有一些共同之處：
</p>
<ul class="org-ul">
<li>它們是所謂的「零和遊戲」(zero-sum games)，即一個玩家所得到的奬勵就來自另一個玩家相對應的損失。另一類相對的遊戲則是指兩位玩家可以選擇合作，如 「囚徒困境」(prisoner&rsquo;s dilemma)。</li>
<li>它們都具有「完全資訊」(perfect information)，兩方不同玩家都知道遊戲的整個狀態；另一種相對的遊戲則是撲克。因為得知目前狀態就可以導出最好的行動，所以這種遊戲可以減少 AI 所需處理問題的複雜度。</li>
<li>兩種遊戲都有「明確性」(deterministic): 如果一個玩家下了一步，這步就會導致一個明確的下一個狀態；另一種相對的遊戲中，玩家下的一步可能是丟一次骰子或是抽一張牌，這就無法導致一個明確的下一步。</li>
</ul>
</div>
</div>
<div id="outline-container-orgd802e2d" class="outline-3">
<h3 id="orgd802e2d"><span class="section-number-3">5.4.</span> 電腦遊戲</h3>
</div>
<div id="outline-container-org2079bb0" class="outline-3">
<h3 id="org2079bb0"><span class="section-number-3">5.5.</span> 異常偵測</h3>
</div>
<div id="outline-container-org6f83984" class="outline-3">
<h3 id="org6f83984"><span class="section-number-3">5.6.</span> 物體偵測</h3>
<div class="outline-text-3" id="text-5-6">
<p>
從影像中分析出物體位置，進行分類。物體偵測比物體辨識的問題更困難，最著名的方式為 R-CNN，R-CNN 的實際處理流程有點複雜，包括把影像變形成正方形，使用 SVM 分類。
</p>
</div>
</div>
<div id="outline-container-orgf3b190c" class="outline-3">
<h3 id="orgf3b190c"><span class="section-number-3">5.7.</span> 影像分割</h3>
<div class="outline-text-3" id="text-5-7">
<p>
指針對影像以像素標籤進行類別分類，利用神經網路進行影像分割，最簡單的方法就是以全部的像素為對象，再依照各個像素進行推論處理。典型做法為 FCN(Fully Convolutional Network)，相對於一般 CNN 含有全連接層的情況，FCN 把全連接層更換成「執行相同動作的卷積層」，在物體辨識的網路全連接層中，中間資料的空間大小當作排列成 1 行節點來處理。
</p>
</div>
</div>
<div id="outline-container-org976b46c" class="outline-3">
<h3 id="org976b46c"><span class="section-number-3">5.8.</span> 產生圖說</h3>
<div class="outline-text-3" id="text-5-8">
<p>
針對影像自動產生說明該影像的內容，代表性方法為 NIC (Neural Image Caption)模型，NIC 是由處理多層 CNN 與自然語言的 RNN(Recurrent Neural Network)所構成，RNN 指擁有遞迴功能的網路，常用在自然語言、時間序列資料等有連續性的資料上。
</p>
</div>
</div>
<div id="outline-container-orga0d90c8" class="outline-3">
<h3 id="orga0d90c8"><span class="section-number-3">5.9.</span> 影像風格轉換</h3>
<div class="outline-text-3" id="text-5-9">
<p>
代表論文為 A Neural Algorithm of Artistic Style。
</p>
</div>
</div>
<div id="outline-container-org5b9061d" class="outline-3">
<h3 id="org5b9061d"><span class="section-number-3">5.10.</span> 產生影像</h3>
<div class="outline-text-3" id="text-5-10">
<p>
從零開始產生「臥室」影像，代表性方法為 DCGAN(Deep Convolutional Generative Adversarial Network)。DCGAN 利用大量影像（如大量拍攝臥室影像）來學習，結束學習後，只要利用該模組就能產生新的影像。DCGAN 運用了 Generator(生成器)與 Discriminator(判別器)等兩個神經網路，Generator 產生與本尊相似的影像，Discriminator 判斷是否為本尊，即，確定是由 Generator 產生的影像或是實際拍攝的影像。兩者彼此制䚘學習，Generator 可以學習到更精巧的偽裝影像技術，Discriminator 則學習更高的鑑定技能，二者相互切磋成長，最終，Generator 能學會畫出與本尊一模一樣的影像。
</p>
</div>
</div>
<div id="outline-container-org2ef3c85" class="outline-3">
<h3 id="org2ef3c85"><span class="section-number-3">5.11.</span> 自動駕駛</h3>
<div class="outline-text-3" id="text-5-11">
<p>
最近在辨識周圍環境的技術中，深度學習的能力頗受期待，例如以 CNN 為基礎的網路 SegNet 即可精確辨識走路的環境。
</p>
</div>
</div>
<div id="outline-container-org1f83acc" class="outline-3">
<h3 id="org1f83acc"><span class="section-number-3">5.12.</span> Deep Q-Network (強化學習)</h3>
<div class="outline-text-3" id="text-5-12">
<p>
人類是透過嚐試錯誤來學習，例如騎腳踏車，在電腦領域中，也有從嚐試錯誤的過程中進行自主學習的例子，稱為強化學習(reinforcement learning)。在強化學習中，代理人(Agent)根據環境狀況來決定要採取的行動，利用該行動讓㼈境變化。隨環境變化，代理人獲得某些報酬。強化學習的目的是決定代理人的行動方針，以獲得更好的報酬。典型的 DQN 可以讓遊戲自動學習，達到超越人類等級的能力，使用 DQN 的 CNN 可以輸入遊戲影像(如連續 4 個畫面)，最後針對遊戲的控制器動作(搖桿的動作與按鈕)分別輸出該動作的「價值」。由於 DQN 的輸入只是影像，所以不用隨著遊戲的不同來改變設定，同一套 DQN 可以學習「小精靈」與「Atari」。DQN 與 AlphaGo 都是 Google Deep Mind 公司的研究。* ex: 入侵偵測系統
</p>
</div>
</div>
</div>

<div id="outline-container-orgb5d2713" class="outline-2">
<h2 id="orgb5d2713"><span class="section-number-2">6.</span> 深度學習的類型</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orgbd21b5f" class="outline-3">
<h3 id="orgbd21b5f"><span class="section-number-3">6.1.</span> VGG</h3>
<div class="outline-text-3" id="text-6-1">
<p>
VGG 為由卷積層與池化層構成的基本 CNN。特色是含權重層（卷積層及全連接層）共 16-19 層，有時會稱為 VGG16 或 VGG19。VGG 由於結構非常簡單，應用性高，所以多數技術人員喜歡使用以 VGG 為最基礎的網路。
</p>
</div>
</div>

<div id="outline-container-org53b6cc0" class="outline-3">
<h3 id="org53b6cc0"><span class="section-number-3">6.2.</span> GoodLeNet</h3>
<div class="outline-text-3" id="text-6-2">
<p>
GoogLeLeNet 基本上與 CNN 相同，其特色是不僅會往垂直方向加深網路，也會往水平方向加深。GoogLeNet 往水平方向的做法稱為「Inception 結構」。
</p>
</div>
</div>

<div id="outline-container-org5f0bca3" class="outline-3">
<h3 id="org5f0bca3"><span class="section-number-3">6.3.</span> ResNet</h3>
<div class="outline-text-3" id="text-6-3">
<p>
ResNet 是由 Microsoft 團隊開發的網路，特色是具有能加深比過去更多層的「結構」，為了解決因加深過多層數無法順利學習的問題，ResNet 導入了「跳躍結構」（也稱為捷徑或分流）。跳躍結構是「直接」傳遞輸入資料，所以在反向傳播時，也會將上層的梯度「直接」傳遞給下層。透過這種跳躍結構，不用擔心梯度變小（或變得太大），可以把「具有意義的梯度」傳遞給上層。因此，跳躍結構能減少之前因為加深層數，使得梯度變小，出現梯度消失的問題。
</p>
</div>
</div>
</div>

<div id="outline-container-orgc03fb24" class="outline-2">
<h2 id="orgc03fb24"><span class="section-number-2">7.</span> 實作範例</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org8bba0c6" class="outline-3">
<h3 id="org8bba0c6"><span class="section-number-3">7.1.</span> 以 Keras 解決分類問題</h3>
<div class="outline-text-3" id="text-7-1">
</div>
<div id="outline-container-org46940b6" class="outline-4">
<h4 id="org46940b6"><span class="section-number-4">7.1.1.</span> 二元分類：IMDB</h4>
<div class="outline-text-4" id="text-7-1-1">
<p>
自 IMDB 資料集中取得 50000 個正/負評論，各 25000 個，該資料集已內建於 Keras 中，且資料已先預處理，電影評論內容為由單字構成的 list 結構，例如，若評論內容為&ldquo;In a Wonderful morning&#x2026;&rdquo;，其 list 結構可能為(8, 3, 386, 1969&#x2026;)，每個單字都會依據其出現頻率給定一個編號，編號越小越常見。(與 IMDb 相關的 paper 參見<a href="https://paperswithcode.com/sota/sentiment-analysis-on-imdb">Sentiment Analysis on IMDb / paperswithcode</a>
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr">2: </span>  (train_data, train_labels), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_labels</span>) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">3: </span>  <span style="color: #c678dd;">print</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">4: </span>  <span style="color: #c678dd;">print</span>(train_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>

<pre class="example">
[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]
1
</pre>


<p>
如上為第一筆評論的單字代號與評論結果，若要將原始資料的單字代號還原，其程式碼如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span id="coderef-imdbLoadData" class="coderef-off"><span class="linenr"> 2: </span>  (train_data, train_labels), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_labels</span>) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">word_index is a dictionary mapping words to an integer index</span>
<span id="coderef-wordIndex" class="coderef-off"><span class="linenr"> 5: </span>  word_index = imdb.get_word_index()</span>
<span class="linenr"> 6: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#23383;&#20856;&#20013;key&#28858;this&#23565;&#25033;&#30340;value:"</span>,word_index[<span style="color: #98be65;">'this'</span>])
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">We reverse it, mapping integer indices to words</span>
<span id="coderef-reverseWordIndex" class="coderef-off"><span class="linenr"> 8: </span>  reverse_word_index = <span style="color: #c678dd;">dict</span>([(value, key) <span style="color: #51afef;">for</span> (key, value) <span style="color: #51afef;">in</span> word_index.items()])</span>
<span class="linenr"> 9: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#21453;&#36681;&#23383;&#20856;&#20013;key&#28858;11&#25152;&#23565;&#25033;&#21040;&#30340;value:"</span>,reverse_word_index[<span style="color: #da8548; font-weight: bold;">11</span>])
<span class="linenr">10: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#21453;&#36681;&#23383;&#20856;&#20013;key&#28858;1&#25152;&#23565;&#25033;&#21040;&#30340;value:"</span>,reverse_word_index[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">11: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#21453;&#36681;&#23383;&#20856;&#20013;key&#28858;2&#25152;&#23565;&#25033;&#21040;&#30340;value:"</span>,reverse_word_index[<span style="color: #da8548; font-weight: bold;">2</span>])
<span class="linenr">12: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">We decode the review; note that our indices were offset by 3</span>
<span class="linenr">13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">because 0, 1 and 2 are reserved indices for "padding", "start of sequence", and "unknown".</span>
<span id="coderef-decodedReview" class="coderef-off"><span class="linenr">14: </span>  decoded_review = <span style="color: #98be65;">' '</span>.join([reverse_word_index.get(i - <span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #98be65;">'?'</span>) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> train_data[<span style="color: #da8548; font-weight: bold;">0</span>]])</span>
<span class="linenr">15: </span>  <span style="color: #c678dd;">print</span>(decoded_review)
</pre>
</div>

<pre class="example" id="org654de60">
字典中key為this對應的value: 11
反轉字典中key為11所對應到的value: this
反轉字典中key為1所對應到的value: the
反轉字典中key為2所對應到的value: and
編號 0的單字: None
編號 1的單字: the
編號 2的單字: and
編號 3的單字: a
編號11的單字: this
? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all
</pre>

<p>
上述程式中第<a href="#coderef-wordIndex" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-wordIndex');" onmouseout="CodeHighlightOff(this, 'coderef-wordIndex');">5</a>行主要負責取得單字(key)的對應數字(value)的字典，再藉由第<a href="#coderef-reverseWordIndex" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-reverseWordIndex');" onmouseout="CodeHighlightOff(this, 'coderef-reverseWordIndex');">8</a>行將(key:value)轉換為(value:key)，最後第<a href="#coderef-decodedReview" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-decodedReview');" onmouseout="CodeHighlightOff(this, 'coderef-decodedReview');">14</a>行將字典中的單字回復至原始評論，程式中(i-3)的原因是第<a href="#coderef-imdbLoadData" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-imdbLoadData');" onmouseout="CodeHighlightOff(this, 'coderef-imdbLoadData');">2</a>的 load 已預留了第 0~2 個位置做特殊用途。
</p>
</div>

<ol class="org-ol">
<li><a id="orgf2f7ef6"></a>準備資料<br />
<div class="outline-text-5" id="text-7-1-1-1">
<p>
由於 IMDB 匯入 train_data 及 test_data 均為 list 型態，要先轉換為 tensor 才能輸入至神經網路，方法有二：
</p>

<ol class="org-ol">
<li>填補資料中每個子 list 內容使其具有相同長度，再轉 shapre。</li>
<li>對每個子 list 做 one-hot 編碼，其程式碼如下：</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  (train_data, train_labels), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_labels</span>) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 8: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create an all-zero matrix of shape (len(sequences), dimension)</span>
<span class="linenr"> 9: </span>      results = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr">10: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr">11: </span>          results[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">set specific indices of results[i] to 1s</span>
<span class="linenr">12: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr">15: </span>  x_train = vectorize_sequences(train_data)
<span class="linenr">16: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr">17: </span>  x_test = vectorize_sequences(test_data)
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #c678dd;">print</span>(x_train[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26368;&#24460;&#20877;&#23559;&#27161;&#31844;&#36039;&#26009;&#20063;&#21521;&#37327;&#21270;</span>
<span class="linenr">22: </span>  y_train = np.asarray(train_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">23: </span>  y_test = np.asarray(test_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">24: </span>
<span class="linenr">25: </span>  <span style="color: #c678dd;">print</span>(y_train[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>


<pre class="example">
[0. 1. 1. ... 0. 0. 0.]
1.0
</pre>
</div>
</li>

<li><a id="org7e22636"></a>建立神經網路<br />
<div class="outline-text-5" id="text-7-1-1-2">
<p>
由於輸入資料為向量、標籤為純量(1, 0)，對這樣的問題，適合用 relu 啟動函數的全連接層(Dense)堆疊架構：Dense(16, activation=&rsquo;relu&rsquo;)。其中 16 指該層神經元的數量(也可看成該層的寬度)，典型旳寫法為：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">output</span> = relu(dot(W, <span style="color: #c678dd;">input</span>)+b)
</pre>
</div>

<p>
擁有 16 個神經單元表示權重矩陣 W 的 shape 為(input_dimension, 10)，在 W 和 input 做內積後，input 資料會被映射到 16 維的空間上，最後加上 b、套用 relu 運算來產生輸出值。每一層的神經元數越多，可以讓神經網路學習更複雜的資料表示法，但也使計算成本更高。
</p>

<p>
要建構一個 Dense 層堆疊架構，要考慮兩個關鍵：
</p>

<ol class="org-ol">
<li>要用多少層？</li>
<li>每一層要有多少神經元？</li>
</ol>

<p>
此處使用兩個中間層、一個輸出層，如圖<a href="#orgd746ec4">23</a>，一般的神經網路中，介於輸入層和輸出層間的習慣稱為隱藏層(hidden layers)，但 Keras 的輸入層也有隱藏層的特性。圖<a href="#orgd746ec4">23</a>的 hidden layer 以 relu 為啟動函數，輸出層以 sigmoid 啟動函數輸出機率值。
</p>


<div id="orgd746ec4" class="figure">
<p><img src="images/nn3-6.png" alt="nn3-6.png" width="200" />
</p>
<p><span class="figure-number">Figure 23: </span>IMDB model 架構</p>
</div>

<p>
為何要有 relu 等啟動函數？原因之一是這類函數為非線性函數(如圖<a href="#orga814191">24</a>)，如果不是線性函數，則 Dense 層的運作就會變成
</p>

<div id="orga814191" class="figure">
<p><img src="images/ReLUPlot.png" alt="ReLUPlot.png" width="500" />
</p>
<p><span class="figure-number">Figure 24: </span>ReLU 函數圖</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #dcaeea;">output</span> = dot(W, <span style="color: #c678dd;">input</span>)+b
</pre>
</div>

<p>
也就是說，該層只能學習輸入資料的線性變換，即使輸入資料的維度再多，也只是這些多維空間的所有可能線性變換，如此一來就算加入再多層的運算，最終仍只是在做線性運算，並無助於複雜學習。
</p>

<p>
圖<a href="#orgd746ec4">23</a>的實作程式如下：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">5: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">6: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">7: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>

<p>
建好 model 後，要選擇一個損失函數和一個優化器，由於要處理的是二元分類問題，所以最好用 binary_crossentropy 損失函數，因為 crossentropy 主要就是用來測量機率分佈之間的距離(差異)。其實作如下：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">2: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">3: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>

<p>
之所以能將 optimizer 和 loss function 以字串方式經由參數傳給 compoile()，這是因為 rmsprop、binary_crossentropy 和 accuracy 均已事先在 Keras 套件中定義好了，若是要進一步自訂參數(如自訂學習率)，做法如下：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35519;&#25972;learning rate</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr"> 5: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 6: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#21478;&#22806;&#30340;&#35413;&#20272;&#20989;&#25976;</span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> losses
<span class="linenr">10: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> metrics
<span class="linenr">11: </span>
<span class="linenr">12: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr">13: </span>                loss=losses.binary_crossentropy,
<span class="linenr">14: </span>                metrics=[metrics.binary_accuracy])
<span class="linenr">15: </span>
<span class="linenr">16: </span>
</pre>
</div>
</div>
</li>

<li><a id="orga3861ef"></a>驗證神經網路的 model<br />
<div class="outline-text-5" id="text-7-1-1-3">
<p>
為了在訓練期間監控 model 對新資料的準確度，可以從原始訓練資料中分離出 10000 個樣本來建立驗證資料集。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">x_val</span> = x_train[:<span style="color: #da8548; font-weight: bold;">10000</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21069;10000&#20491;&#36039;&#26009;&#28858;&#39511;&#35657;&#38598;</span>
<span class="linenr">2: </span>  <span style="color: #dcaeea;">partial_x_train</span> = x_train[<span style="color: #da8548; font-weight: bold;">10000</span>:] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;10000&#20491;&#20197;&#24460;&#28858;&#35347;&#32244;&#38598;</span>
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">y_val</span> = y_train[:<span style="color: #da8548; font-weight: bold;">10000</span>]
<span class="linenr">5: </span>  <span style="color: #dcaeea;">partial_y_train</span> = y_train[<span style="color: #da8548; font-weight: bold;">10000</span>:]
</pre>
</div>

<p>
接下來才是使用 fit()來訓練模型，進行 20 個訓練週期(epoch，即，把 x_train 和 y_train 張量中的所有訓練樣本進行 20 輪的訓練)，以 512 個小樣本的小批量(batch_size)進行訓練，
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">history</span> = model.fit(partial_x_train,
<span class="linenr">2: </span>                      partial_y_train,
<span class="linenr">3: </span>                      epochs=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">4: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr">5: </span>                      validation_data=(x_val, y_val))
</pre>
</div>

<p>
model.fit()會回傳一個 history 物件，這物件本身有一個 history 屬性，為一個包含有關訓練過程中相關數據的字典，這個字期包含有 4 個項目(val_loss, val_acc, loss, acc)，為訓練和驗證時監控的指標。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28310;&#20633;&#36039;&#26009;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr"> 3: </span>  (train_data, train_labels), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_labels</span>) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 6: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create an all-zero matrix of shape (len(sequences), dimension)</span>
<span class="linenr"> 7: </span>      results = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr"> 8: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 9: </span>          results[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">set specific indices of results[i] to 1s</span>
<span class="linenr">10: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr">12: </span>  x_train = vectorize_sequences(train_data)
<span class="linenr">13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr">14: </span>  x_test = vectorize_sequences(test_data)
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26368;&#24460;&#20877;&#23559;&#27161;&#31844;&#36039;&#26009;&#20063;&#21521;&#37327;&#21270;</span>
<span class="linenr">16: </span>  y_train = np.asarray(train_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">17: </span>  y_test = np.asarray(test_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;model</span>
<span class="linenr">19: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">20: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">21: </span>  model = models.Sequential()
<span class="linenr">22: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">23: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">24: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr">25: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">26: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr">27: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">28: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39511;&#35657;&#25976;&#25818;&#38598;</span>
<span class="linenr">30: </span>  x_val = x_train[:<span style="color: #da8548; font-weight: bold;">10000</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21069;10000&#20491;&#36039;&#26009;&#28858;&#39511;&#35657;&#38598;</span>
<span class="linenr">31: </span>  partial_x_train = x_train[<span style="color: #da8548; font-weight: bold;">10000</span>:] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;10000&#20491;&#20197;&#24460;&#28858;&#35347;&#32244;&#38598;</span>
<span class="linenr">32: </span>  y_val = y_train[:<span style="color: #da8548; font-weight: bold;">10000</span>]
<span class="linenr">33: </span>  partial_y_train = y_train[<span style="color: #da8548; font-weight: bold;">10000</span>:]
<span class="linenr">34: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;model</span>
<span class="linenr">35: </span>  history = model.fit(partial_x_train,
<span class="linenr">36: </span>                      partial_y_train,
<span class="linenr">37: </span>                      epochs=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">38: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr">39: </span>                      validation_data=(x_val, y_val),
<span class="linenr">40: </span>                      verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">41: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31168;&#20986;history&#26550;&#27083;</span>
<span class="linenr">42: </span>  history_dict = history.history
<span class="linenr">43: </span>  <span style="color: #c678dd;">print</span>(history_dict.keys())
<span class="linenr">44: </span>
<span class="linenr">45: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">46: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">47: </span>  accuracy = history.history[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr">48: </span>  val_accuracy = history.history[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr">49: </span>  loss = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">50: </span>  val_loss = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">51: </span>  epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(accuracy) + <span style="color: #da8548; font-weight: bold;">1</span>)<span style="color: #5B6268;"># </span><span style="color: #5B6268;">"bo" is for "blue dot"</span>
<span class="linenr">52: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">53: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">b is for "solid blue line"</span>
<span class="linenr">54: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">55: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">56: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">57: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">58: </span>  plt.legend()
<span class="linenr">59: </span>  plt.plot()
<span class="linenr">60: </span>  plt.savefig(<span style="color: #98be65;">"imdb-Keras-1.png"</span>)
<span class="linenr">61: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()plt.clf()   # clear figure</span>
<span class="linenr">62: </span>
<span class="linenr">63: </span>  plt.clf()
<span class="linenr">64: </span>  acc_values = history_dict[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr">65: </span>  val_acc_values = history_dict[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr">66: </span>  plt.plot(epochs, accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">67: </span>  plt.plot(epochs, val_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">68: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">69: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">70: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">71: </span>  plt.legend()
<span class="linenr">72: </span>  plt.plot()
<span class="linenr">73: </span>  plt.savefig(<span style="color: #98be65;">"imdb-Keras-2.png"</span>)
<span class="linenr">74: </span>
<span class="linenr">75: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">76: </span>
</pre>
</div>

<pre class="example">
dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])
</pre>



<div id="org94773a0" class="figure">
<p><img src="images/imdb-Keras-1.png" alt="imdb-Keras-1.png" />
</p>
<p><span class="figure-number">Figure 25: </span>IMDB-Keras-1</p>
</div>


<div id="org9e554a0" class="figure">
<p><img src="images/imdb-Keras-2.png" alt="imdb-Keras-2.png" />
</p>
<p><span class="figure-number">Figure 26: </span>IMDB-Keras-2</p>
</div>
</div>
</li>

<li><a id="orge1d378c"></a>優化 model<br />
<div class="outline-text-5" id="text-7-1-1-4">
<p>
由圖<a href="#org94773a0">25</a>、<a href="#org9e554a0">26</a>可以看出，上述 model 雖然在訓練階段的效能不錯，loss function 隨 epoch 下降、accuracy 也隨 epoch 升高，但在驗證階段的表現卻十分不理想，不僅 accuracy 隨 epoch 的增加呈緩降趨勢，loss function 甚至還往上急升。
</p>

<p>
第二版的 model 加入了兩層 layer 以及 dropout 層，其架構如下:
</p>


<div id="org64876a8" class="figure">
<p><img src="images/nn3-6-2.png" alt="nn3-6-2.png" width="200" />
</p>
<p><span class="figure-number">Figure 27: </span>IMDB model 架構#2</p>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28310;&#20633;&#36039;&#26009;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> imdb
<span class="linenr"> 3: </span>  (train_data, train_labels), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_labels</span>) = imdb.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 6: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Create an all-zero matrix of shape (len(sequences), dimension)</span>
<span class="linenr"> 7: </span>      results = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr"> 8: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 9: </span>          results[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">set specific indices of results[i] to 1s</span>
<span class="linenr">10: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr">12: </span>  x_train = vectorize_sequences(train_data)
<span class="linenr">13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr">14: </span>  x_test = vectorize_sequences(test_data)
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26368;&#24460;&#20877;&#23559;&#27161;&#31844;&#36039;&#26009;&#20063;&#21521;&#37327;&#21270;</span>
<span class="linenr">16: </span>  y_train = np.asarray(train_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">17: </span>  y_test = np.asarray(test_labels).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;model</span>
<span class="linenr">19: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">20: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">21: </span>
<span class="linenr">22: </span>  model = models.Sequential()
<span class="linenr">23: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">24: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">25: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr">26: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">27: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr">28: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr">29: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">30: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> metrics
<span class="linenr">31: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=<span style="color: #da8548; font-weight: bold;">0.0001</span>),
<span class="linenr">32: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">33: </span>                metrics=[metrics.binary_accuracy])
<span class="linenr">34: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39511;&#35657;&#25976;&#25818;&#38598;</span>
<span class="linenr">35: </span>  x_val = x_train[:<span style="color: #da8548; font-weight: bold;">10000</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21069;10000&#20491;&#36039;&#26009;&#28858;&#39511;&#35657;&#38598;</span>
<span class="linenr">36: </span>  partial_x_train = x_train[<span style="color: #da8548; font-weight: bold;">10000</span>:] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;10000&#20491;&#20197;&#24460;&#28858;&#35347;&#32244;&#38598;</span>
<span class="linenr">37: </span>  y_val = y_train[:<span style="color: #da8548; font-weight: bold;">10000</span>]
<span class="linenr">38: </span>  partial_y_train = y_train[<span style="color: #da8548; font-weight: bold;">10000</span>:]
<span class="linenr">39: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;model</span>
<span class="linenr">40: </span>  history = model.fit(partial_x_train,
<span class="linenr">41: </span>                      partial_y_train,
<span class="linenr">42: </span>                      epochs=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">43: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr">44: </span>                      validation_data=(x_val, y_val),
<span class="linenr">45: </span>                      verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">46: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31168;&#20986;history&#26550;&#27083;</span>
<span class="linenr">47: </span>  history_dict = history.history
<span class="linenr">48: </span>  <span style="color: #c678dd;">print</span>(history_dict.keys())
<span class="linenr">49: </span>
<span class="linenr">50: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr">51: </span>  x = model.predict(x_test)
<span class="linenr">52: </span>  <span style="color: #c678dd;">print</span>(x)
<span class="linenr">53: </span>
<span class="linenr">54: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">55: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">56: </span>  plt.clf()
<span class="linenr">57: </span>  binary_accuracy = history.history[<span style="color: #98be65;">'binary_accuracy'</span>]
<span class="linenr">58: </span>  val_binary_accuracy = history.history[<span style="color: #98be65;">'val_binary_accuracy'</span>]
<span class="linenr">59: </span>  loss = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">60: </span>  val_loss = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">61: </span>  epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(binary_accuracy) + <span style="color: #da8548; font-weight: bold;">1</span>)<span style="color: #5B6268;"># </span><span style="color: #5B6268;">"bo" is for "blue dot"</span>
<span class="linenr">62: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">63: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">b is for "solid blue line"</span>
<span class="linenr">64: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">65: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">66: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">67: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">68: </span>  plt.legend()
<span class="linenr">69: </span>  plt.plot()
<span class="linenr">70: </span>  plt.savefig(<span style="color: #98be65;">"imdb-Keras-3.png"</span>)
<span class="linenr">71: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()plt.clf()   # clear figure</span>
<span class="linenr">72: </span>
<span class="linenr">73: </span>  plt.clf()
<span class="linenr">74: </span>  acc_values = history_dict[<span style="color: #98be65;">'binary_accuracy'</span>]
<span class="linenr">75: </span>  val_acc_values = history_dict[<span style="color: #98be65;">'val_binary_accuracy'</span>]
<span class="linenr">76: </span>  plt.plot(epochs, binary_accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">77: </span>  plt.plot(epochs, val_binary_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">78: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">79: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">80: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">81: </span>  plt.legend()
<span class="linenr">82: </span>  plt.plot()
<span class="linenr">83: </span>  plt.savefig(<span style="color: #98be65;">"imdb-Keras-4.png"</span>)
<span class="linenr">84: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">85: </span>
</pre>
</div>

<pre class="example">
dict_keys(['val_loss', 'val_binary_accuracy', 'loss', 'binary_accuracy'])
[[0.1434195 ]
 [0.9996901 ]
 [0.98705375]
 ...
 [0.05256996]
 [0.11039814]
 [0.7423996 ]]
</pre>


<p>
[[file:<img src="images/imdb-Keras-3.png" alt="imdb-Keras-3.png" />images/imdb-Keras-1.png]]
</p>

<p>
[[file:<img src="images/imdb-Keras-4.png" alt="imdb-Keras-4.png" />images/imdb-Keras-2.png]]
</p>

<p>
比較上述兩組結果，可以發現優化版的 model 在 loss function 以及 accuracy 的表現都有進步。
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org66043b1" class="outline-4">
<h4 id="org66043b1"><span class="section-number-4">7.1.2.</span> 多類別分類：數位新聞</h4>
<div class="outline-text-4" id="text-7-1-2">
<p>
目標：將路透社(Reuters)的數位新聞專欄分成 46 個主題，這屬於多類別分類(multiclass classification)問題，每個資料點只會被歸入一個類別；如果每個資料點可能屬於多個類別，則屬於多標籤多類別(multilabel multiclass classification)問題。
</p>
</div>

<ol class="org-ol">
<li><a id="orgf79ce6e"></a>資料集<br />
<div class="outline-text-5" id="text-7-1-2-1">
<p>
和 MNIST、IMDB 一樣，這組由 Reuters 在 1986 年發布的簡短新聞主題資料集也內建在 Keras 中，這個資料集總共分為 46 個不同主題。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> reuters
<span class="linenr">2: </span>  (train_data, train_labels), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_labels</span>) = reuters.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">3: </span>  <span style="color: #c678dd;">print</span>(train_data[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">4: </span>  <span style="color: #c678dd;">print</span>(train_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
</pre>
</div>

<pre class="example">
[1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]
3
</pre>


<p>
將資料向量化有幾種方式：將 label list 轉為整數張量，或是用 one-hot 編碼。以下為使用 pythonh 自訂的編碼程式：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr"> 4: </span>      results = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr"> 5: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 6: </span>          results[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.
<span class="linenr"> 7: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr">10: </span>  x_train = vectorize_sequences(train_data)
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr">12: </span>  x_test = vectorize_sequences(test_data)
<span class="linenr">13: </span>
</pre>
</div>

<p>
另外，Keras 也有一個內建的函式可用：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.utils.np_utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">one_hot_train_labels</span> = to_categorical(train_labels)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">one_hot_test_labels</span> = to_categorical(test_labels)
<span class="linenr">5: </span>
</pre>
</div>
</div>
</li>

<li><a id="org4b1cb59"></a>建立神經網路<br />
<div class="outline-text-5" id="text-7-1-2-2">
<p>
此次面臨的問題不似 IMDB 只分成兩類，而是共有 46 類，若每個 Dense layer 仍只使用 16 個維度，可能無法學會區分 46 個不同類別，故有需要將維度增加：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">5: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">6: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">7: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">46</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">8: </span>
</pre>
</div>

<p>
另外，輸出層將啟動函數由 sigmoid 改為 softmax，以機率值來顯示預測的類別結果，配合這種情境，最適合的損失函數為 categorical_crossentropy，它可以測量兩個機率分佈間的差距（即神經網路輸出的預測機率分佈與真實分佈間的距離），透過最小化這兩個分佈間的距離來訓練神經網路，讓結果接近答案。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">2: </span>                loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr">3: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">4: </span>
</pre>
</div>
</div>
</li>

<li><a id="orgaac35a2"></a>驗證數據集<br />
<div class="outline-text-5" id="text-7-1-2-3">
<p>
由訓練集鵋出 1000 個樣本來驗證：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">x_val</span> = x_train[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr">2: </span>  <span style="color: #dcaeea;">partial_x_train</span> = x_train[<span style="color: #da8548; font-weight: bold;">1000</span>:]
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">y_val</span> = one_hot_train_labels[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr">5: </span>  <span style="color: #dcaeea;">partial_y_train</span> = one_hot_train_labels[<span style="color: #da8548; font-weight: bold;">1000</span>:]
</pre>
</div>
</div>
</li>

<li><a id="orgbe9a710"></a>完整實作<br />
<div class="outline-text-5" id="text-7-1-2-4">
<p>
以下為完整的 model 程式碼
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> reuters
<span class="linenr">  2: </span>
<span class="linenr">  3: </span>  (train_data, train_labels), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_labels</span>) = reuters.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">  4: </span>
<span class="linenr">  5: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  6: </span>
<span class="linenr">  7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr">  8: </span>      results = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr">  9: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 10: </span>          results[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.
<span class="linenr"> 11: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr"> 12: </span>
<span class="linenr"> 13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr"> 14: </span>  x_train = vectorize_sequences(train_data)
<span class="linenr"> 15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr"> 16: </span>  x_test = vectorize_sequences(test_data)
<span class="linenr"> 17: </span>
<span class="linenr"> 18: </span>  <span style="color: #51afef;">from</span> keras.utils.np_utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr"> 19: </span>
<span class="linenr"> 20: </span>  one_hot_train_labels = to_categorical(train_labels)
<span class="linenr"> 21: </span>  one_hot_test_labels = to_categorical(test_labels)
<span class="linenr"> 22: </span>
<span class="linenr"> 23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#27083;&#27169;&#22411;</span>
<span class="linenr"> 24: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 25: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 26: </span>
<span class="linenr"> 27: </span>  model = models.Sequential()
<span class="linenr"> 28: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr"> 29: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 30: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">46</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr"> 31: </span>
<span class="linenr"> 32: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr"> 33: </span>                loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span id="coderef-metricsName" class="coderef-off"><span class="linenr"> 34: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])</span>
<span class="linenr"> 35: </span>
<span class="linenr"> 36: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39511;&#35657;</span>
<span class="linenr"> 37: </span>  x_val = x_train[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr"> 38: </span>  partial_x_train = x_train[<span style="color: #da8548; font-weight: bold;">1000</span>:]
<span class="linenr"> 39: </span>
<span class="linenr"> 40: </span>  y_val = one_hot_train_labels[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr"> 41: </span>  partial_y_train = one_hot_train_labels[<span style="color: #da8548; font-weight: bold;">1000</span>:]
<span class="linenr"> 42: </span>
<span class="linenr"> 43: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;</span>
<span class="linenr"> 44: </span>  history = model.fit(partial_x_train,
<span class="linenr"> 45: </span>                      partial_y_train,
<span class="linenr"> 46: </span>                      epochs=<span style="color: #da8548; font-weight: bold;">9</span>,
<span class="linenr"> 47: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr"> 48: </span>                      validation_data=(x_val, y_val),
<span class="linenr"> 49: </span>                      verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 50: </span>
<span class="linenr"> 51: </span>  history_dict = history.history
<span class="linenr"> 52: </span>  <span style="color: #c678dd;">print</span>(history_dict.keys())
<span class="linenr"> 53: </span>
<span class="linenr"> 54: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35413;&#20272;</span>
<span class="linenr"> 55: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Returns the loss value &amp; metrics values for the model in test mode.</span>
<span id="coderef-modelEvaluate" class="coderef-off"><span class="linenr"> 56: </span>  results = model.evaluate(x_test, one_hot_test_labels)</span>
<span class="linenr"> 57: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#35413;&#20272;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,results)
<span class="linenr"> 58: </span>
<span class="linenr"> 59: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#28204;</span>
<span class="linenr"> 60: </span>  predictions = model.predict(x_test)
<span class="linenr"> 61: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#26550;&#27083;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>].shape)
<span class="linenr"> 62: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 63: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#32080;&#26524;:"</span>,np.argmax(predictions[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr"> 64: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#31572;&#26696;:"</span>,one_hot_test_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 65: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr"> 66: </span>
<span class="linenr"> 67: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 68: </span>
<span class="linenr"> 69: </span>  loss = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 70: </span>  val_loss = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 71: </span>
<span class="linenr"> 72: </span>  epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(loss) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 73: </span>
<span class="linenr"> 74: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr"> 75: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr"> 76: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr"> 77: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr"> 78: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr"> 79: </span>  plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">3</span>])
<span class="linenr"> 80: </span>  plt.legend()
<span class="linenr"> 81: </span>  plt.plot()
<span class="linenr"> 82: </span>  plt.savefig(<span style="color: #98be65;">"reuters-1.png"</span>)
<span class="linenr"> 83: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr"> 84: </span>
<span class="linenr"> 85: </span>  plt.clf()   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">clear figure</span>
<span class="linenr"> 86: </span>
<span class="linenr"> 87: </span>  accuracy = history.history[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr"> 88: </span>  val_accuracy = history.history[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr"> 89: </span>
<span class="linenr"> 90: </span>  plt.plot(epochs, accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training accuracy'</span>)
<span class="linenr"> 91: </span>  plt.plot(epochs, val_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation accuracy'</span>)
<span class="linenr"> 92: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr"> 93: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr"> 94: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr"> 95: </span>  plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr"> 96: </span>  plt.legend()
<span class="linenr"> 97: </span>  plt.plot()
<span class="linenr"> 98: </span>  plt.savefig(<span style="color: #98be65;">"reuters-2.png"</span>)
<span class="linenr"> 99: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">100: </span>
</pre>
</div>

<pre class="example" id="org6b6cfe3">
dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])

  32/2246 [..............................] - ETA: 0s
 672/2246 [=======&gt;......................] - ETA: 0s
1344/2246 [================&gt;.............] - ETA: 0s
2016/2246 [=========================&gt;....] - ETA: 0s
2246/2246 [==============================] - 0s 78us/step
評估資料內容： [0.9810597261783807, 0.7804986834526062]
預測資料架構： (46,)
預測資料內容： [4.7579077e-05 1.2676844e-03 1.5874884e-04 9.6115595e-01 2.2415580e-02
 4.0142340e-06 1.0888425e-04 6.9402384e-05 6.5381191e-04 8.4027524e-05
 1.4560925e-05 1.6368082e-03 9.6688804e-05 4.5832386e-04 2.1395419e-05
 2.1998589e-05 5.2564731e-03 1.6274580e-04 1.8614135e-05 1.5144094e-03
 2.2311162e-03 5.8142754e-04 1.6369991e-05 2.4161035e-04 3.8008704e-05
 1.2996762e-04 9.4583183e-06 1.0127547e-04 1.5613921e-05 2.0752830e-04
 1.2362217e-04 9.5950272e-05 4.5157034e-05 3.6724876e-05 3.9637266e-04
 6.4885942e-05 1.7066645e-04 6.9418798e-05 2.6165835e-05 1.2429565e-04
 1.5218212e-05 7.5062417e-05 1.4183885e-06 6.8154754e-06 2.2027129e-06
 6.0409470e-06]
預測結果: 3
答案: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre>


<div id="orga354653" class="figure">
<p><img src="images/reuters-1.png" alt="reuters-1.png" />
</p>
<p><span class="figure-number">Figure 28: </span>Reuters-1</p>
</div>


<div id="org5328216" class="figure">
<p><img src="images/reuters-2.png" alt="reuters-2.png" />
</p>
<p><span class="figure-number">Figure 29: </span>Reuters-2</p>
</div>

<p>
程式第<a href="#coderef-modelEvaluate" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-modelEvaluate');" onmouseout="CodeHighlightOff(this, 'coderef-modelEvaluate');">56</a>行傳回的值有兩個，一個是 loss value、一個是在建構 model 時(model.compile)所指定的評估標準 metrics（程式第<a href="#coderef-metricsName" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-metricsName');" onmouseout="CodeHighlightOff(this, 'coderef-metricsName');">34</a>行），在此處指的是 accuracy。上述程式在經由 9 個 epoch 後精準度已近 80%(0.79)。
</p>
</div>
</li>

<li><a id="orgec25a0c"></a>優化 model<br />
<div class="outline-text-5" id="text-7-1-2-5">
<p>
上例中的中間層若將神經元數(維度)降到 4，則其驗證準確率會降至 71%，主要原因是因為這樣會壓縮大量資訊到一個低維度的中間層表示空間，雖然神經網路能將大部份必要的資訊塞進這 4 維表示法中，但仍顯不足。若再提升維度、增加層數、加入 Dropout，結果似乎沒有顯著改善，為什麼？
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> reuters
<span class="linenr">  2: </span>
<span class="linenr">  3: </span>  (train_data, train_labels), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_labels</span>) = reuters.load_data(num_words=<span style="color: #da8548; font-weight: bold;">10000</span>)
<span class="linenr">  4: </span>
<span class="linenr">  5: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  6: </span>
<span class="linenr">  7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">vectorize_sequences</span>(sequences, dimension=<span style="color: #da8548; font-weight: bold;">10000</span>):
<span class="linenr">  8: </span>      results = np.zeros((<span style="color: #c678dd;">len</span>(sequences), dimension))
<span class="linenr">  9: </span>      <span style="color: #51afef;">for</span> i, sequence <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(sequences):
<span class="linenr"> 10: </span>          results[i, sequence] = <span style="color: #da8548; font-weight: bold;">1</span>.
<span class="linenr"> 11: </span>      <span style="color: #51afef;">return</span> results
<span class="linenr"> 12: </span>
<span class="linenr"> 13: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized training data</span>
<span class="linenr"> 14: </span>  x_train = vectorize_sequences(train_data)
<span class="linenr"> 15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Our vectorized test data</span>
<span class="linenr"> 16: </span>  x_test = vectorize_sequences(test_data)
<span class="linenr"> 17: </span>
<span class="linenr"> 18: </span>  <span style="color: #51afef;">from</span> keras.utils.np_utils <span style="color: #51afef;">import</span> to_categorical
<span class="linenr"> 19: </span>
<span class="linenr"> 20: </span>  one_hot_train_labels = to_categorical(train_labels)
<span class="linenr"> 21: </span>  one_hot_test_labels = to_categorical(test_labels)
<span class="linenr"> 22: </span>
<span class="linenr"> 23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#27083;&#27169;&#22411;</span>
<span class="linenr"> 24: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 25: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 26: </span>
<span class="linenr"> 27: </span>  model = models.Sequential()
<span class="linenr"> 28: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr"> 29: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">128</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 30: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr"> 31: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 32: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.3</span>))
<span class="linenr"> 33: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 34: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr"> 35: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">46</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr"> 36: </span>
<span class="linenr"> 37: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr"> 38: </span>                loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span id="coderef-metricsName" class="coderef-off"><span class="linenr"> 39: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])</span>
<span class="linenr"> 40: </span>
<span class="linenr"> 41: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39511;&#35657;</span>
<span class="linenr"> 42: </span>  x_val = x_train[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr"> 43: </span>  partial_x_train = x_train[<span style="color: #da8548; font-weight: bold;">1000</span>:]
<span class="linenr"> 44: </span>
<span class="linenr"> 45: </span>  y_val = one_hot_train_labels[:<span style="color: #da8548; font-weight: bold;">1000</span>]
<span class="linenr"> 46: </span>  partial_y_train = one_hot_train_labels[<span style="color: #da8548; font-weight: bold;">1000</span>:]
<span class="linenr"> 47: </span>
<span class="linenr"> 48: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;</span>
<span class="linenr"> 49: </span>  history = model.fit(partial_x_train,
<span class="linenr"> 50: </span>                      partial_y_train,
<span class="linenr"> 51: </span>                      epochs=<span style="color: #da8548; font-weight: bold;">9</span>,
<span class="linenr"> 52: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">512</span>,
<span class="linenr"> 53: </span>                      validation_data=(x_val, y_val),
<span class="linenr"> 54: </span>                      verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 55: </span>
<span class="linenr"> 56: </span>  history_dict = history.history
<span class="linenr"> 57: </span>  <span style="color: #c678dd;">print</span>(history_dict.keys())
<span class="linenr"> 58: </span>
<span class="linenr"> 59: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35413;&#20272;</span>
<span class="linenr"> 60: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Returns the loss value &amp; metrics values for the model in test mode.</span>
<span id="coderef-modelEvaluate" class="coderef-off"><span class="linenr"> 61: </span>  results = model.evaluate(x_test, one_hot_test_labels)</span>
<span class="linenr"> 62: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#35413;&#20272;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,results)
<span class="linenr"> 63: </span>
<span class="linenr"> 64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#28204;</span>
<span class="linenr"> 65: </span>  predictions = model.predict(x_test)
<span class="linenr"> 66: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#26550;&#27083;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>].shape)
<span class="linenr"> 67: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#36039;&#26009;&#20839;&#23481;&#65306;"</span>,predictions[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 68: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#38928;&#28204;&#32080;&#26524;:"</span>,np.argmax(predictions[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr"> 69: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#31572;&#26696;:"</span>,one_hot_test_labels[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 70: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr"> 71: </span>
<span class="linenr"> 72: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 73: </span>
<span class="linenr"> 74: </span>  loss = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 75: </span>  val_loss = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 76: </span>
<span class="linenr"> 77: </span>  epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(loss) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 78: </span>
<span class="linenr"> 79: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr"> 80: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr"> 81: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr"> 82: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr"> 83: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr"> 84: </span>  plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">3</span>])
<span class="linenr"> 85: </span>  plt.legend()
<span class="linenr"> 86: </span>  plt.plot()
<span class="linenr"> 87: </span>  plt.savefig(<span style="color: #98be65;">"reuters-3.png"</span>)
<span class="linenr"> 88: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr"> 89: </span>
<span class="linenr"> 90: </span>  plt.clf()   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">clear figure</span>
<span class="linenr"> 91: </span>
<span class="linenr"> 92: </span>  accuracy = history.history[<span style="color: #98be65;">'accuracy'</span>]
<span class="linenr"> 93: </span>  val_accuracy = history.history[<span style="color: #98be65;">'val_accuracy'</span>]
<span class="linenr"> 94: </span>
<span class="linenr"> 95: </span>  plt.plot(epochs, accuracy, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training accuracy'</span>)
<span class="linenr"> 96: </span>  plt.plot(epochs, val_accuracy, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation accuracy'</span>)
<span class="linenr"> 97: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr"> 98: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr"> 99: </span>  plt.ylabel(<span style="color: #98be65;">'Loss'</span>)
<span class="linenr">100: </span>  plt.axis([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">10</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">101: </span>  plt.legend()
<span class="linenr">102: </span>  plt.plot()
<span class="linenr">103: </span>  plt.savefig(<span style="color: #98be65;">"reuters-4.png"</span>)
<span class="linenr">104: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">105: </span>
<span class="linenr">106: </span>
</pre>
</div>

<pre class="example" id="orgda1e69d">
dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])

  32/2246 [..............................] - ETA: 0s
 544/2246 [======&gt;.......................] - ETA: 0s
1088/2246 [=============&gt;................] - ETA: 0s
1632/2246 [====================&gt;.........] - ETA: 0s
2208/2246 [============================&gt;.] - ETA: 0s
2246/2246 [==============================] - 0s 96us/step
評估資料內容： [1.3893752790197982, 0.7497773766517639]
預測資料架構： (46,)
預測資料內容： [8.8242497e-11 1.8549613e-07 1.2244985e-12 9.9946600e-01 4.8491010e-04
2.4004774e-11 4.0463274e-08 2.8705716e-09 1.4672127e-06 1.2457425e-12
 2.1608832e-08 1.1945065e-06 1.6438412e-08 6.1330823e-08 5.5952110e-10
 2.0300506e-12 4.4415983e-06 5.0839004e-09 3.8925752e-09 2.0913212e-05
 2.0335670e-05 7.7277225e-09 1.0450782e-12 6.6110601e-08 2.6362378e-11
 2.6260804e-07 2.6264095e-12 4.5255667e-11 4.4987689e-10 2.7449030e-09
 1.0358207e-08 7.0458644e-10 1.4057776e-09 6.6201856e-11 9.8362518e-09
 1.4279193e-11 2.3172060e-08 3.4204664e-10 7.4201589e-10 3.5206096e-08
 2.1588344e-09 2.4565621e-09 1.9249602e-11 5.2338623e-11 4.7235077e-14
 9.4377089e-14]
預測結果: 3
答案: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
</pre>

<p>
[[file:<img src="images/reuters-3.png" alt="reuters-3.png" />images/reuters-1.png]]
</p>
<p>
[[file:<img src="images/reuters-4.png" alt="reuters-4.png" />images/reuters-2.png]]
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-org109c4c7" class="outline-3">
<h3 id="org109c4c7"><span class="section-number-3">7.2.</span> 以 Keras 解決迴歸問題：預測房價: Boston</h3>
<div class="outline-text-3" id="text-7-2">
<p>
迴歸(regression)與邏輯斯迴歸(logistic regression)不同，後者為分類法，與迴歸無關。本例使用資料集為 1970 年中期 Boston 郊區資料，包含犯罪率、當地財產稅等，用以預測某郊區房價中位數，本例有 506 筆資料，分為 404 個訓練樣本和 102 個測試樣本，但每個 feature 的單位不同，故須先進行資料預調整。
</p>
</div>

<div id="outline-container-orgf360f06" class="outline-4">
<h4 id="orgf360f06"><span class="section-number-4">7.2.1.</span> 準備資料</h4>
<div class="outline-text-4" id="text-7-2-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> boston_housing
<span class="linenr">2: </span>
<span class="linenr">3: </span>(train_data, train_targets), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_targets</span>) =  boston_housing.load_data()
<span class="linenr">4: </span>
<span class="linenr">5: </span><span style="color: #c678dd;">print</span>(train_data.shape)
<span class="linenr">6: </span><span style="color: #c678dd;">print</span>(test_data.shape)
</pre>
</div>

<pre class="example">
Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz

 8192/57026 [===&gt;..........................] - ETA: 1s
24576/57026 [===========&gt;..................] - ETA: 0s
40960/57026 [====================&gt;.........] - ETA: 0s
57344/57026 [==============================] - 1s 11us/step
(404, 13)
(102, 13)
</pre>


<p>
由於將不同類型不同單位的數值直接輸入神經網路會有問題，故要先資料進行正規化(normalization)處理，即，減去平均值，除以標準差。需留意的是，正規化時要用訓練資料集來計算 mean 和 std，不能使用測試集的資料。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">mean</span> = train_data.mean(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">2: </span>  train_data -= mean
<span class="linenr">3: </span>  std = train_data.std(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">4: </span>  train_data /= std
<span class="linenr">5: </span>
<span class="linenr">6: </span>  test_data -= mean
<span class="linenr">7: </span>  test_data /= std
</pre>
</div>
</div>
</div>

<div id="outline-container-org96da7f9" class="outline-4">
<h4 id="org96da7f9"><span class="section-number-4">7.2.2.</span> 建立神經網路</h4>
<div class="outline-text-4" id="text-7-2-2">
<p>
由於可用的樣本很少，所以使用一個較小的神經網路，一般來說，訓練資料集越少，過度配適的情況會越嚴重。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">build_model</span>():
<span class="linenr"> 5: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Because we will need to instantiate</span>
<span class="linenr"> 6: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">the same model multiple times,</span>
<span class="linenr"> 7: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">we use a function to construct it.</span>
<span class="linenr"> 8: </span>      <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 9: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr">10: </span>                             input_shape=(train_data.shape[<span style="color: #da8548; font-weight: bold;">1</span>],)))
<span class="linenr">11: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span id="coderef-OneUnitLayer" class="coderef-off"><span class="linenr">12: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>))</span>
<span class="linenr">13: </span>      model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>, loss=<span style="color: #98be65;">'mse'</span>, metrics=[<span style="color: #98be65;">'mae'</span>])
<span class="linenr">14: </span>      <span style="color: #51afef;">return</span> model
<span class="linenr">15: </span>
</pre>
</div>

<p>
這裡以 1 unit 的神經網路結束而且沒有啟動函數(第<a href="#coderef-OneUnitLayer" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-OneUnitLayer');" onmouseout="CodeHighlightOff(this, 'coderef-OneUnitLayer');">12</a>行)，代表為線性轉換，這是純量迴歸的基本設定，會輸出一個浮點數型別的數值(即迴歸值)，如果使用啟動函數，則只會輸出 0~1 間的值。另，mse 也是迴歸常用的損失函數，在評量指標的選擇方面，則採用 mae(mean absolute error，即預測值與目標值間差異的絕對值)。
</p>
</div>
</div>

<div id="outline-container-org7aaec61" class="outline-4">
<h4 id="org7aaec61"><span class="section-number-4">7.2.3.</span> 驗證</h4>
<div class="outline-text-4" id="text-7-2-3">
<p>
本例中由於資料點少，驗證集也只有 100 筆資料，故驗證分數可能會因驗證資料點或訓練資料點的選用而有很大的變化，因而阻礙評估 model 優劣的可靠性。在這種情況下，最好的方式是選用 K-fold corss validation，做法如圖<a href="#org4b226f3">30</a>，原理是將資料拆分為 K 個區域(通常 K=4 或 5)，每次取一個區域做為驗證資料集，最後求 K 次驗證分數的平均值。
</p>


<div id="org4b226f3" class="figure">
<p><img src="images/k-fold-validation.png" alt="k-fold-validation.png" />
</p>
<p><span class="figure-number">Figure 30: </span>K-fold 交叉驗證</p>
</div>

<p>
K-fold cross validation 的 python 實作程式碼如下：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">k</span> = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">num_val_samples</span> = <span style="color: #c678dd;">len</span>(train_data) // k
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">num_epochs</span> = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">all_scores</span> = []
<span class="linenr"> 7: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(k):
<span class="linenr"> 8: </span>      <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'processing fold #'</span>, i)
<span class="linenr"> 9: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the validation data: data from partition # k</span>
<span class="linenr">10: </span>      val_data = train_data[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">11: </span>      <span style="color: #dcaeea;">val_targets</span> = train_targets[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">12: </span>
<span class="linenr">13: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the training data: data from all other partitions</span>
<span class="linenr">14: </span>      partial_train_data = np.concatenate(
<span class="linenr">15: </span>          [train_data[:i * num_val_samples],
<span class="linenr">16: </span>           train_data[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">17: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">18: </span>      partial_train_targets = np.concatenate(
<span class="linenr">19: </span>          [train_targets[:i * num_val_samples],
<span class="linenr">20: </span>           train_targets[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">21: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">22: </span>
<span class="linenr">23: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Build the Keras model (already compiled)</span>
<span class="linenr">24: </span>      model = build_model()
<span class="linenr">25: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Train the model (in silent mode, verbose=0)</span>
<span class="linenr">26: </span>      model.fit(partial_train_data, partial_train_targets,
<span class="linenr">27: </span>                epochs=num_epochs, batch_size=<span style="color: #da8548; font-weight: bold;">1</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">28: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Evaluate the model on the validation data</span>
<span class="linenr">29: </span>      val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">30: </span>      all_scores.append(val_mae)
</pre>
</div>



<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> boston_housing
<span class="linenr"> 2: </span>  (train_data, train_targets), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_targets</span>) =  boston_housing.load_data()
<span class="linenr"> 3: </span>  <span style="color: #c678dd;">print</span>(train_data.shape)
<span class="linenr"> 4: </span>  <span style="color: #c678dd;">print</span>(test_data.shape)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">mean</span> = train_data.mean(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 7: </span>  train_data -= mean
<span class="linenr"> 8: </span>  std = train_data.std(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 9: </span>  train_data /= std
<span class="linenr">10: </span>  test_data -= mean
<span class="linenr">11: </span>  test_data /= std
<span class="linenr">12: </span>
<span class="linenr">13: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">14: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">15: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">build_model</span>():
<span class="linenr">16: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Because we will need to instantiate</span>
<span class="linenr">17: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">the same model multiple times,</span>
<span class="linenr">18: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">we use a function to construct it.</span>
<span class="linenr">19: </span>      model = models.Sequential()
<span class="linenr">20: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr">21: </span>                             input_shape=(train_data.shape[<span style="color: #da8548; font-weight: bold;">1</span>],)))
<span class="linenr">22: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">23: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">24: </span>      model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>, loss=<span style="color: #98be65;">'mse'</span>, metrics=[<span style="color: #98be65;">'mae'</span>])
<span class="linenr">25: </span>      <span style="color: #51afef;">return</span> model
<span class="linenr">26: </span>
<span class="linenr">27: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">28: </span>
<span class="linenr">29: </span>  k = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr">30: </span>  num_val_samples = <span style="color: #c678dd;">len</span>(train_data) // k
<span class="linenr">31: </span>  num_epochs = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr">32: </span>  all_scores = []
<span class="linenr">33: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(k):
<span class="linenr">34: </span>      <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'processing fold #'</span>, i)
<span class="linenr">35: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the validation data: data from partition # k</span>
<span class="linenr">36: </span>      val_data = train_data[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">37: </span>      val_targets = train_targets[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">38: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the training data: data from all other partitions</span>
<span class="linenr">39: </span>      partial_train_data = np.concatenate(
<span class="linenr">40: </span>          [train_data[:i * num_val_samples],
<span class="linenr">41: </span>           train_data[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">42: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">43: </span>      partial_train_targets = np.concatenate(
<span class="linenr">44: </span>          [train_targets[:i * num_val_samples],
<span class="linenr">45: </span>           train_targets[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">46: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">47: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Build the Keras model (already compiled)</span>
<span class="linenr">48: </span>      model = build_model()
<span class="linenr">49: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Train the model (in silent mode, verbose=0)</span>
<span class="linenr">50: </span>      model.fit(partial_train_data, partial_train_targets,
<span class="linenr">51: </span>                epochs=num_epochs, batch_size=<span style="color: #da8548; font-weight: bold;">1</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">52: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Evaluate the model on the validation data</span>
<span class="linenr">53: </span>      val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">54: </span>      all_scores.append(val_mae)
<span class="linenr">55: </span>
<span class="linenr">56: </span>  <span style="color: #c678dd;">print</span>(all_scores)
<span class="linenr">57: </span>  <span style="color: #c678dd;">print</span>(np.mean(all_scores))
</pre>
</div>

<pre class="example">
(404, 13)
(102, 13)
processing fold # 0
processing fold # 1
processing fold # 2
processing fold # 3
[1.8689913749694824, 2.581745147705078, 2.9093284606933594, 2.6838433742523193]
2.51097708940506
</pre>


<p>
由上述結果看來，拆成 4 區的驗證分數自 1.87 到 2.91，總平均為 2.51，這個平均值是較為可靠的指標，因為當目標房價的數值很大時，1.87 到 2.91 會變成很大的誤差。
</p>

<p>
可能是因為 MAC 與 Linux 版本的 Anaconda 相容性問題，或是 Keras 版本差異問題，MAC 版與 Linux 下的 history.history 架構略有差異：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Linux with Keras 2.2.5</span>
<span class="linenr">2: </span>dict_keys([<span style="color: #98be65;">'val_loss'</span>, <span style="color: #98be65;">'val_mean_absolute_error'</span>, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'mean_absolute_error'</span>])
<span class="linenr">3: </span><span style="color: #5B6268;"># </span><span style="color: #5B6268;">Mac with Keras 2.3.1</span>
<span class="linenr">4: </span>dict_keys([<span style="color: #98be65;">'val_loss'</span>, <span style="color: #98be65;">'val_mae'</span>, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'mae'</span>])
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> boston_housing
<span class="linenr"> 2: </span>  (train_data, train_targets), (<span style="color: #dcaeea;">test_data</span>, <span style="color: #dcaeea;">test_targets</span>) =  boston_housing.load_data()
<span class="linenr"> 3: </span>  <span style="color: #c678dd;">print</span>(train_data.shape)
<span class="linenr"> 4: </span>  <span style="color: #c678dd;">print</span>(test_data.shape)
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">mean</span> = train_data.mean(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 6: </span>  train_data -= mean
<span class="linenr"> 7: </span>  std = train_data.std(axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 8: </span>  train_data /= std
<span class="linenr"> 9: </span>  test_data -= mean
<span class="linenr">10: </span>  test_data /= std
<span class="linenr">11: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">13: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">build_model</span>():
<span class="linenr">14: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Because we will need to instantiate</span>
<span class="linenr">15: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">the same model multiple times,</span>
<span class="linenr">16: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">we use a function to construct it.</span>
<span class="linenr">17: </span>      model = models.Sequential()
<span class="linenr">18: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr">19: </span>                             input_shape=(train_data.shape[<span style="color: #da8548; font-weight: bold;">1</span>],)))
<span class="linenr">20: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">64</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">21: </span>      model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">22: </span>      model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>, loss=<span style="color: #98be65;">'mse'</span>, metrics=[<span style="color: #98be65;">'mae'</span>])
<span class="linenr">23: </span>      <span style="color: #51afef;">return</span> model
<span class="linenr">24: </span>
<span class="linenr">25: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">26: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> backend <span style="color: #51afef;">as</span> K
<span class="linenr">27: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Some memory clean-up</span>
<span class="linenr">28: </span>  K.clear_session()
<span class="linenr">29: </span>  k = <span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr">30: </span>  num_val_samples = <span style="color: #c678dd;">len</span>(train_data) // k
<span class="linenr">31: </span>  num_epochs = <span style="color: #da8548; font-weight: bold;">500</span>
<span class="linenr">32: </span>  all_mae_histories = []
<span class="linenr">33: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(k):
<span class="linenr">34: </span>      <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'processing fold #'</span>, i)
<span class="linenr">35: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the validation data: data from partition # k</span>
<span class="linenr">36: </span>      val_data = train_data[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">37: </span>      val_targets = train_targets[i * num_val_samples: (i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples]
<span class="linenr">38: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Prepare the training data: data from all other partitions</span>
<span class="linenr">39: </span>      partial_train_data = np.concatenate(
<span class="linenr">40: </span>          [train_data[:i * num_val_samples],
<span class="linenr">41: </span>           train_data[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">42: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">43: </span>      partial_train_targets = np.concatenate(
<span class="linenr">44: </span>          [train_targets[:i * num_val_samples],
<span class="linenr">45: </span>           train_targets[(i + <span style="color: #da8548; font-weight: bold;">1</span>) * num_val_samples:]],
<span class="linenr">46: </span>          axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">47: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Build the Keras model (already compiled)</span>
<span class="linenr">48: </span>      model = build_model()
<span class="linenr">49: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Train the model (in silent mode, verbose=0)</span>
<span class="linenr">50: </span>      history = model.fit(partial_train_data, partial_train_targets,
<span class="linenr">51: </span>                          validation_data=(val_data, val_targets),
<span class="linenr">52: </span>                          epochs=num_epochs, batch_size=<span style="color: #da8548; font-weight: bold;">1</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">53: </span>      mae_history = history.history[<span style="color: #98be65;">'val_mae'</span>]
<span class="linenr">54: </span>      all_mae_histories.append(mae_history)
<span class="linenr">55: </span>
<span class="linenr">56: </span>  average_mae_history = [np.mean([x[i] <span style="color: #51afef;">for</span> x <span style="color: #51afef;">in</span> all_mae_histories]) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(num_epochs)]
<span class="linenr">57: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">58: </span>  plt.plot(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(average_mae_history) + <span style="color: #da8548; font-weight: bold;">1</span>), average_mae_history)
<span class="linenr">59: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">60: </span>  plt.ylabel(<span style="color: #98be65;">'Validation MAE'</span>)
<span class="linenr">61: </span>  plt.plot()
<span class="linenr">62: </span>  plt.savefig(<span style="color: #98be65;">"Boston-House-Price.png"</span>)
<span class="linenr">63: </span>
<span class="linenr">64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25490;&#38500;&#27599;&#36913;&#26399;&#30340;&#21069;10&#20491;&#36039;&#26009;&#40670;</span>
<span class="linenr">65: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">smooth_curve</span>(points, factor=<span style="color: #da8548; font-weight: bold;">0.9</span>):
<span class="linenr">66: </span>    smoothed_points = []
<span class="linenr">67: </span>    <span style="color: #51afef;">for</span> point <span style="color: #51afef;">in</span> points:
<span class="linenr">68: </span>      <span style="color: #51afef;">if</span> smoothed_points:
<span class="linenr">69: </span>        previous = smoothed_points[-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">70: </span>        smoothed_points.append(previous * factor + point * (<span style="color: #da8548; font-weight: bold;">1</span> - factor))
<span class="linenr">71: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">72: </span>        smoothed_points.append(point)
<span class="linenr">73: </span>    <span style="color: #51afef;">return</span> smoothed_points
<span class="linenr">74: </span>
<span class="linenr">75: </span>  smooth_mae_history = smooth_curve(average_mae_history[<span style="color: #da8548; font-weight: bold;">10</span>:])
<span class="linenr">76: </span>  plt.clf()
<span class="linenr">77: </span>  plt.plot(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(smooth_mae_history) + <span style="color: #da8548; font-weight: bold;">1</span>), smooth_mae_history)
<span class="linenr">78: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">79: </span>  plt.ylabel(<span style="color: #98be65;">'Validation MAE'</span>)
<span class="linenr">80: </span>  plt.plot()
<span class="linenr">81: </span>  plt.savefig(<span style="color: #98be65;">"Boston-House-Price-ex10.png"</span>)
<span class="linenr">82: </span>
</pre>
</div>

<pre class="example">
(404, 13)
(102, 13)
processing fold # 0
processing fold # 1
processing fold # 2
processing fold # 3
</pre>



<div id="org6539a84" class="figure">
<p><img src="images/Boston-House-Price.png" alt="Boston-House-Price.png" />
</p>
<p><span class="figure-number">Figure 31: </span>Boston House Price Training MAE</p>
</div>

<p>
圖<a href="#org6539a84">31</a>是由每一訓練週期的平均 MAE 分數所繪出的折線圖，由於單位刻度與 y 軸刻度問題，此圖失去了部份重要細節，經由下列方式進行修正：
</p>
<ul class="org-ul">
<li>省略前 10 個資料點，</li>
<li>把每個資料點替換成前一點的指數移動平均值(exponential moving average, EMA)，讓誤差變平滑。</li>
</ul>

<p>
EMA 常應用於各領域的資料分析中，其核心概念為：現在的資料會被過去的資料所影響，而時間點越近的資料影響越大，反之越小，如股票的漲幅，前 10 年的漲跌與前 10 日的漲跌，自然是後者對未來的影響更大。
</p>

<p>
EMA 的數學函式如下：
</p>

<p>
\( E_t = a \times V_t + (1-a) \times E_{t-1} \)，其中
</p>

<ul class="org-ul">
<li>\(E_t\)為時間點\(t\)的指數移動平均值</li>
<li>\(a\)為平滑係數，通常介於 0 到 1 之間</li>
<li>\(V_t\)為時間點\(t\)的原始數值</li>
<li>\(E_{t-1}\)為時間點\(t-1\)的指數移動平均值</li>
</ul>

<p>
為什麼前例中前 10 筆數據的與其他數據差異如此巨大？我們以前 10 天的資料(一天一筆)來看，第 10 天的 EMA 為：
\( E_{10} = aV_{10} + (1-a)E_9 \)
展開第 9 天的\(E_9\)後
\( E_{10} = aV_{10} + (1-a)[aV_9 + (1-a)E_8] \)
整理後變成
\( E_{10} = a(V_{10} + (1-a)V_9) + (1-a)^{2}E_8 \)
若繼續展開所有天數，將得到
\( E_{10} = a(V_{10} + (1-a)V_9) + (1-a)^{2}E_8+ \dots + (1-a)^{9}V_{1}) + (1-a)^{9}E_1 \)
通常上式的最後一項會因為時間很長而變太小，故可忽略不計，而由此也可看出，\(E_{10}\)的值會被每天的原始資料\((V_{10} \dots V_{1}\))影響，每多一天，原始數值就會多乘(1-a)倍，成指數關係，故時間越久遠的事件，影響越小。
</p>

<p>
[[<img src="images/Boston-House-Price-ex10.png" alt="Boston-House-Price-ex10.png" />]
</p>

<p>
由圖<a href="#orgd222806">37</a>是可看出 MAE 在 80 個週期後已停止改善，然後開始往上升，即，過了這點就開始發生過度適配的情況。
</p>
</div>
</div>

<div id="outline-container-org563e19f" class="outline-4">
<h4 id="org563e19f"><span class="section-number-4">7.2.4.</span> 小結</h4>
<div class="outline-text-4" id="text-7-2-4">
<p>
由此範例可知：
</p>
<ul class="org-ul">
<li>進行迴歸分木卜竹一中時，常以 MSE 做為損失函數、以 MAE 做為評估指標(而非 accuracy).</li>
<li>當輸入資料的特徵有不同刻度時，應先將每個特徵進行轉換。</li>
<li>當可用資料很少時，使用 K-fold 驗證來評估模式。</li>
<li>當可用資料很少時，最好使用隠藏層較少(較淺)的小型神經網路，如一個或兩個，以免產生過渡配適。</li>
</ul>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> keras
<span class="linenr">2: </span>  <span style="color: #c678dd;">print</span>(keras.__version__)
</pre>
</div>

<pre class="example">
2.3.1
</pre>


<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-orgdaf7752" class="outline-3">
<h3 id="orgdaf7752"><span class="section-number-3">7.3.</span> 以神經網路重跑鳶尾花問題</h3>
<div class="outline-text-3" id="text-7-3">
</div>
<div id="outline-container-orgaaaa569" class="outline-4">
<h4 id="orgaaaa569"><span class="section-number-4">7.3.1.</span> 鳶尾花分類問題</h4>
<div class="outline-text-4" id="text-7-3-1">
</div>
<ol class="org-ol">
<li><a id="orga55fc1a"></a>DataSet<br />
<div class="outline-text-5" id="text-7-3-1-1">
<p>
收集了3種鳶尾花的四個特徵，分別是花萼(sepal)長寬、花瓣(petal)長寬度，以及對應的鳶尾花種類。
</p>

<div id="org72fa480" class="figure">
<p><img src="images/iris-1.png" alt="iris-1.png" width="400" />
</p>
<p><span class="figure-number">Figure 32: </span>鳶尾花的花萼與花瓣</p>
</div>
</div>
</li>
<li><a id="org35a1bf7"></a>Mission<br />
<div class="outline-text-5" id="text-7-3-1-2">
<p>
輸入花萼和花瓣數據後，推測所屬的鳶尾花類型。
</p>

<div id="org1f9c7b7" class="figure">
<p><img src="images/iris-2.png" alt="iris-2.png" width="600" />
</p>
<p><span class="figure-number">Figure 33: </span>三種鳶尾花</p>
</div>
</div>
</li>
</ol>
</div>
<div id="outline-container-org17ce525" class="outline-4">
<h4 id="org17ce525"><span class="section-number-4">7.3.2.</span> 實作</h4>
<div class="outline-text-4" id="text-7-3-2">
<ol class="org-ol">
<li><p>
讀取資料集
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> sklearn <span style="color: #51afef;">import</span> datasets

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#36039;&#26009;</span>
<span style="color: #dcaeea;">iris</span> = datasets.load_iris()
<span style="color: #c678dd;">print</span>(iris.DESCR)
</pre>
</div></li>

<li><p>
取出特徵與標籤
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #dcaeea;">x</span> = iris.data
<span style="color: #dcaeea;">y</span> = iris.target
<span style="color: #c678dd;">print</span>(x[:<span style="color: #da8548; font-weight: bold;">5</span>])
<span style="color: #c678dd;">print</span>(y[:<span style="color: #da8548; font-weight: bold;">5</span>])
</pre>
</div></li>
<li><p>
資料觀察
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span style="color: #51afef;">import</span> seaborn <span style="color: #51afef;">as</span> sns
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#25226;nupmy ndarray&#36681;&#28858;pandas dataFrame,&#21152;&#19978;columns title</span>
<span style="color: #dcaeea;">npx</span> = pd.DataFrame(x, columns=[<span style="color: #98be65;">'fac1'</span>,<span style="color: #98be65;">'fac2'</span>,<span style="color: #98be65;">'fac3'</span>,<span style="color: #98be65;">'fac4'</span>])
npy = pd.DataFrame(y.astype(<span style="color: #c678dd;">int</span>), columns=[<span style="color: #98be65;">'category'</span>])
<span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#21512;&#20341;</span>
dataPD = pd.concat([npx, npy], axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span style="color: #c678dd;">print</span>(dataPD)
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
sns.lmplot(<span style="color: #98be65;">'fac1'</span>, <span style="color: #98be65;">'fac2'</span>, data=dataPD, hue=<span style="color: #98be65;">'category'</span>, fit_reg=<span style="color: #a9a1e1;">False</span>)
plt.show()
</pre>
</div></li>

<li><p>
分割資料集
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> sklearn.model_selection <span style="color: #51afef;">import</span> train_test_split
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21123;&#20998;&#36039;&#26009;&#38598;</span>
<span style="color: #dcaeea;">x_train</span>, <span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_train</span>, <span style="color: #dcaeea;">y_test</span> = train_test_split(iris.data, iris.target, random_state=<span style="color: #da8548; font-weight: bold;">6</span>)
</pre>
</div>
<ul class="org-ul">
<li>train_test_split()
所接受的變數其實非常單純，基本上為 3 項：『原始的資料』、『Seed』、『比例』
<ol class="org-ol">
<li>原始的資料：就如同上方的 data 一般，是我們打算切成 Training data 以及 Test data 的原始資料</li>
<li>Seed： 亂數種子，可以固定我們切割資料的結果</li>
<li>比例：可以設定 train_size 或 test_size，只要設定一邊即可，範圍在 [0-1] 之間</li>
</ol></li>
<li><p>
scikit-learn.org: sklearn.model_selection.train_test_split
</p>

<p>
Split arrays or matrices into random train and test subsets
</p>

<p>
Quick utility that wraps input validation and next(ShuffleSplit().split(X, y)) and application to input data into a single call for splitting (and optionally subsampling) data in a oneliner.
</p>
<div class="org-src-container">
<pre class="src src-python"> sklearn.model_selection.train_test_split(*arrays, test_size=<span style="color: #a9a1e1;">None</span>, train_size=<span style="color: #a9a1e1;">None</span>, random_state=<span style="color: #a9a1e1;">None</span>, shuffle=<span style="color: #a9a1e1;">True</span>, stratify=<span style="color: #a9a1e1;">None</span>)[source]
</pre>
</div>
<ul class="org-ul">
<li><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">online docs</a></li>
</ul></li>
</ul></li>

<li><p>
資料標準化
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#36039;&#26009;&#27161;&#28310;&#21270;: &#21033;&#29992;preprocessing&#27169;&#32068;&#35041;&#30340;StandardScaler&#39006;&#21029;</span>
<span style="color: #51afef;">from</span> sklearn.preprocessing <span style="color: #51afef;">import</span> StandardScaler
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21033;&#29992;fit&#26041;&#27861;&#65292;&#23565;X_train&#20013;&#27599;&#20491;&#29305;&#24501;&#20540;&#20272;&#24179;&#22343;&#25976;&#21644;&#27161;&#28310;&#24046;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28982;&#24460;&#23565;&#27599;&#20491;&#29305;&#24501;&#20540;&#36914;&#34892;&#27161;&#28310;&#21270;(train&#21644;test&#37117;&#35201;&#20570;)</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29305;&#24501;&#24037;&#31243;&#65306;&#27161;&#28310;&#21270;</span>
transfer = StandardScaler()
<span style="color: #dcaeea;">x_train</span> = transfer.fit_transform(x_train)
<span style="color: #dcaeea;">x_test</span> = transfer.fit_transform(x_test)
</pre>
</div></li>

<li><p>
分類
</p>
<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> sklearn.neighbors <span style="color: #51afef;">import</span> KNeighborsClassifier
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">KNN &#20998;&#39006;&#22120;</span>
<span style="color: #dcaeea;">estimator</span> = KNeighborsClassifier(n_neighbors=<span style="color: #da8548; font-weight: bold;">1</span>)
estimator.fit(x_train, y_train)

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27169;&#22411;&#35413;&#20272;</span>
<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26041;&#27861;&#19968;&#65306;&#30452;&#25509;&#23565;&#27604;&#30495;&#23526;&#20540;&#21644;&#38928;&#28204;&#20540;</span>
y_predict = estimator.predict(x_test)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'y_predict&#65306;\n'</span>, y_predict)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#30452;&#25509;&#23565;&#27604;&#30495;&#23526;&#20540;&#21644;&#38928;&#28204;&#20540;:\n'</span>, y_test == y_predict)

<span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26041;&#27861;&#20108;&#65306;&#35336;&#31639;&#28310;&#30906;&#29575;</span>
score = estimator.score(x_test, y_test)
<span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#28310;&#30906;&#29575;:\n'</span>, score)
</pre>
</div></li>
</ol>
</div>
</div>
</div>
</div>

<div id="outline-container-org3c206fb" class="outline-2">
<h2 id="org3c206fb"><span class="section-number-2">8.</span> 以少量資料集實做 CNN</h2>
<div class="outline-text-2" id="text-8">
<p>
使用少量資料訓練影像分類在實務的電腦視覺應用上十分常見，此處所謂少量樣本從幾百到幾萬張都算在內。此處以 4000 張為例(2000 cats v.s. 2000 dogs)，過程中使用 2000 張來訓練、1000 張用來驗證、1000 張用來測試。接下來導入以下技術來克服 overfitting:
</p>

<ul class="org-ul">
<li>資料擴增法(data augmentation):這是常用於減輕電腦視覺 overfitting 的強大技術，可以改善神經網路的成效，提升到 82%的準確率。</li>
<li>預先訓練神經網路的特徵萃取法(feature extraction with a pretrained network):應用於少量資料集的基本技術，可使神經網路成效達到 90%~96%的準確度。</li>
<li>微調預先訓練神經網路法(fine-tuning a pretrained network):也是常用於深度學習少量資料集的技術，將使神經網路準確率提升到 97%。</li>
</ul>
</div>

<div id="outline-container-org853f458" class="outline-3">
<h3 id="org853f458"><span class="section-number-3">8.1.</span> 深度學習與少量資料的相關性</h3>
<div class="outline-text-3" id="text-8-1">
<p>
深度學習的基本特色是在它能自行在訓練資料中找到有趣的特徵，而不需要人為介入，但這只有在具備大量訓練樣本時才成立，特別是對於像圖片這類高維度(high-dimensional)的輸入樣本。所以也有人說深度學習一定要有大量資料才能進行。
</p>

<p>
然而樣本數與神經網路的大小與深度息息相關。只用幾十個樣本不可能訓練出可以解決複雜問題的卷積神經網路；相反的，如果只是要用來解決簡單任務，而且已經做好了 well-regularized 的小 model，那麼幾百個樣本或許就足夠了。因為卷積神經網路可以學習局部 pattern 且具平移不變性，所以在感知問題上具有高度的資料效率性。
</p>

<p>
此外，本質上，深度學習 model 是可高度再利用的。例如，使用大規模資料集訓練的影像 model 或語音轉文字的 model，只要進行小小的更改，便可以重新用於其他不同問題上。以電腦視覺的應用而言，許多預先訓練好的 model(通常是使用 Image-Net 資料集進行訓練)都是可公開下載的，以這些預先訓練好的 model 為基礎，再加以少量資料的訓練，就能產出更強大的 model。
</p>
</div>
</div>

<div id="outline-container-orgddd0eda" class="outline-3">
<h3 id="orgddd0eda"><span class="section-number-3">8.2.</span> 實作</h3>
<div class="outline-text-3" id="text-8-2">
</div>
<div id="outline-container-org4688d82" class="outline-4">
<h4 id="org4688d82"><span class="section-number-4">8.2.1.</span> 下載資料</h4>
<div class="outline-text-4" id="text-8-2-1">
<p>
2013 年的 Kaggle 貓狗辨識大賽，最佳 model 即是使用 CNN，當時準確率達 95%，2013 年後的準確率已提高至 98%。本案例之資料來源：<a href="https://www.kaggle.com/c/dogs-vs-cats/data">https://www.kaggle.com/c/dogs-vs-cats/data</a>，由於原始圖片尺寸未做修改，大小各異，故需先額外處理，複製圖片到訓練、驗證和測試目錄的程式碼如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">original_dataset_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#20786;&#23384;&#23569;&#37327;&#36039;&#26009;&#38598;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(base_dir): os.mkdir(base_dir)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22914;&#26524;&#30446;&#37636;&#19981;&#23384;&#22312;, &#25165;&#24314;&#31435;&#30446;&#37636;</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#25286;&#25104;&#35347;&#32244;&#12289;&#39511;&#35657;&#33287;&#28204;&#35430;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">11: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(train_dir): os.mkdir(train_dir)
<span class="linenr">12: </span>
<span class="linenr">13: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr">14: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(validation_dir): os.mkdir(validation_dir)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">17: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(test_dir): os.mkdir(test_dir)
<span class="linenr">18: </span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #dcaeea;">train_cats_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">21: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(train_cats_dir):
<span class="linenr">22: </span>      os.mkdir(train_cats_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#35347;&#32244;&#35987;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #dcaeea;">train_dogs_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">25: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(train_dogs_dir):
<span class="linenr">26: </span>      os.mkdir(train_dogs_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#35347;&#32244;&#29399;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">27: </span>
<span class="linenr">28: </span>  <span style="color: #dcaeea;">validation_cats_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">29: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(validation_cats_dir):
<span class="linenr">30: </span>      os.mkdir(validation_cats_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#39511;&#35657;&#35987;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">31: </span>
<span class="linenr">32: </span>  <span style="color: #dcaeea;">validation_dogs_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">33: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(validation_dogs_dir):
<span class="linenr">34: </span>      os.mkdir(validation_dogs_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#39511;&#35657;&#29399;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #dcaeea;">test_cats_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">37: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(test_cats_dir):
<span class="linenr">38: </span>      os.mkdir(test_cats_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#28204;&#35430;&#35987;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">39: </span>
<span class="linenr">40: </span>  <span style="color: #dcaeea;">test_dogs_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">41: </span>  <span style="color: #51afef;">if</span> <span style="color: #51afef;">not</span> os.path.isdir(test_dogs_dir):
<span class="linenr">42: </span>      os.mkdir(test_dogs_dir) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#28204;&#35430;&#29399;&#22294;&#29255;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">43: </span>
<span class="linenr">44: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#21069;&#38754; 1000 &#24373;&#35987;&#22294;&#29255;&#21040; train_cats_dir &#35347;&#32244;&#30446;&#37636;</span>
<span class="linenr">45: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'cat.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1000</span>)]
<span class="linenr">46: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">47: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">48: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(train_cats_dir, fname)
<span class="linenr">49: </span>      shutil.copyfile(src, dst)
<span class="linenr">50: </span>
<span class="linenr">51: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#19979; 500 &#24373;&#35987;&#22294;&#29255;&#21040; validation_cats_dir &#39511;&#35657;&#30446;&#37636;</span>
<span class="linenr">52: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'cat.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">1500</span>)]
<span class="linenr">53: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">54: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">55: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(validation_cats_dir, fname)
<span class="linenr">56: </span>      shutil.copyfile(src, dst)
<span class="linenr">57: </span>
<span class="linenr">58: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#19979; 500 &#24373;&#35987;&#22294;&#29255;&#21040; test_cats_dir &#28204;&#35430;&#30446;&#37636;</span>
<span class="linenr">59: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'cat.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1500</span>, <span style="color: #da8548; font-weight: bold;">2000</span>)]
<span class="linenr">60: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">61: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">62: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(test_cats_dir, fname)
<span class="linenr">63: </span>      shutil.copyfile(src, dst)
<span class="linenr">64: </span>
<span class="linenr">65: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#21069;&#38754; 1000 &#24373;&#29399;&#22294;&#29255;&#21040; train_dogs_dir &#35347;&#32244;&#30446;&#37636;</span>
<span class="linenr">66: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'dog.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1000</span>)]
<span class="linenr">67: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">68: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">69: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(train_dogs_dir, fname)
<span class="linenr">70: </span>      shutil.copyfile(src, dst)
<span class="linenr">71: </span>
<span class="linenr">72: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#19979; 500 &#24373;&#29399;&#22294;&#29255;&#21040; validation_dogs_dir &#39511;&#35657;&#30446;&#37636;</span>
<span class="linenr">73: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'dog.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">1500</span>)]
<span class="linenr">74: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">75: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">76: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(validation_dogs_dir, fname)
<span class="linenr">77: </span>      shutil.copyfile(src, dst)
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35079;&#35069;&#19979; 500 &#24373;&#29399;&#22294;&#29255;&#21040; test_dogs_dir &#28204;&#35430;&#30446;&#37636;</span>
<span class="linenr">80: </span>  <span style="color: #dcaeea;">fnames</span> = [<span style="color: #98be65;">'dog.{}.jpg'</span>.<span style="color: #c678dd;">format</span>(i) <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1500</span>, <span style="color: #da8548; font-weight: bold;">2000</span>)]
<span class="linenr">81: </span>  <span style="color: #51afef;">for</span> fname <span style="color: #51afef;">in</span> <span style="color: #dcaeea;">fnames</span>:
<span class="linenr">82: </span>      src = os.path.join(original_dataset_dir, fname)
<span class="linenr">83: </span>      <span style="color: #dcaeea;">dst</span> = os.path.join(test_dogs_dir, fname)
<span class="linenr">84: </span>      shutil.copyfile(src, dst)
<span class="linenr">85: </span>
<span class="linenr">86: </span>      <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#35079;&#35069;&#23436;&#25104;'</span>)
</pre>
</div>

<pre class="example">
複製完成
</pre>


<p>
上述程式會產生三組資料集：訓練集狗貓各 1000、驗證集各 500、測試集各 500，可再以下列程式驗證：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">original_dataset_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#20786;&#23384;&#23569;&#37327;&#36039;&#26009;&#38598;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#25286;&#25104;&#35347;&#32244;&#12289;&#39511;&#35657;&#33287;&#28204;&#35430;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">10: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr">11: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">train_cats_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">13: </span>  <span style="color: #dcaeea;">train_dogs_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">14: </span>  <span style="color: #dcaeea;">validation_cats_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">15: </span>  <span style="color: #dcaeea;">validation_dogs_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">16: </span>  <span style="color: #dcaeea;">test_cats_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">17: </span>  <span style="color: #dcaeea;">test_dogs_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#35347;&#32244;&#29992;&#30340;&#35987;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(train_cats_dir)))
<span class="linenr">20: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#35347;&#32244;&#29992;&#30340;&#29399;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(train_dogs_dir)))
<span class="linenr">21: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#39511;&#35657;&#29992;&#30340;&#35987;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(validation_cats_dir)))
<span class="linenr">22: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#39511;&#35657;&#29992;&#30340;&#29399;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(validation_dogs_dir)))
<span class="linenr">23: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#28204;&#35430;&#29992;&#30340;&#35987;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(test_cats_dir)))
<span class="linenr">24: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#28204;&#35430;&#29992;&#30340;&#29399;&#29031;&#29255;&#24373;&#25976;:'</span>, <span style="color: #c678dd;">len</span>(os.listdir(test_dogs_dir)))
</pre>
</div>

<pre class="example">
訓練用的貓照片張數: 1000
訓練用的狗照片張數: 1000
驗證用的貓照片張數: 500
驗證用的狗照片張數: 500
測試用的貓照片張數: 500
測試用的狗照片張數: 500
</pre>
</div>
</div>

<div id="outline-container-orgcc6272d" class="outline-4">
<h4 id="orgcc6272d"><span class="section-number-4">8.2.2.</span> 建立神經網路</h4>
<div class="outline-text-4" id="text-8-2-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 5: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">32</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr"> 6: </span>                          input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>)))
<span class="linenr"> 7: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 8: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">64</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 9: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">10: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">11: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">12: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">13: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">14: </span>  model.add(layers.Flatten())
<span class="linenr">15: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">16: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr">17: </span>  model.summary()  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26597;&#30475;&#27169;&#22411;&#25688;&#35201;</span>
<span class="linenr">18: </span>
</pre>
</div>

<pre class="example" id="org74218e9">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
</pre>

<p>
在編譯時，以 RMSProp 優化器，由於使用 sigmoid 單元結束神經網路，所以配合使用 binary_crossentropy 二元交叉熵作為損失基準。
</p>

<div class="org-src-container">
<pre class="src src-python" id="org89cf867"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">2: </span>
<span class="linenr">3: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">4: </span>                optimizer=optimizers.RMSprop(lr=1e-<span style="color: #da8548; font-weight: bold;">4</span>),
<span class="linenr">5: </span>                metrics=[<span style="color: #98be65;">'acc'</span>])
</pre>
</div>
</div>
</div>

<div id="outline-container-org063fe48" class="outline-4">
<h4 id="org063fe48"><span class="section-number-4">8.2.3.</span> 資料預處理</h4>
<div class="outline-text-4" id="text-8-2-3">
<p>
資料在送入神經網路前應先將 JPEG 檔案格式化成適當的浮點數張量，其步驟如下：
</p>
<ol class="org-ol">
<li>讀取影像檔</li>
<li>將 JPEG 內容解碼為 RGB 的像素</li>
<li>將 RGB 像素轉為浮點數張量</li>
<li>將像素值(0~255)壓縮到[0,1]區間</li>
</ol>

<p>
上述過程可以用 Keras 的 keras.preprocessing.image 模組來處理，它包含 ImageDataGenerator 類別，過程如下：
</p>

<div class="org-src-container">
<pre class="src src-python" id="orga33edef"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">original_dataset_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#20786;&#23384;&#23569;&#37327;&#36039;&#26009;&#38598;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#25286;&#25104;&#35347;&#32244;&#12289;&#39511;&#35657;&#33287;&#28204;&#35430;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">10: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">13: </span>
<span id="coderef-ImageDataGenerator" class="coderef-off"><span class="linenr">14: </span>  <span style="color: #dcaeea;">train_datagen</span> = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#35347;&#32244;&#12289;&#28204;&#35430;&#36039;&#26009;&#30340; Python &#29986;&#29983;&#22120;&#65292;&#20006;&#23559;&#22294;&#29255;&#20687;&#32032;&#20540;&#20381; 1/255 &#27604;&#20363;&#37325;&#26032;&#22739;&#32302;&#21040; [0, 1]</span></span>
<span class="linenr">15: </span>  test_datagen = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>)
<span class="linenr">16: </span>
<span class="linenr">17: </span>  train_generator = train_datagen.flow_from_directory(
<span class="linenr">18: </span>      train_dir,              <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#27161;&#30446;&#37636;</span>
<span class="linenr">19: </span>      target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35519;&#25972;&#25152;&#26377;&#24433;&#20687;&#22823;&#23567;&#25104; 150x150</span>
<span class="linenr">20: </span>      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">21: </span>      class_mode=<span style="color: #98be65;">'binary'</span>)    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22240;&#28858;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109; binary_crossentropy &#20316;&#28858;&#25613;&#22833;&#20540;&#65292;&#25152;&#20197;&#38656;&#35201;&#20108;&#20301;&#20803;&#27161;&#31844;</span>
<span class="linenr">22: </span>
<span class="linenr">23: </span>
<span class="linenr">24: </span>  validation_generator = test_datagen.flow_from_directory(
<span class="linenr">25: </span>      validation_dir,
<span class="linenr">26: </span>      target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr">27: </span>      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">28: </span>      class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr">29: </span>
<span class="linenr">30: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35264;&#23519;&#29986;&#29983;&#22120;&#30340;&#32080;&#26524;</span>
<span class="linenr">31: </span>  <span style="color: #51afef;">for</span> data_batch, labels_batch <span style="color: #51afef;">in</span> train_generator:
<span class="linenr">32: </span>      <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'data batch shape:'</span>, data_batch.shape)
<span class="linenr">33: </span>      <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'labels batch shape:'</span>, labels_batch.shape)
<span id="coderef-DataGeneratorBreak" class="coderef-off"><span class="linenr">34: </span>      <span style="color: #51afef;">break</span></span>
<span class="linenr">35: </span>
</pre>
</div>

<pre class="example">
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
data batch shape: (20, 150, 150, 3)
labels batch shape: (20,)
</pre>


<p>
結果顯示每批次產生出的資料為 20 張 150&times;150 的 RGB 影像以及 20 個 label(即答案)，需留意的是此處的 generator 會無 止盡的生成批次量樣本，也就會不停的持續循環產生影像到目標目錄中，所以要放 break。而上述程式中的 ImageDataGenerator(第<a href="#coderef-ImageDataGenerator" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-ImageDataGenerator');" onmouseout="CodeHighlightOff(this, 'coderef-ImageDataGenerator');">14</a>行)是一種產生器(Generator)，在 Python 中是一個持續迭代運作的物件，是一個可以與 for&#x2026;in 一起使用的物件，產生器是使用 yield 建構的。典型的產生器範例如下：
</p>

<div class="org-src-container">
<pre class="src src-python" id="orge80fef0"><span class="linenr"> 1: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">generator</span>():
<span class="linenr"> 2: </span>      <span style="color: #dcaeea;">i</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 3: </span>      <span style="color: #51afef;">while</span> <span style="color: #a9a1e1;">True</span>:
<span class="linenr"> 4: </span>          i += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 5: </span>          <span style="color: #51afef;">yield</span> i
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">for</span> item <span style="color: #51afef;">in</span> generator():
<span class="linenr"> 8: </span>      <span style="color: #c678dd;">print</span>(item)
<span class="linenr"> 9: </span>      <span style="color: #51afef;">if</span> item &gt; <span style="color: #da8548; font-weight: bold;">3</span>:
<span class="linenr">10: </span>          <span style="color: #51afef;">break</span>
</pre>
</div>

<pre class="example">
1
2
3
4
</pre>


<p>
建構好 model、整理完資料，接下來就可以調整 model 來搭配產生器所產生的資料，我們可以應用 model 的 fit_generator 方法，這個方法的第 1 個參數即是一個 Python 的產生器，然而由於資料是無止盡地產生，所以在宣告訓練時期之前，Keras model 需要知道從產生器抽取多少樣本，這就是 steps_per_epoch 參數的功能，它指定了從產生器取得的批次量，也就是說，model 在運行了 steps_per_epoch 次的梯度下降步驟後，訓練過程將進入下一個訓練週期(epochs)。在以下的例子中，每個批次量包含 20 個樣本，而目標樣本有 2000 個，所以就需要有 100 個批次量。
</p>

<div class="org-src-container">
<pre class="src src-python" id="org2da63d9"><span class="linenr">1: </span>  <span style="color: #dcaeea;">history</span> = model.fit_generator(
<span class="linenr">2: </span>      train_generator,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#29986;&#29983;&#22120;</span>
<span class="linenr">3: </span>      steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#24478;&#29986;&#29983;&#22120;&#25277;&#21462;100&#20491;&#25209;&#27425;&#37327;</span>
<span class="linenr">4: </span>      epochs=<span style="color: #da8548; font-weight: bold;">30</span>,
<span class="linenr">5: </span>      validation_data=validation_generator,
<span class="linenr">6: </span>      validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr">7: </span>
<span class="linenr">8: </span>  model.save(<span style="color: #98be65;">'cats_and_dogs_small_i.h5'</span>)
</pre>
</div>

<p>
使用上述 fit_generator 時，還可以傳遞 validation_data 參數，此參數可以接收一個資料產生器，也可以接收 Numpy 陣列，如果接收的資料來自產生器，則還要指定 validation_steps 參數，告訴程式要從產生器中抽取多少次批量進行評估。在完成訓練後把 model 存起來，並繪製訓練週期與驗證週期的 model 損失值與準確度。
</p>
</div>
</div>

<div id="outline-container-org0b3f95a" class="outline-4">
<h4 id="org0b3f95a"><span class="section-number-4">8.2.4.</span> 完整程式</h4>
<div class="outline-text-4" id="text-8-2-4">
<div class="org-src-container">
<pre class="src src-python" id="org6b611af"><span class="linenr">  1: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr">  2: </span>
<span class="linenr">  3: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr">  4: </span>  <span style="color: #dcaeea;">original_dataset_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr">  5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#20786;&#23384;&#23569;&#37327;&#36039;&#26009;&#38598;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">  6: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">  7: </span>
<span class="linenr">  8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#25286;&#25104;&#35347;&#32244;&#12289;&#39511;&#35657;&#33287;&#28204;&#35430;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr">  9: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr"> 10: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr"> 11: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr"> 12: </span>  <span style="color: #dcaeea;">train_cats_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 13: </span>  <span style="color: #dcaeea;">train_dogs_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 14: </span>  <span style="color: #dcaeea;">validation_cats_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 15: </span>  <span style="color: #dcaeea;">validation_dogs_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 16: </span>  <span style="color: #dcaeea;">test_cats_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 17: </span>  <span style="color: #dcaeea;">test_dogs_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 18: </span>
<span class="linenr"> 19: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;&#27169;&#32068;</span>
<span class="linenr"> 20: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 21: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 22: </span>
<span class="linenr"> 23: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 24: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">32</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr"> 25: </span>                          input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>)))
<span class="linenr"> 26: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 27: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">64</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 28: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 29: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 30: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 31: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 32: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 33: </span>  model.add(layers.Flatten())
<span class="linenr"> 34: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 35: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr"> 36: </span>  model.summary()  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26597;&#30475;&#27169;&#22411;&#25688;&#35201;</span>
<span class="linenr"> 37: </span>
<span class="linenr"> 38: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#37197;&#32622; model &#20197;&#36914;&#34892;&#35347;&#32244;</span>
<span class="linenr"> 39: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 40: </span>
<span class="linenr"> 41: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 42: </span>                optimizer=optimizers.RMSprop(lr=1e-<span style="color: #da8548; font-weight: bold;">4</span>),
<span class="linenr"> 43: </span>                metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr"> 44: </span>
<span class="linenr"> 45: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992; ImageDataGenerator &#29986;&#29983;&#22120;&#24478;&#30446;&#37636;&#20013;&#35712;&#21462;&#24433;&#20687;</span>
<span class="linenr"> 46: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr"> 47: </span>
<span id="coderef-ImageDataGenerator" class="coderef-off"><span class="linenr"> 48: </span>  train_datagen = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#35347;&#32244;&#12289;&#28204;&#35430;&#36039;&#26009;&#30340; Python &#29986;&#29983;&#22120;&#65292;&#20006;&#23559;&#22294;&#29255;&#20687;&#32032;&#20540;&#20381; 1/255 &#27604;&#20363;&#37325;&#26032;&#22739;&#32302;&#21040; [0, 1]</span></span>
<span class="linenr"> 49: </span>  test_datagen = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>)
<span class="linenr"> 50: </span>
<span class="linenr"> 51: </span>  train_generator = train_datagen.flow_from_directory(
<span class="linenr"> 52: </span>      train_dir,              <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#27161;&#30446;&#37636;</span>
<span class="linenr"> 53: </span>      target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35519;&#25972;&#25152;&#26377;&#24433;&#20687;&#22823;&#23567;&#25104; 150x150</span>
<span class="linenr"> 54: </span>      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr"> 55: </span>      class_mode=<span style="color: #98be65;">'binary'</span>)    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22240;&#28858;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109; binary_crossentropy &#20316;&#28858;&#25613;&#22833;&#20540;&#65292;&#25152;&#20197;&#38656;&#35201;&#20108;&#20301;&#20803;&#27161;&#31844;</span>
<span class="linenr"> 56: </span>
<span class="linenr"> 57: </span>
<span class="linenr"> 58: </span>  validation_generator = test_datagen.flow_from_directory(
<span class="linenr"> 59: </span>      validation_dir,
<span class="linenr"> 60: </span>      target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr"> 61: </span>      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr"> 62: </span>      class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr"> 63: </span>
<span class="linenr"> 64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;model</span>
<span class="linenr"> 65: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 66: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 67: </span>
<span class="linenr"> 68: </span>  history = model.fit_generator(
<span class="linenr"> 69: </span>      train_generator,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#29986;&#29983;&#22120;</span>
<span class="linenr"> 70: </span>      steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450;&#24478;&#29986;&#29983;&#22120;&#25277;&#21462;100&#20491;&#25209;&#27425;&#37327;</span>
<span class="linenr"> 71: </span>      epochs=<span style="color: #da8548; font-weight: bold;">30</span>, verbose=<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #5B6268;">#</span><span style="color: #5B6268;">verbose=1, &#19981;&#39023;&#31034;&#35347;&#32244;&#36942;&#31243;</span>
<span class="linenr"> 72: </span>      validation_data=validation_generator,
<span class="linenr"> 73: </span>      validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr"> 74: </span>
<span class="linenr"> 75: </span>  model.save(<span style="color: #98be65;">'cats_and_dogs_small_i.h5'</span>)
<span class="linenr"> 76: </span>
<span class="linenr"> 77: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32362;&#35069;model&#30340;&#25613;&#22833;&#29575;&#33287;&#31934;&#30906;&#29575;</span>
<span class="linenr"> 78: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span>  acc = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr"> 81: </span>  val_acc = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr"> 82: </span>  loss = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 83: </span>  val_loss = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 84: </span>
<span class="linenr"> 85: </span>  epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 86: </span>
<span class="linenr"> 87: </span>  plt.clf()
<span class="linenr"> 88: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr"> 89: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr"> 90: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr"> 91: </span>  plt.legend()
<span class="linenr"> 92: </span>  plt.plot()
<span class="linenr"> 93: </span>  plt.savefig(<span style="color: #98be65;">"cats-and-dogs-accuracy-v1.png"</span>)
<span class="linenr"> 94: </span>  plt.figure()
<span class="linenr"> 95: </span>
<span class="linenr"> 96: </span>  plt.clf()
<span class="linenr"> 97: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr"> 98: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr"> 99: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">100: </span>  plt.legend()
<span class="linenr">101: </span>  plt.plot()
<span class="linenr">102: </span>  plt.savefig(<span style="color: #98be65;">"cats-and-dogs-loss-v1.png"</span>)
<span class="linenr">103: </span>
</pre>
</div>

<pre class="example" id="orgb91546a">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
</pre>


<div id="org3b7abc7" class="figure">
<p><img src="images/cats-and-dogs-accuracy-v1.png" alt="cats-and-dogs-accuracy-v1.png" />
</p>
<p><span class="figure-number">Figure 34: </span>Cats and Dogs Accuracy V1</p>
</div>


<div id="org0ba6b27" class="figure">
<p><img src="images/cats-and-dogs-loss-v1.png" alt="cats-and-dogs-loss-v1.png" />
</p>
<p><span class="figure-number">Figure 35: </span>Cats and Dogs Loss V1</p>
</div>

<p>
由圖<a href="#org3b7abc7">34</a>看出訓練準確度成線性成長直到逼近 100%，但驗證準確度則在第三個訓練週期後就停留在 70%；訓練損失分數也呈線性下降，但驗證損失分數則約在第 12 週期後達到最低點。這些都是明顯的 overfitting 訊號。
</p>

<p>
由於訓練樣本數(2000)相對較少，overfitting 將成為訓練 model 的首要顧慮因素，幾種緩解 overfitting 的技術有：
</p>
<ul class="org-ul">
<li>dropout</li>
<li>權重調整(L2 regularization)</li>
<li>資料擴增法(data augmentation)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org615c1dd" class="outline-3">
<h3 id="org615c1dd"><span class="section-number-3">8.3.</span> 改善#1: 使用資料擴增法(data augmentation)</h3>
<div class="outline-text-3" id="text-8-3">
<p>
Overfitting 的部份成因是由於樣本太少導致無法訓練出具備普適性、可套用到新資料的 model，想像一下如果有無限量的資料，則 model 將會因應用手邊資料的各種可能面向，也就不致於 overfitting。資料擴增就是由現有訓練樣本生成更多訓練資料的方法，主要是透過隨機變換原始資料，以產生相似的影像，進而增加訓練樣本數。最終目標是在訓練時，model 不會看到兩次完全相同的影像。
</p>

<p>
在 Keras 中，我們可以藉由設定 ImageDataGenerator，在讀取影像時執行隨機變換(random transformation)來達到資料擴增，至於變換的方向則可以在 ImageDataGenerator 的參數中進一步指定。以下例來看：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">datagen</span> = ImageDataGenerator(
<span class="linenr">2: </span>      rotation_range=<span style="color: #da8548; font-weight: bold;">40</span>,       <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#26059;&#36681;&#35282;&#24230;&#20540;(0~180)</span>
<span class="linenr">3: </span>      width_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#27700;&#24179;&#38568;&#27231;&#24179;&#31227;(&#22294;&#29255;&#23532;&#24230;&#20043;&#30334;&#20998;&#27604;)</span>
<span class="linenr">4: </span>      height_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#22402;&#30452;&#38568;&#27231;&#24179;&#31227;(&#22294;&#29255;&#39640;&#24230;&#20043;&#30334;&#20998;&#27604;)</span>
<span class="linenr">5: </span>      shear_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,         <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#20670;&#26012;(&#38918;&#26178;&#37912;&#20670;&#26012;&#35282;&#24230;)</span>
<span class="linenr">6: </span>      zoom_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,          <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#32302;&#25918;(&#32302;&#25918;&#30334;&#20998;&#27604;)</span>
<span class="linenr">7: </span>      horizontal_flip=<span style="color: #a9a1e1;">True</span>,    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#27700;&#24179;&#32763;&#36681;(&#24433;&#20687;&#38750;&#24038;&#21491;&#23565;&#31281;&#25165;&#26377;&#25928;)</span>
<span class="linenr">8: </span>      fill_mode=<span style="color: #98be65;">'nearest'</span>)     <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#26032;&#24314;&#24433;&#20687;&#22635;&#35036;&#20687;&#32032;&#26041;&#27861;</span>
</pre>
</div>

<p>
上述程式之 fill__{}mode 共提供四種像素填補方法：
</p>
<ul class="org-ul">
<li>constant: 依照輸入的 cval(浮點數或整數)將影像邊界之外都以該值填補，例如 cval=k，則影像填補為 kkkkkkkk|abcd|kkkkkkkk</li>
<li>nearest: 以最接近的像素值填補，如：aaaaaaaa|abcd|dddddddd</li>
<li>reflect: 以影像重複填補(影像以一正一反方向)，如 abcddcba|abcd|dcbaabcd</li>
<li>wrap: 以影像重複填補，如：abcdabcd|abcd|abcdabcd</li>
</ul>

<p>
以下為實際運作的示範：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> matplotlib
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> platform
<span class="linenr"> 3: </span>  <span style="color: #51afef;">if</span> platform.system() == <span style="color: #98be65;">'Darwin'</span>:
<span class="linenr"> 4: </span>      matplotlib.use(<span style="color: #98be65;">'MacOSX'</span>)
<span class="linenr"> 5: </span>  <span style="color: #51afef;">else</span>:
<span class="linenr"> 6: </span>      matplotlib.use(<span style="color: #98be65;">'TkAgg'</span>)
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  datagen = ImageDataGenerator(
<span class="linenr">11: </span>      rotation_range=<span style="color: #da8548; font-weight: bold;">40</span>,       <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#26059;&#36681;&#35282;&#24230;&#20540;(0~180)</span>
<span class="linenr">12: </span>      width_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,   <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#27700;&#24179;&#38568;&#27231;&#24179;&#31227;(&#22294;&#29255;&#23532;&#24230;&#20043;&#30334;&#20998;&#27604;)</span>
<span class="linenr">13: </span>      height_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#22402;&#30452;&#38568;&#27231;&#24179;&#31227;(&#22294;&#29255;&#39640;&#24230;&#20043;&#30334;&#20998;&#27604;)</span>
<span class="linenr">14: </span>      shear_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,         <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#20670;&#26012;(&#38918;&#26178;&#37912;&#20670;&#26012;&#35282;&#24230;)</span>
<span class="linenr">15: </span>      zoom_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,          <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#32302;&#25918;(&#32302;&#25918;&#30334;&#20998;&#27604;)</span>
<span class="linenr">16: </span>      horizontal_flip=<span style="color: #a9a1e1;">True</span>,    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38568;&#27231;&#27700;&#24179;&#32763;&#36681;(&#24433;&#20687;&#38750;&#24038;&#21491;&#23565;&#31281;&#25165;&#26377;&#25928;)</span>
<span class="linenr">17: </span>      fill_mode=<span style="color: #98be65;">'nearest'</span>)     <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#26032;&#24314;&#24433;&#20687;&#22635;&#35036;&#20687;&#32032;&#26041;&#27861;</span>
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr">22: </span>  original_dataset_dir = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr">23: </span>  base_dir = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">24: </span>
<span class="linenr">25: </span>  train_dir = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">26: </span>  train_cats_dir = os.path.join(train_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr">27: </span>
<span class="linenr">28: </span>  <span style="color: #51afef;">from</span> keras.preprocessing <span style="color: #51afef;">import</span> image
<span class="linenr">29: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">30: </span>
<span class="linenr">31: </span>  fnames = [os.path.join(train_cats_dir, fname) <span style="color: #51afef;">for</span>
<span class="linenr">32: </span>      fname <span style="color: #51afef;">in</span> os.listdir(train_cats_dir)]
<span class="linenr">33: </span>
<span class="linenr">34: </span>  img_path = fnames[<span style="color: #da8548; font-weight: bold;">3</span>] <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#36984;&#19968;&#24373;&#24433;&#20687;&#20358;&#25844;&#20805;</span>
<span class="linenr">35: </span>  <span style="color: #c678dd;">print</span>(img_path)
<span class="linenr">36: </span>
<span class="linenr">37: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35712;&#21462;&#24433;&#20687;&#12289;&#35519;&#25972;&#22823;&#23567;</span>
<span class="linenr">38: </span>  img = image.load_img(img_path, target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">39: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23559;&#20854;&#35519;&#25972;&#28858;shape=(150, 150, 3)</span>
<span class="linenr">40: </span>  x = image.img_to_array(img)
<span class="linenr">41: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35519;&#25972;shape&#28858;(1, 150, 150, 3)</span>
<span class="linenr">42: </span>  x = x.reshape((<span style="color: #da8548; font-weight: bold;">1</span>, ) + x.shape)
<span class="linenr">43: </span>  <span style="color: #c678dd;">print</span>(x.shape)
<span class="linenr">44: </span>
<span class="linenr">45: </span>  i = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">46: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32362;&#35069;model&#30340;&#25613;&#22833;&#29575;&#33287;&#31934;&#30906;&#29575;</span>
<span class="linenr">47: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">48: </span>
<span class="linenr">49: </span>  <span style="color: #51afef;">for</span> batch <span style="color: #51afef;">in</span> datagen.flow(x, batch_size=<span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr">50: </span>      plt.figure(i)
<span class="linenr">51: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">imgplot = plt.imshow(image.array_to_img(batch[0]))</span>
<span class="linenr">52: </span>      plt.imshow(image.array_to_img(batch[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr">53: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.clf()</span>
<span class="linenr">54: </span>      plt.plot()
<span class="linenr">55: </span>      plt.savefig(<span style="color: #98be65;">"CatsAugmentation"</span>+<span style="color: #c678dd;">str</span>(i)+<span style="color: #98be65;">".png"</span>)
<span class="linenr">56: </span>      i += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">57: </span>      <span style="color: #51afef;">if</span> i % <span style="color: #da8548; font-weight: bold;">4</span> == <span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">58: </span>          <span style="color: #51afef;">break</span>
<span class="linenr">59: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">60: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.savefig("CatsAugmentation.png")</span>
</pre>
</div>

<pre class="example">
/Volumes/Vanessa/dogs-vs-cats/small/train/cats/cat.100.jpg
(1, 150, 150, 3)
</pre>


<p>
[[file:<img src="images/CatsAugmentation0.png" alt="CatsAugmentation0.png" />images/CatsAugmentation1.png]]
[[file:<img src="images/CatsAugmentation2.png" alt="CatsAugmentation2.png" />images/CatsAugmentation3.png]]
</p>

<p>
雖然資料擴增能擴充來自少量的原始圖片，但終究無法自行產生資訊，只能重新混合現有資訊，影像間仍是高度相關，仍不足以完全擺脫 overfitting 問題，所以進一步在密集連接的分類器前，在 model 中增加 Dropout 層(Fatten 層後)。
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">import</span> matplotlib
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> platform
<span class="linenr">  3: </span>  <span style="color: #51afef;">if</span> platform.system() == <span style="color: #98be65;">'Darwin'</span>:
<span class="linenr">  4: </span>      matplotlib.use(<span style="color: #98be65;">'MacOSX'</span>)
<span class="linenr">  5: </span>  <span style="color: #51afef;">else</span>:
<span class="linenr">  6: </span>      matplotlib.use(<span style="color: #98be65;">'TkAgg'</span>)
<span class="linenr">  7: </span>
<span class="linenr">  8: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">  9: </span>
<span class="linenr"> 10: </span>  <span style="color: #51afef;">import</span> os, shutil
<span class="linenr"> 11: </span>
<span class="linenr"> 12: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35299;&#22739;&#32302;&#36039;&#26009;&#22846;&#25152;&#22312;&#30340;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr"> 13: </span>  original_dataset_dir = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/train/train'</span>
<span class="linenr"> 14: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29992;&#20358;&#20786;&#23384;&#23569;&#37327;&#36039;&#26009;&#38598;&#30340;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 15: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr"> 16: </span>
<span class="linenr"> 17: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#25286;&#25104;&#35347;&#32244;&#12289;&#39511;&#35657;&#33287;&#28204;&#35430;&#30446;&#37636;&#20301;&#32622;</span>
<span class="linenr"> 18: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr"> 19: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr"> 20: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr"> 21: </span>  <span style="color: #dcaeea;">train_cats_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 22: </span>  <span style="color: #dcaeea;">train_dogs_dir</span> = os.path.join(train_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 23: </span>  <span style="color: #dcaeea;">validation_cats_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 24: </span>  <span style="color: #dcaeea;">validation_dogs_dir</span> = os.path.join(validation_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 25: </span>  <span style="color: #dcaeea;">test_cats_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'cats'</span>)
<span class="linenr"> 26: </span>  <span style="color: #dcaeea;">test_dogs_dir</span> = os.path.join(test_dir, <span style="color: #98be65;">'dogs'</span>)
<span class="linenr"> 27: </span>
<span class="linenr"> 28: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24314;&#31435;&#27169;&#32068;</span>
<span class="linenr"> 29: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 30: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 31: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> regularizers
<span class="linenr"> 32: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 33: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">32</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>,
<span class="linenr"> 34: </span>                          input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>)))
<span class="linenr"> 35: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 36: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">64</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 37: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 38: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 39: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 40: </span>  model.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">128</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 41: </span>  model.add(layers.MaxPooling2D((<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 42: </span>  model.add(layers.Flatten())
<span class="linenr"> 43: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr"> 44: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 45: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr"> 46: </span>  model.summary()  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26597;&#30475;&#27169;&#22411;&#25688;&#35201;</span>
<span class="linenr"> 47: </span>
<span class="linenr"> 48: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#37197;&#32622; model &#20197;&#36914;&#34892;&#35347;&#32244;</span>
<span class="linenr"> 49: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 50: </span>
<span class="linenr"> 51: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 52: </span>                optimizer=optimizers.RMSprop(lr=1e-<span style="color: #da8548; font-weight: bold;">4</span>),
<span class="linenr"> 53: </span>                metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr"> 54: </span>
<span class="linenr"> 55: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#36039;&#26009;&#25844;&#22686;</span>
<span class="linenr"> 56: </span>  train_datagen = ImageDataGenerator(
<span class="linenr"> 57: </span>      rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>,
<span class="linenr"> 58: </span>      rotation_range=<span style="color: #da8548; font-weight: bold;">40</span>,
<span class="linenr"> 59: </span>      width_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 60: </span>      height_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 61: </span>      shear_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 62: </span>      zoom_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 63: </span>      horizontal_flip=<span style="color: #a9a1e1;">True</span>, )
<span class="linenr"> 64: </span>
<span class="linenr"> 65: </span>  test_datagen = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35531;&#27880;&#24847;&#65281;&#39511;&#35657;&#36039;&#26009;&#19981;&#25033;&#35442;&#25844;&#20805;!!!</span>
<span class="linenr"> 66: </span>
<span class="linenr"> 67: </span>  train_generator = train_datagen.flow_from_directory(
<span class="linenr"> 68: </span>    train_dir,    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#27161;&#30446;&#37636;</span>
<span class="linenr"> 69: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>), <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25152;&#26377;&#22294;&#20687;&#22823;&#23567;&#35519;&#25972;&#25104; 150&#215;150</span>
<span class="linenr"> 70: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">32</span>,
<span class="linenr"> 71: </span>    class_mode=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22240;&#28858;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109; binary_crossentropy &#20316;&#28858;&#25613;&#22833;&#65292;&#25152;&#20197;&#38656;&#35201;&#20108;&#20803;&#27161;&#31844;</span>
<span class="linenr"> 72: </span>
<span class="linenr"> 73: </span>
<span class="linenr"> 74: </span>  validation_generator = test_datagen.flow_from_directory(
<span class="linenr"> 75: </span>    validation_dir,
<span class="linenr"> 76: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr"> 77: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">32</span>,
<span class="linenr"> 78: </span>    class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;</span>
<span class="linenr"> 81: </span>  history = model.fit_generator(
<span class="linenr"> 82: </span>    train_generator,
<span class="linenr"> 83: </span>    steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr"> 84: </span>    epochs=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr"> 85: </span>      verbose=<span style="color: #da8548; font-weight: bold;">1</span>,
<span class="linenr"> 86: </span>    validation_data=validation_generator,
<span class="linenr"> 87: </span>    validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr"> 88: </span>
<span class="linenr"> 89: </span>  model.save(<span style="color: #98be65;">'cats_and_dogs_small_data_augmentation.h5'</span>)
<span class="linenr"> 90: </span>
<span class="linenr"> 91: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32362;&#35069;model&#30340;&#25613;&#22833;&#29575;&#33287;&#31934;&#30906;&#29575;</span>
<span class="linenr"> 92: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 93: </span>
<span class="linenr"> 94: </span>  acc = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr"> 95: </span>  val_acc = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr"> 96: </span>  loss = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 97: </span>  val_loss = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 98: </span>
<span class="linenr"> 99: </span>  epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">100: </span>  plt.clf()
<span class="linenr">101: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">102: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">103: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">104: </span>  plt.legend()
<span class="linenr">105: </span>  plt.plot()
<span class="linenr">106: </span>  plt.savefig(<span style="color: #98be65;">"CatsDogsDataAugmentation-acc.png"</span>)
<span class="linenr">107: </span>  plt.figure()
<span class="linenr">108: </span>
<span class="linenr">109: </span>  plt.clf()
<span class="linenr">110: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">111: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">112: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">113: </span>  plt.legend()
<span class="linenr">114: </span>  plt.plot()
<span class="linenr">115: </span>  plt.savefig(<span style="color: #98be65;">"CatsDogsDataAugmentation-loss.png"</span>)
<span class="linenr">116: </span>
</pre>
</div>

<pre class="example" id="orgc5a4390">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
100/100 [==============================] - 101s 1s/step - loss: 0.3521 - acc: 0.8438 - val_loss: 0.4726 - val_acc: 0.8061
</pre>


<div id="orgd4f8e17" class="figure">
<p><img src="images/CatsDogsDataAugmentation-acc.png" alt="CatsDogsDataAugmentation-acc.png" />
</p>
<p><span class="figure-number">Figure 36: </span>Cats and Dogs Data Augmentation - Accuracy</p>
</div>


<div id="orgceaef86" class="figure">
<p><img src="images/CatsDogsDataAugmentation-loss.png" alt="CatsDogsDataAugmentation-loss.png" />
</p>
<p><span class="figure-number">Figure 37: </span>Cats and Dogs Data Augmentation - Loss</p>
</div>


<p>
由圖<a href="#orgd4f8e17">36</a>和<a href="#orgceaef86">37</a>可以發現，在加入了 data augmentation 和 dropout 後，訓練曲線與驗證曲線漸趨一致，不再 overfitting，model 的準確度也達到 84%。但值的一題的是，同樣的資料集與演算法，在 Google colab 上以 GPU 執行的結果(下圖)與在本機執行(上圖)時並不相同。
</p>


<div id="orgf1516c1" class="figure">
<p><img src="images/Cat-Dog-Data-Augmentation-Acc-colab.png" alt="Cat-Dog-Data-Augmentation-Acc-colab.png" />
</p>
<p><span class="figure-number">Figure 38: </span>Cats and Dogs Data Augmentation on Google colab - Accuracy</p>
</div>


<div id="orgdbfcfa0" class="figure">
<p><img src="images/Cat-Dog-Data-Augmentation-loss-colab.png" alt="Cat-Dog-Data-Augmentation-loss-colab.png" />
</p>
<p><span class="figure-number">Figure 39: </span>Cats and Dogs Data Augmentation on Google colab - Loss</p>
</div>

<p>
在透過進一步 regularization 技術的使用，以及調整神經網路參數(如每個卷積層的過濾器數量、神經網路中的層數)，我們就能獲得更高的準確度(86%或 87%)，但在資料不及的情況下(如本例)，我們仍很難進一步提升準確度，此時，就要使用預先訓練 model。
</p>
</div>
</div>

<div id="outline-container-orgfee191f" class="outline-3">
<h3 id="orgfee191f"><span class="section-number-3">8.4.</span> 改善 2: 使用 pretrained network</h3>
<div class="outline-text-3" id="text-8-4">
<p>
Pretrained network，以簡單的話來說，就是「站在巨人的肩膀」<sup><a id="fnr.8" class="footref" href="#fn.8" role="doc-backlink">8</a></sup>，所謂「巨人」，就是別人已經用 ImageNet 訓練好的模型，例如 Google 的 Inception Model、Microsoft 的 Resnet Model 等等，把它當作 Pre-trained Model，幫助我們提取出照片的特徵(feature)。順帶一提，所謂的 Transfer Learning 就是把 Pre-trained Model 最後一層拔掉 (註：最後一層是用來分類的)，加入新的一層，然後用新資料訓練新層的參數。
</p>

<p>
能夠用來被當成 pretrained netwrok 的 model 通常是擁有大量資料集的大規模圖片分類模型，如果這個原始資料集足夠大量且具通用性，那麼 pretrained network 學習的空間層次特徵(spartial hierarchy features)就足以充當視覺世界的通用 model，其特徵對於許多不同的電腦視覺問題都同樣有效，即便是要辨識與原始任務完全不同的類別也能通用。
</p>

<p>
例如，以 ImageNet 先訓練出一個神經網路(其辨識項目為日常生活用品)，然後重新訓練這個已訓練完成的神經網路，去識別和原始樣本天差地別的家具產品等。和許多淺層的神經網路相較，深度學習的關鍵優勢在於學習到的特徵可移植到不同問題上。
</p>

<p>
以下由 Karen Simonyan 和 Andrew Zisserman 於 2014 年開發的 VGG16 架構。使用 pretrain network 有兩種方式：特徵萃取(feature extraction)和徵調(fine-tuning)。
</p>
</div>

<div id="outline-container-org942f22c" class="outline-4">
<h4 id="org942f22c"><span class="section-number-4">8.4.1.</span> 特徵萃取</h4>
<div class="outline-text-4" id="text-8-4-1">
<p>
Feature extraction 是使用 pretrained network 學習到的表示法，以這些表示法從新樣本中萃取有趣的特徵，然後將這些特徵輸入到從頭訓練的新分類器中進行處理。用於影像分類的 CNN 分為以下兩部份：以一系列的卷積層和池化層開始，以密集連接的分類器結束。第一部分稱為 model 的 convolutional base (卷積基底)，在 CNN 的情況下，特徵萃取以一個 pretrained network 做為 convolutional base，透過 convolutional base 處理新資料，
</p>


<div id="orgb670cc1" class="figure">
<p><img src="images/img-191126103936.jpg" alt="img-191126103936.jpg" />
</p>
<p><span class="figure-number">Figure 40: </span>套用同樣的 convolutional base，交換分類器</p>
</div>

<p>
為何不連分類器也預先訓練？原因是 CNN 的特徵圖是來自影像上通用 pattern 的概念，因此無論面臨何種電腦視覺問題，都能通用；而分類器學習到的表示法可能只適用於 model 所訓練的類別，僅關於整個影像中該類別相關的機率。此外，卷積特徵圖仍會描述物件出現的位置，但密集層並沒有空間的概念，密集層學習到的表示法不再包含物件在輸入影像中位罝的任何訊息，所以只要是和物件出現位置相關的問題，密集層產生的特徵絕大多數是沒有用的。
</p>

<p>
特定卷積層所萃取出來的表示法，其普適程度取於該層的深度，model 中較早出現的層會萃取局部、高度通用的特徵圖（例如可視邊緣、顏色或紋理），而較深入的層則會萃取更抽象的概念（如貓耳朵、狗眼），如果新的資料集與訓練原始 model 的資料集有很大的差別，最好使用 model 的前幾層來進行特徵萃取，而不是使用整個 convolutional base。以下以 ImageNet 訓練的 VGG16 所產生的 convolutional base 來實作，類似 pretrained 的影像分類 model 還有 Xception、Inception V3、ResNet50、VGG19、MobileNet，均已收錄於 keras.applications。
</p>
</div>

<ol class="org-ol">
<li><a id="orgd4d3e1e"></a>1. 初始化 model<br />
<div class="outline-text-5" id="text-8-4-1-1">
<p>
要使用這個 pretrained model，還需要傳三個參數給 VGG16 建構式：
</p>
<ul class="org-ul">
<li>weights: 用於初始化 model 的權重檢查點</li>
<li>include_top: 指在神經網路頂部有沒有包含密集連接的分類器。預設情況下，密集連接分類器對應於 ImageNet 的 1000 個類別。然而，我們實際想分類的可能沒這麼多層，所以這裡不一定要包含預設分類器。</li>
<li>input_shape: qpaqamo 供給神經網路的影像張量 shape。這個參數為 optional，如果不傳，則神經網路能處理任何 shape 的輸入張量。</li>
</ul>

<div class="org-src-container">
<pre class="src src-python" id="orgd472916"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr">4: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr">5: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">6: </span>  conv_base.summary()
</pre>
</div>

<pre class="example" id="orge4e13a2">
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
</pre>

<p>
由上述輸出觀察，最終特徵圖的 shape 為(4, 4, 512)，這算是神經網路的 top 層特徵，這個預訓練的 model 共有 13 層 Conv2D 層，最後要再接上密集連接分類器。做法有二：
</p>


<ol class="org-ol">
<li>在資料集上執行 convolutional base，將輸出記錄到硬碟上的 Numpy 陣列，然後再輸入到獨立的密集分類層。這種解決方案只需要為每個輪入影像執行一次 convolutional base，而 convolutional base 是處理過程中成本最高的部份，所以這種做法速度快成本低。但也因如此，這種做法不允許使用資料擴增法。</li>
<li>在頂部(最後端)增加 Dnese 層來擴展 model (conv_base)，並從輸入資料開始，從頭到尾執行整個處理過程。這種方式允許資料擴增技術，因為每次輸入影像在執行 convolutional base 時都會在 model 處理到。但這種方式的成本較高。</li>
</ol>
</div>
</li>

<li><a id="orgbfa94c0"></a>2. 快速特徵萃取<br />
<div class="outline-text-5" id="text-8-4-1-2">
<p>
先執行 ImageDataGenerator，將影像轉換為 Numpy 陣列及其 label 向量，然後呼叫 conv_base model 的 predict 方法從這些影像中萃取特徵。
</p>


<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr"> 4: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 5: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">import</span> os
<span class="linenr"> 8: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 9: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">10: </span>
<span class="linenr">11: </span>  base_dir = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">12: </span>  train_dir = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">13: </span>  validation_dir = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr">14: </span>  test_dir = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">15: </span>
<span class="linenr">16: </span>  datagen = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>)
<span class="linenr">17: </span>  batch_size = <span style="color: #da8548; font-weight: bold;">20</span>
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">extract_features</span>(directory, sample_count):
<span class="linenr">20: </span>      features = np.zeros(shape=(sample_count, <span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">21: </span>      labels = np.zeros(shape=(sample_count))
<span class="linenr">22: </span>      generator = datagen.flow_from_directory(directory,
<span class="linenr">23: </span>                                              target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr">24: </span>                                              batch_size=batch_size,
<span class="linenr">25: </span>                                              class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr">26: </span>      i = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">27: </span>      <span style="color: #51afef;">for</span> inputs_batch, labels_batch <span style="color: #51afef;">in</span> generator:
<span class="linenr">28: </span>          features_batch = conv_base.predict(inputs_batch)
<span class="linenr">29: </span>          features[i * batch_size : (i + <span style="color: #da8548; font-weight: bold;">1</span>) * batch_size] = features_batch
<span class="linenr">30: </span>          labels[i * batch_size : (i + <span style="color: #da8548; font-weight: bold;">1</span>) * batch_size] = labels_batch
<span class="linenr">31: </span>          i += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">32: </span>          <span style="color: #c678dd;">print</span>(i, end=<span style="color: #98be65;">' '</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30001;&#26044;&#33795;&#21462;&#38656;&#35201;&#36611;&#38263;&#30340;&#26178;&#38291;&#65292;&#25105;&#20497;&#21360;&#20986; i &#20358;&#27298;&#35222;&#36914;&#24230;</span>
<span class="linenr">33: </span>          <span style="color: #51afef;">if</span> i * batch_size &gt;= sample_count:
<span class="linenr">34: </span>              <span style="color: #51afef;">break</span>
<span class="linenr">35: </span>      <span style="color: #51afef;">return</span> features, labels
<span class="linenr">36: </span>
<span class="linenr">37: </span>  train_features, train_labels = extract_features(train_dir, <span style="color: #da8548; font-weight: bold;">2000</span>)
<span class="linenr">38: </span>  validation_features, validation_labels = extract_features(validation_dir, <span style="color: #da8548; font-weight: bold;">1000</span>)
<span class="linenr">39: </span>  test_features, test_labels = extract_features(test_dir, <span style="color: #da8548; font-weight: bold;">1000</span>)
</pre>
</div>

<pre class="example">
Found 2000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50
</pre>
</div>
</li>

<li><a id="orgb94dbce"></a>3. 展平資料<br />
<div class="outline-text-5" id="text-8-4-1-3">
<p>
由於目前的萃取特徵 shape = (樣本數, 4, 4, 512)，為了要提供給密集層分類器，必須將資料展平為(樣本數, 8192)。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">train_features</span> = np.reshape(train_features, (<span style="color: #da8548; font-weight: bold;">2000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">2: </span>  <span style="color: #dcaeea;">validation_features</span> = np.reshape(validation_features, (<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">3: </span>  <span style="color: #dcaeea;">test_features</span> = np.reshape(test_features, (<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
</pre>
</div>
</div>
</li>

<li><a id="org45a28b9"></a>4. 訓練<br />
<div class="outline-text-5" id="text-8-4-1-4">
<p>
接下來就可以建立我們的密集分類層（使用 dropout 和 regularization)在剛剛萃取的資料和標籤上進行訓練。因為只有兩個密集層，所以訓練的速度會很快。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 6: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>, input_dim=<span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr"> 7: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19999;&#26820;&#27861;</span>
<span class="linenr"> 8: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=2e-<span style="color: #da8548; font-weight: bold;">5</span>),
<span class="linenr">11: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">12: </span>                metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr">13: </span>
<span class="linenr">14: </span>  history = model.fit(train_features,
<span class="linenr">15: </span>                      train_labels,epochs=<span style="color: #da8548; font-weight: bold;">30</span>,
<span class="linenr">16: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">17: </span>                      validation_data=(validation_features, validation_labels))
</pre>
</div>
</div>
</li>

<li><a id="orgecd6f85"></a>5. 繪圖<br />
<div class="outline-text-5" id="text-8-4-1-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">acc</span> = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">val_acc</span> = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">loss</span> = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">val_loss</span> = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">epochs</span> = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">11: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">12: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">13: </span>  plt.legend()
<span class="linenr">14: </span>
<span class="linenr">15: </span>  plt.figure()
<span class="linenr">16: </span>
<span class="linenr">17: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">18: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">19: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">20: </span>  plt.legend()
<span class="linenr">21: </span>
<span class="linenr">22: </span>  plt.show()
</pre>
</div>
</div>
</li>

<li><a id="orgef4da4d"></a>6. 實際執行結果<br />
<div class="outline-text-5" id="text-8-4-1-6">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#####</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr"> 5: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 6: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">import</span> os
<span class="linenr"> 9: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">10: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">11: </span>
<span class="linenr">12: </span>  base_dir = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">13: </span>  train_dir = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">14: </span>  validation_dir = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr">15: </span>  test_dir = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">16: </span>
<span class="linenr">17: </span>  datagen = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>)
<span class="linenr">18: </span>  batch_size = <span style="color: #da8548; font-weight: bold;">20</span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">extract_features</span>(directory, sample_count):
<span class="linenr">21: </span>      features = np.zeros(shape=(sample_count, <span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">4</span>, <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">22: </span>      labels = np.zeros(shape=(sample_count))
<span class="linenr">23: </span>      generator = datagen.flow_from_directory(directory,
<span class="linenr">24: </span>                                              target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr">25: </span>                                              batch_size=batch_size,
<span class="linenr">26: </span>                                              class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr">27: </span>      i = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">28: </span>      <span style="color: #51afef;">for</span> inputs_batch, labels_batch <span style="color: #51afef;">in</span> generator:
<span class="linenr">29: </span>          features_batch = conv_base.predict(inputs_batch)
<span class="linenr">30: </span>          features[i * batch_size : (i + <span style="color: #da8548; font-weight: bold;">1</span>) * batch_size] = features_batch
<span class="linenr">31: </span>          labels[i * batch_size : (i + <span style="color: #da8548; font-weight: bold;">1</span>) * batch_size] = labels_batch
<span class="linenr">32: </span>          i += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">33: </span>          <span style="color: #c678dd;">print</span>(i, end=<span style="color: #98be65;">' '</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30001;&#26044;&#33795;&#21462;&#38656;&#35201;&#36611;&#38263;&#30340;&#26178;&#38291;&#65292;&#25105;&#20497;&#21360;&#20986; i &#20358;&#27298;&#35222;&#36914;&#24230;</span>
<span class="linenr">34: </span>          <span style="color: #51afef;">if</span> i * batch_size &gt;= sample_count:
<span class="linenr">35: </span>              <span style="color: #51afef;">break</span>
<span class="linenr">36: </span>      <span style="color: #51afef;">return</span> features, labels
<span class="linenr">37: </span>
<span class="linenr">38: </span>  train_features, train_labels = extract_features(train_dir, <span style="color: #da8548; font-weight: bold;">2000</span>)
<span class="linenr">39: </span>  validation_features, validation_labels = extract_features(validation_dir, <span style="color: #da8548; font-weight: bold;">1000</span>)
<span class="linenr">40: </span>  test_features, test_labels = extract_features(test_dir, <span style="color: #da8548; font-weight: bold;">1000</span>)
<span class="linenr">41: </span>
<span class="linenr">42: </span>  <span style="color: #5B6268;">#####</span>
<span class="linenr">43: </span>  train_features = np.reshape(train_features, (<span style="color: #da8548; font-weight: bold;">2000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">44: </span>  validation_features = np.reshape(validation_features, (<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">45: </span>  test_features = np.reshape(test_features, (<span style="color: #da8548; font-weight: bold;">1000</span>, <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">46: </span>
<span class="linenr">47: </span>
<span class="linenr">48: </span>  <span style="color: #5B6268;">#####</span>
<span class="linenr">49: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">50: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">51: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">52: </span>
<span class="linenr">53: </span>  model = models.Sequential()
<span class="linenr">54: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>, input_dim=<span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">4</span> * <span style="color: #da8548; font-weight: bold;">512</span>))
<span class="linenr">55: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#19999;&#26820;&#27861;</span>
<span class="linenr">56: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
<span class="linenr">57: </span>
<span class="linenr">58: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=2e-<span style="color: #da8548; font-weight: bold;">5</span>),
<span class="linenr">59: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">60: </span>                metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr">61: </span>
<span class="linenr">62: </span>  history = model.fit(train_features,
<span class="linenr">63: </span>                      train_labels,epochs=<span style="color: #da8548; font-weight: bold;">30</span>,
<span class="linenr">64: </span>                      batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">65: </span>                      validation_data=(validation_features, validation_labels))
<span class="linenr">66: </span>
<span class="linenr">67: </span>
<span class="linenr">68: </span>  <span style="color: #5B6268;">#####</span>
<span class="linenr">69: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">70: </span>
<span class="linenr">71: </span>  acc = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr">72: </span>  val_acc = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr">73: </span>  loss = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">74: </span>  val_loss = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">75: </span>
<span class="linenr">76: </span>  epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">77: </span>  plt.clf()
<span class="linenr">78: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">79: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">80: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">81: </span>  plt.legend()
<span class="linenr">82: </span>  plt.plot()
<span class="linenr">83: </span>  plt.savefig(<span style="color: #98be65;">"Pretrained-VGG16-1-acc.png"</span>)
<span class="linenr">84: </span>  plt.figure()
<span class="linenr">85: </span>
<span class="linenr">86: </span>  plt.clf()
<span class="linenr">87: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">88: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">89: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">90: </span>  plt.legend()
<span class="linenr">91: </span>  plt.plot()
<span class="linenr">92: </span>  plt.savefig(<span style="color: #98be65;">"Pretrained-VGG16-1-loss.png"</span>)
<span class="linenr">93: </span>
</pre>
</div>

<pre class="example" id="orgb5611ee">
Found 2000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Found 1000 images belonging to 2 classes.
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 Train on 2000 samples, validate on 1000 samples
Epoch 30/30
1980/2000 [============================&gt;.] - ETA: 0s - loss: 0.0909 - acc: 0.9722
2000/2000 [==============================] - 3s 2ms/step - loss: 0.0905 - acc: 0.9725 - val_loss: 0.2450 - val_acc: 0.9020
</pre>


<div id="org6191565" class="figure">
<p><img src="images/Pretrained-VGG16-1-acc.png" alt="Pretrained-VGG16-1-acc.png" />
</p>
<p><span class="figure-number">Figure 41: </span>簡單特徵萃取的訓練和驗證準確度</p>
</div>


<div id="orgaaad921" class="figure">
<p><img src="images/Pretrained-VGG16-1-loss.png" alt="Pretrained-VGG16-1-loss.png" />
</p>
<p><span class="figure-number">Figure 42: </span>簡單特徵萃取的訓練和驗證損失</p>
</div>

<p>
圖中顯示可以達到 90%的驗證準確度，比較原來的 model 成效，雖然準確度有提高，但仍可看到 overfitting 的情況，即便 model 裡已套用了 dropout，也許是因為無法使用資料擴增法，對 overfitting 的防治仍然有限。
</p>
</div>
</li>

<li><a id="orgc8ed845"></a>7. 加入資料擴增的特徵萃取<br />
<div class="outline-text-5" id="text-8-4-1-7">
<p>
將資料擴增加入特徵萃取的作法是擴展 conv_base model 並從輸入資料開始，從頭到尾執行整個處理過程，這種做法的運算成本非常昂貴，只能在 GPU 上執行，在 CPU 上絕對難以處理。由於 model 的行為與 layer 類似，因此可以將 model(如 conv\uunder{}base)視為 layer，增加到 Sequential model 中，就如同增加神經網路的 layer 一樣。其作法如下：
</p>

<div class="org-src-container">
<pre class="src src-python" id="orgae5fa98"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21367;&#31309;&#22522;&#24213;</span>
<span class="linenr"> 6: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 7: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  model = models.Sequential()
<span class="linenr">10: </span>  model.add(conv_base)        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#21367;&#31309;&#22522;&#24213;&#35222;&#28858;&#23652;&#21152;&#20837; Sequential &#27169;&#22411;&#20013;</span>
<span class="linenr">11: </span>  model.add(layers.Flatten()) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25892;&#24179;</span>
<span class="linenr">12: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">13: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22686;&#21152;&#23494;&#38598;&#23652;&#20998;&#39006;&#22120;</span>
<span class="linenr">14: </span>  model.summary() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26597;&#30475;&#27169;&#22411;&#25688;&#35201;</span>
</pre>
</div>

<pre class="example" id="org17ddab8">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
vgg16 (Model)                (None, 4, 4, 512)         14714688
_________________________________________________________________
flatten_1 (Flatten)          (None, 8192)              0
_________________________________________________________________
dense_1 (Dense)              (None, 256)               2097408
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 257
=================================================================
Total params: 16,812,353
Trainable params: 16,812,353
Non-trainable params: 0
_________________________________________________________________
</pre>

<p>
如上圖，VGG16 的 convolutional base 有 14714688 個參數，在頂部(後端)增加的分類器有 200 多萬個參數。在加入資料擴增之前，凍結 convolutional base 是非常重要的，凍結(freeze)表示在訓練期間禁止更新權重，如果不這樣做，則 convolutional base 先前學習到的表示法就會在訓練期間被修改掉，因為頂部的 Dense 層是隨機初始化的，所以非常大量的權重更新將透過神經網路傳播，會導致先前學習到的表示法被破壞掉。
</p>

<p>
在 Keras 中，可以透過設定模型的 trainable 屬性為 False 來凍結 convolutional base 神經網路：
</p>

<div class="org-src-container">
<pre class="src src-python" id="org9cc4937"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21367;&#31309;&#22522;&#24213;</span>
<span class="linenr"> 6: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 7: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  model = models.Sequential()
<span class="linenr">10: </span>  model.add(conv_base)        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#21367;&#31309;&#22522;&#24213;&#35222;&#28858;&#23652;&#21152;&#20837; Sequential &#27169;&#22411;&#20013;</span>
<span class="linenr">11: </span>  model.add(layers.Flatten()) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25892;&#24179;</span>
<span class="linenr">12: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">13: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22686;&#21152;&#23494;&#38598;&#23652;&#20998;&#39006;&#22120;</span>
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">freeze convolutional base</span>
<span class="linenr">16: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'This is the number of trainable weights '</span>
<span class="linenr">17: </span>  <span style="color: #98be65;">'before freezing the conv base:'</span>, <span style="color: #c678dd;">len</span>(model.trainable_weights))
<span class="linenr">18: </span>  conv_base.trainable = <span style="color: #a9a1e1;">False</span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20941;&#32080;&#27402;&#37325;</span>
<span class="linenr">19: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'This is the number of trainable weights '</span>
<span class="linenr">20: </span>  <span style="color: #98be65;">'after freezing the conv base:'</span>, <span style="color: #c678dd;">len</span>(model.trainable_weights))
</pre>
</div>

<pre class="example">
This is the number of trainable weights before freezing the conv base: 30
This is the number of trainable weights after freezing the conv base: 4
</pre>


<p>
由於 conv_base 被凍結更新權重，所以 model 只會訓練增力的兩個 Dense 層權重，每層有兩個參數要更新(主要權重矩陣和偏差向量)，所以一共剩 4 個 trainable weights，原本的 pretrained model 有 13 層 Conv2D，共 26 個 trainable weights。
</p>

<p>
接下來就可以使用資料擴增來訓練 model:
</p>

<div class="org-src-container">
<pre class="src src-python" id="org7109ca0"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21367;&#31309;&#22522;&#24213;</span>
<span class="linenr"> 6: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 7: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #51afef;">import</span> os
<span class="linenr">10: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">11: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">12: </span>
<span class="linenr">13: </span>  base_dir = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">14: </span>  train_dir = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">15: </span>  validation_dir = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr">16: </span>  test_dir = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">17: </span>
<span class="linenr">18: </span>  model = models.Sequential()
<span class="linenr">19: </span>  model.add(conv_base)        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#21367;&#31309;&#22522;&#24213;&#35222;&#28858;&#23652;&#21152;&#20837; Sequential &#27169;&#22411;&#20013;</span>
<span class="linenr">20: </span>  model.add(layers.Flatten()) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25892;&#24179;</span>
<span class="linenr">21: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">22: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22686;&#21152;&#23494;&#38598;&#23652;&#20998;&#39006;&#22120;</span>
<span class="linenr">23: </span>
<span class="linenr">24: </span>  conv_base.trainable = <span style="color: #a9a1e1;">False</span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20941;&#32080;&#27402;&#37325;</span>
<span class="linenr">25: </span>
<span class="linenr">26: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">data augmentation</span>
<span class="linenr">27: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">28: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">29: </span>
<span class="linenr">30: </span>  train_datagen = ImageDataGenerator( <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25844;&#20805;&#35347;&#32244;&#36039;&#26009;</span>
<span class="linenr">31: </span>    rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>,
<span class="linenr">32: </span>    rotation_range=<span style="color: #da8548; font-weight: bold;">40</span>,
<span class="linenr">33: </span>    width_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">34: </span>    height_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">35: </span>    shear_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">36: </span>    zoom_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">37: </span>    horizontal_flip=<span style="color: #a9a1e1;">True</span>,
<span class="linenr">38: </span>    fill_mode=<span style="color: #98be65;">'nearest'</span>)
<span class="linenr">39: </span>
<span class="linenr">40: </span>  test_datagen = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35531;&#27880;&#24847;&#39511;&#35657;&#36039;&#26009;&#19981;&#25033;&#35442;&#25844;&#20805;</span>
<span class="linenr">41: </span>
<span class="linenr">42: </span>
<span class="linenr">43: </span>  train_generator = train_datagen.flow_from_directory(
<span class="linenr">44: </span>    train_dir, <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#27161;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr">45: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>), <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35519;&#25972;&#25152;&#26377;&#22294;&#20687;&#22823;&#23567;&#25104; 150&#215;150</span>
<span class="linenr">46: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">47: </span>    class_mode=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22240;&#28858;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109; binary_crossentropy &#20316;&#28858;&#25613;&#22833;&#20998;&#25976;&#65292;&#25152;                       &#20197;&#38656;&#35201;&#20108;&#20803;&#27161;&#31844;</span>
<span class="linenr">48: </span>
<span class="linenr">49: </span>  validation_generator = test_datagen.flow_from_directory(
<span class="linenr">50: </span>    validation_dir,
<span class="linenr">51: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr">52: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr">53: </span>    class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr">54: </span>
<span class="linenr">55: </span>  model.<span style="color: #c678dd;">compile</span>( loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">56: </span>         optimizer=optimizers.RMSprop(lr=2e-<span style="color: #da8548; font-weight: bold;">5</span>),
<span class="linenr">57: </span>         metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr">58: </span>
<span class="linenr">59: </span>  history = model.fit_generator(
<span class="linenr">60: </span>    train_generator,
<span class="linenr">61: </span>    steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">62: </span>    epochs=<span style="color: #da8548; font-weight: bold;">30</span>,
<span class="linenr">63: </span>    validation_data=validation_generator,
<span class="linenr">64: </span>    validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr">65: </span>
<span class="linenr">66: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32362;&#35069;model&#30340;&#25613;&#22833;&#29575;&#33287;&#31934;&#30906;&#29575;</span>
<span class="linenr">67: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">68: </span>
<span class="linenr">69: </span>  acc = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr">70: </span>  val_acc = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr">71: </span>  loss = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr">72: </span>  val_loss = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr">73: </span>
<span class="linenr">74: </span>  epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">75: </span>  plt.clf()
<span class="linenr">76: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr">77: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr">78: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">79: </span>  plt.legend()
<span class="linenr">80: </span>  plt.plot()
<span class="linenr">81: </span>  plt.savefig(<span style="color: #98be65;">"CatsDogsDataAugmentationPretrained-acc.png"</span>)
<span class="linenr">82: </span>  plt.figure()
<span class="linenr">83: </span>
<span class="linenr">84: </span>  plt.clf()
<span class="linenr">85: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">86: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">87: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">88: </span>  plt.legend()
<span class="linenr">89: </span>  plt.plot()
<span class="linenr">90: </span>  plt.savefig(<span style="color: #98be65;">"CatsDogsDataAugmentationPretrained-loss.png"</span>)
<span class="linenr">91: </span>
</pre>
</div>

<pre class="example" id="org248f544">
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
Epoch 1/30
2900  1/100 [..............................] - ETA: 8:14 - loss: 0.6634 - acc: 0.5500
100/100 [==============================] - 575s 6s/step - loss: 0.2791 - acc: 0.8820 - val_loss: 0.4186 - val_acc: 0.8990
</pre>


<div id="org35b6c4c" class="figure">
<p><img src="images/CatsDogsDataAugmentationPretrained-acc.png" alt="CatsDogsDataAugmentationPretrained-acc.png" />
</p>
<p><span class="figure-number">Figure 43: </span>Cats and Dogs Data Augmentation / Pretrained- Accuracy</p>
</div>


<div id="orga295a38" class="figure">
<p><img src="images/CatsDogsDataAugmentationPretrained-loss.png" alt="CatsDogsDataAugmentationPretrained-loss.png" />
</p>
<p><span class="figure-number">Figure 44: </span>Cats and Dogs Data Augmentation - Loss</p>
</div>

<p>
實作結果，驗證準確率達 90%，優於從頭訓練小型神經網路（結果與原書中達 96%有所出入）。
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org1899efe" class="outline-4">
<h4 id="org1899efe"><span class="section-number-4">8.4.2.</span> 微調</h4>
<div class="outline-text-4" id="text-8-4-2">
<p>
微調(fine-tuning)為另一種廣泛使用的 model reuse 技術，本質上是特徵萃取的變化版，其做法是在特徵萃取的過程中不凍結整個 convolutional base，而是解凍 convolutional base 頂部的某些層以用於特徵萃取，並對於新增加於 model 的部份(如密集層分類器)與被解凍的部份層一起進行聯合訓練。
</p>

<p>
微調神經網路的步驟如下：
</p>
<ol class="org-ol">
<li>在已訓練過的基礎神經網路(即 convolutional base)上增加自定義神經網路</li>
<li>凍結 convolutional base</li>
<li>訓練步驟 1 增加的部份(即最頂端的分類器)</li>
<li>解凍 convolutional base 的某幾層</li>
<li>共同訓練解凍層和分類器</li>
</ol>

<p>
以 VGG16 的模組架構為例，其分層架構如下：
</p>

<div class="org-src-container">
<pre class="src src-python" id="orgf4270b0"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr">4: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr">5: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">6: </span>  conv_base.summary()
</pre>
</div>

<pre class="example" id="org149fbf4">
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
</pre>

<p>
我們可以調整這個 convolutional base 的最頂層(block5)三層的卷積層，即 block5_conv1、block5_conv2、block5_conv3 三層，然後凍結 block4_pool 以下的所有層。之所以選擇只解凍 convolutional base 的最頂層，幾個考量原因如下：
</p>

<ul class="org-ul">
<li>相對於 convolutional base 中的低層主要是對更通用、可重複使用的特徵進行編碼；更高層則是對更特定的特徵進行編碼，所以這些特徵需要重新調整才能適用於新的問題。如果是對低層進行微調，則會出現反效果。</li>
</ul>
<ul class="org-ul">
<li>訓練的參數越多，就越可能 overfitting。convolutional base 有近 1500 萬個參數，因此在少量資料集上訓練會有風險。</li>
</ul>

<p>
解凍部份 convolutional base 的方式如下：
</p>

<div class="org-src-container">
<pre class="src src-python" id="org1cdaf8c"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr"> 4: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 5: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 6: </span>  conv_base.summary()
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  conv_base.trainable = <span style="color: #a9a1e1;">True</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20808;&#35373;&#23450;&#25152;&#26377;layer&#37117;&#21487;&#35347;&#32244;?</span>
<span class="linenr"> 9: </span>  set_trainable = <span style="color: #a9a1e1;">False</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38928;&#35373;&#28858;&#20941;&#32080;</span>
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #51afef;">for</span> layer <span style="color: #51afef;">in</span> conv_base.layers: <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30001;&#20302;&#21040;&#39640;</span>
<span class="linenr">12: </span>      <span style="color: #51afef;">if</span> layer.name == <span style="color: #98be65;">'block5_conv1'</span>: <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30452;&#21040;&#20986;&#29694;block5_conv1&#36889;&#23652;&#24460;&#38283;&#22987;&#35299;&#20941;</span>
<span class="linenr">13: </span>          set_trainable = <span style="color: #a9a1e1;">True</span>
<span class="linenr">14: </span>      <span style="color: #51afef;">if</span> set_trainable:
<span class="linenr">15: </span>          layer.trainable = <span style="color: #a9a1e1;">True</span>
<span class="linenr">16: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">17: </span>          layer.trainable = <span style="color: #a9a1e1;">False</span>
</pre>
</div>

<p>
解凍完部份 layer 後即可開始徵調神經網路，這裡使用 RMSProp 優化器以非常低的學習率來微調，降低學習率的目的在減小 3 個解凍層的修改幅度，以免因為過大的修改損害到這些表示法。
</p>

<div class="org-src-container">
<pre class="src src-python" id="orga32a1b9"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32232;&#35695;&#27169;&#22411;</span>
<span class="linenr"> 2: </span>  model.<span style="color: #c678dd;">compile</span>(
<span class="linenr"> 3: </span>      loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 4: </span>      optimizer=optimizers.RMSprop(lr=1e-<span style="color: #da8548; font-weight: bold;">5</span>),
<span class="linenr"> 5: </span>      metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr"> 8: </span>  history = model.fit_generator(
<span class="linenr"> 9: </span>      train_generator,
<span class="linenr">10: </span>      steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">11: </span>      epochs=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr">12: </span>      validation_data=validation_generator,
<span class="linenr">13: </span>      validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
</pre>
</div>





<div class="org-src-container">
<pre class="src src-python" id="org03cfc1a"><span class="linenr">  1: </span>  <span style="color: #51afef;">import</span> os
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  3: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr">  4: </span>
<span class="linenr">  5: </span>  <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small'</span>
<span class="linenr">  6: </span>  <span style="color: #dcaeea;">train_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'train'</span>)
<span class="linenr">  7: </span>  <span style="color: #dcaeea;">validation_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'validation'</span>)
<span class="linenr">  8: </span>  <span style="color: #dcaeea;">test_dir</span> = os.path.join(base_dir, <span style="color: #98be65;">'test'</span>)
<span class="linenr">  9: </span>
<span class="linenr"> 10: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#37096;&#20221;&#20941;&#32080;</span>
<span class="linenr"> 11: </span>
<span class="linenr"> 12: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 13: </span>
<span class="linenr"> 14: </span>  <span style="color: #dcaeea;">conv_base</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr"> 15: </span>                    include_top=<span style="color: #a9a1e1;">False</span>,
<span class="linenr"> 16: </span>                    input_shape=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 17: </span>  conv_base.summary()
<span class="linenr"> 18: </span>
<span class="linenr"> 19: </span>  conv_base.trainable = <span style="color: #a9a1e1;">True</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20808;&#35373;&#23450;&#25152;&#26377;layer&#37117;&#21487;&#35347;&#32244;?</span>
<span class="linenr"> 20: </span>  set_trainable = <span style="color: #a9a1e1;">False</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38928;&#35373;&#28858;&#20941;&#32080;</span>
<span class="linenr"> 21: </span>
<span class="linenr"> 22: </span>  <span style="color: #51afef;">for</span> layer <span style="color: #51afef;">in</span> conv_base.layers: <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30001;&#20302;&#21040;&#39640;</span>
<span class="linenr"> 23: </span>      <span style="color: #51afef;">if</span> layer.name == <span style="color: #98be65;">'block5_conv1'</span>: <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30452;&#21040;&#20986;&#29694;block5_conv1&#36889;&#23652;&#24460;&#38283;&#22987;&#35299;&#20941;</span>
<span class="linenr"> 24: </span>          set_trainable = <span style="color: #a9a1e1;">True</span>
<span class="linenr"> 25: </span>      <span style="color: #51afef;">if</span> set_trainable:
<span class="linenr"> 26: </span>          layer.trainable = <span style="color: #a9a1e1;">True</span>
<span class="linenr"> 27: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr"> 28: </span>          layer.trainable = <span style="color: #a9a1e1;">False</span>
<span class="linenr"> 29: </span>
<span class="linenr"> 30: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">data augmentation</span>
<span class="linenr"> 31: </span>  <span style="color: #51afef;">from</span> keras.preprocessing.image <span style="color: #51afef;">import</span> ImageDataGenerator
<span class="linenr"> 32: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 33: </span>
<span class="linenr"> 34: </span>  train_datagen = ImageDataGenerator( <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25844;&#20805;&#35347;&#32244;&#36039;&#26009;</span>
<span class="linenr"> 35: </span>    rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>,
<span class="linenr"> 36: </span>    rotation_range=<span style="color: #da8548; font-weight: bold;">40</span>,
<span class="linenr"> 37: </span>    width_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 38: </span>    height_shift_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 39: </span>    shear_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 40: </span>    zoom_range=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 41: </span>    horizontal_flip=<span style="color: #a9a1e1;">True</span>,
<span class="linenr"> 42: </span>    fill_mode=<span style="color: #98be65;">'nearest'</span>)
<span class="linenr"> 43: </span>
<span class="linenr"> 44: </span>  test_datagen = ImageDataGenerator(rescale=<span style="color: #da8548; font-weight: bold;">1</span>./<span style="color: #da8548; font-weight: bold;">255</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35531;&#27880;&#24847;&#39511;&#35657;&#36039;&#26009;&#19981;&#25033;&#35442;&#25844;&#20805;</span>
<span class="linenr"> 45: </span>
<span class="linenr"> 46: </span>
<span class="linenr"> 47: </span>  train_generator = train_datagen.flow_from_directory(
<span class="linenr"> 48: </span>    train_dir, <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#27161;&#30446;&#37636;&#36335;&#24465;</span>
<span class="linenr"> 49: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>), <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35519;&#25972;&#25152;&#26377;&#22294;&#20687;&#22823;&#23567;&#25104; 150&#215;150</span>
<span class="linenr"> 50: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr"> 51: </span>    class_mode=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22240;&#28858;&#20351;&#29992;&#20108;&#20803;&#20132;&#21449;&#29109; binary_crossentropy &#20316;&#28858;&#25613;&#22833;&#20998;&#25976;&#65292;&#25152;                       &#20197;&#38656;&#35201;&#20108;&#20803;&#27161;&#31844;</span>
<span class="linenr"> 52: </span>
<span class="linenr"> 53: </span>  validation_generator = test_datagen.flow_from_directory(
<span class="linenr"> 54: </span>    validation_dir,
<span class="linenr"> 55: </span>    target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>),
<span class="linenr"> 56: </span>    batch_size=<span style="color: #da8548; font-weight: bold;">20</span>,
<span class="linenr"> 57: </span>    class_mode=<span style="color: #98be65;">'binary'</span>)
<span class="linenr"> 58: </span>
<span class="linenr"> 59: </span>
<span class="linenr"> 60: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr"> 61: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr"> 62: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr"> 63: </span>
<span class="linenr"> 64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model&#36996;&#26159;&#35201;&#21152;&#24460;&#38754;&#30340;layer?</span>
<span class="linenr"> 65: </span>  model = models.Sequential()
<span class="linenr"> 66: </span>  model.add(conv_base)        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#21367;&#31309;&#22522;&#24213;&#35222;&#28858;&#23652;&#21152;&#20837; Sequential &#27169;&#22411;&#20013;</span>
<span class="linenr"> 67: </span>  model.add(layers.Flatten()) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25892;&#24179;</span>
<span class="linenr"> 68: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">256</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 69: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22686;&#21152;&#23494;&#38598;&#23652;&#20998;&#39006;&#22120;</span>
<span class="linenr"> 70: </span>
<span class="linenr"> 71: </span>
<span class="linenr"> 72: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24494;&#35519;</span>
<span class="linenr"> 73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32232;&#35695;&#27169;&#22411;</span>
<span class="linenr"> 74: </span>  model.<span style="color: #c678dd;">compile</span>(
<span class="linenr"> 75: </span>      loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 76: </span>      optimizer=optimizers.RMSprop(lr=1e-<span style="color: #da8548; font-weight: bold;">5</span>),
<span class="linenr"> 77: </span>      metrics=[<span style="color: #98be65;">'acc'</span>])
<span class="linenr"> 78: </span>
<span class="linenr"> 79: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr"> 80: </span>  history = model.fit_generator(
<span class="linenr"> 81: </span>      train_generator,
<span class="linenr"> 82: </span>      steps_per_epoch=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr"> 83: </span>      epochs=<span style="color: #da8548; font-weight: bold;">100</span>,
<span class="linenr"> 84: </span>      validation_data=validation_generator,
<span class="linenr"> 85: </span>      validation_steps=<span style="color: #da8548; font-weight: bold;">50</span>)
<span class="linenr"> 86: </span>
<span class="linenr"> 87: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32362;&#35069;model&#30340;&#25613;&#22833;&#29575;&#33287;&#31934;&#30906;&#29575;</span>
<span class="linenr"> 88: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 89: </span>
<span class="linenr"> 90: </span>  acc = history.history[<span style="color: #98be65;">'acc'</span>]
<span class="linenr"> 91: </span>  val_acc = history.history[<span style="color: #98be65;">'val_acc'</span>]
<span class="linenr"> 92: </span>  loss = history.history[<span style="color: #98be65;">'loss'</span>]
<span class="linenr"> 93: </span>  val_loss = history.history[<span style="color: #98be65;">'val_loss'</span>]
<span class="linenr"> 94: </span>
<span class="linenr"> 95: </span>  epochs = <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(acc) + <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 96: </span>  plt.clf()
<span class="linenr"> 97: </span>  plt.plot(epochs, acc, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training acc'</span>)
<span class="linenr"> 98: </span>  plt.plot(epochs, val_acc, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation acc'</span>)
<span class="linenr"> 99: </span>  plt.title(<span style="color: #98be65;">'Training and validation accuracy'</span>)
<span class="linenr">100: </span>  plt.legend()
<span class="linenr">101: </span>  plt.plot()
<span class="linenr">102: </span>  plt.savefig(<span style="color: #98be65;">"FineTune-acc-1.png"</span>)
<span class="linenr">103: </span>  plt.figure()
<span class="linenr">104: </span>
<span class="linenr">105: </span>  plt.clf()
<span class="linenr">106: </span>  plt.plot(epochs, loss, <span style="color: #98be65;">'bo'</span>, label=<span style="color: #98be65;">'Training loss'</span>)
<span class="linenr">107: </span>  plt.plot(epochs, val_loss, <span style="color: #98be65;">'b'</span>, label=<span style="color: #98be65;">'Validation loss'</span>)
<span class="linenr">108: </span>  plt.title(<span style="color: #98be65;">'Training and validation loss'</span>)
<span class="linenr">109: </span>  plt.legend()
<span class="linenr">110: </span>  plt.plot()
<span class="linenr">111: </span>  plt.savefig(<span style="color: #98be65;">"FineTune-loss-1.png"</span>)
</pre>
</div>

<pre class="example" id="orgaa91ff1">
Model: "vgg16"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
input_1 (InputLayer)         (None, 150, 150, 3)       0
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
Found 2000 images belonging to 2 classes.
Found 1000 images belonging to 2 classes.
100/100 [==============================] - 602s 6s/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0559 - val_acc: 0.9380
</pre>


<div id="org4ec16bf" class="figure">
<p><img src="images/FineTune-acc-1.png" alt="FineTune-acc-1.png" />
</p>
<p><span class="figure-number">Figure 45: </span>VGG16 Fine Tune Acc</p>
</div>


<div id="orgad41f1b" class="figure">
<p><img src="images/FineTune-loss-1.png" alt="FineTune-loss-1.png" />
</p>
<p><span class="figure-number">Figure 46: </span>VGG16 Fine Tune Loss</p>
</div>

<p>
微調的訓練準確率來到 99%，驗證準確率也有 94%，這是使用 2000 個訓練樣本就達到的結果。
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org05a6222" class="outline-2">
<h2 id="org05a6222"><span class="section-number-2">9.</span> 視覺化呈現 CNN 的學習內容</h2>
<div class="outline-text-2" id="text-9">
<p>
CNN 學習的表示法非常適合以視覺化呈現，因為它們大部份就是視覺概念的表示法(represnetations of visual concepts)，幾種常用的視得化技術如下：
</p>
<ul class="org-ul">
<li>視覺化中間層 convnet 的輸出(中間啟動函數)：有助於理解 convnet 是如何一層一層的轉化資料，以及對過濾器(filter)的含義。</li>
<li>視覺化 CNN 過濾器：用於準確理解 CNN 中每個過濾器所要接受的視覺 patter 或概念中</li>
<li>視覺化類別激活熱圖(heatmaps of class activation): 有助於了解影像的哪些部份被識別為某個類別，藉以定位影像中的物件。</li>
</ul>
</div>

<div id="outline-container-orgeb7f78b" class="outline-3">
<h3 id="orgeb7f78b"><span class="section-number-3">9.1.</span> 中間層輸出視覺化</h3>
<div class="outline-text-3" id="text-9-1">
<p>
這部份工作主要是在給定輸入影像後，顯示 convnet 各個卷積層和池化層輸出的特徵層。主要是讓我們能看到在 convnet 的學習過程中，輸入資料是如何經由逐層分解到不同的過濾器，雖然輸入資料為三個維度(width, height, channel)，但其實每個 channel 會針對相對獨立的 feature 進行編碼，所以此處是將每個 channel 的內容獨立繪製成 2D 圖形秀出，此即響應圖(比喻為將吐司切片。
</p>

<p>
以下先載入之前儲存好的 model，取一張測試集中的照片(未經訓練過)，秀出原始內容，然後萃取出特徵圖，因為我們只要看一張圖，所以要建新一個新的 Keras model。
</p>

<div class="org-src-container">
<pre class="src src-python" id="org7b0b557"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35712;&#20986;&#20043;&#21069;&#23384;&#30340;model</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'cats_and_dogs_small_data_augmentation.h5'</span>)
<span class="linenr"> 4: </span>  model.summary()
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38928;&#34389;&#29702;&#21934;&#24373;&#29031;&#29255;</span>
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">img_path</span> = <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small/test/cats/cat.1556.jpg'</span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> keras.preprocessing <span style="color: #51afef;">import</span> image
<span class="linenr"> 9: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">img</span> = image.load_img(img_path, target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">12: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20197;&#19979;&#23559;&#36889;&#24373;&#29031;&#29255;&#38928;&#34389;&#29702;&#25104;4D&#24373;&#37327;&#65292;&#23559;&#20540;&#38480;&#21046;&#21040;0~1&#38291;</span>
<span class="linenr">13: </span>  img_tensor = image.img_to_array(img)
<span class="linenr">14: </span>  img_tensor = np.expand_dims(img_tensor, axis = <span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">15: </span>  img_tensor /= <span style="color: #da8548; font-weight: bold;">255</span>.
<span class="linenr">16: </span>  <span style="color: #c678dd;">print</span>(img_tensor.shape)
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#39023;&#31034;&#21407;&#22294;</span>
<span class="linenr">19: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">20: </span>  plt.imshow(img_tensor[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">21: </span>  plt.plot()
<span class="linenr">22: </span>  plt.savefig(<span style="color: #98be65;">"origin.cat.png"</span>)
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#29992;&#19968;&#20491;&#36664;&#20837;&#24373;&#37327;&#21644;&#19968;&#20491;&#36664;&#20986;&#24373;&#37327;list&#20358;&#24314;&#19968;&#20491;&#26032;model</span>
<span class="linenr">25: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">26: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#33795;&#21462;model&#30340;&#21069;8&#23652;&#36664;&#20986;&#24373;&#37327;</span>
<span class="linenr">27: </span>  layer_outputs = [layer.output <span style="color: #51afef;">for</span> layer <span style="color: #51afef;">in</span> model.layers[:<span style="color: #da8548; font-weight: bold;">8</span>]]
<span class="linenr">28: </span>  <span style="color: #51afef;">for</span> op <span style="color: #51afef;">in</span> layer_outputs:
<span class="linenr">29: </span>      <span style="color: #c678dd;">print</span>(op)
<span class="linenr">30: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#22312;&#32102;&#23450;&#36664;&#20837;&#24373;&#37327;&#30340;&#26781;&#20214;&#19979;&#65292;&#24314;&#31435;&#26371;&#29986;&#29983;&#36889;&#20123;&#36664;&#20986;&#30340;model</span>
<span class="linenr">31: </span>  activation_model = models.Model(inputs=model.<span style="color: #c678dd;">input</span>, outputs=layer_outputs)
<span class="linenr">32: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30070;&#39221;&#20837;&#19968;&#24373;&#24433;&#20687;&#24460;&#65292;&#27492;model&#26371;&#36664;&#20986;&#21407;model&#20013;&#21069;8&#23652;&#21855;&#21205;&#20989;&#25976;&#30340;&#20540;</span>
<span class="linenr">33: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#25925;&#27492;&#34389;&#26371;&#20659;&#22238;&#19968;&#20491;&#21547;8&#20491;&#36664;&#20986;&#24373;&#37327;&#30340;list&#65292;&#21363;8&#20491;layer&#30340;&#21855;&#21205;&#20989;&#25976;&#36664;&#20986;&#20540;</span>
<span class="linenr">34: </span>  activations = activation_model.predict(img_tensor)
<span class="linenr">35: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">len</span>(activations))
<span class="linenr">36: </span>
<span class="linenr">37: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20363;&#22914;&#65292;&#22312;&#39221;&#20837;&#24433;&#20687;&#24460;&#65292;&#31532;&#19968;&#20491;&#21367;&#31309;&#23652;(index=0)&#30340;&#21855;&#21205;&#20989;&#25976;&#24373;&#37327;&#28858;</span>
<span class="linenr">38: </span>  first_layer_activation = activations[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">39: </span>  <span style="color: #c678dd;">print</span>(first_layer_activation.shape)
<span class="linenr">40: </span>
<span class="linenr">41: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#32362;&#20986;&#31532;4&#20491;channel&#30340;&#38911;&#25033;&#22294;</span>
<span class="linenr">42: </span>  plt.matshow(first_layer_activation[<span style="color: #da8548; font-weight: bold;">0</span>, :, :, <span style="color: #da8548; font-weight: bold;">4</span>], cmap=<span style="color: #98be65;">'viridis'</span>)
<span class="linenr">43: </span>  plt.plot()
<span class="linenr">44: </span>  plt.savefig(<span style="color: #98be65;">"origin_channel_4_map.png"</span>)
<span class="linenr">45: </span>
<span class="linenr">46: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#32362;&#20986;&#31532;4&#20491;channel&#30340;&#38911;&#25033;&#22294;</span>
<span class="linenr">47: </span>  plt.matshow(first_layer_activation[<span style="color: #da8548; font-weight: bold;">0</span>, :, :, <span style="color: #da8548; font-weight: bold;">7</span>], cmap=<span style="color: #98be65;">'viridis'</span>)
<span class="linenr">48: </span>  plt.plot()
<span class="linenr">49: </span>  plt.savefig(<span style="color: #98be65;">"origin_channel_7_map.png"</span>)
</pre>
</div>

<pre class="example" id="org454d48d">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
(1, 150, 150, 3)
Tensor("conv2d_1/Relu:0", shape=(?, 148, 148, 32), dtype=float32)
Tensor("max_pooling2d_1/MaxPool:0", shape=(?, 74, 74, 32), dtype=float32)
Tensor("conv2d_2/Relu:0", shape=(?, 72, 72, 64), dtype=float32)
Tensor("max_pooling2d_2/MaxPool:0", shape=(?, 36, 36, 64), dtype=float32)
Tensor("conv2d_3/Relu:0", shape=(?, 34, 34, 128), dtype=float32)
Tensor("max_pooling2d_3/MaxPool:0", shape=(?, 17, 17, 128), dtype=float32)
Tensor("conv2d_4/Relu:0", shape=(?, 15, 15, 128), dtype=float32)
Tensor("max_pooling2d_4/MaxPool:0", shape=(?, 7, 7, 128), dtype=float32)
8
(1, 148, 148, 32)
</pre>

<p>
由上述結果可看出 firtst_layer_activation 的輸出為一個 148&times;148 的特徵圖，共有 32 個 channel，即原 model 中第一層的 conv2d 的輸出內容。其中第 1556 張原圖如下：
</p>


<div id="org6f988dd" class="figure">
<p><img src="images/origin.cat.png" alt="origin.cat.png" />
</p>
<p><span class="figure-number">Figure 47: </span>Origin cat</p>
</div>

<p>
其中第 4 個 channel 似乎是編碼成一個邊緣特徵偵測器，如圖<a href="#org4430ac3">48</a>。
</p>


<div id="org4430ac3" class="figure">
<p><img src="images/origin_channel_4_map.png" alt="origin_channel_4_map.png" />
</p>
<p><span class="figure-number">Figure 48: </span>Effect of channel 4</p>
</div>

<p>
而第 7 個 channel 看起來則像個亮綠點偵測器，可以用來編碼貓眼特徵。
</p>


<div id="org74c70b1" class="figure">
<p><img src="images/origin_channel_7_map.png" alt="origin_channel_7_map.png" />
</p>
<p><span class="figure-number">Figure 49: </span>Effect of channel 7</p>
</div>

<p>
如果想看出神經網路中所有啟動函數輸出的完整圖形、每個 channel 的響應圖，其相對應程式碼如下：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35712;&#20986;&#20043;&#21069;&#23384;&#30340;model</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'cats_and_dogs_small_data_augmentation.h5'</span>)
<span class="linenr"> 4: </span>  model.summary()
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38928;&#34389;&#29702;&#21934;&#24373;&#29031;&#29255;</span>
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">img_path</span> = <span style="color: #dcaeea;">base_dir</span> = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/small/test/cats/cat.1556.jpg'</span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> keras.preprocessing <span style="color: #51afef;">import</span> image
<span class="linenr"> 9: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">img</span> = image.load_img(img_path, target_size=(<span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>))
<span class="linenr">12: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20197;&#19979;&#23559;&#36889;&#24373;&#29031;&#29255;&#38928;&#34389;&#29702;&#25104;4D&#24373;&#37327;&#65292;&#23559;&#20540;&#38480;&#21046;&#21040;0~1&#38291;</span>
<span class="linenr">13: </span>  img_tensor = image.img_to_array(img)
<span class="linenr">14: </span>  img_tensor = np.expand_dims(img_tensor, axis = <span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">15: </span>  img_tensor /= <span style="color: #da8548; font-weight: bold;">255</span>.
<span class="linenr">16: </span>  <span style="color: #c678dd;">print</span>(img_tensor.shape)
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#39023;&#31034;&#21407;&#22294;</span>
<span class="linenr">19: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">20: </span>  plt.imshow(img_tensor[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">21: </span>  plt.plot()
<span class="linenr">22: </span>  plt.savefig(<span style="color: #98be65;">"origin.cat.png"</span>)
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#29992;&#19968;&#20491;&#36664;&#20837;&#24373;&#37327;&#21644;&#19968;&#20491;&#36664;&#20986;&#24373;&#37327;list&#20358;&#24314;&#19968;&#20491;&#26032;model</span>
<span class="linenr">25: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">26: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#33795;&#21462;model&#30340;&#21069;8&#23652;&#36664;&#20986;&#24373;&#37327;</span>
<span class="linenr">27: </span>  layer_outputs = [layer.output <span style="color: #51afef;">for</span> layer <span style="color: #51afef;">in</span> model.layers[:<span style="color: #da8548; font-weight: bold;">8</span>]]
<span class="linenr">28: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#22312;&#32102;&#23450;&#36664;&#20837;&#24373;&#37327;&#30340;&#26781;&#20214;&#19979;&#65292;&#24314;&#31435;&#26371;&#29986;&#29983;&#36889;&#20123;&#36664;&#20986;&#30340;model</span>
<span class="linenr">29: </span>  activation_model = models.Model(inputs=model.<span style="color: #c678dd;">input</span>, outputs=layer_outputs)
<span class="linenr">30: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#30070;&#39221;&#20837;&#19968;&#24373;&#24433;&#20687;&#24460;&#65292;&#27492;model&#26371;&#36664;&#20986;&#21407;model&#20013;&#21069;8&#23652;&#21855;&#21205;&#20989;&#25976;&#30340;&#20540;</span>
<span class="linenr">31: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#25925;&#27492;&#34389;&#26371;&#20659;&#22238;&#19968;&#20491;&#21547;8&#20491;&#36664;&#20986;&#24373;&#37327;&#30340;list&#65292;&#21363;8&#20491;layer&#30340;&#21855;&#21205;&#20989;&#25976;&#36664;&#20986;&#20540;</span>
<span class="linenr">32: </span>  activations = activation_model.predict(img_tensor)
<span class="linenr">33: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20363;&#22914;&#65292;&#22312;&#39221;&#20837;&#24433;&#20687;&#24460;&#65292;&#31532;&#19968;&#20491;&#21367;&#31309;&#23652;(index=0)&#30340;&#21855;&#21205;&#20989;&#25976;&#24373;&#37327;&#28858;</span>
<span class="linenr">34: </span>  first_layer_activation = activations[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #5B6268;">###</span><span style="color: #5B6268;">=========================</span>
<span class="linenr">37: </span>  layer_names = []
<span class="linenr">38: </span>
<span class="linenr">39: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21462;&#24471;&#21508;&#23652;&#30340;&#21517;&#23383;&#65292;&#36889;&#27171;&#25165;&#21487;&#20197;&#25104;&#28858;&#22294;&#34920;&#30340;&#19968;&#37096;&#20998;</span>
<span class="linenr">40: </span>  <span style="color: #51afef;">for</span> layer <span style="color: #51afef;">in</span> model.layers[:<span style="color: #da8548; font-weight: bold;">8</span>]:
<span class="linenr">41: </span>      layer_names.append(layer.name)
<span class="linenr">42: </span>
<span class="linenr">43: </span>  images_per_row = <span style="color: #da8548; font-weight: bold;">16</span>
<span class="linenr">44: </span>
<span class="linenr">45: </span>  <span style="color: #51afef;">for</span> layer_name, layer_activation <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(layer_names, activations):
<span class="linenr">46: </span>      n_features = layer_activation.shape[-<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">47: </span>      size = layer_activation.shape[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr">48: </span>
<span class="linenr">49: </span>      n_cols = n_features // images_per_row
<span class="linenr">50: </span>      display_grid = np.zeros((size * n_cols, images_per_row * size))
<span class="linenr">51: </span>
<span class="linenr">52: </span>      <span style="color: #51afef;">for</span> col <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(n_cols):
<span class="linenr">53: </span>          <span style="color: #51afef;">for</span> row <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(images_per_row):
<span class="linenr">54: </span>              channel_image = layer_activation[<span style="color: #da8548; font-weight: bold;">0</span>, :, :, col * images_per_row + row]
<span class="linenr">55: </span>              channel_image -= channel_image.mean()
<span class="linenr">56: </span>              channel_image /= channel_image.std()
<span class="linenr">57: </span>              channel_image *= <span style="color: #da8548; font-weight: bold;">64</span>
<span class="linenr">58: </span>              channel_image += <span style="color: #da8548; font-weight: bold;">128</span>
<span class="linenr">59: </span>              channel_image = np.clip(channel_image, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">255</span>).astype(<span style="color: #98be65;">'uint8'</span>)
<span class="linenr">60: </span>              display_grid[col * size : (col + <span style="color: #da8548; font-weight: bold;">1</span>) * size,
<span class="linenr">61: </span>                           row * size : (row + <span style="color: #da8548; font-weight: bold;">1</span>) * size] = channel_image
<span class="linenr">62: </span>
<span class="linenr">63: </span>      scale = <span style="color: #da8548; font-weight: bold;">1</span>. / size
<span class="linenr">64: </span>      plt.figure(figsize=(scale * display_grid.shape[<span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">65: </span>      scale * display_grid.shape[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr">66: </span>      plt.title(layer_name)
<span class="linenr">67: </span>      plt.grid(<span style="color: #a9a1e1;">False</span>)
<span class="linenr">68: </span>      plt.imshow(display_grid, aspect=<span style="color: #98be65;">'auto'</span>, cmap=<span style="color: #98be65;">'viridis'</span>)
<span class="linenr">69: </span>      plt.plot()
<span class="linenr">70: </span>      plt.savefig(layer_name+<span style="color: #98be65;">".png"</span>)
<span class="linenr">71: </span>      plt.clf()
</pre>
</div>

<pre class="example" id="orgf3d2673">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 148, 148, 32)      896
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0
_________________________________________________________________
flatten_1 (Flatten)          (None, 6272)              0
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0
_________________________________________________________________
dense_1 (Dense)              (None, 512)               3211776
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
(1, 150, 150, 3)
</pre>


<div id="org3ffdc80" class="figure">
<p><img src="images/conv2d_1.png" alt="conv2d_1.png" />
</p>
<p><span class="figure-number">Figure 50: </span>conv2d_1.png</p>
</div>



<div id="org8afeae8" class="figure">
<p><img src="images/max_pooling2d_1.png" alt="max_pooling2d_1.png" />
</p>
<p><span class="figure-number">Figure 51: </span>max_pooling2d_1.png</p>
</div>



<div id="org65842e9" class="figure">
<p><img src="images/conv2d_2.png" alt="conv2d_2.png" />
</p>
<p><span class="figure-number">Figure 52: </span>conv2d_2.png</p>
</div>



<div id="orga5af961" class="figure">
<p><img src="images/max_pooling2d_2.png" alt="max_pooling2d_2.png" />
</p>
<p><span class="figure-number">Figure 53: </span>max_pooling2d_2.png</p>
</div>


<div id="org6b4ce5d" class="figure">
<p><img src="images/conv2d_3.png" alt="conv2d_3.png" />
</p>
<p><span class="figure-number">Figure 54: </span>conv2d_3.png</p>
</div>


<div id="org6135514" class="figure">
<p><img src="images/max_pooling2d_3.png" alt="max_pooling2d_3.png" />
</p>
<p><span class="figure-number">Figure 55: </span>max_pooling2d_3.png</p>
</div>


<div id="org8bcf107" class="figure">
<p><img src="images/conv2d_4.png" alt="conv2d_4.png" />
</p>
<p><span class="figure-number">Figure 56: </span>conv2d_4.png</p>
</div>


<div id="org42ee987" class="figure">
<p><img src="images/max_pooling2d_4.png" alt="max_pooling2d_4.png" />
</p>
<p><span class="figure-number">Figure 57: </span>max_pooling2d_4.png</p>
</div>

<p>
由上圖可知，隨層數越來越高，啟動函數的輸出變得越來越抽象，視覺上也越來越難解釋，model 開始編碼出更高階的概念。此外，啟動函數輸出的稀疏性也隨著層數的深度而增加，在第一層中，所有的過濾器都被輸入影響所驅動(都有值)，但接下來就有越來越多的 filter 的值是空的(全黑)，這表示在這些層的輸入影像中已經找不到過濾器要編碼的圖案 pattern 了。
</p>

<p>
上述示例也證明了深度神經網路所學習到的表示法有一個重要特性：各層萃取的特徵隨著層的𣶶度而變的越來越抽象，越高階的啟動函數越不會帶有關於特定輸入的資訊，卻具備更多關於目標的資訊（此例中指的是貓或狗）。這和人或動物感知世界的方式很像：在觀察一個場景幾秒中後閉眼，我們可以記得場景中有哪些抽象事物，但不會記得每個物體的特殊外觀，因為大腦也會將事物抽象化。
</p>
</div>
</div>

<div id="outline-container-orgc8d1763" class="outline-3">
<h3 id="orgc8d1763"><span class="section-number-3">9.2.</span> 視覺化 convnet 的 filter</h3>
<div class="outline-text-3" id="text-9-2">
<p>
另一種視覺化的方法：convnet 是去看各 filter 要過濾的視覺化圖案(visual pattern)。我們可以先餵給 convnet 一張空白的影像，然後將梯度下降法套用到 convnet 上，一直到所指定的層、所指定的 filter 對輸入影像的響應達到最大化，如此所得到的輸入影像就是讓該 filter 產生最大化響的影像，也就是 filter 的長相。方法如下：
</p>

<ol class="org-ol">
<li>先建立一個損失函數，讓 convnet 指定層中指定的濾波器的啟動函數輸出最大化</li>
<li>使用隨機梯度下降(SGD)來調整輸入影像像素值，以便最大化這個啟動函數輸出值</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.applications <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> backend <span style="color: #51afef;">as</span> K <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#36617;&#20837;Keras&#30340;&#24460;&#31471;&#65292;&#29992;&#20358;&#25805;&#20316;&#24373;&#37327;(&#21462;&#24179;&#22343;)</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #dcaeea;">model</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>,
<span class="linenr"> 5: </span>                include_top=<span style="color: #a9a1e1;">False</span>)
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  layer_name = <span style="color: #98be65;">'block3_conv1'</span>
<span class="linenr"> 8: </span>  filter_index = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  layer_output = model.get_layer(layer_name).output
<span class="linenr">11: </span>  loss = K.mean(layer_output[:, :, :, filter_index]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23450;&#32681;&#25613;&#22833;&#20989;&#25976;&#24373;&#37327;, &#20854;&#28858;&#23652;&#36664;&#20986;&#24373;&#37327;&#25976;&#20540;&#21462;&#24179;&#22343;</span>
<span class="linenr">12: </span>
<span class="linenr">13: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#24471;&#21040;&#25613;&#22833;&#20989;&#25976;&#24373;&#37327;&#24460;&#65292;&#25509;&#33879;&#20351;&#29992;backend&#27169;&#32068;gradients()&#20989;&#25976;&#20358;&#21462;&#24471;&#25613;&#22833;&#20989;&#25976;&#24373;&#37327;&#30456;&#23565;&#26044;model&#36664;&#20837;&#30340;&#26799;&#24230;</span>
<span class="linenr">14: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">gradients() &#26371;&#20659;&#22238;&#19968;&#20491;&#30001;&#24373;&#37327;&#32068;&#25104;&#30340;list, &#22312;&#26412;&#20363;&#20013;, list &#30340;&#22823;&#23567;&#28858; 1, &#22240;&#27492;, &#21482;&#21462;&#20986;&#20854;&#31532; 0 &#20491;&#20803;&#32032;, &#21363; grads &#26159; 1 &#20491;&#26799;&#24230;&#24373;&#37327;</span>
<span class="linenr">15: </span>  grads = K.gradients(loss, model.<span style="color: #c678dd;">input</span>)[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#28858;&#30906;&#20445;&#26799;&#24230;&#19979;&#38477;&#36942;&#31243;&#38918;&#21033;&#65292;&#27492;&#34389;&#25226;&#26799;&#24230;&#24373;&#37327;&#38500;&#20197;&#20854;L2 norm&#65292;&#36889;&#26159;&#28858;&#20102;&#35731;&#26356;&#26032;&#24133;&#24230;&#38480;&#21046;&#22312;&#35215;&#31684;&#30340;&#31684;&#22285;&#20839;</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22312;&#20570;&#38500;&#27861;&#20043;&#21069;&#20808;&#21152;&#19978; 1e-5 &#20197;&#36991;&#20813;&#24847;&#22806;&#22320;&#38500;&#20197; 0</span>
<span class="linenr">19: </span>  grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-<span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23450;&#32681;&#19968;&#20491;Keras&#24460;&#31471;&#31243;&#24335;&#20358;&#35336;&#31639;&#25613;&#22833;&#24373;&#37327;&#21644;&#26799;&#24230;&#24373;&#37327;&#30340;&#25976;&#20540;:iterate</span>
<span class="linenr">22: </span>  iterate = K.function([model.<span style="color: #c678dd;">input</span>], [loss, grads]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23450;&#32681;&#19968;&#20491; Keras &#24460;&#31471;&#20989;&#24335;</span>
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">25: </span>  <span style="color: #dcaeea;">loss_value</span>, <span style="color: #dcaeea;">grads_value</span> = iterate([np.zeros((<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>))])
<span class="linenr">26: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#36664;&#20986;&#25613;&#22833;&#24373;&#37327;&#33287;&#26799;&#24230;&#24373;&#37327;</span>
<span class="linenr">27: </span>
<span class="linenr">28: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23450;&#32681;&#19968;&#20491;&#36852;&#22280;&#20358;&#36914;&#34892;SGD</span>
<span class="linenr">29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24478;&#24118;&#26377;&#38620;&#35338;&#30340;&#28784;&#38542;&#22294;&#20687;&#38283;&#22987;</span>
<span class="linenr">30: </span>  input_img_data = np.random.random((<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">150</span>, <span style="color: #da8548; font-weight: bold;">3</span>)) * <span style="color: #da8548; font-weight: bold;">20</span> + <span style="color: #da8548; font-weight: bold;">128</span>. <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1...</span>
<span class="linenr">31: </span>
<span class="linenr">32: </span>  step = <span style="color: #da8548; font-weight: bold;">1</span>. <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27599;&#20491;&#26799;&#24230;&#26356;&#26032;&#30340;&#22823;&#23567;</span>
<span class="linenr">33: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">40</span>): <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22519;&#34892;&#26799;&#24230;&#19978;&#21319; 40 &#27493;</span>
<span class="linenr">34: </span>    <span style="color: #dcaeea;">loss_value</span>, <span style="color: #dcaeea;">grads_value</span> = iterate([input_img_data]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;&#25613;&#22833;&#20540;&#21644;&#26799;&#24230;&#20540;</span>
<span class="linenr">35: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2. &#20197;&#26397;&#21521;&#26368;&#22823;&#21270;&#25613;&#22833;&#35519;&#25972;&#36664;&#20837;&#22294;&#20687; (&#20197;&#21069;SGD &#26159;&#29992; -= &#31639;&#31526;, &#29694;&#22312;&#21453;&#36942;&#20358;&#26159;&#29992; += &#31639;&#31526;)</span>
<span class="linenr">36: </span>    input_img_data += grads_value * step
<span class="linenr">37: </span>
<span class="linenr">38: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#24471;&#21040;&#30340;&#22294;&#20687;&#24373;&#37327;&#26159;shape&#28858;(1, 150, 150, 3)&#30340;&#28014;&#40670;&#24373;&#37327;&#65292;&#23565;&#20043;&#36914;&#34892;&#24460;&#34389;&#29702;&#36681;&#28858;&#21487;&#39023;&#31034;&#26684;&#24335;</span>
<span class="linenr">39: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">deprocess_image</span>(x):
<span class="linenr">40: </span>      x -= x.mean()
<span class="linenr">41: </span>      x /= (x.std() + 1e-<span style="color: #da8548; font-weight: bold;">5</span>)             <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. &#24373;&#37327;&#27491;&#35215;&#21270;&#65306;&#20197; 0 &#28858;&#20013;&#24515;, &#30906;&#20445; std &#28858; 0.1</span>
<span class="linenr">42: </span>      x *= <span style="color: #da8548; font-weight: bold;">0.1</span>
<span class="linenr">43: </span>      x += <span style="color: #da8548; font-weight: bold;">0.5</span>
<span class="linenr">44: </span>      x = np.clip(x, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20462;&#27491;&#25104; [0, 1], &#21363; 0-1 &#20043;&#38291;</span>
<span class="linenr">45: </span>      x *= <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">46: </span>      x = np.clip(x, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">255</span>).astype(<span style="color: #98be65;">'uint8'</span>)        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2.&#36681;&#25563;&#25104; RGB &#38499;&#21015;</span>
<span class="linenr">47: </span>      <span style="color: #51afef;">return</span> x
<span class="linenr">48: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">49: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23559;&#23652;&#30340;&#21517;&#31281;&#21644;&#36942;&#28670;&#22120;&#32034;&#24341;&#20316;&#28858;&#36664;&#20837;&#21443;&#25976;&#65292;&#22238;&#20659;&#26377;&#25928;&#24433;&#20687;&#24373;&#37327;</span>
<span class="linenr">50: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">generate_pattern</span>(layer_name, filter_index, size=<span style="color: #da8548; font-weight: bold;">150</span>):
<span class="linenr">51: </span>    layer_output = model.get_layer(layer_name).output <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21462;&#24471;&#25351;&#23450;&#23652;&#30340;&#36664;&#20986;&#24373;&#37327;</span>
<span class="linenr">52: </span>    loss = K.mean(layer_output[:, :, :, filter_index]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. &#21462;&#24471;&#25351;&#23450;&#36942;&#28670;&#22120;&#30340;&#36664;&#20986;&#24373;&#37327;, &#20006;&#20197;&#26368;&#22823;&#21270;&#27492;&#24373;&#37327;&#30340;&#22343;&#20540;&#20570;&#28858;&#25613;&#22833;</span>
<span class="linenr">53: </span>    grads = K.gradients(loss, model.<span style="color: #c678dd;">input</span>)[<span style="color: #da8548; font-weight: bold;">0</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26681;&#25818;&#27492;&#25613;&#22833;&#35336;&#31639;&#36664;&#20837;&#24433;&#20687;&#30340;&#26799;&#24230;</span>
<span class="linenr">54: </span>    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-<span style="color: #da8548; font-weight: bold;">5</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27161;&#28310;&#21270;&#25216;&#24039;&#65306;&#26799;&#24230;&#27161;&#28310;&#21270;</span>
<span class="linenr">55: </span>    iterate = K.function([model.<span style="color: #c678dd;">input</span>], [loss, grads]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">2.&#24314;&#31435; Keras function &#20358;&#37341;&#23565;&#32102;&#23450;&#30340;&#36664;&#20837;&#24433;&#20687;&#22238;&#20659;&#25613;&#22833;&#21644;&#26799;&#24230;</span>
<span class="linenr">56: </span>    input_img_data = np.random.random((<span style="color: #da8548; font-weight: bold;">1</span>, size, size, <span style="color: #da8548; font-weight: bold;">3</span>)) * <span style="color: #da8548; font-weight: bold;">20</span> + <span style="color: #da8548; font-weight: bold;">128</span>. <span style="color: #5B6268;"># </span><span style="color: #5B6268;">3. &#24478;&#24118;&#26377;&#38620;&#35338;&#30340;&#28784;&#38542;&#24433;&#20687;&#38283;&#22987;</span>
<span class="linenr">57: </span>
<span class="linenr">58: </span>    step = <span style="color: #da8548; font-weight: bold;">1</span>.
<span class="linenr">59: </span>    <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">40</span>): <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22519;&#34892;&#26799;&#24230;&#19978;&#21319; 40 &#27493;</span>
<span class="linenr">60: </span>      loss_value, grads_value = iterate([input_img_data]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">4. &#37341;&#23565;&#32102;&#23450;&#30340;&#36664;&#20837;&#24433;&#20687;&#22238;&#20659;&#25613;&#22833;&#21644;&#26799;&#24230;</span>
<span class="linenr">61: </span>      input_img_data += grads_value * step
<span class="linenr">62: </span>    img = input_img_data[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">63: </span>    <span style="color: #51afef;">return</span> deprocess_image(img)   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#22294;&#20687;&#24460;&#34389;&#29702;&#24460;&#22238;&#20659;</span>
<span class="linenr">64: </span>
<span class="linenr">65: </span>  plt.imshow(generate_pattern(<span style="color: #98be65;">'block3_conv1'</span>, <span style="color: #da8548; font-weight: bold;">0</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25105;&#20497;&#20358;&#30475;&#30475; block3_conv1 &#23652;&#20013;&#30340;&#36942;&#28670;&#22120; 0 &#30340;&#29305;&#24501;&#22294;</span>
<span class="linenr">66: </span>  plt.plot()
<span class="linenr">67: </span>  plt.savefig(<span style="color: #98be65;">"block3_conv1_filter_feature.png"</span>)
<span class="linenr">68: </span>
<span class="linenr">69: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;&#19968;&#23652;&#20013;&#25152;&#26377;&#36942;&#28670;&#22120;&#38911;&#25033;&#22294;</span>
<span class="linenr">70: </span>  <span style="color: #51afef;">for</span> layer_name <span style="color: #51afef;">in</span> [<span style="color: #98be65;">'block1_conv1'</span>, <span style="color: #98be65;">'block2_conv1'</span>, <span style="color: #98be65;">'block3_conv1'</span>, <span style="color: #98be65;">'block4_conv1'</span>]:
<span class="linenr">71: </span>      size = <span style="color: #da8548; font-weight: bold;">64</span>
<span class="linenr">72: </span>      margin = <span style="color: #da8548; font-weight: bold;">5</span>
<span class="linenr">73: </span>
<span class="linenr">74: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. &#29992;&#26044;&#20786;&#23384;&#32080;&#26524;&#30340;&#31354;(&#40657;&#33394;)&#24433;&#20687;</span>
<span class="linenr">75: </span>      results = np.zeros((<span style="color: #da8548; font-weight: bold;">8</span> * size + <span style="color: #da8548; font-weight: bold;">7</span> * margin, <span style="color: #da8548; font-weight: bold;">8</span> * size + <span style="color: #da8548; font-weight: bold;">7</span> * margin, <span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">76: </span>
<span class="linenr">77: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">8</span>):  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#8592; &#36845;&#20195;&#29986;&#29983;&#32178;&#26684;&#30340;&#34892;</span>
<span class="linenr">78: </span>          <span style="color: #51afef;">for</span> j <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">8</span>):  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#8592;&#36845;&#20195;&#29986;&#29983;&#32178;&#26684;&#30340;&#21015;</span>
<span class="linenr">79: </span>              <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22312; layer_name &#20013;&#29986;&#29983;&#36942;&#28670;&#22120; i +(j * 8) &#30340; pattern</span>
<span class="linenr">80: </span>              filter_img = generate_pattern(layer_name, i + (j * <span style="color: #da8548; font-weight: bold;">8</span>), size=size)
<span class="linenr">81: </span>
<span class="linenr">82: </span>              <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#32080;&#26524;&#25918;&#22312;&#32080;&#26524;&#32178;&#26684;&#30340;&#26041;&#24418;(i, j)&#20013;</span>
<span class="linenr">83: </span>              horizontal_start = i * size + i * margin
<span class="linenr">84: </span>              horizontal_end = horizontal_start + size
<span class="linenr">85: </span>              vertical_start = j * size + j * margin
<span class="linenr">86: </span>              vertical_end = vertical_start + size
<span class="linenr">87: </span>              results[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img
<span class="linenr">88: </span>
<span class="linenr">89: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#32178;&#26684;&#32080;&#26524;</span>
<span class="linenr">90: </span>      plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">20</span>, <span style="color: #da8548; font-weight: bold;">20</span>))
<span class="linenr">91: </span>      plt.imshow(results)
<span class="linenr">92: </span>      plt.plot()
<span class="linenr">93: </span>      plt.savefig(layer_name+<span style="color: #98be65;">"_pattern.png"</span>)
</pre>
</div>


<div id="org5cd6a3c" class="figure">
<p><img src="images/block3_conv1_filter_feature.png" alt="block3_conv1_filter_feature.png" />
</p>
<p><span class="figure-number">Figure 58: </span>block3_conv1 層中的第 0 個 channel 的最大回應 pattern</p>
</div>

<p>
block3_conv1 層中的過濾器 0 似乎對波爾卡圓點圖案有回應，再來就是開始對每一層中的每個 filter/channel，為簡化起見，只每每一層中的前 64 個過濾器，將結果以 8&times;8 網格方式呈現。
</p>

<p>
結果和課本的圖差異很大&#x2026;&#x2026;
</p>


<div id="org5a8ec3d" class="figure">
<p><img src="images/block1_conv1_pattern.png" alt="block1_conv1_pattern.png" width="400" />
</p>
<p><span class="figure-number">Figure 59: </span>pattern of block1_conv1</p>
</div>


<div id="org1306da4" class="figure">
<p><img src="images/block1_conv2_pattern.png" alt="block1_conv2_pattern.png" width="400" />
</p>
<p><span class="figure-number">Figure 60: </span>pattern of block_conv1</p>
</div>


<div id="org0a527b4" class="figure">
<p><img src="images/block3_conv1_pattern.png" alt="block3_conv1_pattern.png" width="400" />
</p>
<p><span class="figure-number">Figure 61: </span>pattern of block3_conv1</p>
</div>


<div id="org8d85991" class="figure">
<p><img src="images/block4_conv1_pattern.png" alt="block4_conv1_pattern.png" width="400" />
</p>
<p><span class="figure-number">Figure 62: </span>pattern of block4_conv1</p>
</div>
</div>
</div>

<div id="outline-container-orga8353a6" class="outline-3">
<h3 id="orga8353a6"><span class="section-number-3">9.3.</span> 視覺化類別激活熱圖 heatmap of class activation</h3>
<div class="outline-text-3" id="text-9-3">
<p>
CMA(class activation map)可以用來理解影像中的哪些部份會讓 convnet 做出最終分類的決策，這有助於 convnet 決策過程的偵錯。CAM 主要是針對輸入影像產生類別激活熱圖(heatmap of class)，這是一個 2D 的分數網格圖，針對輸入影像的每個位置(網格)進行計算，然後指出每個位置相對於目前類別的重要性。我們可以使用&ldquo;Graid-CAM: Visual Explanations from Deep Networks via Gradient-based Localization&rdquo;這篇論文提到的方法，即，給定影像，取得卷積層的輸出特徵，以&ldquo;這個類別對每個 channel 的梯度值&rdquo;對這個特徵圖中的每個 channel 做加權。進而產生「某張圖片激活某類別的強度」的 2D 分數網格圖。做法如下：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.applications.vgg16 <span style="color: #51afef;">import</span> VGG16
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">model</span> = VGG16(weights=<span style="color: #98be65;">'imagenet'</span>)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35531;&#27880;&#24847;, &#22312;&#38914;&#37096;&#21253;&#21547;&#20102;&#23494;&#38598;&#36899;&#25509;&#30340;&#20998;&#39006;&#22120; (&#38928;&#35373; include_top=True)</span>
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#20808;&#34389;&#29702; VGG16 &#30340;&#36664;&#20837;&#24433;&#20687;</span>
<span class="linenr"> 6: </span>  <span style="color: #51afef;">from</span> keras.preprocessing <span style="color: #51afef;">import</span> image
<span class="linenr"> 7: </span>  <span style="color: #51afef;">from</span> keras.applications.vgg16 <span style="color: #51afef;">import</span> preprocess_input, decode_predictions
<span class="linenr"> 8: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  img_path = r<span style="color: #98be65;">'/Volumes/Vanessa/dogs-vs-cats/african_elephants.jpg'</span>
<span class="linenr">11: </span>
<span class="linenr">12: </span>  img = image.load_img(img_path, target_size=(<span style="color: #da8548; font-weight: bold;">224</span>, <span style="color: #da8548; font-weight: bold;">224</span>))
<span class="linenr">13: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #c678dd;">type</span>(img))  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30446;&#21069;&#22294;&#29255;&#28858; &lt;class 'PIL.Image.Image'&gt; &#29289;&#20214;</span>
<span class="linenr">14: </span>  <span style="color: #c678dd;">print</span>(img.size)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21487;&#20197;&#29992; size &#23660;&#24615;&#26597;&#30475;&#23610;&#23544; -&gt; (224, 224)</span>
<span class="linenr">15: </span>
<span class="linenr">16: </span>  x = image.img_to_array(img)   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559; PIL &#29289;&#20214;&#36681;&#28858; float32 &#30340; Numpy &#38499;&#21015;</span>
<span class="linenr">17: </span>  <span style="color: #c678dd;">print</span>(x.shape)                <span style="color: #5B6268;"># </span><span style="color: #5B6268;">shape=(224, 224, 3)</span>
<span class="linenr">18: </span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559; x &#38499;&#21015; (&#21487;&#35222;&#28858;&#24373;&#37327;) &#22686;&#21152;&#19968;&#20491;&#25209;&#27425;&#36600;, shape=(1, 224, 224, 3)</span>
<span class="linenr">21: </span>  x = np.expand_dims(x, axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">22: </span>  <span style="color: #c678dd;">print</span>(x.shape)
<span class="linenr">23: </span>
<span class="linenr">24: </span>  x = preprocess_input(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38928;&#34389;&#29702;&#25209;&#27425;&#37327; (&#36889;&#26371;&#23565;&#27599;&#19968; channel &#20570;&#38991;&#33394;&#20540;&#27491;&#35215;&#21270;)</span>
<span class="linenr">25: </span>
<span class="linenr">26: </span>
<span class="linenr">27: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20351;&#29992; VGG &#31070;&#32147;&#32178;&#36335;&#38928;&#28204;&#22294;&#29255;&#39006;&#21029;</span>
<span class="linenr">28: </span>  preds = model.predict(x)
<span class="linenr">29: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#38928;&#28204;&#32080;&#26524;:'</span>, decode_predictions(preds, top=<span style="color: #da8548; font-weight: bold;">3</span>)[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">30: </span>
<span class="linenr">31: </span>  np.argmax(preds[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">32: </span>
<span class="linenr">33: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35373;&#23450; Gard-CAM &#28436;&#31639;&#27861;</span>
<span class="linenr">34: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> backend <span style="color: #51afef;">as</span> K
<span class="linenr">35: </span>
<span class="linenr">36: </span>  african_elephant_output = model.output[:, <span style="color: #da8548; font-weight: bold;">386</span>] <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#8592; &#38928;&#28204;&#21521;&#37327;&#20013;&#30340; "&#38750;&#27954;&#35937;" &#38917;&#30446;</span>
<span class="linenr">37: </span>
<span class="linenr">38: </span>  last_conv_layer = model.get_layer(<span style="color: #98be65;">'block5_conv3'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">block5_conv3 &#23652;&#30340;&#36664;&#20986;&#29305;&#24501;&#22294;, &#20854;&#28858; VGG16 &#20013;&#30340;&#26368;&#24460;&#19968;&#20491;&#21367;&#31309;&#23652;</span>
<span class="linenr">39: </span>
<span class="linenr">40: </span>  grads = K.gradients(african_elephant_output, last_conv_layer.output)[<span style="color: #da8548; font-weight: bold;">0</span>] <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">block5_conv3 &#30340;&#36664;&#20986;&#29305;&#24501;&#22294;&#20013;&#38364;&#26044; "&#38750;&#27954;&#35937;" &#39006;&#21029;&#30340;&#26799;&#24230;</span>
<span class="linenr">41: </span>
<span class="linenr">42: </span>  pooled_grads = K.mean(grads, axis=(<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>)) <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">&#36681;&#25563;&#25104;&#21521;&#37327; shape = (512, ), &#20854;&#20013;&#27599;&#20491;&#38917;&#30446;&#26159;&#29305;&#23450;&#29305;&#24501;&#22294; channel &#30340;&#26799;&#24230;&#24179;&#22343;&#24375;&#24230;(&#20540;)</span>
<span class="linenr">43: </span>
<span class="linenr">44: </span>  <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">&#32102;&#23450;&#36664;&#20837;&#24433;&#20687;&#30340;&#26781;&#20214;&#19979;, &#35731;&#25105;&#20497;&#21487;&#20197;&#23384;&#21462;&#21083;&#21083;&#23450;&#32681;&#30340;&#25976;&#20540;&#65306;pooled_grads &#21644; block5_conv3 &#30340;&#36664;&#20986;&#29305;&#24501;&#22294;</span>
<span class="linenr">45: </span>  iterate = K.function([model.<span style="color: #c678dd;">input</span>],
<span class="linenr">46: </span>      [pooled_grads, last_conv_layer.output[<span style="color: #da8548; font-weight: bold;">0</span>]])
<span class="linenr">47: </span>
<span class="linenr">48: </span>  <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">&#23565;&#26044;&#32102;&#23450;&#30340;&#20841;&#38587;&#22823;&#35937;&#27171;&#26412;&#24433;&#20687;, &#29986;&#29983;&#36889;&#20841;&#20491;&#37327;&#20540;, &#20197; Numpy &#38499;&#21015;&#21576;&#29694;</span>
<span class="linenr">49: </span>  pooled_grads_value, conv_layer_output_value = iterate([x])
<span class="linenr">50: </span>
<span class="linenr">51: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#29305;&#24501;&#22294;&#38499;&#21015;&#20013;&#30340;&#27599;&#20491; channel &#33287; "&#22823;&#35937;" &#39006;&#21029;&#30456;&#38364;&#30340; "&#27492; channel &#30340;&#37325;&#35201;&#31243;&#24230;" &#30456;&#20056;</span>
<span class="linenr">52: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">512</span>):
<span class="linenr">53: </span>      conv_layer_output_value[:, :, i] *= pooled_grads_value[i]
<span class="linenr">54: </span>
<span class="linenr">55: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29305;&#24501;&#22294;&#30340;&#36328; channel &#24179;&#22343;&#20540;&#26159;&#39006;&#21029;&#28608;&#27963;&#20989;&#25976;&#36664;&#20986;&#30340;&#29105;&#22294;</span>
<span class="linenr">56: </span>  heatmap = np.mean(conv_layer_output_value, axis=-<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">57: </span>
<span class="linenr">58: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#29105;&#22294;&#24460;&#26399;&#34389;&#29702;</span>
<span class="linenr">59: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">60: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">61: </span>
<span class="linenr">62: </span>  heatmap = np.maximum(heatmap, <span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">63: </span>  heatmap /= np.<span style="color: #c678dd;">max</span>(heatmap)
<span class="linenr">64: </span>  plt.matshow(heatmap)
<span class="linenr">65: </span>  plt.plot()
<span class="linenr">66: </span>  plot.savefig(<span style="color: #98be65;">"heapmapOfClassActivation.png"</span>)
<span class="linenr">67: </span>
<span class="linenr">68: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#29105;&#22294;&#33287;&#21407;&#22987;&#24433;&#20687;&#30090;&#21152;&#22312;&#19968;&#36215;</span>
<span class="linenr">69: </span>  <span style="color: #51afef;">import</span> cv2
<span class="linenr">70: </span>
<span class="linenr">71: </span>  img = cv2.imread(img_path)
<span class="linenr">72: </span>
<span class="linenr">73: </span>  heatmap = cv2.resize(heatmap, (img.shape[<span style="color: #da8548; font-weight: bold;">1</span>], img.shape[<span style="color: #da8548; font-weight: bold;">0</span>]))
<span class="linenr">74: </span>
<span class="linenr">75: </span>  heatmap = np.uint8(<span style="color: #da8548; font-weight: bold;">255</span> * heatmap)
<span class="linenr">76: </span>
<span class="linenr">77: </span>  heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)
<span class="linenr">78: </span>
<span class="linenr">79: </span>  superimposed_img = heatmap * <span style="color: #da8548; font-weight: bold;">0.4</span> + img <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36889;&#35041; 0.4 &#26159;&#29105;&#22294;&#24375;&#24230;&#22240;&#23376;</span>
<span class="linenr">80: </span>
<span class="linenr">81: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#26159;&#21542;&#20786;&#23384;&#25104;&#21151;:'</span>, cv2.imwrite(<span style="color: #98be65;">'elephant_cam.jpg'</span>, superimposed_img))
<span class="linenr">82: </span>
</pre>
</div>


<div id="orga43aabe" class="figure">
<p><img src="images/heapmapOfClassActivation.png" alt="heapmapOfClassActivation.png" />
</p>
<p><span class="figure-number">Figure 63: </span>Heapmap of class activation</p>
</div>


<div id="orgaef37ef" class="figure">
<p><img src="images/elephant_cam-1.jpg" alt="elephant_cam-1.jpg" />
</p>
<p><span class="figure-number">Figure 64: </span>將激活熱圖與影像叠加</p>
</div>

<p>
上圖的視覺化技術回答了兩個重要問題：
</p>
<ul class="org-ul">
<li>為什麼神經網路認為這個影像裡有非洲象?</li>
<li>非洲象位於影像中的哪個位置?</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgac6f2a7" class="outline-2">
<h2 id="orgac6f2a7"><span class="section-number-2">10.</span> MLP 神經網路模型實作：以 Keras 為實作工具</h2>
<div class="outline-text-2" id="text-10">
</div>
<div id="outline-container-orgd38e85c" class="outline-3">
<h3 id="orgd38e85c"><span class="section-number-3">10.1.</span> 簡介</h3>
<div class="outline-text-3" id="text-10-1">
<p>
Keras 是 Python 的深度學習框架，提供一種便利的方式來定義和訓練幾秬所有類型的深度學習模型。
</p>
</div>

<div id="outline-container-org78056c8" class="outline-4">
<h4 id="org78056c8"><span class="section-number-4">10.1.1.</span> 優點</h4>
<div class="outline-text-4" id="text-10-1-1">
<ul class="org-ul">
<li>相同的程式碼可在 CPU 或 GPU 上執行</li>
<li>內建程式庫支擾了卷積神經網路(用於電腦視覺)、循環神經網路(用於序列資料處理)，以及二者的任何組合。</li>
<li>Keras 可以使用最少的程式碼，花最少的時間，就能建立深度學習模型，並進行培訓、評做準確率；相對的，如果使用 TensorFlow，則需要更多程式碼，花費更多時間。</li>
<li>採用寬鬆的 MIT 授權條款，所以可以自由使用在商業專案上。</li>
</ul>

<p>
Keras 是一個 model-level 模型級的深度學習程式庫，Keras 只處理模型的建立、訓練、預測等功能。深度學習程式庫的運作（如張量運算），Keras 必須配合使用「後端引擎」(backend Engine)進行運算。目前 Keras 提供了兩種 backend engine：Theano 與 TensorFlow。其基本架構如下圖所示：
</p>


<div id="org6874331" class="figure">
<p><img src="images/KerasArch.png" alt="KerasArch.png" width="360" />
</p>
<p><span class="figure-number">Figure 65: </span>深度學習軟硬體架構</p>
</div>

<p>
由圖<a href="#org6874331">65</a>可看出，Keras 並未被綁定在特定張量程式庫中，而是改以模組方式處理，目前可用的後端引擎有 Montreal 大學 MILA 實驗室的 Theano、Google 的 TensorFlow、Microsoft 的 CNTK&#x2026;等，這些後端引擎在應用不同硬體(CPU/GPU)時則會採用不同的低階程式庫(CUDA/Eigen)。
</p>
</div>
</div>
</div>

<div id="outline-container-org252149f" class="outline-3">
<h3 id="org252149f"><span class="section-number-3">10.2.</span> Keras 程式設計模式</h3>
<div class="outline-text-3" id="text-10-2">
<p>
Keras 的開發流程大致如下：
</p>
<ol class="org-ol">
<li>定義問題並建立資料集</li>
<li>選擇一種評量成功的準則(metrics)</li>
<li>決定驗證(validation)程序</li>
<li>準備資料：定義訓練資料：即 input tensor 和 target tensors(label tensors)</li>
<li>開發出優於基準(baseline)的模型：定義神經網路模型的 layers，以便將 input tensor 對應到預測值</li>
<li>選擇 loss function, optimizer 和監控的評量準則(metrics)來建立學習過程</li>
<li>呼叫模型中的 fit()方法來迭代訓練資料</li>
<li>擴大規模：開發一個過度適配的模型</li>
<li>常規化模型並調整參數</li>
</ol>
</div>

<div id="outline-container-org3fd13f7" class="outline-4">
<h4 id="org3fd13f7"><span class="section-number-4">10.2.1.</span> 定義問題並建立資料集</h4>
<div class="outline-text-4" id="text-10-2-1">
<p>
進行模型建構之初，我們首先要評估的是：
</p>
<ul class="org-ul">
<li>輸入資料是什麼？想要預測什麼？有什麼樣的訓練資料，就只能學習預測該類問題。例如，手上只有電影評論和情緒標註資料，就只能學習對電影評論的情緒分類。</li>
<li>面臨什麼樣的問題？是二元分類？多類別分類？純量迴歸？向量迴歸？多類別多標籤？分群？生成式學習？增強式學習？不同的問題類型會引導我們如何選擇模型架構與損失函數。</li>
</ul>

<p>
在確認上述兩項問題後，我們是基於以下兩個假設來進行模型的建立：
</p>

<ul class="org-ul">
<li>假設機器可以根據給定的輸入資料預測結果</li>
<li>假設手上的資料能提供足夠的資訊，讓機器能學習到輸入與輸出間的關係</li>
</ul>

<p>
在真正建構出一個可用模型之前，上述兩個假設依然只是假設，必須經過驗證後才能確定成立與否。重點是：並非所有的問題都能透過模型來解決，例如：試圖以最近的歷史價格來預測股票市場的走勢就很難成功，因為光是參考歷史價格並不足以提供預測股價所需資訊。
</p>

<p>
另一種要特別留意的問題類型為非平穩問題(nonstationary problems)，例如分析服裝的消售/推薦，這當中存在的最大問題在於人們購買的衣服種類會隨季節而變化，所以服裝購買在幾個月內是非平穩現象，建立的模型內容會隨著時間而變化。在這種情況下，解決方法有：
</p>

<ul class="org-ul">
<li>不斷以最近的資料重新訓練模型，或</li>
<li>在相對平穩的時間區間(具有規律的週期間)收集資料，以購買衣服為例，應該以年為單位進行資料收集才足以補捉到季節變化的規律。</li>
</ul>

<p>
最後，切記：機器學習只能用於學習訓練資料中已存在的模式，也就是只能認出以前見過的模式。通常我們所謂以過去的資料預測未來，是假設未來的行為在過去曾發生過，但實際情況則未必如此。
</p>
</div>
</div>

<div id="outline-container-orgf7c6630" class="outline-4">
<h4 id="orgf7c6630"><span class="section-number-4">10.2.2.</span> 選擇一種評量成功的準則(metrics)</h4>
<div class="outline-text-4" id="text-10-2-2">
<p>
選好評量成功的準則，才有選擇損失函數的依據。在 Keras 中，所謂選擇評量準則，就是在 compile 時選擇適當的 metrics 參數。大概的選擇原則如下：
</p>

<ul class="org-ul">
<li>二元分類問題：accuracy 和 ROC AUC(area under the receiver operating characteristic curve)為兩種常用的度量。</li>
<li>類別不均(class-imbalanced)問題：使用 precision 和 recall 來做度量。</li>
<li>排名問題或多標籤問題：使用平均精度</li>
<li>少問的問題：自行定義指標</li>
</ul>
</div>
</div>

<div id="outline-container-org72e3bd7" class="outline-4">
<h4 id="org72e3bd7"><span class="section-number-4">10.2.3.</span> 決定驗證(validation)程序</h4>
<div class="outline-text-4" id="text-10-2-3">
<p>
一旦決定目標，就要決定驗證學習進度的方法，三種常見的驗證方法如下所述，但在大多數情況下，第一個方法就有不錯的效能。
</p>

<ul class="org-ul">
<li>Simple hold-out: 資料量大時適用</li>
<li>K-fold cross validation: 樣本資料不夠多時用</li>
<li>Iterated K-fold validation with shuffling: 資料量非常少時用</li>
</ul>
</div>
</div>

<div id="outline-container-orgbd30f76" class="outline-4">
<h4 id="orgbd30f76"><span class="section-number-4">10.2.4.</span> 準備資料</h4>
<div class="outline-text-4" id="text-10-2-4">
<p>
一旦知道要訓練什麼、要優化什麼、以及如評估效能，就可以著手準備建構模型，但首先要把資料整理成可以輸入神經網路的格式（張量），以監督式學習而言，其輸入的訓練資料會有以下兩類：即 input tensor 和 target tensors(label tensors)
</p>
</div>
</div>

<div id="outline-container-org85b2c53" class="outline-4">
<h4 id="org85b2c53"><span class="section-number-4">10.2.5.</span> 開發優於基準(baseline)的模型</h4>
<div class="outline-text-4" id="text-10-2-5">
<p>
此階段的目標在於實現統計功效(statistical power)，以 MNIST 資料集為例，任何準確度大於 0.1 的模型都可以說具有統計功效的（因為一共有 10 個答案類鞏）；而在 IMDB 範例中，只要準確度大於 0.5 即算。雖然 baseline 是一個很低的標準，但我們不見得都能實現這個目標，如果在嚐試過多個合理架構後模型表現仍無法優於隨機基準能力，則很有可能問題出在輸入資料，也許輸入資料沒有所需答案。
</p>

<p>
如果一切順利，則接下來我們要做出三個關鍵選擇來建構第一個模型：
</p>

<ul class="org-ul">
<li>選擇最後一層的啟動函數：這將為神經網路建立輸出的形式。例如，IMDB 分類最後使用 sigmoid 分成兩個值、MNIST 則以 softmax 分為 10 類。</li>
<li>損失函數：要配合問題類型，如 IMDB 使用 binary_crossentropy、迴歸則使用 mse。</li>
<li>優化器設定：大多數情況下，rmsprop 可做為預設選項搭配預設學習率</li>
</ul>

<p>
下表為選擇啟動函數與損失函數的參考
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">問題類型</th>
<th scope="col" class="org-left">輸出層啟動函數</th>
<th scope="col" class="org-left">損失函數</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Binary classification</td>
<td class="org-left">Sigmoid</td>
<td class="org-left">binary_crossentropy</td>
</tr>

<tr>
<td class="org-left">Multiclass, single-label classification</td>
<td class="org-left">softmax</td>
<td class="org-left">categorical_crossentropy</td>
</tr>

<tr>
<td class="org-left">Multiclass, multi-label classification</td>
<td class="org-left">sigmoid</td>
<td class="org-left">binary_crossentropy</td>
</tr>

<tr>
<td class="org-left">Regression to arbitrary values</td>
<td class="org-left">None</td>
<td class="org-left">mse</td>
</tr>

<tr>
<td class="org-left">Regression to values between 0 and 1</td>
<td class="org-left">sigmoid</td>
<td class="org-left">mse or binary_crossentropy</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org8d58459" class="outline-4">
<h4 id="org8d58459"><span class="section-number-4">10.2.6.</span> 擴大規模：開發一個過度適配的模型</h4>
<div class="outline-text-4" id="text-10-2-6">
<p>
一旦成功建構了一個超越 baseline 的模型，問題就變成：這個模型夠不夠強大？有沒有足夠的 layer 和參數來正確模擬手上的問題？例如，只有兩個 units 的單隱藏層也許有辨識 MNIST 的統計功效，但不足以很好的解決該問題。而機器學習就是在最佳化和普適性之間做取捨，理想的模型是介於 underfitting 和 overfitting 的交界、介於模型太小(undercapacity)和模型太大(overcapacity)之間，要找出這個位置，勢必要先越過它再回來。所以，要搞清楚需要多大的模型，就要開發一個太大的模型，有幾種方法可以達到這點：
</p>

<ul class="org-ul">
<li>加入更多的 layer</li>
<li>增加每一層的 capacity</li>
<li>訓練更多的週期</li>
</ul>
</div>
</div>

<div id="outline-container-org24cb627" class="outline-4">
<h4 id="org24cb627"><span class="section-number-4">10.2.7.</span> 常規化模型並調整參數</h4>
<div class="outline-text-4" id="text-10-2-7">
<p>
這裡會花掉最多時間：要反覆修改模型、訓練模型、評估驗證資料，然後再次修改，以下有幾種做法：
</p>

<ul class="org-ul">
<li>加入 dropout</li>
<li>嘗試不同架構：新增或刪除 layer</li>
<li>添加 L1 或 L2 regularization，或同時使用</li>
<li>嘗試使用不同的超參數，如每一曾的 units 數或優化器的學習率</li>
<li>著重於特徵工程，如加入新特徵、刪除似乎沒用的特徵</li>
</ul>

<p>
一旦開發出令人滿意的模型配置，就可以在所有可用資料(訓練和驗證)上訓練最終產出的模型，並在測試集上最後一次評估它。
</p>
</div>
</div>
</div>

<div id="outline-container-org3b1fe28" class="outline-3">
<h3 id="org3b1fe28"><span class="section-number-3">10.3.</span> 基本流程</h3>
<div class="outline-text-3" id="text-10-3">
<p>
在 Keras 定義 model 有兩種方法：
</p>

<ul class="org-ul">
<li>Sequential class: 適用於線性堆叠的模型</li>
<li>functional API: 適用任何有向無環的神經網路架構</li>
</ul>

<p>
以下為建立 sequential model 的例子：
</p>

<ol class="org-ol">
<li>建立模型</li>
</ol>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26032;&#22686;&#19968;&#20491;&#36664;&#20837;&#28858;874&#32173;&#12289;&#36664;&#20986;&#28858;32&#32173;&#30340;Dense layer</span>
<span class="linenr">6: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">32</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">784</span>,)))
<span class="linenr">7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25509;&#25976;&#20358;&#33258;&#19978;&#23652;32&#32173;&#30340;&#36664;&#20837;&#65292;&#36664;&#20986;&#19968;&#20491;10&#32173;&#30340;&#36039;&#26009;</span>
<span class="linenr">8: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>))
</pre>
</div>

<p>
若使用 API 來定義相同的模型，其語法如下：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> models
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">3: </span>
<span class="linenr">4: </span>  <span style="color: #dcaeea;">input_tensor</span> = layers.Input(shape=(<span style="color: #da8548; font-weight: bold;">784</span>,))
<span class="linenr">5: </span>  x = layers.Dense(<span style="color: #da8548; font-weight: bold;">32</span>, activation=<span style="color: #98be65;">'relu'</span>)(input_tensor)
<span class="linenr">6: </span>  output_tensor = layers.Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>)(x)
<span class="linenr">7: </span>  model = models.Model(inputs=input_tensor, outputs=output_tensor)
<span class="linenr">8: </span>
</pre>
</div>

<ol class="org-ol">
<li>一旦建立好模型架構，則無論是使用 Sequential 或 API，其餘步驟均相同。神經網路是在編譯(model.compile)時建立的，我們可以在其中指定使用的 optimizer 和 loss function，以及訓練期間監看的評量準則(metrics)，典型的範例如下：</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> optimizers
<span class="linenr">2: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=optimizers.RMSprop(lr=<span style="color: #da8548; font-weight: bold;">0.001</span>), loss=<span style="color: #98be65;">'mse'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>

<ol class="org-ol">
<li>最後，整個學習程序經由 fit()將輸入資料以 Numpy 陣列的形式傳給模型：</li>
</ol>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  model.fit(input_tensor, target_tensor, batch_size=<span style="color: #da8548; font-weight: bold;">128</span>, ephchs=<span style="color: #da8548; font-weight: bold;">10</span>)
</pre>
</div>
</div>
</div>

<div id="outline-container-org35c6c58" class="outline-3">
<h3 id="org35c6c58"><span class="section-number-3">10.4.</span> 以 Keras 實作 MNist 手寫數字辨識資料集</h3>
<div class="outline-text-3" id="text-10-4">
</div>
<div id="outline-container-org7755c00" class="outline-4">
<h4 id="org7755c00"><span class="section-number-4">10.4.1.</span> 讀入資料與預處理</h4>
<div class="outline-text-4" id="text-10-4-1">
<p>
MNist 手寫數字辨識資料集是由 Yann LeCun 所蒐集，他也是 CNN 的創始人。MNist 資料集共有訓練資料集 60000 筆、測試資料集 10000 筆，每筆資料都由一 28*28 的 image 以及相對應的 label 組成。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #98be65;">'''###1. &#19979;&#36617;MNist&#36039;&#26009;###'''</span>
<span class="linenr"> 2: </span>  <span style="color: #98be65;">'''1.1 &#28377;&#20837;Keras&#21450;&#30456;&#38364;&#25152;&#38656;&#36039;&#28304;'''</span>
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pf
<span class="linenr"> 5: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35201;&#23559;table&#36681;&#28858;one-hot encoding</span>
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #98be65;">'''1.2 &#21295;&#20837;Keras&#27169;&#32068;&#20197;&#19979;&#36617;MNist&#36039;&#26009;&#38598;'''</span>
<span class="linenr">10: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr">11: </span>  <span style="color: #98be65;">'''1.3 &#35712;&#21462;MNist&#36039;&#26009;&#38598;'''</span>
<span class="linenr">12: </span>  (x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #83898d;">'''1.4 &#26597;&#30475;MNist&#36039;&#26009;&#38598;&#31558;&#25976;'''</span>
<span class="linenr">15: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'4. &#26597;&#30475;MNist&#36039;&#26009;&#38598;&#31558;&#25976;'</span>)
<span class="linenr">16: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'train data='</span>, <span style="color: #c678dd;">len</span>(x_train_image))
<span class="linenr">17: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">' test data='</span>, <span style="color: #c678dd;">len</span>(x_test_image))
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #98be65;">'''###2.  &#26597;&#30475;&#35347;&#32244;&#36039;&#26009;'''</span>
<span class="linenr">20: </span>  <span style="color: #98be65;">'''2.1 &#36664;&#20986;&#35347;&#32244;&#36039;&#26009;&#26684;&#24335;'''</span>
<span class="linenr">21: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'2.1 &#26597;&#30475;&#35347;&#32244;&#36039;&#26009;&#26684;&#24335;'</span>)
<span class="linenr">22: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'train image='</span>, x_train_image.shape)
<span class="linenr">23: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">' test image='</span>, y_train_label.shape)
<span class="linenr">24: </span>
<span class="linenr">25: </span>  <span style="color: #98be65;">'''2.2 &#23450;&#32681;plot_image&#20989;&#25976;&#39023;&#31034;&#25976;&#23383;&#24433;&#20687;'''</span>
<span class="linenr">26: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">27: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_image</span>(imgname, image):
<span class="linenr">28: </span>      fig = plt.gcf() <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23450;&#35373;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr">29: </span>      fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">30: </span>      plt.imshow(image, cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">cmap&#21443;&#25976;&#35373;&#23450;&#28858;binary&#20197;&#40657;&#30333;&#28784;&#38542;&#39023;&#31034;</span>
<span class="linenr">31: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() #for jupyter or colab</span>
<span class="linenr">32: </span>      plt.plot()
<span class="linenr">33: </span>      plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">34: </span>
<span class="linenr">35: </span>  <span style="color: #98be65;">'''2.3 &#22519;&#34892;plot_image&#20989;&#25976;&#26597;&#30475;&#31532;0&#31558;&#25976;&#23383;&#24433;&#20687;&#21450;&#23565;&#25033;label'''</span>
<span class="linenr">36: </span>  plot_image(<span style="color: #98be65;">"Keras-mnist-0"</span>, x_train_image[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">37: </span>  <span style="color: #c678dd;">print</span>(y_train_label[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">38: </span>
<span class="linenr">39: </span>  <span style="color: #98be65;">'''###3. &#26597;&#30475;&#22810;&#31558;&#36039;&#26009;'''</span>
<span class="linenr">40: </span>  <span style="color: #98be65;">'''3.1 &#24314;&#31435;plot_images_labels_prediction()&#20989;&#24335;'''</span>
<span class="linenr">41: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_images_labels_prediction</span>(imgname, images, labels, prediction, idx, num = <span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr">42: </span>      fig = plt.gcf()
<span class="linenr">43: </span>      fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">8</span>,<span style="color: #da8548; font-weight: bold;">4</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450;&#39023;&#31034;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr">44: </span>      <span style="color: #51afef;">if</span> num&gt;<span style="color: #da8548; font-weight: bold;">25</span>: num=<span style="color: #da8548; font-weight: bold;">25</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#36039;&#26009;&#31558;&#25976;&#19978;&#38480;</span>
<span class="linenr">45: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, num):
<span class="linenr">46: </span>          ax=plt.subplot(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">1</span>+i) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27492;&#34389;&#39023;&#31034;10&#31558;&#22294;&#24418;&#65292;2*5&#20491;</span>
<span class="linenr">47: </span>          ax.imshow(images[idx], cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">48: </span>          title= <span style="color: #98be65;">"label="</span> +<span style="color: #c678dd;">str</span>(labels[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21152;&#20837;&#23376;&#22294;&#24418;title</span>
<span class="linenr">49: </span>          <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(prediction)&gt;<span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">50: </span>              title+=<span style="color: #98be65;">",predict="</span>+<span style="color: #c678dd;">str</span>(prediction[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27161;&#38988;title&#21152;&#20837;&#38928;&#28204;&#32080;&#26524;</span>
<span class="linenr">51: </span>          ax.set_title(title,fontsize=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">52: </span>          ax.set_xticks([]);ax.set_yticks([]) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#19981;&#39023;&#31034;&#21051;&#24230;</span>
<span class="linenr">53: </span>          idx+=<span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">54: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">55: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.plot()</span>
<span class="linenr">56: </span>      plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">57: </span>
<span class="linenr">58: </span>  <span style="color: #98be65;">'''3.2 &#22519;&#34892;plot_images_labels_prediction&#20989;&#25976;&#26597;&#30475;&#22810;&#31558;images&#21450;labels'''</span>
<span class="linenr">59: </span>  plot_images_labels_prediction(<span style="color: #98be65;">"Keras-mnist-1"</span>,x_train_image,y_train_label,[],<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">60: </span>
<span class="linenr">61: </span>  <span style="color: #98be65;">'''###4. &#22810;&#23652;&#24863;&#30693;&#22120;(Multilayer perception, MLP)&#27169;&#22411;&#36039;&#26009;&#38928;&#34389;&#29702;'''</span>
<span class="linenr">62: </span>  <span style="color: #98be65;">'''4.1 &#20197;reshape&#36681;&#25563;image&#30697;&#38499;'''</span>
<span class="linenr">63: </span>  x_Train = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">64: </span>  x_Test = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">65: </span>  <span style="color: #c678dd;">print</span>(x_Train.shape)
<span class="linenr">66: </span>  <span style="color: #c678dd;">print</span>(x_Test.shape)
<span class="linenr">67: </span>  <span style="color: #98be65;">'''4.2 &#23559;&#24433;&#20687;&#20043;&#25976;&#23383;&#30697;&#38499;&#27491;&#35215;&#21270;'''</span>
<span class="linenr">68: </span>  x_Train_normalize = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">69: </span>  x_Test_normalize = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">70: </span>
<span class="linenr">71: </span>  <span style="color: #83898d;">'''4.3 &#21407;&#22987;label&#27396;&#20301;'''</span>
<span class="linenr">72: </span>  <span style="color: #c678dd;">print</span>(y_train_label[:<span style="color: #da8548; font-weight: bold;">5</span>]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;&#21069;5&#31558;</span>
<span class="linenr">73: </span>  <span style="color: #98be65;">'''4.4 &#36914;&#34892;One-hot encoding'''</span>
<span class="linenr">74: </span>  y_TrainOneHot = np_utils.to_categorical(y_train_label)
<span class="linenr">75: </span>  y_TestOneHot = np_utils.to_categorical(y_test_label)
<span class="linenr">76: </span>  <span style="color: #83898d;">'''4.5 &#36681;&#25563;&#24460;&#20043;label&#27396;&#20301;'''</span>
<span class="linenr">77: </span>  <span style="color: #c678dd;">print</span>(y_TrainOneHot[:<span style="color: #da8548; font-weight: bold;">5</span>]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;&#21069;5&#31558;</span>
<span class="linenr">78: </span>
<span class="linenr">79: </span>  <span style="color: #98be65;">'''###5. Keras MLP'''</span>
</pre>
</div>

<pre class="example" id="org586980f">
4. 查看MNist資料集筆數
train data= 60000
 test data= 10000
2.1 查看訓練資料格式
train image= (60000, 28, 28)
 test image= (60000,)
5
(60000, 784)
(10000, 784)
[5 0 4 1 9]
[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]
</pre>


<div id="org08b98e9" class="figure">
<p><img src="images/Keras-mnist-0.png" alt="Keras-mnist-0.png" />
</p>
<p><span class="figure-number">Figure 66: </span>MNist 第一筆資料影像</p>
</div>


<div id="orga157999" class="figure">
<p><img src="images/Keras-mnist-1.png" alt="Keras-mnist-1.png" />
</p>
<p><span class="figure-number">Figure 67: </span>MNist 前十筆資料影像</p>
</div>
</div>
</div>

<div id="outline-container-org5612574" class="outline-4">
<h4 id="org5612574"><span class="section-number-4">10.4.2.</span> Keras MLP 辨識 MNist</h4>
<div class="outline-text-4" id="text-10-4-2">
</div>
<ol class="org-ol">
<li><a id="org3b4118b"></a>多層感知器模型<br />
<div class="outline-text-5" id="text-10-4-2-1">
<p>
MNist 的初始模型分為輸入、隠藏及輸出三層, 輸入層有 784 個輸入神經元(\(x_1,x_2,...,x_{784}\))，接收被 reshape 為一維矩陣的手寫圖片(28*28)；隠藏層內部有 256 個神經元，隱藏層的層數與每層的神經元各數在神經網路的建構中主要取決於設計者；輸出層共有 10 個神經元，代表預測的結果(0~9)。
</p>

<div id="orga683bf7" class="figure">
<p><img src="images/MLP-2.jpg" alt="MLP-2.jpg" />
</p>
<p><span class="figure-number">Figure 68: </span>MNist MLP 模型</p>
</div>
</div>
</li>

<li><a id="org48bdf26"></a>多層感知器的訓練與預測<br />
<div class="outline-text-5" id="text-10-4-2-2">
<p>
多層感知器模型建立後，必須先訓練模型才能夠進行預測(辨識)手寫數字。而這裡所謂的訓練模型，對於神經網路而言，就是學習。
</p>
</div>
<ol class="org-ol">
<li><a id="orgf9a91a1"></a>訓練(Traning)<br />
<div class="outline-text-6" id="text-10-4-2-2-1">
<p>
MNist 的資料訓練集共 60000 筆，經資料預處理後會產生 features(數字特徵集)與 label(數字的真實值)，然後將這些資料輸入 MLP 模型進行訓練，訓練完成後的模型才能進行預測。
</p>
</div>
</li>
<li><a id="org9cf16ba"></a>預測(Predict)<br />
<div class="outline-text-6" id="text-10-4-2-2-2">
<p>
將測試資料集匯入訓練完成的 MLP 模型，最後產生預測結果(此例中為 0~9 的數字)。
</p>
</div>
</li>
</ol>
</li>
</ol>
</div>

<div id="outline-container-org8433cad" class="outline-4">
<h4 id="org8433cad"><span class="section-number-4">10.4.3.</span> MLP 模型旳建立步驟</h4>
<div class="outline-text-4" id="text-10-4-3">
</div>
<ol class="org-ol">
<li><a id="org5e4f592"></a>進行資料預處理(preprocess)<br />
<ol class="org-ol">
<li><a id="org3b08efa"></a>匯入所需模組<br />
<div class="outline-text-6" id="text-10-4-3-1-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr">2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr">3: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
</pre>
</div>
</div>
</li>
<li><a id="org166480e"></a>讀取 mnist 資料<br />
<div class="outline-text-6" id="text-10-4-3-1-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr">2: </span>  (x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
</pre>
</div>
</div>
</li>
<li><a id="org225b272"></a>利用 reshape 轉換影像特徵值(features)<br />
<div class="outline-text-6" id="text-10-4-3-1-3">
<p>
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  x_Train = x_train_image.reshape(60000, 784).astype(&rsquo;float32&rsquo;)
  x_Test = x_test_image.reshape(10000, 784).astype(&rsquo;float32&rsquo;)
  #+END_SRC***** 建立模型
</p>
</div>
</li>
<li><a id="org1505664"></a>將 feature 標準化<br />
<div class="outline-text-6" id="text-10-4-3-1-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">2: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
</pre>
</div>
</div>
</li>
<li><a id="orgb934652"></a>以 one-hot encoding 轉換數字真實值(label)<br />
<div class="outline-text-6" id="text-10-4-3-1-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">2: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
</pre>
</div>
</div>
</li>
</ol>
</li>
<li><a id="orgf0f3587"></a>建立模型<br />
<ol class="org-ol">
<li><a id="orgc9952b4"></a>匯入所需模組<br />
<div class="outline-text-6" id="text-10-4-3-2-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">2: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
</pre>
</div>
<p>
在 Keras 在 Keras 中有兩類主要的模型：Sequential 順序模型 和 使用函數式 API 的 Model 類模型。
</p>
</div>
</li>
<li><a id="org4d6434f"></a>建立 Sequential 模型<br />
<div class="outline-text-6" id="text-10-4-3-2-2">
<p>
建立一個線性堆叠模型，後續再使用 model.add()方法將各神經網路層加入模型中即可。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
</pre>
</div>
</div>
</li>
<li><a id="orgdb16b5f"></a>建立「輸入層」與「隠藏層」<br />
<div class="outline-text-6" id="text-10-4-3-2-3">
<p>
Dense 神經網路層的特色：所有的上一層與下一層的神經元都完全連接。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#24120;&#24907;&#20998;&#20296;&#30340;&#20098;&#25976;&#21021;&#22987;&#21270;weight&#21644;bias</span>
<span class="linenr">2: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">256</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
</pre>
</div>
</div>
</li>
<li><a id="org7d6626b"></a>建立「輸出層」<br />
<div class="outline-text-6" id="text-10-4-3-2-4">
<p>
10 個神經元分別對應 0~9 的答案，softmax 可以將神經元的輸出結果轉換為預測每一個數字的機率。建立這裡的 Dense 網路層時無需設定 input_data，因為 Keras 會自動依照上一層的 units 神經元個數(256)來設定這一層的 input_dim 神經元個數。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#24120;&#24907;&#20998;&#20296;&#30340;&#20098;&#25976;&#21021;&#22987;&#21270;weight&#21644;bias</span>
<span class="linenr">2: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
</pre>
</div>
</div>
</li>
<li><a id="orgbee1316"></a>查看模型摘要<br />
<div class="outline-text-6" id="text-10-4-3-2-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #c678dd;">print</span>(model.summary())
</pre>
</div>
</div>
</li>
<li><a id="org0283723"></a>執行結果<br />
<div class="outline-text-6" id="text-10-4-3-2-6">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 3: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 5: </span>  (x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">11: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">13: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">14: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">15: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">256</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">16: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">17: </span>  <span style="color: #c678dd;">print</span>(model.summary())
</pre>
</div>

<pre class="example" id="org86ac967">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 256)               200960
_________________________________________________________________
dense_2 (Dense)              (None, 10)                2570
=================================================================
Total params: 203,530
Trainable params: 203,530
Non-trainable params: 0
_________________________________________________________________
None
</pre>

<p>
以上每一層 Param 稱為超參數(Hyper-Parameters)，計算方式為：Param=(上一層神經元數量)\(\times\)(本層的神經元數量)\(+\)(本層的神經元數量)。其中：
</p>
<ul class="org-ul">
<li>隠藏層的 Param 為 200960，即 784(輸入層神經元數量)\(\times\)256(隠藏層神經元數量)+256(隠藏層神經元數量)=200960</li>
<li>輸出層的 Param 為 2570，即 256(隠藏層神經元數量)\(\times\)10(輸出層神經元數量)+10(輸出層神經元數量)=2570</li>
</ul>
</div>
</li>
</ol>
</li>

<li><a id="org4ec621e"></a>進行訓練<br />
<div class="outline-text-5" id="text-10-4-3-3">
<p>
模型建立後，即可利用 Back Propagation 來進行訓練，其步驟如下：
</p>
</div>

<ol class="org-ol">
<li><a id="orgb898c0e"></a>定義訓練方式<sup><a id="fnr.9" class="footref" href="#fn.9" role="doc-backlink">9</a></sup><br />
<div class="outline-text-6" id="text-10-4-3-3-1">
<p>
在訓練模型前，我們必須使用 compile 方式，設定訓練模式
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
</pre>
</div>
<p>
model.compile()接收三個參數：
</p>

<ul class="org-ul">
<li>優化器 optimizer。它可以是現有優化器的字符串標識符，如 rmsprop 或 adagrad，也可以是 Optimizer 類的實例。詳見：optimizers。</li>
<li>損失函數 loss，模型試圖最小化的目標函數。它可以是現有損失函數的字符串標識符，如 categorical_crossentropy 或 mse，也可以是一個目標函數。詳見：losses。</li>
<li>評估標準 metrics。對於任何分類問題，你都希望將其設置為 metrics = [&rsquo;accuracy&rsquo;]。評估標準可以是現有的標準的字符串標識符，也可以是自定義的評估標準函數。</li>
</ul>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22810;&#20998;&#39006;&#21839;&#38988;</span>
<span class="linenr"> 2: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr"> 3: </span>                loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr"> 4: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20108;&#20998;&#39006;&#21839;&#38988;</span>
<span class="linenr"> 7: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr"> 8: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr"> 9: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22343;&#26041;&#35492;&#24046;&#22238;&#27512;&#21839;&#38988;</span>
<span class="linenr">12: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">13: </span>                loss=<span style="color: #98be65;">'mse'</span>)
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#33258;&#23450;&#32681;&#35413;&#20272;&#27161;&#28310;&#20989;&#25976;</span>
<span class="linenr">16: </span>  <span style="color: #51afef;">import</span> keras.backend <span style="color: #51afef;">as</span> K
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">mean_pred</span>(y_true, y_pred):
<span class="linenr">19: </span>      <span style="color: #51afef;">return</span> K.mean(y_pred)
<span class="linenr">20: </span>
<span class="linenr">21: </span>  model.<span style="color: #c678dd;">compile</span>(optimizer=<span style="color: #98be65;">'rmsprop'</span>,
<span class="linenr">22: </span>                loss=<span style="color: #98be65;">'binary_crossentropy'</span>,
<span class="linenr">23: </span>                metrics=[<span style="color: #98be65;">'accuracy'</span>, mean_pred])
<span class="linenr">24: </span>
</pre>
</div>
</div>
</li>

<li><a id="org01ec566"></a>開始訓練<br />
<div class="outline-text-6" id="text-10-4-3-3-2">
<p>
x,y 分別為輸入之訓練參數資料，split=0.2 表示該批資料的 80%作為訓練用、20%作為驗證用，共執行 10 次訓練週期、verbose=2 則表示要顯示訓練過程。
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">train_history</span> = model.fit(x=x_Train_normalize, y=y_Train_OneHot,
<span class="linenr">2: </span>                            validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
</pre>
</div>

<p>
fit 完整語法如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  fit(<span style="color: #51afef;">self</span>, x=<span style="color: #a9a1e1;">None</span>, y=<span style="color: #a9a1e1;">None</span>, batch_size=<span style="color: #a9a1e1;">None</span>,
<span class="linenr">2: </span>      epochs=<span style="color: #da8548; font-weight: bold;">1</span>, verbose=<span style="color: #da8548; font-weight: bold;">1</span>, callbacks=<span style="color: #a9a1e1;">None</span>,
<span class="linenr">3: </span>      validation_split=<span style="color: #da8548; font-weight: bold;">0.0</span>, validation_data=<span style="color: #a9a1e1;">None</span>,
<span class="linenr">4: </span>      shuffle=<span style="color: #a9a1e1;">True</span>, class_weight=<span style="color: #a9a1e1;">None</span>, sample_weight=<span style="color: #a9a1e1;">None</span>,
<span class="linenr">5: </span>      initial_epoch=<span style="color: #da8548; font-weight: bold;">0</span>, steps_per_epoch=<span style="color: #a9a1e1;">None</span>, validation_steps=<span style="color: #a9a1e1;">None</span>)
</pre>
</div>
<p>
對應參數分別為：<sup><a id="fnr.10" class="footref" href="#fn.10" role="doc-backlink">10</a></sup>
</p>
<ul class="org-ul">
<li>x：輸入數據。如果模型只有一個輸入，那麼 x 的類型是 numpy array，如果模型有多個輸入，那麼 x 的類型應當為 list，list 的元素是對應於各個輸入的 numpy array。如果模型的每個輸入都有名字，則可以傳入一個字典，將輸入名與其輸入數據對應起來。</li>
<li>y：標籤，numpy array。如果模型有多個輸出，可以傳入一個 numpy array 的 list。如果模型的輸出擁有名字，則可以傳入一個字典，將輸出名與其標籤對應起來。</li>
<li>batch_size：整數，指定進行梯度下降時每個 batch 包含的樣本數。訓練時一個 batch 的樣本會被計算一次梯度下降，使目標函數優化一步。</li>
<li>epochs：整數，訓練終止時的 epoch 值，訓練將在達到該 epoch 值時停止，當沒有設置 initial_epoch 時，它就是訓練的總輪數，否則訓練的總輪數為 epochs - inital_epoch</li>
<li>verbose：日誌顯示，0為不在標準輸出流輸出日誌信息，1為輸出進度條記錄，2為每個 epoch 輸出一行記錄</li>
<li>callbacks：list，其中的元素是 keras.callbacks.Callback 的對象。這個 list 中的回調函數將會在訓練過程中的適當時機被調用，參考回調函數</li>
<li>validation_split：0~1 之間的浮點數，用來指定訓練集的一定比例數據作為驗證集。驗證集將不參與訓練，並在每個 epoch 結束後測試的模型的指標，如損失函數、精確度等。注意，validation_split 的劃分在 shuffle 之後，因此如果你的數據本身是有序的，需要先手工打亂再指定 validation_split，否則可能會出現驗證集樣本不均勻。</li>
<li>validation_data：形式為（X，y）或（X，y，sample_weights）的 tuple，是指定的驗證集。此參數將覆蓋 validation_spilt。</li>
<li>shuffle：布爾值，表示是否在訓練過程中每個 epoch 前隨機打亂輸入樣本的順序。</li>
<li>class_weight：字典，將不同的類別映射為不同的權值，該參數用來在訓練過程中調整損失函數（只能用於訓練）。該參數在處理非平衡的訓練數據（某些類的訓練樣本數很少）時，可以使得損失函數對樣本數不足的數據更加關注。</li>
<li>sample_weight：權值的 numpy array，用於在訓練時調整損失函數（僅用於訓練）。可以傳遞一個 1D 的與樣本等長的向量用於對樣本進行 1 對 1 的加權，或者在面對時序數據時，傳遞一個的形式為（samples，sequence_length）的矩陣來為每個時間步上的樣本賦不同的權。這種情況下請確定在編譯模型時添加了 sample_weight_mode=&rsquo;temporal&rsquo;。</li>
<li>initial_epoch: 從該參數指定的 epoch 開始訓練，在繼續之前的訓練時有用。</li>
<li>steps_per_epoch: 一個 epoch 包含的步數（每一步是一個 batch 的數據送入），當使用如 TensorFlow 數據 Tensor 之類的輸入張量進行訓練時，預設的 None 代表自動分割，即數據集樣本數/batch 樣本數。</li>
<li>validation_steps: 僅當 steps_per_epoch 被指定時有用，在驗證集上的 step 總數。</li>
</ul>
</div>
</li>

<li><a id="org6adb42e"></a>建立、顯示訓練過程：show_train_history<br />
<div class="outline-text-6" id="text-10-4-3-3-3">
<p>
上述過程包括 accuracy 及 loss 均儲存於 train_history 變數中，可以下列程式碼將其轉變為圖表：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
<span class="linenr">3: </span>      plt.plot(train_history.history[train])
<span class="linenr">4: </span>      plt.plot(train_history.history[validation])
<span class="linenr">5: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">6: </span>      plt.ylabel(train)
<span class="linenr">7: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">8: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], toc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">9: </span>      plt.show()
</pre>
</div>
</div>
</li>

<li><a id="orgbb20534"></a>畫出 accuracy 執行結果<br />
<div class="outline-text-6" id="text-10-4-3-3-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  show_train_history(train_history,<span style="color: #98be65;">'train_acc'</span>,<span style="color: #98be65;">'val_acc'</span>)
</pre>
</div>
</div>
</li>

<li><a id="org16620a1"></a>完整執行結果<br />
<div class="outline-text-6" id="text-10-4-3-3-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> warnings
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  warnings.filterwarnings(<span style="color: #98be65;">'ignore'</span>)
<span class="linenr"> 5: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 7: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 9: </span>  (x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr">10: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">11: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">13: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">14: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">15: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
<span class="linenr">16: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">17: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">18: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">19: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">256</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">20: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">21: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">===&#36914;&#34892;&#35347;&#32244;===</span>
<span class="linenr">22: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">23: </span>  train_history = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
<span class="linenr">24: </span>                                validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">20</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">25: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">26: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
<span class="linenr">27: </span>      plt.plot(train_history.history[train])
<span class="linenr">28: </span>      plt.plot(train_history.history[validation])
<span class="linenr">29: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">30: </span>      plt.ylabel(train)
<span class="linenr">31: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">32: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">33: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
<span class="linenr">34: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
<span class="linenr">35: </span>      img = plt.plot()
<span class="linenr">36: </span>      <span style="color: #51afef;">return</span> img
<span class="linenr">37: </span>
<span class="linenr">38: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span class="linenr">39: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span class="linenr">40: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span class="linenr">41: </span>  img = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span class="linenr">42: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-1.png"</span>)
<span class="linenr">43: </span>  img = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
<span class="linenr">44: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-2.png"</span>) <span style="color: #5B6268;">#</span>
<span class="linenr">45: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span class="linenr">46: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">save</span>
<span class="linenr">47: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'test before save model: '</span>, model.predict(x_Test[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">5</span>]))
<span class="linenr">48: </span>
<span class="linenr">49: </span>  scores = model.evaluate(x_Test_normalize, y_TestOneHot)
<span class="linenr">50: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'accuracy'</span>,scores)
<span class="linenr">51: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'accuracy'</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">52: </span>
<span class="linenr">53: </span>  model.save(<span style="color: #98be65;">'Keras_MNist_model.h5'</span>)   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">HDF5 file, you have to pip3 install h5py if don't have it</span>
<span class="linenr">54: </span>  <span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example" id="org98e246f">
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 9s - loss: 0.2697 - accuracy: 0.9225 - val_loss: 0.1326 - val_accuracy: 0.9611
Epoch 2/20
 - 8s - loss: 0.1075 - accuracy: 0.9683 - val_loss: 0.1038 - val_accuracy: 0.9694
Epoch 3/20
 - 8s - loss: 0.0710 - accuracy: 0.9777 - val_loss: 0.0929 - val_accuracy: 0.9726
Epoch 4/20
 - 8s - loss: 0.0513 - accuracy: 0.9843 - val_loss: 0.0826 - val_accuracy: 0.9762
Epoch 5/20
 - 8s - loss: 0.0377 - accuracy: 0.9882 - val_loss: 0.0786 - val_accuracy: 0.9757
Epoch 6/20
 - 8s - loss: 0.0262 - accuracy: 0.9920 - val_loss: 0.0790 - val_accuracy: 0.9787
Epoch 7/20
 - 8s - loss: 0.0202 - accuracy: 0.9938 - val_loss: 0.0863 - val_accuracy: 0.9770
Epoch 8/20
 - 8s - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.0873 - val_accuracy: 0.9779
Epoch 9/20
 - 8s - loss: 0.0138 - accuracy: 0.9957 - val_loss: 0.0984 - val_accuracy: 0.9753
Epoch 10/20
 - 8s - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.1103 - val_accuracy: 0.9750
Epoch 11/20
 - 8s - loss: 0.0114 - accuracy: 0.9960 - val_loss: 0.0920 - val_accuracy: 0.9787
Epoch 12/20
 - 8s - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.1012 - val_accuracy: 0.9777
Epoch 13/20
 - 8s - loss: 0.0077 - accuracy: 0.9977 - val_loss: 0.1182 - val_accuracy: 0.9758
Epoch 14/20
 - 8s - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0993 - val_accuracy: 0.9799
Epoch 15/20
 - 8s - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.1104 - val_accuracy: 0.9786
Epoch 16/20
 - 8s - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.1036 - val_accuracy: 0.9797
Epoch 17/20
 - 8s - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.1288 - val_accuracy: 0.9767
Epoch 18/20
 - 8s - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.1279 - val_accuracy: 0.9773
Epoch 19/20
 - 9s - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.1226 - val_accuracy: 0.9763
Epoch 20/20
 - 9s - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.1365 - val_accuracy: 0.9772
test before save model:  [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]

   32/10000 [..............................] - ETA: 0s
 1664/10000 [===&gt;..........................] - ETA: 0s
 3168/10000 [========&gt;.....................] - ETA: 0s
 4832/10000 [=============&gt;................] - ETA: 0s
 6432/10000 [==================&gt;...........] - ETA: 0s
 8032/10000 [=======================&gt;......] - ETA: 0s
 9760/10000 [============================&gt;.] - ETA: 0s
10000/10000 [==============================] - 0s 31us/step
accuracy [0.11840327789002267, 0.9767000079154968]
accuracy 0.9767000079154968
</pre>


<div id="org5fbc2b1" class="figure">
<p><img src="images/Keras-MNist-Train-1.png" alt="Keras-MNist-Train-1.png" width="400" />
</p>
<p><span class="figure-number">Figure 69: </span>Keras Mnist Model 訓練#1: accuracy</p>
</div>

<p>
圖<a href="#org5fbc2b1">69</a>為執行 10 次週期後的預測精確度變化，可以看出以下現象：
</p>
<ol class="org-ol">
<li>訓練與驗證的精確率均隨訓練週期增加而提高</li>
<li>訓練精確度較驗證精確度高，原因是用來評估訓練精確率的資料已在訓練階段用過了；而用來評做驗證精確率的資料則否；這就類似，考試時考學過的練習題，學生得分較高。</li>
<li>驗證精確率雖然低，但較符合現實情況，即，考試時考學生沒有做過的題目。</li>
<li>如果訓練精確率持續增高，但驗證精率卻無法提升，可能是出現過度擬合(overfitting)的現象。,</li>
</ol>

<div id="org536487d" class="figure">
<p><img src="images/Keras-MNist-Train-2.png" alt="Keras-MNist-Train-2.png" width="400" />
</p>
<p><span class="figure-number">Figure 70: </span>Keras Mnist Model 訓練#1: lost function</p>
</div>

<p>
由圖<a href="#org536487d">70</a>亦可看出，訓練誤差與驗證誤差亦隨週期增加而降低，且訓練襄差最終低於驗證誤差。
</p>

<p>
訓練完成後，再以測試資料來評估模型準確率。
</p>
</div>
</li>
</ol>
</li>

<li><a id="org3ab8481"></a>進行預測<br />
<div class="outline-text-5" id="text-10-4-3-4">
<p>
模型在訓練、驗證、測試後，即可以此訓練完之模型進行預測，預測方式如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  (x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Define func</span>
<span class="linenr">10: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_images_labels_prediction</span>(imgname, images, labels, prediction, idx, num = <span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr">11: </span>        fig = plt.gcf()
<span class="linenr">12: </span>        fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">8</span>,<span style="color: #da8548; font-weight: bold;">4</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450;&#39023;&#31034;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr">13: </span>        <span style="color: #51afef;">if</span> num&gt;<span style="color: #da8548; font-weight: bold;">25</span>: num=<span style="color: #da8548; font-weight: bold;">25</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#36039;&#26009;&#31558;&#25976;&#19978;&#38480;</span>
<span class="linenr">14: </span>        <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, num):
<span class="linenr">15: </span>            ax=plt.subplot(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">1</span>+i) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27492;&#34389;&#39023;&#31034;10&#31558;&#22294;&#24418;&#65292;2*5&#20491;</span>
<span class="linenr">16: </span>            ax.imshow(images[idx], cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">17: </span>            title= <span style="color: #98be65;">"label="</span> +<span style="color: #c678dd;">str</span>(labels[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21152;&#20837;&#23376;&#22294;&#24418;title</span>
<span class="linenr">18: </span>            <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(prediction)&gt;<span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">19: </span>                title+=<span style="color: #98be65;">",predict="</span>+<span style="color: #c678dd;">str</span>(prediction[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27161;&#38988;title&#21152;&#20837;&#38928;&#28204;&#32080;&#26524;</span>
<span class="linenr">20: </span>            ax.set_title(title,fontsize=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">21: </span>            ax.set_xticks([]);ax.set_yticks([]) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#19981;&#39023;&#31034;&#21051;&#24230;</span>
<span class="linenr">22: </span>            idx+=<span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">23: </span>        <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">24: </span>        <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.plot()</span>
<span class="linenr">25: </span>        plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">26: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36617;&#20837;&#20786;&#23384;&#20043;&#27169;&#22411;</span>
<span class="linenr">27: </span>  model = load_model(<span style="color: #98be65;">'Keras_MNist_model.h5'</span>)
<span class="linenr">28: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;340-349&#20849;10&#31558;&#36039;&#26009;</span>
<span class="linenr">29: </span>  prediction = model.predict_classes(x_Test)
<span class="linenr">30: </span>  plot_images_labels_prediction(<span style="color: #98be65;">'Keras-MNist-Train-3'</span>,x_test_image, y_test_label,
<span class="linenr">31: </span>                                prediction, idx=<span style="color: #da8548; font-weight: bold;">340</span>)
<span class="linenr">32: </span>
</pre>
</div>
<p width="400">
<img src="images/Keras-MNist-Train-3.png" alt="Keras-MNist-Train-3.png" width="400" />
上述程式碼將訓練好後儲存的模型取出，拿 10 筆記錄去預測，發現第一筆有誤（真實值 label 為 5、預測值為 3）。
</p>
</div>
</li>

<li><a id="org2a73ae8"></a>顯示混淆矩陣(confusion matrix)<br />
<div class="outline-text-5" id="text-10-4-3-5">
<p>
若想進一步得知哪些數字容易被混淆，可以使用混淆矩陣(confustion matrix, 也稱為誤差矩陣 error matrix)。實務上可以利用 pandas crosstab 來建立，程式碼如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  (x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'Keras_MNist_model.h5'</span>)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">prediction</span> = model.predict_classes(x_Test)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">15: </span>  <span style="color: #dcaeea;">confuse</span> = pd.crosstab(y_test_label, prediction, rownames=[<span style="color: #98be65;">'label'</span>], colnames=[<span style="color: #98be65;">'predict'</span>])
<span class="linenr">16: </span>  <span style="color: #c678dd;">print</span>(confuse)
</pre>
</div>

<pre class="example" id="org02bce68">
  predict    0     1     2    3    4    5    6     7    8    9
  label
  0        972     1     2    0    0    0    2     1    2    0
  1          0  1126     3    1    0    0    2     1    2    0
  2          7     1  1005    4    2    0    2     7    4    0
  3          1     0     1  999    0    3    0     3    2    1
  4          2     0     3    1  958    1    3     3    1   10
  5          5     0     1   29    1  842    5     3    5    1
  6          7     3     1    1    3    2  941     0    0    0
  7          2     3     5    3    1    0    0  1009    1    4
  8         10     0     4   10    2    1    3     5  938    1
  9          5     5     0    4    9    0    0    11    0  975
</pre>

<p>
由輸出結果可以得知，5很容易被誤判為 3(共 29 次)。若想進一步得知到底有那幾張圖為上述狀況，則可以加入限制條件，程式碼如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> load_model
<span class="linenr"> 5: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>  (x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">model</span> = load_model(<span style="color: #98be65;">'Keras_MNist_model.h5'</span>)
<span class="linenr">11: </span>  <span style="color: #dcaeea;">prediction</span> = model.predict_classes(x_Test)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">df</span> = pd.DataFrame({<span style="color: #98be65;">'label'</span>: y_test_label, <span style="color: #98be65;">'predict'</span>:prediction})
<span class="linenr">13: </span>  <span style="color: #c678dd;">print</span>(df[(df.label==<span style="color: #da8548; font-weight: bold;">5</span>)&amp;(df.predict==<span style="color: #da8548; font-weight: bold;">3</span>)])
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_image</span>(imgname, image):
<span class="linenr">16: </span>      fig = plt.gcf() <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23450;&#35373;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr">17: </span>      fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">18: </span>      plt.imshow(image, cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">cmap&#21443;&#25976;&#35373;&#23450;&#28858;binary&#20197;&#40657;&#30333;&#28784;&#38542;&#39023;&#31034;</span>
<span class="linenr">19: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() #for jupyter or colab</span>
<span class="linenr">20: </span>      plt.plot()
<span class="linenr">21: </span>      plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">22: </span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#31532;340&#31558;&#36039;&#26009;</span>
<span class="linenr">24: </span>  plot_image(<span style="color: #98be65;">'Keras-MNist-Train-4'</span>, x_test_image[<span style="color: #da8548; font-weight: bold;">340</span>])
<span class="linenr">25: </span>
</pre>
</div>

<pre class="example" id="org31a3cf4">
      label  predict
340       5        3
1003      5        3
1082      5        3
1393      5        3
2035      5        3
2291      5        3
2369      5        3
2526      5        3
2597      5        3
2604      5        3
2810      5        3
2970      5        3
3117      5        3
3414      5        3
4255      5        3
4271      5        3
4360      5        3
5874      5        3
5891      5        3
5913      5        3
5937      5        3
5972      5        3
5982      5        3
5985      5        3
6028      5        3
6042      5        3
6043      5        3
6598      5        3
9982      5        3
</pre>

<div id="orgad23b52" class="figure">
<p><img src="images/Keras-MNist-Train-4.png" alt="Keras-MNist-Train-4.png" width="400" />
</p>
<p><span class="figure-number">Figure 71: </span>Keras Mnist Model 訓練#1: error sample</p>
</div>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orge1c4529" class="outline-3">
<h3 id="orge1c4529"><span class="section-number-3">10.5.</span> 強化 MLP 辨識 solution #1: 增加隠藏層神經元數</h3>
<div class="outline-text-3" id="text-10-5">
<p>
為了增加 MLP 的準確率，其中一種方法可以增加隠藏層的神經元數至 1000 個，程式碼如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 3: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 5: </span>  (x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">11: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
<span class="linenr">12: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">13: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">14: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">15: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31070;&#32147;&#20803;&#25976;</span>
<span class="linenr">16: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">17: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">18: </span>  <span style="color: #c678dd;">print</span>(model.summary())
<span class="linenr">19: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#26597;&#30475;&#35347;&#32244;&#36942;&#31243;&#21450;&#32080;&#26524;</span>
<span class="linenr">20: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">21: </span>  train_history = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
<span class="linenr">22: </span>                                validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">23: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">24: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
<span class="linenr">25: </span>      plt.plot(train_history.history[train])
<span class="linenr">26: </span>      plt.plot(train_history.history[validation])
<span class="linenr">27: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">28: </span>      plt.ylabel(train)
<span class="linenr">29: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">30: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">31: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
<span class="linenr">32: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
<span class="linenr">33: </span>      img = plt.plot()
<span class="linenr">34: </span>      <span style="color: #51afef;">return</span> img
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span class="linenr">37: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span class="linenr">38: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span class="linenr">39: </span>  img = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span class="linenr">40: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-5.png"</span>)
<span class="linenr">41: </span>  img = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
<span class="linenr">42: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-6.png"</span>) <span style="color: #5B6268;">#</span>
<span class="linenr">43: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span class="linenr">44: </span>  scores = model.evaluate(x_Test_normalize, y_TestOneHot)
<span class="linenr">45: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'accuracy'</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">46: </span>
<span class="linenr">47: </span>  model.save(<span style="color: #98be65;">'Keras_MNist_model-1.h5'</span>)
<span class="linenr">48: </span>  <span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example" id="org08e9714">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 1000)              785000
_________________________________________________________________
dense_2 (Dense)              (None, 10)                10010
=================================================================
Total params: 795,010
Trainable params: 795,010
Non-trainable params: 0
_________________________________________________________________
None
Train on 48000 samples, validate on 12000 samples
Epoch 1/10
 - 22s - loss: 0.2047 - accuracy: 0.9385 - val_loss: 0.1026 - val_accuracy: 0.9691
Epoch 2/10
 - 22s - loss: 0.0797 - accuracy: 0.9754 - val_loss: 0.0866 - val_accuracy: 0.9734
Epoch 3/10
 - 23s - loss: 0.0504 - accuracy: 0.9833 - val_loss: 0.0917 - val_accuracy: 0.9737
Epoch 4/10
 - 22s - loss: 0.0356 - accuracy: 0.9888 - val_loss: 0.0820 - val_accuracy: 0.9772
Epoch 5/10
 - 23s - loss: 0.0259 - accuracy: 0.9914 - val_loss: 0.0763 - val_accuracy: 0.9785
Epoch 6/10
 - 22s - loss: 0.0201 - accuracy: 0.9931 - val_loss: 0.0898 - val_accuracy: 0.9792
Epoch 7/10
 - 23s - loss: 0.0171 - accuracy: 0.9942 - val_loss: 0.0934 - val_accuracy: 0.9789
Epoch 8/10
 - 22s - loss: 0.0148 - accuracy: 0.9950 - val_loss: 0.0965 - val_accuracy: 0.9790
Epoch 9/10
 - 22s - loss: 0.0148 - accuracy: 0.9951 - val_loss: 0.1181 - val_accuracy: 0.9771
Epoch 10/10
 - 22s - loss: 0.0117 - accuracy: 0.9961 - val_loss: 0.1280 - val_accuracy: 0.9741

   32/10000 [..............................] - ETA: 0s
 1120/10000 [==&gt;...........................] - ETA: 0s
 2112/10000 [=====&gt;........................] - ETA: 0s
 3008/10000 [========&gt;.....................] - ETA: 0s
 4000/10000 [===========&gt;..................] - ETA: 0s
 5152/10000 [==============&gt;...............] - ETA: 0s
 6208/10000 [=================&gt;............] - ETA: 0s
 7296/10000 [====================&gt;.........] - ETA: 0s
 8448/10000 [========================&gt;.....] - ETA: 0s
 9568/10000 [===========================&gt;..] - ETA: 0s
10000/10000 [==============================] - 0s 48us/step
accuracy 0.9760000109672546
</pre>


<div id="org84eedb9" class="figure">
<p><img src="images/Keras-MNist-Train-5.png" alt="Keras-MNist-Train-5.png" width="400" />
</p>
<p><span class="figure-number">Figure 72: </span>Keras Mnist Model 訓練#2: accuracy</p>
</div>

<div id="org0e40bfa" class="figure">
<p><img src="images/Keras-MNist-Train-6.png" alt="Keras-MNist-Train-6.png" width="400" />
</p>
<p><span class="figure-number">Figure 73: </span>Keras Mnist Model 訓練#2: loss information</p>
</div>

<p>
將神經元數增加至 1000 後，精確率由 0.97 提升至 0.98。但測驗準確率仍未提升，可見 overfitting 仍然嚴重。
</p>
</div>
</div>

<div id="outline-container-org15b9c2c" class="outline-3">
<h3 id="org15b9c2c"><span class="section-number-3">10.6.</span> 強化 MLP 辨識 solution #2: 加入 DropOut 以避免 overfitting</h3>
<div class="outline-text-3" id="text-10-6">
<p>
為解決 Overfitting 問題，此處再加入 Dropout(0.5)指令，其功能為在每次訓練迭代時，會隨機地在隱藏層中放棄 50%的神經元，以避免 overfitting，程式碼如下：
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 5: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 6: </span>  (x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">15: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">16: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">17: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31070;&#32147;&#20803;&#25976;</span>
<span class="linenr">18: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">19: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;Dropout</span>
<span class="linenr">20: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dropout
<span class="linenr">21: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">22: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;&#36664;&#20986;&#23652;</span>
<span class="linenr">23: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">24: </span>  <span style="color: #c678dd;">print</span>(model.summary())
<span class="linenr">25: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#35347;&#32244;&#27169;&#22411;&#12289;&#26597;&#30475;&#32080;&#26524;</span>
<span class="linenr">26: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">27: </span>  train_history = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
<span class="linenr">28: </span>                                validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">29: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">30: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
<span class="linenr">31: </span>      plt.plot(train_history.history[train])
<span class="linenr">32: </span>      plt.plot(train_history.history[validation])
<span class="linenr">33: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">34: </span>      plt.ylabel(train)
<span class="linenr">35: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">36: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">37: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
<span class="linenr">38: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
<span class="linenr">39: </span>      img = plt.plot()
<span class="linenr">40: </span>      <span style="color: #51afef;">return</span> img
<span class="linenr">41: </span>
<span class="linenr">42: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span class="linenr">43: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span class="linenr">44: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span class="linenr">45: </span>  img = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span class="linenr">46: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-7.png"</span>)
<span class="linenr">47: </span>  img = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
<span class="linenr">48: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-8.png"</span>) <span style="color: #5B6268;">#</span>
<span class="linenr">49: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span class="linenr">50: </span>  scores = model.evaluate(x_Test_normalize, y_TestOneHot)
<span class="linenr">51: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'accuracy='</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">52: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr">53: </span>  prediction=model.predict_classes(x_Test)
<span class="linenr">54: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;confuse mqtrix</span>
<span class="linenr">55: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">56: </span>  pd.crosstab(y_test_label,prediction,
<span class="linenr">57: </span>              rownames=[<span style="color: #98be65;">'label'</span>],colnames=[<span style="color: #98be65;">'predict'</span>])
<span class="linenr">58: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20786;&#23384;&#35347;&#32244;&#22909;&#30340;&#27169;&#24335;</span>
<span class="linenr">59: </span>  model.save(<span style="color: #98be65;">'Keras_MNist_model-2.h5'</span>)
<span class="linenr">60: </span>  <span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example" id="orgae2e796">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 1000)              785000
_________________________________________________________________
dropout_1 (Dropout)          (None, 1000)              0
_________________________________________________________________
dense_2 (Dense)              (None, 10)                10010
=================================================================
Total params: 795,010
Trainable params: 795,010
Non-trainable params: 0
_________________________________________________________________
None
Train on 48000 samples, validate on 12000 samples
Epoch 1/10
 - 25s - loss: 0.2673 - accuracy: 0.9171 - val_loss: 0.1145 - val_accuracy: 0.9659
Epoch 2/10
 - 25s - loss: 0.1319 - accuracy: 0.9581 - val_loss: 0.1046 - val_accuracy: 0.9689
Epoch 3/10
 - 26s - loss: 0.1008 - accuracy: 0.9686 - val_loss: 0.1004 - val_accuracy: 0.9690
Epoch 4/10
 - 26s - loss: 0.0884 - accuracy: 0.9719 - val_loss: 0.0826 - val_accuracy: 0.9763
Epoch 5/10
 - 26s - loss: 0.0764 - accuracy: 0.9752 - val_loss: 0.0858 - val_accuracy: 0.9763
Epoch 6/10
 - 25s - loss: 0.0703 - accuracy: 0.9779 - val_loss: 0.0826 - val_accuracy: 0.9779
Epoch 7/10
 - 25s - loss: 0.0617 - accuracy: 0.9811 - val_loss: 0.0849 - val_accuracy: 0.9772
Epoch 8/10
 - 25s - loss: 0.0597 - accuracy: 0.9817 - val_loss: 0.0816 - val_accuracy: 0.9795
Epoch 9/10
 - 26s - loss: 0.0529 - accuracy: 0.9829 - val_loss: 0.0878 - val_accuracy: 0.9787
Epoch 10/10
 - 28s - loss: 0.0516 - accuracy: 0.9838 - val_loss: 0.0833 - val_accuracy: 0.9805

   32/10000 [..............................] - ETA: 0s
 1088/10000 [==&gt;...........................] - ETA: 0s
 2016/10000 [=====&gt;........................] - ETA: 0s
 2848/10000 [=======&gt;......................] - ETA: 0s
 3904/10000 [==========&gt;...................] - ETA: 0s
 5024/10000 [==============&gt;...............] - ETA: 0s
 6144/10000 [=================&gt;............] - ETA: 0s
 7264/10000 [====================&gt;.........] - ETA: 0s
 8384/10000 [========================&gt;.....] - ETA: 0s
 9472/10000 [===========================&gt;..] - ETA: 0s
10000/10000 [==============================] - 0s 48us/step
accuracy= 0.9815000295639038
</pre>

<p>
由訓練過程可以看出，驗證精確率(0.9805)已接近訓練精確率(0.9838)，可見已改善了 overfitting 問題。
</p>


<div id="orgd473563" class="figure">
<p><img src="images/Keras-MNist-Train-7.png" alt="Keras-MNist-Train-7.png" width="400" />
</p>
<p><span class="figure-number">Figure 74: </span>Keras Mnist Model 訓練#3: accuracy</p>
</div>

<div id="org8126598" class="figure">
<p><img src="images/Keras-MNist-Train-8.png" alt="Keras-MNist-Train-8.png" width="400" />
</p>
<p><span class="figure-number">Figure 75: </span>Keras Mnist Model 訓練#3: loss information</p>
</div>

<p>
由圖<a href="#orgd473563">74</a>也可看出，驗證精確率已隨訓練週期提高，驗證誤差也隨訓練週期降低。
</p>
</div>
</div>

<div id="outline-container-org64f7ca3" class="outline-3">
<h3 id="org64f7ca3"><span class="section-number-3">10.7.</span> 強化 MLP 辨識 solution #3: 增加隱藏層層數</h3>
<div class="outline-text-3" id="text-10-7">
<p>
本例 MLP 模型的預測能力仍有改善空間：增加隠藏層層數。以下程式碼將隠藏層數提高至 2 層：
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25903;&#25588;&#32173;&#24230;&#38499;&#21015;&#20043;&#30697;&#38499;&#36939;&#31639;</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 5: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 6: </span>  (x_train_image, y_train_label), (<span style="color: #dcaeea;">x_test_image</span>, <span style="color: #dcaeea;">y_test_label</span>) = mnist.load_data()
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">x_Train</span> = x_train_image.reshape(<span style="color: #da8548; font-weight: bold;">60000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">x_Test</span> = x_test_image.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_Train_normalize</span> = x_Train/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">x_Test_normalize</span> = x_Test/ <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_train_label)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_test_label)
<span class="linenr">13: </span>
<span class="linenr">14: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">15: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense
<span class="linenr">16: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dropout
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">19: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31532;1&#23652;&#38577;&#34255;&#23652;&#31070;&#32147;&#20803;&#25976;</span>
<span class="linenr">20: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, input_dim=<span style="color: #da8548; font-weight: bold;">784</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">21: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;Dropout</span>
<span class="linenr">22: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">23: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#22686;&#21152;&#31532;2&#23652;&#38577;&#34255;&#23652;</span>
<span class="linenr">24: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">1000</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">25: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;&#31532;2&#23652;Dropout</span>
<span class="linenr">26: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">27: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#21152;&#20837;&#36664;&#20986;&#23652;</span>
<span class="linenr">28: </span>  model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>, kernel_initializer=<span style="color: #98be65;">'normal'</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">29: </span>  <span style="color: #c678dd;">print</span>(model.summary())
<span class="linenr">30: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====&#35347;&#32244;&#27169;&#22411;&#12289;&#26597;&#30475;&#32080;&#26524;</span>
<span class="linenr">31: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>, optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">32: </span>  train_history = model.fit(x=x_Train_normalize, y=y_TrainOneHot,
<span class="linenr">33: </span>                                validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>, epochs=<span style="color: #da8548; font-weight: bold;">10</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">34: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">35: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_history, train, validation):
<span class="linenr">36: </span>      plt.plot(train_history.history[train])
<span class="linenr">37: </span>      plt.plot(train_history.history[validation])
<span class="linenr">38: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">39: </span>      plt.ylabel(train)
<span class="linenr">40: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">41: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'validation'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">42: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show() # for jupyter notebook</span>
<span class="linenr">43: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#20462;&#25913;for console run</span>
<span class="linenr">44: </span>      img = plt.plot()
<span class="linenr">45: </span>      <span style="color: #51afef;">return</span> img
<span class="linenr">46: </span>
<span class="linenr">47: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;acc</span>
<span class="linenr">48: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#19979;&#30340;val_accuracy&#22312;Linux/Windows&#19979;&#35201;&#25913;&#28858;val_acc</span>
<span class="linenr">49: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">show_train_history(train_history, 'accuracy', 'val_accuracy')</span>
<span class="linenr">50: </span>  img = show_train_history(train_history, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span class="linenr">51: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-9.png"</span>)
<span class="linenr">52: </span>  img = show_train_history(train_history, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
<span class="linenr">53: </span>  plt.savefig(<span style="color: #98be65;">"Keras-MNist-Train-a.png"</span>) <span style="color: #5B6268;">#</span>
<span class="linenr">54: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20197;&#28204;&#35430;&#36039;&#26009;&#35413;&#20272;&#31934;&#30906;&#29575;</span>
<span class="linenr">55: </span>  scores = model.evaluate(x_Test_normalize, y_TestOneHot)
<span class="linenr">56: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'accuracy='</span>,scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">57: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr">58: </span>  prediction=model.predict_classes(x_Test)
<span class="linenr">59: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;confuse mqtrix</span>
<span class="linenr">60: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">61: </span>  pd.crosstab(y_test_label,prediction,
<span class="linenr">62: </span>              rownames=[<span style="color: #98be65;">'label'</span>],colnames=[<span style="color: #98be65;">'predict'</span>])
<span class="linenr">63: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20786;&#23384;&#35347;&#32244;&#22909;&#30340;&#27169;&#24335;</span>
<span class="linenr">64: </span>  model.save(<span style="color: #98be65;">'Keras_MNist_model-3.h5'</span>)
<span class="linenr">65: </span>  <span style="color: #51afef;">del</span> model  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">deletes the existing model</span>
</pre>
</div>

<pre class="example" id="org293d473">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 1000)              785000
_________________________________________________________________
dropout_1 (Dropout)          (None, 1000)              0
_________________________________________________________________
dense_2 (Dense)              (None, 1000)              1001000
_________________________________________________________________
dropout_2 (Dropout)          (None, 1000)              0
_________________________________________________________________
dense_3 (Dense)              (None, 10)                10010
=================================================================
Total params: 1,796,010
Trainable params: 1,796,010
Non-trainable params: 0
_________________________________________________________________
None
Train on 48000 samples, validate on 12000 samples
Epoch 1/10
 - 51s - loss: 0.3197 - accuracy: 0.9040 - val_loss: 0.1234 - val_accuracy: 0.9613
Epoch 2/10
 - 53s - loss: 0.1753 - accuracy: 0.9463 - val_loss: 0.1277 - val_accuracy: 0.9635
Epoch 3/10
 - 50s - loss: 0.1493 - accuracy: 0.9557 - val_loss: 0.0965 - val_accuracy: 0.9725
Epoch 4/10
 - 49s - loss: 0.1321 - accuracy: 0.9616 - val_loss: 0.1069 - val_accuracy: 0.9693
Epoch 5/10
 - 50s - loss: 0.1265 - accuracy: 0.9632 - val_loss: 0.1008 - val_accuracy: 0.9714
Epoch 6/10
 - 50s - loss: 0.1204 - accuracy: 0.9658 - val_loss: 0.0802 - val_accuracy: 0.9772
Epoch 7/10
 - 50s - loss: 0.1086 - accuracy: 0.9681 - val_loss: 0.0860 - val_accuracy: 0.9768
Epoch 8/10
 - 50s - loss: 0.1018 - accuracy: 0.9709 - val_loss: 0.0904 - val_accuracy: 0.9755
Epoch 9/10
 - 50s - loss: 0.1055 - accuracy: 0.9708 - val_loss: 0.0878 - val_accuracy: 0.9768
Epoch 10/10
 - 50s - loss: 0.0973 - accuracy: 0.9739 - val_loss: 0.0886 - val_accuracy: 0.9771

   32/10000 [..............................] - ETA: 0s
  768/10000 [=&gt;............................] - ETA: 0s
 1472/10000 [===&gt;..........................] - ETA: 0s
 2016/10000 [=====&gt;........................] - ETA: 0s
 2656/10000 [======&gt;.......................] - ETA: 0s
 3360/10000 [=========&gt;....................] - ETA: 0s
 4032/10000 [===========&gt;..................] - ETA: 0s
 4736/10000 [=============&gt;................] - ETA: 0s
 5472/10000 [===============&gt;..............] - ETA: 0s
 6176/10000 [=================&gt;............] - ETA: 0s
 6912/10000 [===================&gt;..........] - ETA: 0s
 7648/10000 [=====================&gt;........] - ETA: 0s
 8384/10000 [========================&gt;.....] - ETA: 0s
 9120/10000 [==========================&gt;...] - ETA: 0s
 9856/10000 [============================&gt;.] - ETA: 0s
10000/10000 [==============================] - 1s 73us/step
accuracy= 0.9793999791145325
</pre>


<div id="orge6cb169" class="figure">
<p><img src="images/Keras-MNist-Train-9.png" alt="Keras-MNist-Train-9.png" width="400" />
</p>
<p><span class="figure-number">Figure 76: </span>Keras Mnist Model 訓練#3: accuracy</p>
</div>

<div id="org5e261d7" class="figure">
<p><img src="images/Keras-MNist-Train-a.png" alt="Keras-MNist-Train-a.png" width="400" />
</p>
<p><span class="figure-number">Figure 77: </span>Keras Mnist Model 訓練#3: loss information</p>
</div>

<p>
由訓練成果判斷，雖然精確率並未提升，但是由圖<a href="#orge6cb169">76</a>可以看出驗證精確率已經比訓練精確率高，已確實可以解決 overfitting 的問題。
</p>

<p>
隨著 MLP 模型的改進，雖然精確率可逐步提升，也可藉由加入 Dropout 解決 overfitting 的問題，但 MLP 仍有其極限，如果要進一步提升準確率，就要使用卷積神經網路 CNN (convolutional neural network)。
</p>
</div>
</div>
</div>

<div id="outline-container-orgbf30f89" class="outline-2">
<h2 id="orgbf30f89"><span class="section-number-2">11.</span> 深度學習的高速化</h2>
<div class="outline-text-2" id="text-11">
<p>
由於大數據與大型網路的關係，使得深度學習必須進行大量運算，過去我們使用 CPU 來進行運算，如今多數深度學習的框架多支援 GPU，甚至支援以多個 GPU 與多台裝置進行分散式學習。GPU 原本是圖形專用處理器，可以快速處理平行運算，GPU 運算的目標是把其強大的效能運用在各種用途。比較 CPU 與 GPU 在 AlexNet 的學習，CPU 需花費 40 天以上，GPU 則可以在 6 天內完成。
</p>

<p>
利用 GPU 除了可以大幅提升深度學習的運算速度，但是一旦變成多層網路時，就需要花費數天或數週的時間來學習，Google 的 TensorFlow、Microsoft 的 CNTK 便是針對分散式學習來開發的，100 個分散式的 GPU 可以提升比單一 GPU 高到 56 倍的速度，意味著原本要有天才能完成的學習，只要 3 小時就可以結束。
</p>

<p>
在深度學習的高速化過程中，包含運算量在內，記憶體容量、匯流排頻寬等，都會造成瓶頸，就記憶體容量來說，必須考慮到大量權重參數及中間資料會儲存在記憶體的情況。至於匯流排頻寛，一旦通過 GPU(或 CPU)的匯流排資料超過一定的限制，該處就會形成瓶頸，所以，最好能儘量減少通過網路的資料位元數。
</p>
</div>

<div id="outline-container-org039b8cb" class="outline-3">
<h3 id="org039b8cb"><span class="section-number-3">11.1.</span> GPU v.s. CPU</h3>
<div class="outline-text-3" id="text-11-1">
<ul class="org-ul">
<li>CPU 是由幾個每次可處理數個獨立「執行緒」(threads)的核心(core)所組成；GPU 則有數百個這樣的核心，同時可以處理上千個執行緒</li>
<li>CPU 主要是線性執行； GPU 則是個高度平行化的單元</li>
<li>CPU 的發展主要致力於最佳化系統的遲滯時間，讓系統能有迅速流暢的反應；GPU 的發展則是朝頻寬最佳化努力。在深度神經網路中，頻寬為主要的系統瓶頸</li>
<li>GPU 的 Level 1 cache 比 CPU 快且大，在深度神經網路中，大部份的資料都會再次被使用到</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org4725c69" class="outline-2">
<h2 id="org4725c69"><span class="section-number-2">12.</span> 深度學習的未來方向</h2>
<div class="outline-text-2" id="text-12">
<p>
對於大多數深度學習實作者，推動深度學習的最佳途徑並不是創造出更高級的最佳化演算法，相反的，過去幾十年來絕大多數深度學習的突破，都是因為發現了更容易訓練的架構，而不是因為與那些討厭的誤差曲面搏鬥所得到的成果。
</p>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://kknews.cc/zh-tw/tech/b4zkbom.html">主流的深度學習模型有哪些？</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://morvanzhou.github.io/tutorials/machine-learning/ML-intro/3-09-l1l2regularization/">L1 / L2 正規化</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://ithelp.ithome.com.tw/articles/10219648?sc=rss.iron">Google ML課程筆記 - Overfitting 與 L1 /L2 Regularization </a>
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.itread01.com/content/1549579879.html">機器學習十大演算法&#x2014;8. 隨機森林演算法</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Goodfello, Ian J., Oriol Vinyals &amp; Andrew M. Saxe, Qualitatively characterizing neural ntwork optimization problems, arXiv preprint arXiv: 1412.6544 (2014).
</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E7%B5%B1%E8%A8%88%E5%AD%B8%E7%BF%92-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-principle-component-analysis-pca-58229cd26e71">機器/統計學習:主成分分析(Principal Component Analysis, PCA)</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7" role="doc-backlink">7</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://blog.csdn.net/dongyanwen6036/article/details/78311071">LDA與PCA都是常用的降維方法，二者的區別</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.8" class="footnum" href="#fnr.8" role="doc-backlink">8</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://medium.com/yiyi-network/transfer-learning-1f87d4f1886f">Kaggle Learn | Deep Learning 深度學習 | 學習資源介紹 (Part 2)</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.9" class="footnum" href="#fnr.9" role="doc-backlink">9</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://keras.io/zh/getting-started/sequential-model-guide/">Sequential 順序模型指引</a>
</p></div></div>

<div class="footdef"><sup><a id="fn.10" class="footnum" href="#fnr.10" role="doc-backlink">10</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://keras-cn.readthedocs.io/en/latest/models/model/">函數式模型接口</a>
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2022-07-05 Tue 09:42</p>
</div>
</body>
</html>
