:PROPERTIES:
:ID:       2df41f8a-38d5-44f5-8775-624aea317233
:END:
#+TITLE: AI Introduction
#+INCLUDE: ../pdf.org
#+TAGS: AI
#+OPTIONS: toc:2 ^:nil num:5
#+PROPERTY: header-args :eval never-export
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="../css/muse.css" />
#+EXCLUDE_TAGS: noexport
#+latex:\newpage


* 和AI聊聊天
** ELIZA
- [[https://en.wikipedia.org/wiki/ELIZA#cite_note-turing-1][What is it]]
- [[https://web.njit.edu/~ronkowit/eliza.html][web-based version]]
** ALICE
- [[https://en.wikipedia.org/wiki/Artificial_Linguistic_Internet_Computer_Entity][About ALICE]]
- [[http://www.mfellmann.net/content/alice.html][web-based version]]
** Mitsuku
- [[https://en.wikipedia.org/wiki/Mitsuku][About Mitsuku]]
- [[https://chat.kuki.ai/][Try it]]

* AI, [[file:MachineLearning.org][Machine Learning]]與[[file:DeepLearning.org][Deep Learning]]
人工智慧、機器學習與深度學習是三個常被混為一談的概念，如圖[[fig:AMD]]，深度學習是機器學習的一種類型，而機器學習又是人工智慧的一個分支，相較於機器學習，早期實作人工智慧的一種策略是專家系統(Expert System)。
#+BEGIN_SRC ditaa :file images/AMD.png
+-------------------------------------+
| Artificial Intelligence             |
| +-----------------------+           |
| | Machine Learning      |           |
| | +-- ------------+     |           |
| | | Deep Learning |     |           |
| | +---------------+     |           |
| +-----------------------+           |
| +-------------------------------+   |
| | Traditional Machine Learning  |   |
| | +---------------+             |   |
| | | Expert System |             |   |
| | +---------------+             |   |
| +-------------------------------+   |
+-------------------------------------+
#+END_SRC
#+CAPTION: AI, Machine, Deep Learning
#+name: fig:AMD
#+ATTR_HTML: :width 500
#+ATTR_LATEX: :width 600
#+ATTR_ORG: :width 600
[[file:images/AMD.png]]

** AI與早期專家系統的差異
- 專家系統:由人訂規則，告訴電腦判別的方法：狗鼻子較長、耳朵較大...
- [[file:MachineLearning.org][機器學習]]:給電腦大量標註貓狗的照片，由[[file:MachineLearning.org][機器學習]]演算法自行歸納辨別二者的方法。

** AI
AI是一個涵蓋面極廣的名詞，從1964年[[https://en.wikipedia.org/wiki/MIT_Computer_Science_and_Artificial_Intelligence_Laboratory][MIT AI Lab]]的[[https://web.njit.edu/~ronkowit/eliza.html][ELIZA]]對話機器人，到最近的自駕車，再到科幻電影中俱備人類情感的機器人都可以是AI的範圍。在實作上，AI 可以是簡單的 decision tree 或 rule-based 的專家系統(知識庫 + 推理機制)，也可以是包含數十億神經元的類神經網路。那麼，這和我們常聽到的機器學習、深度學習、神經網絡又有什麼關係呢？

** 機器學習
在AI的發展中，人們想過以各種方式來達成讓機器具備人類智慧的目的，有人希望能將大量的人類智慧教給電腦，這部份包含了人類在各領域的知識以及推理規則；另一派學者則認為人類的智識大過於廣泛而且不斷的有新知識生成，與其把所有的知識教給電腦，不如讓電腦具備學習的能力，如此電腦就可以自己去學習新的知識，這便是所謂的機器學習。

在開發機器學習模型時，我們會基於觀測值計算出一些衍生變數(derived variables)，再將其加入決策判斷的條件中，以增加 model 的預測準確度。例如，由男生的身高體重判斷高血壓的機率，而 BMI 即為一更佳的衍生變數。而[[file:MachineLearning.org][機器學習]]模型的成效往往取決於特徵工程的品質，但在某些領域下，特徵工程很難靠領域專家取得好的結果，例如非結構化資料以及序列資料：
- 非結構化資料：聲音、影像、影片
- 序列資料：sensor 資料、金融市場資料、交易資料

機器學習有各種不同的實作策略（演算法），而類神經網路就是其中之一。

** 類神經網路與深度學習
如何讓電腦俱備學習能力？在實作上也有多不同策略，類神經網路就是希望藉由模擬人類腦神經結構的方式來達到這個目的的一種方式，Hinton 於 2006 年提出的 Boltzmann Machine 為一種多層神經網路。典型的類神經網路架構(如圖[[fig:NN]])由輸入層、隱藏層、輸出層組成，學術界稱層數大於3的類神經網路為深度學習。
#+CAPTION: 類神經網路架構
#+LABEL:fig:NN
#+name: fig:NN
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 500
[[file:images/ANN-640x314.jpeg]]

所以，當你聽到深度學習這個名詞時，有兩件事是可以確定的：
1. 這一定是機器學習
2. 這一定是類神經網路

前面提到 *在某些領域下，特徵工程很難靠領域專家取得好的結果* ，深度學習的強大之處就在於深度學習連特徵工程也可以自行完成，即，由原始資中自行產生衍生變數。

** 深度學習
#+begin_verse
深度學習與其他機器學習方式最主要的差異在於能否自動進行「特徵工程」(feature engineering)
#+end_verse

考慮採取傳統[[file:MachineLearning.org][機器學習]]或深度學習時，一個重要關鍵是資料量，若資料量太小，深度學習不一定會有更好的表現。Google Translate 在訓練文件量少於一億篇時，傳統[[file:MachineLearning.org][機器學習]]表現較佳；在文件量超過十億後，深度學習效果就超越傳統[[file:MachineLearning.org][機器學習]]。
#+CAPTION: BLEU scores for English-Spanish systems trained on 0.4M to 385.7M words of parallel data. Source: Koehn and Knowles (2017) and GPU
#+name: fig:BLEU
#+ATTR_LATEX: :width 400
#+ATTR_ORG: :width 400
#+ATTR_HTML: :width 600
[[file:images/BLEU.png]]

** Deep Learning 的概念於 2006 年提出，何以至 2012 年才得到有效應用？
*** 計算速度: GPU 的計算能力由 2018 年起才有突破性的成長[fn:1]
#+CAPTION: Floating-point operations per second for the CPU and GPU
#+name: fig:GPUCPU-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 600
#+RESULTS:
[[file:images/GPUCPU1.jpg]]
#+CAPTION: Memory bandwidth for the CPU and GPU
#+name: fig:GPUCPU-1
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 600
#+RESULTS:
[[file:images/GPUCPU2.jpg]]
*** 大量數據
*** 軟體
數學模型、軟體工具(Tensorflow)

* AI的發展沿革
不論是從 1942 年的[[https://zh.wikipedia.org/zh-tw/%E9%98%BF%E5%A1%94%E7%BA%B3%E7%B4%A2%E5%A4%AB-%E8%B4%9D%E7%91%9E%E8%AE%A1%E7%AE%97%E6%9C%BA][ABC]]或是 1944 年的[[https://zh.wikipedia.org/wiki/%E9%A6%AC%E5%85%8B%E4%B8%80%E8%99%9F][MarK I]]，電腦的發明都過去半個世紀了，為何到 2010 年後，人工智慧才成為熱門話題？
** 三個里程碑
1. 1950: 設定 AI 目標
2. 1980: 以 Machine Learning 為主要手段
3. 1980 - 1990: Neural Network
4. 2006: Deep learning (ImageNet Classification with deep convolutioanl neural networks, 引用達上萬次)
5. 2010: Deep Learning
** AI 發展的起落
*** 第一波(符號還輯)：把人類的知識與思考放入電腦
- 1956 年，John McCarthy
- 1957 年，Herbert A. Somin(諾貝爾經濟學奬得主)預言電腦能在十年內敗人類(西洋棋)，此預言於 1997由[[https://zh.wikipedia.org/zh-tw/%E6%B7%B1%E8%97%8D_(%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6)][IBM Deep]]實現。
- 這階段的失敗原因：連人類自己都還搞不清楚自己的思考過程
*** 第二波(專家系統)：讓電腦按照人類定義的規則做決策
- 1970 年，專家系統，一連串條件判斷的推導
- 第一波失敗原因：野心太大，這次讓電腦依照人類設定好的規則來思考
- expert system 在 1980 年代廣受應用，Fortune 500 大公司有三分之二將之應用於營運工作中，如訂單處理、信用卡徵審、稅務處理。
- 1990 年後 expoert system 逐漸勢微，原因是能力有限，距離人類心目中的人工智慧差距尚大。
- Polanyi's Paradox(博藍尼悖論): We can know more than we cantell, i.e., many of the tasks we perform rely on tacit, intuitive knowledge that is diffucult to codify and automate.
*** 第三波([[file:MachineLearning.org][機器學習]])：電腦從資枓歸納規則，關鍵要素為資料與演算法
- 2006 年：Geofffrey Hinton 提出 Restricted Boltzmann Machine，成功訓練多層神經網路(multi-layer neural networks)，可用來描述更複雜的非線性函數，並稱之為深度學習(Deep Learning)。
- 2012 年 10 月,Hinton 帶兩個學生參力 ILSVRC 比賽，以深度學習配合 GPU 的運算速度拿下冠軍。
- ILSVRC: ImageNet Large Scale Visual Recongnition Challenge, 先讓程式看 120 萬張訓練照片，共 1000 種分類，接下來要求程式為 15 萬張測試照片進行分類。
- 2013 年，Google 收購 Hinton 和他兩位學生創立的公司：DNNresearch
- 2015 年，Microsoft 在 ILSVRC 以 3.5%的錯誤率奪冠，首次超過人類(5%)。
以上參考: [[https://www.books.com.tw/products/0010821934][人工智慧在台灣]]
** AI Development 大事記
*** IBM Deep Blue
*** IBM Watson
*** AlphaGo
*** AI Method
*** Google AI: 以平行處理來加速 Deep learning 的 tried and error
*** Deep learning 能快速發展的原因
- Deep Network Architectures & Training Strategies: 網路架構越來越大
- GPUs: 運算能力越來越強
- Data: 可參與運算的資料越來越多
*** GAN: Generative Adversarial Network:
**** 生成對抗網路
**** generator 與 discrimator 二者透過相互競爭來改善
** AI 應用與影響
*** Weak AI v.s. Strong AI
- Strong AI: 能思考、有主觀意識，又稱 General AI, FUll AI
- Tesla CEO Elon Musk: 2017 年 7 月在美國提倡規管 AI 發展的法案
*** AI 研究趨勢
- 大規模[[file:MachineLearning.org][機器學習]]
- 深度學習
- 強化學習
- 計算機視覺(偵測)
- 自然語言處理
- 協作系統
- Iot 物聯網
- 交通 / 無人機
- 家庭 / 服務機器人
- 醫療: 長照、疾病判斷
- 教育
- 低資源社區
- 公共安全: 監視器
- 就業和勞資
- 娛樂

* AI 的三大學派
** 符號主義學派(知識圖譜: 模仿人類邏輯與抽像推理),
是指基於符號運算的人工智慧學派，他們認為知識可以用符號來表示，認知可以通過符號運算來實現。例如，專家系統等。
*** 主要觀點：思維的基本是符號，思維過程即符號運算；智能的核心是知識，利用知識推理進行問題求解；智能活動的基礎是物理符號系統，人腦、電腦都是物理符號系統；知識可用符號表示，可建立基於符號邏輯的智能理論體系。該學派認為人工智慧源於數理邏輯，其主要的理論基礎是物理符號假設。
*** 主要科學方法：基於實驗心理學與計算軟體計算相結合的，以思維過程的功能模擬為重點的「黑箱」方法。
*** 代表性成果：1956 年問世的第一個啟發程序 LT 邏輯理論機；1968 年發表的第一個專家系統 DENTRAL 化學分析專家系統。
*** 發展途徑：啟發程序→專家系統。
** 連接主義學派(深度學習: 模仿大腦皮層神經網路)
是指神經網絡學派，在神經網絡方面，繼魯梅爾哈特研製出 BP 網絡之後，1987 年，首屆國際人工神經網絡學術大會在聖迭戈（San-Diego）舉行，掀起了人工神經網絡的第二次高潮。之後，隨著模糊邏輯和進化計算的逐步成熟，又形成了「計算智能」這個統一的學科範疇。
*** 主要觀點：智能活動的基元是神經細胞，智能活動過程是神經網絡的狀態演化過程，智能活動的基礎是神經細胞的突觸聯結機制，智能系統的工作模式仿人腦模式。該學派認為人工智慧源於仿生學，特別是對人腦模型的研究，其主要理論基礎為神經網絡及神經網絡間的連接機制與學習算法。
*** 主要科學方法：基於神經生理學與生理學的、以神經系統的結構模擬為重點的數學模擬與物理模擬方法。
*** 代表性成果：1943 年問世的第一個人工神經細胞——MP 模型；1960 年研製的感知機；1982 年提出的全互連型人工神經網絡——Hopfield 網絡；1986 年開發的多層感知機——BP 神經網絡。
*** 發展途徑：人工神經細胞→人工神經網絡。
** 行為主義學派(強化學習: 模仿生物奬懲學習機制)
是指進化主義學派，在行為模擬方面，麻省理工學院的布魯克教授 1991 年研製成功了能在未知的動態環境中漫遊的有 6 條腿的機器蟲。
*** 主要觀點：智能行為的基礎是「感知——行動」的反應機制，智能系統的智能行為，需要在真實世界的複雜境遇中進行學習和訓練，在與周圍環境的信息交互與適應過程中不斷進化和體現。該學派認為人工智慧應著重研究在複雜環境下對行為的控制，其主要的理論基礎是控制論。
*** 主要科學方法：基於智能控制系統的理論、方法和技術，以生物控制系統的智能行為模擬為重點，研究擬人的智能控制行為。
*** 代表性成果：1952 年研製成功的第一個「控制論動物」——香農老鼠；1991 年布魯克斯演示的新型智能機器人。
*** 發展途徑：控制論動物→智能機器人。

(以上參考網址：原文網址：https://kknews.cc/tech/pp8kvlz.html、https://kknews.cc/tech/5gn2gll.html)

#+latex:\newpage

* AI 的五大迷思
** 迷思一：資料等於價值
資料若沒有經過妥善的加工處理和萃取分析，本身並無太大價值，需要將對的資料用在對的場景。例如，電信公司的通聯記錄，行銷公司只會拿來做行銷，治安機關則可以拿來追查詐騙集團；又如 X 光片的判斷品質決定了 AI model 的成效。資料等於價值的另一反例為 AlphaZero。
** 迷思二：牽涉電腦與資料就是 MIS 部門的工作
AI 的導入需要跨部門支持，其開發團隊需要資料科學家(數學、統計)、領域專家(領域知識)、資訊人員(程式設計、資料庫)，最後在驗證模型成效時更需要跨部門的支持。
** 迷思三：資料分析就是產出報表
資料分析不應只限於公司內部資料庫中的結構化資料，而應包含非結構化資料(影像、聲音、影片、文字、互動)
** 迷思四：電腦決策不可能贏過人的專業經驗
主要原因在人類的短期記憶有限、能留意到的弱訊號太少，此外，有些工作需要極快的反應時間(如股市交易)。1995 年 Amazon 曾讓 50 位資深編輯就「推薦書單」與演算法進行 PK，自此後 Amazon 所有商品推薦都由[[file:MachineLearning.org][機器學習]]進行。
** 迷思五：導入系統或平台就可以解決營運問題
AI 不是一個資訊系統(如 ERP)，而是一種根據已知預測未知的方法，它沒有標準做法，其應用情境與方式會隨著企業的狀況與及需求有所不同。因此，問題不在「有沒有導入 AI」，而是「AI 應用的深度與廣度」。

* AI 擅長的解題領域
** 與情境無關的領域
- 如棋類遊戲等封閉系統就是與情境無關；反之，個人商品推薦則否，因為影響使用者是否購買特定商品的因素有太多是電商觀測不到的，例如，當天的心情。同理，戰爭的爆發其背後的因素也有可能出人意料之外，如特洛伊。
- 一些工作雖然與情境相關，但卻因為這些情境可人為控制，所以也適合以 AI 解決，如，人臉辨識可能因為拍照時人的角度、戴口罩、太陽眼鏡、帽子、背景光線、天氣等因素而導致辨識困難，但這些情境因素都可以事先控制，如：要求對象拿下口罩正向面對攝影機。
** 樣本數多的領域
- 如颱風一年最多 20 個，累積 50 年也不過 1000 個，不足以建立高複雜度且精確的學習模型(尤其牽涉的的變數很多時)
#+CAPTION: AI 擅長的解題領域
#+name: fig:AI4Domain
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 600
#+RESULTS:
[[file:images/AITW.jpg]]

* AI 各項產業應用
#+CAPTION: 各產業投資 AI 效益
#+name: fig:AIEnterprise
#+ATTR_LATEX: :width 300
#+ATTR_ORG: :width 300
#+ATTR_HTML: :width 600
#+RESULTS:
[[file:images/enterprise.jpg]]
** 製造業
1. 瑕疵檢測：金屬表面、玻璃、印刷電路、電子產品、牛仔褲、農產品，由 AI 取代人眼。在某家製造商的資料中，人眼檢測瑕疵漏網率為 5%、AI 為 0.01%；人眼檢測速度為每天 30 萬張影像、一台 10 萬左右的電腦每天可檢測 1440 萬張。
2. 自動流程控制：製造業共通的挑戰為設人竹廿月參數的調控及最佳化，或稱為自動流程控制。生產流程中，如馬達轉速、電流、電壓、環境溫度...等等需要監控、會影響產品良率的因素可能高達上千個，這些高維度的因素彼此又有交互作用(通常維度高過 5 個，且參數間有交互作用，人類就無法精確掌握)，而且製程可能很長，調整參數後可能隔天才能確認。AI 介入化工製程的例子可以將良率由六成調至 98%[fn:2]。
3. 預測性維護：包括預測機器何時會出錯以提前進廠保養、預測耗才何時更換最為有利。此類工作涉及訊號鄋理，如：監控馬達電壓、轉速、震動、聲音來判斷馬達是否即將固障；監控機器手臂行程順暢度、夾具穩定度來判斷機器手臂是否有固障徵兆。[fn:2]
4. 原料組合最佳化：製造業的工作在於取得一種或多種原料，經過物理或化學加工過程後製成產品；但每批原枓可能來自不同供應商、品質、等級或特性可能有所差異，如何在各原料、供應商、等級、成本的排列中找出最高 CP 值的組合即為重要工作。以染整業為例，新的布料與顏色平均要花 3~7 天的打色嚐試才能達到客戶允收範圍，以第一次打色為例，軟體模擬加上師傅經驗調整，成功率約七成；而藉由以深度學習建出模型來描述布料、目標顏色及染料濃度間的關係，可以將成功率達到九成[fn:2]。
** 零售與金融業
零售及金融之所以相對容易切入 AI 是因為這兩個產業的核心業務就是在處理資訊流。
依據 Gartner 的報告，資料分析可以分四個層次：
1. 描述：評估現況及了解問題。解釋發生了什麼？
2. 解釋：提供問題的初步診斷。解釋為什麼發生？
3. 預測：提供改善和解決問題的工具。未來會不會發生？
4. 最佳化：提供改善和解決問題的工具。如何讓他發生？
*** 圖表式的決策反而可能誤導
以零售業的產銷量問題為例，假設影響因素有：店點、擺設位置、售價，折扣活動、集點活動、包裝、季節...，若以圖表顯示，每張圖表一次頂多呈現 1~2 個變數的關係，無法同時呈現所有變數[fn:2]。

* AI v.s. security

** 釣魚網站偵測實戰

[[https://github.com/PacktPublishing/Mastering-Machine-Learning-for-Penetration-Testing/tree/master/Chapter02][github data]]

*** 資料集
UCI Machine Learning Repository (Phishing Websites Data Set).
https://archive.ics.uci.edu/ml/datasets/Phishing+Websites

The dataset is provided as an arff file

處理過的資料集:
{30 Attributes (having_IP_Address URL_Length, abnormal_URL and so on)}+ {1 Attribute (Result)}

-1,1,1,1,-1,-1,-1,-1,-1,1,1,-1,1,-1,1,-1,-1,-1,0,1,1,1,1,-1,-1,-1,-1,1,1,-1,-1

真正要能上線跑的演算法不多，因為會面臨資料量太大(流量)的問題，會導致記憶體不足....

*** papers

- Mohammad, Rami, McCluskey, T.L. and Thabtah, Fadi (2012). An Assessment of Features Related to Phishing Websites using an Automated Technique. In: International Conferece For Internet Technology And Secured Transactions. ICITST 2012 . IEEE, London, UK, pp. 492-497. ISBN 978-1-4673-5325-0
- Mohammad, Rami, Thabtah, Fadi Abdeljaber and McCluskey, T.L. (2014). Predicting phishing websites based on self-structuring neural network. Neural Computing and Applications, 25 (2). pp. 443-458. ISSN 0941-0643
- Mohammad, Rami, McCluskey, T.L. and Thabtah, Fadi Abdeljaber (2014). Intelligent Rule based Phishing Websites Classification. IET Information Security, 8 (3). pp. 153-160. ISSN 1751-8709

*** 使用 LogisticRegression

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import numpy as np
  from sklearn import *
  from sklearn.linear_model import LogisticRegression
  from sklearn.metrics import accuracy_score

  training_data = np.genfromtxt('dataset.csv', delimiter=',', dtype=np.int32)
  inputs = training_data[:,:-1]
  outputs = training_data[:, -1]

  training_inputs = inputs[:2000]
  training_outputs = outputs[:2000]
  testing_inputs = inputs[2000:]
  testing_outputs = outputs[2000:]

  classifier = LogisticRegression()
  classifier.fit(training_inputs, training_outputs)
  predictions = classifier.predict(testing_inputs)
  accuracy = 100.0 * accuracy_score(testing_outputs, predictions)
  print ("The accuracy of your Logistic Regression on testing data is: " + str(accuracy))
#+END_SRC

*** 使用 DecisionTreeClassifier

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  from sklearn import tree
  from sklearn.metrics import accuracy_score
  import numpy as np

  training_data = np.genfromtxt('dataset.csv', delimiter=',', dtype=np.int32)
  inputs = training_data[:,:-1]
  outputs = training_data[:, -1]

  training_inputs = inputs[:2000]
  training_outputs = outputs[:2000]
  testing_inputs = inputs[2000:]
  testing_outputs = outputs[2000:]

  classifier = tree.DecisionTreeClassifier()
  classifier.fit(training_inputs, training_outputs)
  predictions = classifier.predict(testing_inputs)
  accuracy = 100.0 * accuracy_score(testing_outputs, predictions)
  print ("The accuracy of your decision tree on testing data is: " + str(accuracy))
#+END_SRC

** Text Classification

[[https://github.com/MyDearGreatTeacher/TensorSecurity/blob/master/code/AI_security/3_TextClassification%E8%88%87%E5%9E%83%E5%9C%BE%E7%9F%AD%E4%BF%A1%E9%A0%90%E6%B8%AC.md][Text classification github]]

二元分類: binary classification

spam detection[email, SMS]

*** papers

- MS Spam Collection Dataset, Collection of SMS messages tagged as spam or legitimate, https://www.kaggle.com/uciml/sms-spam-collection-dataset/data
- The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.
- 2009 年博士論文, A CORPUS LINGUISTICS STUDY OF SMS TEXT MESSAGING, CAROLINE TAGG, https://etheses.bham.ac.uk/id/eprint/253/1/Tagg09PhD.pdf

*** 資料集

https://github.com/CorkyMaigre/sms-spam-ml/blob/master/dataset/SMSSpamCollection

#+BEGIN_SRC sh
  ham	Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...
  ham	Ok lar... Joking wif u oni...
  spam	Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's
  ham	U dun say so early hor... U c already then say...
  ham	Nah I don't think he goes to usf, he lives around here though
  spam	FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv
  ham	Even my brother is not like to speak with me. They treat me like aids patent.
  ham	As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune
  spam	WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.
  spam	Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030
  ham	I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.
  spam	SIX chances to win CASH! From 100 to 20,000 pounds txt> CSH11 and send to 87575. Cost 150p/day, 6days, 16+ TsandCs apply Reply HL 4 info
  spam	URGENT! You have won a 1 week FREE membership in our £100,000 Prize Jackpot! Txt the word: CLAIM to No: 81010 T&C www.dbuk.net LCCLTD POBOX 4403LDNW1A7RW18
  ham	I've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.
  ham	I HAVE A DATE ON SUNDAY WITH WILL!!
  spam	XXXMobileMovieClub: To use your credit, click the WAP link in the next txt message or click here>> http://wap. xxxmobilemovieclub.com?n=QJKGIGHJJGCBL
  ham	Oh k...i'm watching here:)
  ham	Eh u remember how 2 spell his name... Yes i did. He v naughty make until i v wet.
  ham	Fine if that�s the way u feel. That�s the way its gota b
  spam	England v Macedonia - dont miss the goals/team news. Txt ur national team to 87077 eg ENGLAND to 87077 Try:WALES, SCOTLAND 4txt/ú1.20 POBOXox36504W45WQ 16+
  ham	Is that seriously how you spell his name?
  ham	I‘m going to try for 2 months ha ha only joking
  ham	So ü pay first lar... Then when is da stock comin...
  ham	Aft i finish my lunch then i go str down lor. Ard 3 smth lor. U finish ur lunch already?
  ham	Ffffffffff. Alright no way I can meet up with you sooner?
  ham	Just forced myself to eat a slice. I'm really not hungry tho. This sucks. Mark is getting worried. He knows I'm sick when I turn down pizza. Lol
  ham	Lol your always so convincing.
  ham	Did you catch the bus ? Are you frying an egg ? Did you make a tea? Are you eating your mom's left over dinner ? Do you feel my Love ?
  ham	I'm back &amp; we're packing the car now, I'll let you know if there's room
  ham	Ahhh. Work. I vaguely remember that! What does it feel like? Lol
  ham	Wait that's still not all that clear, were you not sure about me being sarcastic or that that's why x doesn't want to live with us
  ham	Yeah he got in at 2 and was v apologetic. n had fallen out and she was actin like spoilt child and he got caught up in that. Till 2! But we won't go there! Not doing too badly cheers. You?

#+END_SRC

*** 使用 LogisticRegression

- Hands-on-Machine-Learning-for-Cyber-Security/Chapter05/sms_spam.py /
- https://github.com/PacktPublishing/Hands-on-Machine-Learning-for-Cyber-Security/blob/master/Chapter05/sms_spam.py

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import pandas as pd
  import numpy as np
  from sklearn.feature_extraction.text import TfidfVectorizer
  from sklearn.linear_model.logistic import LogisticRegression
  from sklearn.model_selection import train_test_split, cross_val_score

  dataframe = pd.read_csv('SMSSpamCollectionDataSet', delimiter='\t',header=None)

  X_train_dataset, X_test_dataset, y_train_dataset, y_test_dataset = train_test_split(dataframe[1],dataframe[0])

  vectorizer = TfidfVectorizer()
  X_train_dataset = vectorizer.fit_transform(X_train_dataset)

  classifier_log = LogisticRegression()
  classifier_log.fit(X_train_dataset, y_train_dataset)

  X_test_dataset = vectorizer.transform( ['URGENT! Your Mobile No 1234 was awarded a Prize', 'Hey honey, whats up?'] )

  predictions_logistic = classifier.predict(X_test_dataset)
  print(predictions)
#+END_SRC

*** TensorFlow_RNN for 垃圾短信預測

TensorFlow 機器學習實戰指南 (美)尼克‧麥克盧爾
 9.2 用 TensorFlow 實現 RNN 模型進行垃圾短信預測
 https://github.com/PacktPublishing/TensorFlow-Machine-Learning-Cookbook-Second-Edition


#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import os
  import re
  import io
  import requests
  import numpy as np
  import matplotlib.pyplot as plt
  import tensorflow as tf
  from zipfile import ZipFile
  from tensorflow.python.framework import ops
  ops.reset_default_graph()

  # Start a graph
  sess = tf.Session()

  # Set RNN parameters
  epochs = 20
  batch_size = 250
  max_sequence_length = 25
  rnn_size = 10
  embedding_size = 50
  min_word_frequency = 10
  learning_rate = 0.0005
  dropout_keep_prob = tf.placeholder(tf.float32)


  # Download or open data
  data_dir = 'temp'
  data_file = 'text_data.txt'
  if not os.path.exists(data_dir):
      os.makedirs(data_dir)

  if not os.path.isfile(os.path.join(data_dir, data_file)):
      zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'
      r = requests.get(zip_url)
      z = ZipFile(io.BytesIO(r.content))
      file = z.read('SMSSpamCollection')
      # Format Data
      text_data = file.decode()
      text_data = text_data.encode('ascii', errors='ignore')
      text_data = text_data.decode().split('\n')

      # Save data to text file
      with open(os.path.join(data_dir, data_file), 'w') as file_conn:
          for text in text_data:
              file_conn.write("{}\n".format(text))
  else:
      # Open data from text file
      text_data = []
      with open(os.path.join(data_dir, data_file), 'r') as file_conn:
          for row in file_conn:
              text_data.append(row)
      text_data = text_data[:-1]

  text_data = [x.split('\t') for x in text_data if len(x) >= 1]
  [text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]


  # Create a text cleaning function
  def clean_text(text_string):
      text_string = re.sub(r'([^\s\w]|_|[0-9])+', '', text_string)
      text_string = " ".join(text_string.split())
      text_string = text_string.lower()
      return text_string


  # Clean texts
  text_data_train = [clean_text(x) for x in text_data_train]

  # Change texts into numeric vectors
  vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length,
                                                                       min_frequency=min_word_frequency)
  text_processed = np.array(list(vocab_processor.fit_transform(text_data_train)))

  # Shuffle and split data
  text_processed = np.array(text_processed)
  text_data_target = np.array([1 if x == 'ham' else 0 for x in text_data_target])
  shuffled_ix = np.random.permutation(np.arange(len(text_data_target)))
  x_shuffled = text_processed[shuffled_ix]
  y_shuffled = text_data_target[shuffled_ix]

  # Split train/test set
  ix_cutoff = int(len(y_shuffled)*0.80)
  x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]
  y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]
  vocab_size = len(vocab_processor.vocabulary_)
  print("Vocabulary Size: {:d}".format(vocab_size))
  print("80-20 Train Test split: {:d} -- {:d}".format(len(y_train), len(y_test)))

  # Create placeholders
  x_data = tf.placeholder(tf.int32, [None, max_sequence_length])
  y_output = tf.placeholder(tf.int32, [None])

  # Create embedding
  embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))
  embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data)

  # Define the RNN cell
  # tensorflow change >= 1.0, rnn is put into tensorflow.contrib directory. Prior version not test.
  if tf.__version__[0] >= '1':
      cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)
  else:
      cell = tf.nn.rnn_cell.BasicRNNCell(num_units=rnn_size)

  output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)
  output = tf.nn.dropout(output, dropout_keep_prob)

  # Get output of RNN sequence
  output = tf.transpose(output, [1, 0, 2])
  last = tf.gather(output, int(output.get_shape()[0]) - 1)

  weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))
  bias = tf.Variable(tf.constant(0.1, shape=[2]))
  logits_out = tf.matmul(last, weight) + bias

  # Loss function
  losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output)
  loss = tf.reduce_mean(losses)

  accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits_out, 1), tf.cast(y_output, tf.int64)), tf.float32))

  optimizer = tf.train.RMSPropOptimizer(learning_rate)
  train_step = optimizer.minimize(loss)

  init = tf.global_variables_initializer()
  sess.run(init)

  train_loss = []
  test_loss = []
  train_accuracy = []
  test_accuracy = []
  # Start training
  for epoch in range(epochs):

      # Shuffle training data
      shuffled_ix = np.random.permutation(np.arange(len(x_train)))
      x_train = x_train[shuffled_ix]
      y_train = y_train[shuffled_ix]
      num_batches = int(len(x_train)/batch_size) + 1
      # TO DO CALCULATE GENERATIONS ExACTLY
      for i in range(num_batches):
          # Select train data
          min_ix = i * batch_size
          max_ix = np.min([len(x_train), ((i+1) * batch_size)])
          x_train_batch = x_train[min_ix:max_ix]
          y_train_batch = y_train[min_ix:max_ix]

          # Run train step
          train_dict = {x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob:0.5}
          sess.run(train_step, feed_dict=train_dict)

      # Run loss and accuracy for training
      temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)
      train_loss.append(temp_train_loss)
      train_accuracy.append(temp_train_acc)

      # Run Eval Step
      test_dict = {x_data: x_test, y_output: y_test, dropout_keep_prob:1.0}
      temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)
      test_loss.append(temp_test_loss)
      test_accuracy.append(temp_test_acc)
      print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}'.format(epoch+1, temp_test_loss, temp_test_acc))

  # Plot loss over time
  epoch_seq = np.arange(1, epochs+1)
  plt.plot(epoch_seq, train_loss, 'k--', label='Train Set')
  plt.plot(epoch_seq, test_loss, 'r-', label='Test Set')
  plt.title('Softmax Loss')
  plt.xlabel('Epochs')
  plt.ylabel('Softmax Loss')
  plt.legend(loc='upper left')
  plt.show()

  # Plot accuracy over time
  plt.plot(epoch_seq, train_accuracy, 'k--', label='Train Set')
  plt.plot(epoch_seq, test_accuracy, 'r-', label='Test Set')
  plt.title('Test Accuracy')
  plt.xlabel('Epochs')
  plt.ylabel('Accuracy')
  plt.legend(loc='upper left')
  plt.show()
#+END_SRC

** AI and Botnet Detection

[[https://github.com/MyDearGreatTeacher/TensorSecurity/tree/master/code/AI_security/%E7%99%BC%E5%B1%95%E8%B6%A8%E5%8B%A2/Botnet][Botnet github]]
IOT honey pot

*** 案例分析

Hands-On Artificial Intelligence for Cybersecurity
Alessandro Parisi August 2019
CH 5 Network Anomaly Detection with AI

*** 資料集

https://github.com/MyDearGreatTeacher/AI201909/blob/master/data/network-logs.csv

!wget https://raw.githubusercontent.com/MyDearGreatTeacher/AI201909/master/data/network-logs.csv



#+BEGIN_SRC csv
  REMOTE_PORT	LATENCY	THROUGHPUT	ANOMALY
  21	15.94287532	16.20299807	0
  20	12.66645095	15.89908374	1
  80	13.89454962	12.95800822	0
  21	13.62081292	15.45947525	0
  21	15.70548485	15.33956527	0
  23	15.59318973	15.61238106	0
  21	15.48906755	15.64087368	0
  80	15.52704801	15.63568031	0
  21	14.07506707	15.76531533	0
  ......
#+END_SRC

#+BEGIN_SRC csv
  延遲（Latency）：一個封包從來源端送出後，到目的端接收到這個封包，中間所花的時間。
  頻寬（Bandwidth）：傳輸媒介的最大吞吐量（throughput）。

  https://blog.gtwang.org/web-development/network-lantency-and-bandwidth/
#+END_SRC

*** 基本統計分析
#+BEGIN_SRC python -r -n :results output :exports both :eval no
!wget https://raw.githubusercontent.com/MyDearGreatTeacher/AI201909/master/data/network-logs.csv

  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  %matplotlib inline

  dataset = pd.read_csv('network-logs.csv')
  hist_dist = dataset[['LATENCY', 'THROUGHPUT']].hist(grid=False, figsize=(10,4))

  data = dataset[['LATENCY', 'THROUGHPUT']].values

  plt.scatter(data[:, 0], data[:, 1], alpha=0.6)
  plt.xlabel('LATENCY')
  plt.ylabel('THROUGHPUT')
  plt.title('DATA FLOW')
  plt.show()
#+END_SRC

*** 機器學習
#+BEGIN_SRC python -r -n :results output :exports both :eval no
  import numpy as np
  import pandas as pd

  from sklearn.linear_model import *
  from sklearn.tree import *
  from sklearn.naive_bayes import *
  from sklearn.neighbors import *
  from sklearn.metrics import accuracy_score

  from sklearn.model_selection import train_test_split

  import matplotlib.pyplot as plt
  %matplotlib inline

  # Load the data.
  dataset = pd.read_csv('network-logs.csv')


  samples = dataset.iloc[:, [1, 2]].values #只取第1、2欄的資料當features
  targets = dataset['ANOMALY'].values

  training_samples, testing_samples, training_targets, testing_targets = train_test_split(
           samples, targets, test_size=0.3, random_state=0)

#+END_SRC

接下來就可以套用各種分類演算法

**** 使用 k-Nearest Neighbors model

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  knc = KNeighborsClassifier(n_neighbors=2)
  knc.fit(training_samples,training_targets)
  knc_prediction = knc.predict(testing_samples)
  knc_accuracy = 100.0 * accuracy_score(testing_targets, knc_prediction)
  print ("K-Nearest Neighbours accuracy: " + str(knc_accuracy))


#+END_SRC

**** 使用 Decision tree model

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  dtc = DecisionTreeClassifier(random_state=0)
  dtc.fit(training_samples,training_targets)
  dtc_prediction = dtc.predict(testing_samples)
  dtc_accuracy = 100.0 * accuracy_score(testing_targets, dtc_prediction)
  print ("Decision Tree accuracy: " + str(dtc_accuracy))
#+END_SRC

**** 使用 Gaussian Naive Bayes model

#+BEGIN_SRC python -r -n :results output :exports both :eval no
  gnb = GaussianNB()
  gnb.fit(training_samples,training_targets)
  gnb_prediction = gnb.predict(testing_samples)
  gnb_accuracy = 100.0 * accuracy_score(testing_targets, gnb_prediction)
  print ("Gaussian Naive Bayes accuracy: " + str(gnb_accuracy))

#+END_SRC

*** 結果

- K-Nearest Neighbours accuracy: 95.90163934426229
- Decision Tree accuracy: 96.72131147540983
- Gaussian Naive Bayes accuracy: 98.36065573770492

#+latex:\newpage

* AI 的學習之路
** About 學習 AI
*** 做資料分析時，難的不是演算法，而是如何訂定一些 feature
*** 要做 AI，沒有資料怎麼辦？
- 拍照
- 找專家先分類
- 把資料丟進去跑
** AI Application Ideas
*** Video
**** 錄影時自動偵測被錄者(講者)的位置，然後進行錄影架的左右旋轉、鏡頭的 zoom in/out
**** 電影中某角色出現的片斷擷取
**** 手語解讀
**** 黑白電影轉彩色
*** 圖片
**** 自行車選手編號自動識別
**** 輸入物品圖片判斷價格
*** 聲音
**** 用別人的聲音唱歌
*** 文字
**** 劇本產生器
*** 測驗
*** 指數
**** 股票與 PTT 關係
** 相關的比賽
*** 台灣的比賽：AIDEA: 人工智慧平台競賽
**** 參加比賽
- [[https://aidea-web.tw/aicup_mango][台灣高經濟作物 - 愛文芒果影像辨識正式賽]]
*** 國外的比賽
- https://www.kaggle.com/
** AI 時代的 I
- 能做什麼？什麼會被取代？要能歸維、推理、跨領域、具備創造力
** 學習資源
*** 書單
- [[https://www.sanmin.com.tw/Product/Index/005848506][OpenCV 3計算機視覺：Python語言實現(原書第2版)]]
- [[https://www.books.com.tw/products/0010761759][Deep Learning：用Python進行深度學習的基礎理論實作]]
- [[https://paperswithcode.com/sota][Browse State-of-the-Art]]
- [[https://www.books.com.tw/products/0010822932][Deep learning 深度學習必讀：Keras 大神帶你用 Python 實作]]
*** 線上資源
- [[https://github.com/MyDearGreatTeacher/TensorSecurity][MyDearGreatTeacher/TensorSecurity]]
- [[https://github.com/MyDearGreatTeacher/PyTorch/blob/master/code/LinearRegression.py][MyDearGreatTeacher/PyTorch]]
- [[https://github.com/MyDearGreatTeacher/AI201909][MyDearGreatTeacher/AI201909]]
- [[https://github.com/MyDearGreatTeacher?tab=repositories][MyDearGreatTeacher]]
- [[https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv][MIT Convolutional Neural Networks for Visual Recognition (Spring 2017)]]
- [[https://www.packtpub.com/catalogsearch/result/?q=python&released=Available&language=Python][packt電子書]]
- [[https://www.youtube.com/watch?v=J6Ok8p463C4][Getting Started with Keras (AI Adventures) Youtube]]
- [[https://blog.csdn.net/sunqiande88/article/details/80100891][PyTorch實戰2: ResNet-18實現Cifar-10圖像分類]]
- [[https://github.com/activatedgeek/LeNet-5][LeNet-5]]
- [[https://arxiv.org/abs/1409.1556][Very Deep Convolutional Networks for Large-Scale Image Recognition]]
- [[https://github.com/nlpinaction/learning-nlp][自然语言处理算法与实战]]

#+latex:\newpage

* AI 的實作平台
** Google Colab
*** 基本操作
- Google 提供的 VM
#+begin_src shell -r -n :results output :exports both
!pwd
!cat /proc/meminfo
!cat  /proc/cpuinfo
#+end_src
- use colab to mount google drive
- 同時最多 5 個 session
- 每個 session 最多 24hr
- 查看 colab 已安裝了哪些 package
#+begin_src shell -r -n :results output :exports both
!pip list
#+end_src
*** 如何確定有在跑 GPU: [[file:TensorFlow.org][Tensorflow 2020]]
#+begin_src python -r -n :results output :exports both
import tensorflow as tf
gpu_name = tf.test.gpu_device_name()
if gpu_name != '/device:GPU:0':
  raise SystemError('無 GPU')
print(f'有 GPU: {gpu_name}')
#+end_src

#+RESULTS:
*** 如何用 python 上傳本機檔案到 colab
#+begin_src python -r -n :results output :exports both
from google.colab import files
uploaded = files.upload()
#+end_src
*** 如何 mount google drive
DEMO
** Jupyter Notebook
*** [[file:\[AI\]CNN.org][深度學習:以CNN為例]]

* 學習資源
** Machine Learning [台大李宏毅]
*** Lecture 0
- [[http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML19.html][ML19: 台大[[file:MachineLearning.org][機器學習]]
- [[https://www.youtube.com/watch?v=CXgbekl66jc][ML Lecture 0-1: Introduction of Machine Learning]]
- [[https://www.youtube.com/watch?v=On1N8u1z2Ng][ML Lecture 0-2: Why we need to learn machine learning?]]
*** Lecture 1
- [[https://www.youtube.com/watch?v=fegAeph9UaA][ML Lecture 1: Regression - Case Study]]
- [[https://www.youtube.com/watch?v=1UqCjFQiiy0][ML Lecture 1: Regression - Demo]]
*** Lecture 2
- [[https://www.youtube.com/watch?v=D_S6y0Jm6dQ][ML Lecture 2: Where does the error come from?]]
*** Lecture 3
- [[https://www.youtube.com/watch?v=yKKNr-QKz2Q][ML Lecture 3-1: Gradient Descent]]
- [[https://www.youtube.com/watch?v=1_HBTJyWgNA][ML Lecture 3-2: Gradient Descent (Demo by AOE)]]
- [[https://www.youtube.com/watch?v=wzPAInDF_gI][ML Lecture 3-3: Gradient Descent (Demo by Minecraft)]]
*** Lecture 4
- [[https://www.youtube.com/watch?v=fZAZUYEeIMg][ML Lecture 4: Classification]]
*** Lecture 5
- [[https://www.youtube.com/watch?v=hSXFuypLukA][ML Lecture 5: Logistic Regression]]
*** Lecture 6
- [[https://www.youtube.com/watch?v=Dr-WRlEFefw][ML Lecture 6: Brief Introduction of Deep Learning]]
*** Lecture 7
- [[https://www.youtube.com/watch?v=ibJpTrp5mcE][ML Lecture 7: Backpropagation]]
- [[https://www.youtube.com/watch?v=gDp2LXGnVLQ&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=3&t=0s][Anomaly Detection (1/7)]]
- [[https://www.youtube.com/playlist?list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4][Next Step of Machine Learning (Hung-yi Lee, NTU, 2019)]]
*** Lecture 8
- [[https://www.youtube.com/watch?v=Lx3l4lOrquw][ML Lecture 8-1: “Hello world” of deep learning]]
- [[https://www.youtube.com/watch?v=5BJDJd-dzzg][ML Lecture 8-2: Keras 2.0]]
- [[https://www.youtube.com/watch?v=L8unuZNpWw8][ML Lecture 8-3: Keras Demo]]
*** Explainable ML
  1) [[https://www.youtube.com/watch?v=lnjrn3bF9lA][Explainable ML (1/8)]]
  2) [[https://www.youtube.com/watch?v=pNpk6DPYUh8&list=PL9McrqOpq3mUCXF5E8rVLjw8f878zkBfX&index=16][Explainable ML (2/8)]]
  3) Explainable ML (3/8)
  4) [[https://www.youtube.com/watch?v=yORbWn7UsBs][Explainable ML (4/8)]]
  5) [[https://www.youtube.com/watch?v=1xnhQbAV1m0][Explainable ML (5/8)]]
  6) [[https://www.youtube.com/watch?v=K1mWgthGS-A][Explainable ML (6/8)]]
  7) [[https://www.youtube.com/watch?v=1xnhQbAV1m0][Explainable ML (7/8)]]
  8) [[https://www.youtube.com/watch?v=ah_Ttx6cIVU][Explainable ML (8/8)]]
*** TODO Attack ML Models
  1) [[https://www.youtube.com/watch?v=NI6yb0WgMBM][Attack ML Models (1/8)]]
  2) [[https://www.youtube.com/watch?v=zOdg05BwE7I][Attack ML Models (2/8)]]
  3) [[https://www.youtube.com/watch?v=F9N5zF7N0qY][Attack ML Models (3/8)]]
  4) [[https://www.youtube.com/watch?v=qjnMoWmn1FQ][Attack ML Models (4/8)]]
  5) [[https://www.youtube.com/watch?v=2mgLPZJOHNk][Attack ML Models (5/8)]]
  6) [[https://www.youtube.com/watch?v=z2nmPDLEXI0][Attack ML Models (6/8)]]
  7) [[https://www.youtube.com/watch?v=KH48zq2RfBA&t=1s][Attack ML Models (7/8)]]
  8) [[https://www.youtube.com/watch?v=ah_Ttx6cIVU][Attack ML Models (8/8)]]
*** Lecture 9
- [[https://www.youtube.com/watch?v=xki61j7z-30][ML Lecture 9-1: Tips for Training DNN]]
- [[https://www.youtube.com/watch?v=Ky1ku1miDow][ML Lecture 9-2: Keras Demo 2]]
- [[https://www.youtube.com/watch?v=F1vek6ULo9w][ML Lecture 9-3: Fizz Buzz in Tensorflow (sequel)]]
*** Lecture 10
- [[https://www.youtube.com/watch?v=FrKWiRv254g][ML Lecture 10: Convolutional Neural Network]]
- [[https://www.youtube.com/watch?v=XsC9byQkUH8][ML Lecture 11: Why Deep?]]
- [[https://www.youtube.com/watch?v=fX_guE7JNnY][ML Lecture 12: Semi-supervised]]
- [[https://www.youtube.com/watch?v=iwh5o_M4BNU][ML Lecture 13: Unsupervised Learning - Linear Methods]]
- [[https://www.youtube.com/watch?v=X7PH3NuYW0Q][ML Lecture 14: Unsupervised Learning - Word Embedding]]
- [[https://www.youtube.com/watch?v=GBUEjkpoxXc][ML Lecture 15: Unsupervised Learning - Neighbor Embedding]]
- [[https://www.youtube.com/watch?v=yyKaACh_j3M&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=45&t=0s][Meta Learning – Metric-based (1/3)]]
- [[https://www.youtube.com/watch?v=Tk5B4seA-AU][ML Lecture 16: Unsupervised Learning - Auto-encoder]]
- [[https://www.youtube.com/watch?v=YNUek8ioAJk][ML Lecture 17: Unsupervised Learning - Deep Generative Model (Part I)]]
- [[https://www.youtube.com/watch?v=8zomhgKrsmQ][ML Lecture 18: Unsupervised Learning - Deep Generative Model (Part II)]]
- [[https://www.youtube.com/watch?v=6ZWu4L7XOiQ&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=48&t=0s][More about Auto-encoder (1/4)]]
- [[https://www.youtube.com/watch?v=qD6iD4TFsdQ][ML Lecture 19: Transfer Learning]]
- [[https://www.youtube.com/watch?v=7qT5P9KJnWo&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=25][Life Long Learning (1/7)]]
- [[https://www.youtube.com/watch?v=ZjfjPzXw6og&feature=youtu.be][Sequence-to-sequence Learning]]
- [[https://www.youtube.com/watch?v=EkAqYbpCYAc&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=33&t=0s][Meta Learning – MAML (1/9)]]
- [[https://www.youtube.com/watch?v=QSEPStBgwRQ][ML Lecture 20: Support Vector Machine (SVM)]]
- [[https://www.youtube.com/watch?v=xCGidAeyS4M][ML Lecture 21-1: Recurrent Neural Network (Part I)]]
- [[https://www.youtube.com/watch?v=rTqmWlnwz_0][ML Lecture 21-2: Recurrent Neural Network (Part II)]]
- [[https://www.youtube.com/watch?v=YIuBHB9Ejok&feature=youtu.be][Unsupervised Syntactic Parsing (ft. 莊永松同學)]]
- [[https://www.youtube.com/watch?v=tH9FH1DH5n0][ML Lecture 22: Ensemble]]
- [[https://www.youtube.com/watch?v=W8XF3ME8G2I][ML Lecture 23-1: Deep Reinforcement Learning]]
- [[https://www.youtube.com/watch?v=y8UPGr36ccI][ML Lecture 23-2: Policy Gradient (Supplementary Explanation)]]
- [[https://www.youtube.com/watch?v=2-JNBzCq77c][ML Lecture 23-3: Reinforcement Learning (including Q-learning)]]
- [[https://www.youtube.com/playlist?list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_][Deep Reinforcement Learning, 2018]]
- [[https://www.youtube.com/watch?v=dPp8rCAnU_A&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=52&t=0s][Network Compression (1/6)]]
- [[https://www.youtube.com/watch?v=ufcKFjdpT98&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=58&t=0s][GAN (Quick Review)]]
- [[https://www.youtube.com/playlist?list=PLJV_el3uVTsMq6JEFPW35BCiOQTsoqwNw][Generative Adversarial Network (GAN), 2018]]
- [[https://www.youtube.com/watch?v=ugWDIIOHtPA&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=58][Transformer]]
- [[https://www.youtube.com/watch?v=UYPa347-DdE&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=62&t=0s][ELMO, BERT, GPT]]
- [[https://www.youtube.com/watch?v=uXY18nzdSsM&list=PLJV_el3uVTsOK_ZK5L0Iv_EQoL1JefRL4&index=59][Flow-based Generative Model]]
- [[https://brohrer.mcknote.com/zh-Hant/statistics/how_bayesian_inference_works.html][貝葉斯推斷的運作原理]]
** Deep Learning for Human Language Processing (DLHLP) 2020
- [[https://www.youtube.com/watch?v=nER51ZyJaCQ&list=PLJV_el3uVTsO07RpBYFsXg-bN5Lu0nhdG][[DLHLP 2020] Deep Learning for Human Language Processing (Course Overview)]]
- [[https://www.youtube.com/watch?v=AIKu43goh-8&list=PLJV_el3uVTsO07RpBYFsXg-bN5Lu0nhdG&index=2][[DLHLP 2020] Speech Recognition (1/7) - Overview]]
- [[https://www.youtube.com/watch?v=BdUeBa6NbXA&list=PLJV_el3uVTsO07RpBYFsXg-bN5Lu0nhdG&index=3][[DLHLP 2020] Speech Recognition (2/7) - Listen, Attend, Spell]]
- [[https://www.youtube.com/watch?v=CGuLuBaLIeI&list=PLJV_el3uVTsO07RpBYFsXg-bN5Lu0nhdG&index=4][[DLHLP 2020] Speech Recognition (3/7) - CTC, RNN-T and more]]
- [[https://www.youtube.com/watch?v=XWTGY_PNABo&list=PLJV_el3uVTsO07RpBYFsXg-bN5Lu0nhdG&index=5][[DLHLP 2020] Speech Recognition (4/7) - HMM (optional)]]
- [[https://www.youtube.com/watch?v=5SSVra6IJY4&list=PLJV_el3uVTsO07RpBYFsXg-bN5Lu0nhdG&index=6][[DLHLP 2020] Speech Recognition (5/7) - Alignment of HMM, CTC and RNN-T (optional)]]
- [[https://www.youtube.com/watch?v=gRfTfXCe3LA][[DLHLP 2020] Deep Learning for Question Answering (1/2)]]
- [[https://www.youtube.com/watch?v=h_Lptoq8spQ][[DLHLP 2020] Deep Learning for Question Answering (2/2)]]
- [[https://www.youtube.com/watch?v=DOG1L9lvsDY][[DLHLP 2020] 來自獵人暗黑大陸的模型 GPT-3]]
- [[https://www.youtube.com/watch?v=Bywo7m6ySlk][[DLHLP 2020] BERT and its family - ELMo, BERT, GPT, XLNet, MASS, BART, UniLM, ELECTRA, and more]]
- [[https://www.youtube.com/watch?v=ugWDIIOHtPA][Transformer]]
** Digital Speech Processing
- [[http://ocw.aca.ntu.edu.tw/ntu-ocw/ocw/cou/104S204/1][第一章 Introduction to Digital Speech Processing  ]]
** test
#+begin_src C -r -n :results output :exports both
#include "stdlib.h"
#include "stdio.h"
int main() {
    printf("TEST");
}
#+end_src

#+RESULTS:
: TEST

* Page Infos
#+INCLUDE: "../css/footer.html" export html

* Footnotes

[fn:1] [[https://www.hindawi.com/journals/mpe/2019/2053156/][A Multi-GPU Parallel Algorithm in Hypersonic Flow Computations]]

[fn:2] [[https://www.books.com.tw/products/0010821934?sloc=main][人工智慧在台灣：產業轉型的契機與挑戰，AI應用無所不在，你跟上了嗎？]]

[fn:7] [[https://youtu.be/VwVg9jCtqaU][Machine Learning Zero to Hero (Google I/O'19)]]

[fn:6] [[https://www.books.com.tw/products/F014278520][Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems]]

[fn:5] [[https://github.com/rasbt/python-machine-learning-book-2nd-edition][python-machine-learning-book-2nd-edition]]

[fn:4] [[https://zh.wikipedia.org/zh-tw/AlphaGo_Zero][AlphaGo Zero]]

[fn:3] [[https://zh.wikipedia.org/wiki/AlphaZero][AlphaZero]]
