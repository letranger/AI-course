<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2022-07-03 Sun 20:38 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>感知器與神經網路</title>
<meta name="author" content="Yung-Chin Yen" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="../css/white.css" />
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">感知器與神經網路</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#FromPerceptrionToMLP">1. Story of gate: From perceptron to MLP</a>
<ul>
<li><a href="#orgf0d4f14">1.1. Example #1</a></li>
<li><a href="#org2f386a9">1.2. Example #2</a></li>
<li><a href="#orgcfa82ca">1.3. XOR Problem</a></li>
<li><a href="#orgeb9cab5">1.4. MLP (Multilayer perceptron)</a></li>
</ul>
</li>
<li><a href="#org9328760">2. 類神經網路</a>
<ul>
<li><a href="#org54242cd">2.1. 概念</a></li>
<li><a href="#org5aa3d07">2.2. NN如何學習</a></li>
<li><a href="#org8fed99e">2.3. 為什麼沒有Activation Function的perceptron為linear model</a></li>
<li><a href="#org6eed8f5">2.4. Backprogation: 如何快速計算 w 與 b 的更新</a></li>
<li><a href="#org6e4459e">2.5. Activation Function</a></li>
</ul>
</li>
<li><a href="#orgeb83d0b">3. 感知器(Perception)</a>
<ul>
<li><a href="#org71c0a28">3.1. 何謂感知器</a></li>
<li><a href="#orgd7e8c0c">3.2. 感知器工作原理</a></li>
<li><a href="#orgb57da11">3.3. 執行感知器: 邏輯閘實作</a></li>
</ul>
</li>
<li><a href="#PerceptronDemo">4. 以感知器解決分類問題(監督式學習範例)</a></li>
<li><a href="#org25af096">5. 單層感知器的極限與多層感知器: XOR gate</a>
<ul>
<li><a href="#orgfd9a2b5">5.1. 多層感知器(Multilayer perceptron, MLP): XOR gate 實作</a></li>
</ul>
</li>
<li><a href="#org40e669c">6. 神經網路的學習</a>
<ul>
<li><a href="#orgf60a4ef">6.1. 從資料中學習</a></li>
<li><a href="#orgda43946">6.2. Overfitting v.s. underfitting</a></li>
<li><a href="#org75933f7">6.3. 三層神經網路</a></li>
<li><a href="#orgf76312e">6.4. 損失函數</a></li>
<li><a href="#org0da7f73">6.5. 數值微分</a></li>
<li><a href="#orgee634ad">6.6. Forward propagation</a></li>
<li><a href="#org4d8681f">6.7. Backward propagation</a></li>
<li><a href="#org2bb8588">6.8. 輸出層的設計：恆等函數與 softmax 函數</a></li>
<li><a href="#org34585b6">6.9. softmax 函數的特色</a></li>
<li><a href="#org1fcc3f7">6.10. 梯度</a></li>
<li><a href="#orgb592c2c">6.11. 梯度下降法</a></li>
<li><a href="#orgdc64721">6.12. 學習演算法</a></li>
</ul>
</li>
<li><a href="#orgb3422ea">7. 深度神經網路 DNN (Deep Neural Network)</a>
<ul>
<li><a href="#org5d9e180">7.1. 學習與參數:以迴歸問題為例</a></li>
<li><a href="#org3534063">7.2. 如何調整參數</a></li>
<li><a href="#org6dbb808">7.3. 模型的極限</a></li>
<li><a href="#org1d5b4e1">7.4. 神經網路為什麼要有那麼多層</a></li>
</ul>
</li>
<li><a href="#orgc4682b3">8. CNN 卷積神經網路:&#xa0;&#xa0;&#xa0;<span class="tag"><span class="CNN">CNN</span>&#xa0;<span class=""></span>&#xa0;<span class="Keeras">Keeras</span></span></a>
<ul>
<li><a href="#orge74389f">8.1. CNN v.s. MLP</a></li>
<li><a href="#org0b55554">8.2. CNN 模型架構</a></li>
<li><a href="#org13aea44">8.3. 卷積運算</a></li>
<li><a href="#org986dc0b">8.4. 池化層(Pooling Layer)</a></li>
<li><a href="#org3dc9344">8.5. 以 CNN 實作 MNIST</a></li>
<li><a href="#org1b7480b">8.6. 以 Keras 實作 Cifar-10</a></li>
</ul>
</li>
<li><a href="#org68dd221">9. 辨識手寫數字: MNIST 資料集</a>
<ul>
<li><a href="#org75a6347">9.1. MNIST 資料集</a></li>
<li><a href="#org9795c6c">9.2. MNIST 資料集:以 DNN Sequential 模型為例</a></li>
</ul>
</li>
<li><a href="#orga084c0a">10. 名詞解釋</a>
<ul>
<li><a href="#orgb2db0c6">10.1. Supervised neural networks</a></li>
<li><a href="#org80a8ecb">10.2. Unsupervised Pre-trained Neural Networks</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-FromPerceptrionToMLP" class="outline-2">
<h2 id="FromPerceptrionToMLP"><span class="section-number-2">1.</span> Story of gate: From perceptron to MLP</h2>
<div class="outline-text-2" id="text-FromPerceptrionToMLP">
</div>
<div id="outline-container-orgf0d4f14" class="outline-3">
<h3 id="orgf0d4f14"><span class="section-number-3">1.1.</span> Example #1</h3>
<div class="outline-text-3" id="text-1-1">
</div>
<ol class="org-ol">
<li><a id="orgc5187e6"></a>Question<br />
<div class="outline-text-4" id="text-1-1-1">
<p>
有三個A、一個B，如何進行分類?<br />
</p>

<div id="org6197405" class="figure">
<p><img src="images/2021-05-24_00-48-56.jpg" alt="2021-05-24_00-48-56.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 1: </span>分類任務:問題</p>
</div>
</div>
</li>
<li><a id="orgf364220"></a>Idea<br />
<div class="outline-text-4" id="text-1-1-2">
<p>
最簡單的分類方式是在A和B中間直接找條直線(\(w_1x_1+w_2x_2+b=0\))就可以將A和B完整切出兩個區塊，然後再搭配階梯函數(step function)將&gt;0與&lt;=0分別設為1與0，用來代表類別0與1。該直線方程式如下：<br />
</p>
\begin{equation}
\label{org9d238d7}
y = \begin{cases}
1, & w_1x_1 + w_2x_2-b>0 \\
0, & w_1x_1 + w_2x_2-b\leq0 \\
\end{cases}
\end{equation}
</div>
</li>
<li><a id="org4809042"></a>Solution<br />
<div class="outline-text-4" id="text-1-1-3">
<p>
經過無數的嚐試錯誤，也許我們可以矇到一個如下的方程式<br />
</p>

<div id="org80b4667" class="figure">
<p><img src="images/2021-05-24_00-50-07.jpg" alt="2021-05-24_00-50-07.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 2: </span>分類任務:Solution</p>
</div>

<p>
如果畫成Perceptron的圖:<br />
</p>
<p width="500">
<img src="./images/orGatePerceptron.png" alt="orGatePerceptron.png" width="500" /><br />
如果將圖<a href="#org80b4667">2</a>的四點點代入y(方程式\eqref{org9d238d7}):<br />
</p>
\begin{align*}
A(0,1) \rightarrow y &= f(0,1) = f(1\times0+1\times1-0.5) = f(0.5) = 1 \\
A(1,0) \rightarrow y &= f(1,0) = f(1\times1+1\times0–0.5) = f(0.5) = 1\\
A(1,1) \rightarrow y &= f(1,1) = f(1\times1+1\times1–0.5) = f(1.5) = 1\\
B(0,0) \rightarrow y &= f(0,0) = f(1\times0+1\times0–0.5) = f(-0.5) = 0\\
\end{align*}
</div>
</li>
<li><a id="org4913599"></a>OR gate<br />
<div class="outline-text-4" id="text-1-1-4">
<p>
有點計概基礎的同學，應該可以發現圖<a href="#org6197405">1</a>與OR邏輯閘一致，其真值表如下<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">A</th>
<th scope="col" class="org-right">B</th>
<th scope="col" class="org-right">A OR B</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
</div>
</li>
<li><a id="orgde1bdfc"></a>Python實作<br />
<div class="outline-text-4" id="text-1-1-5">
<p>
上述 OR gate的python實作如下<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">step_function</span>(x):
<span class="linenr"> 4: </span>    <span style="color: #51afef;">return</span> np.array(x&gt;<span style="color: #da8548; font-weight: bold;">0</span>, dtype=np.<span style="color: #c678dd;">int</span>)
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">OR</span>(x1, x2):
<span class="linenr"> 7: </span>    x = np.array([x1, x2])
<span class="linenr"> 8: </span>    w = np.array([<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr"> 9: </span>    b = -<span style="color: #da8548; font-weight: bold;">0.5</span>
<span class="linenr">10: </span>    theta = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">11: </span>    y = np.<span style="color: #c678dd;">sum</span>(w*x) + b
<span class="linenr">12: </span>    <span style="color: #51afef;">return</span> step_function(y)
<span class="linenr">13: </span>
<span class="linenr">14: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"0 OR 0 -&gt; "</span>, OR(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">0</span>))
<span class="linenr">15: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"0 OR 1 -&gt; "</span>, OR(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">16: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"1 OR 0 -&gt; "</span>, OR(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">0</span>))
<span class="linenr">17: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"1 OR 1 -&gt; "</span>, OR(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">1</span>))
</pre>
</div>

<pre class="example">
0 OR 0 -&gt;  0
0 OR 1 -&gt;  1
1 OR 0 -&gt;  1
1 OR 1 -&gt;  1
</pre>
</div>
</li>
</ol>
</div>

<div id="outline-container-org2f386a9" class="outline-3">
<h3 id="org2f386a9"><span class="section-number-3">1.2.</span> Example #2</h3>
<div class="outline-text-3" id="text-1-2">
<p>
上述範例中，我們以瞎貓精神找出了一組solution解決了OR gate的分類問題，請比照辦理，解決AND gate，建構出perceptro，實作出python code.<br />
已知AND gate真值表如下<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">A</th>
<th scope="col" class="org-right">B</th>
<th scope="col" class="org-right">A AND B</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-orgcfa82ca" class="outline-3">
<h3 id="orgcfa82ca"><span class="section-number-3">1.3.</span> XOR Problem</h3>
<div class="outline-text-3" id="text-1-3">
</div>
<ol class="org-ol">
<li><a id="orgc68f02c"></a>Question<br />
<div class="outline-text-4" id="text-1-3-1">
<p>
XOR(互斥或)真值表如下:<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">A</th>
<th scope="col" class="org-right">B</th>
<th scope="col" class="org-right">A XOR B</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">0</td>
<td class="org-right">0</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-right">0</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">0</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-right">1</td>
<td class="org-right">1</td>
<td class="org-right">1</td>
</tr>
</tbody>
</table>
<p>
其輸入/輸出分佈圖為<br />
</p>

<div id="org40da80d" class="figure">
<p><img src="images/2021-05-24_14-15-53.jpg" alt="2021-05-24_14-15-53.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 3: </span>XOR Gate</p>
</div>
</div>
</li>
<li><a id="orgdd8ba19"></a>Idea<br />
<div class="outline-text-4" id="text-1-3-2">
<p>
這個時候一般線性的分類就沒有辦法很完美分割(如下圖)，所以就需要一些變形的方法來達到目的。<br />
</p>

<div id="orgc946b9d" class="figure">
<p><img src="images/2021-05-24_14-18-51.jpg" alt="2021-05-24_14-18-51.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 4: </span>XOR Gate Solution ideas</p>
</div>

<p class="verse">
一個便當吃不飽那就吃兩個阿<br />
&#x2013;馬惠帝<br />
</p>
<p>
即便一個人再如何bumbler，仍有可能提出一些明智的話語，就如同星爺告訴我們的<br />
</p>
<p width="500">
<img src="images/2021-05-24_14-25-08.jpg" alt="2021-05-24_14-25-08.jpg" width="500" /><br />
所以，一條線無法分割&#x2013;那就用兩條啊<br />
</p>
</div>
</li>
<li><a id="orga798868"></a>Solution<br />
<div class="outline-text-4" id="text-1-3-3">
<p width="500">
<img src="images/2021-05-24_14-28-04.jpg" alt="2021-05-24_14-28-04.jpg" width="500" /><br />
如前所述，一條線為一個perceptron，這裡會用到兩個<br />
</p>
<ul class="org-ul">
<li>\(h_1(x) = x_1 + x_2 - 0.5\)<br /></li>
<li>\(h_2(x) = x_1 + x_2 - 1.5\)<br /></li>
</ul>

<div id="orgd73af6c" class="figure">
<p><img src="images/2021-05-24_14-36-07.jpg" alt="2021-05-24_14-36-07.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 5: </span>XOR Gate Solution 2</p>
</div>

<p>
將圖<a href="#orgc946b9d">4</a>的4個點代入\(h_1\):<br />
</p>
\begin{align*}
h_1(0,0) &= f(1\times0+1\times0–0.5) = f(-0.5) = 0\\
h_1(0,1) &= f(1\times0+1\times1-0.5) = f(0.5) = 1\\
h_1(1,0) &= f(1\times1+1\times0–0.5) = f(0.5) = 1\\
h_1(1,1) &= f(1\times1+1\times1–0.5) = f(1.5) = 1\\
\end{align*}

<p>
將圖<a href="#orgc946b9d">4</a>的4個點代入\(h_2\):<br />
</p>
\begin{align*}
h_2(0,0) &= f(1\times0+1\times0–1.5) = f(-1.5) = 0\\
h_2(0,1) &= f(1\times0+1\times1-1.5) = f(-0.5) = 0\\
h_2(1,0) &= f(1\times1+1\times0–1.5) = f(-0.5) = 0\\
h_2(1,1) &= f(1\times1+1\times1–1.5) = f(0.5) = 1\\
\end{align*}

<p>
由上可知:<br />
</p>
<ul class="org-ul">
<li>(0, 0)帶入第1個perceptron \(h_1(0,0)\)輸出-0.5、帶入第2個perceptron \(h_2(0,0)\)輸出-1.5；(-0.5, -1.5)再經由step function轉換輸出(0,0)<br /></li>
<li>(0, 1)帶入第1個perceptron \(h_1(0,1)\)輸出0.5、帶入第2個perceptron \(h_2(0,1)\)輸出-0.5；(0.5, -0.5)再經由step function轉換輸出(1,0)<br /></li>
<li>(1, 0)帶入第1個perceptron \(h_1(1,0)\)輸出0.5、帶入第2個perceptron \(h_2(1,0)\)輸出-0.5；(0.5, -0.5)再經由step function轉換輸出(1,0)<br /></li>
<li>(1, 1)帶入第1個perceptron \(h_1(1,1)\)輸出1.5、帶入第2個perceptron \(h_2(1,1)\)輸出0.5；(1.5, 0.5)再經由step function轉換輸出(1,1)<br /></li>
</ul>

<p>
即<br />
</p>
\begin{align*}
data(0,0) &= f(h_1,h_2) = (0,0) \\
data(0,1) &= f(h_1,h_2) = (1,0) \\
data(1,0) &= f(h_1,h_2) = (1,0) \\
data(1,1) &= f(h_1,h_2) = (1,1) \\
\end{align*}

<p>
這相當於透過兩個perceptron將原本的輸入做特徵空間轉換，如圖<a href="#org25c3b6d">6</a>:<br />
</p>

<div id="org25c3b6d" class="figure">
<p><img src="images/2021-05-24_16-12-36.jpg" alt="2021-05-24_16-12-36.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 6: </span>XOR Gate Solution 3</p>
</div>

<p>
這個時候只要設計一個線性分類器就可以完美分割兩類的資料了阿，如圖<a href="#orgad13443">7</a>:<br />
</p>

<div id="orgad13443" class="figure">
<p><img src="images/2021-05-24_16-13-44.jpg" alt="2021-05-24_16-13-44.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 7: </span>XOR Gate Solution 4</p>
</div>

<p>
XOR問題的神經網路結構如下圖:<br />
</p>

<div id="org7d269f3" class="figure">
<p><img src="images/2021-05-24_16-15-02.jpg" alt="2021-05-24_16-15-02.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 8: </span>XOR Gate Solution 5</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgeb9cab5" class="outline-3">
<h3 id="orgeb9cab5"><span class="section-number-3">1.4.</span> MLP (Multilayer perceptron)</h3>
<div class="outline-text-3" id="text-1-4">
<p>
由XOR問題的例子可以知道，第一層兩個Perceptron在做的事情其實是將資料投影到另一個特徵空間去,最後再把h1和h2的結果當作另一個Perceptron的輸入，再做一個下一層的Perceptron就可以完美分類XOR問題。<br />
</p>

<p>
上例其實就是一個Two-Layer Perceptrons，第一層的Perceptron輸出其實就是每個hidden node，所以如果hidden layer再多一層就是Three-Layer Perceptrons，所以很多層的Perceptrons組合起來就是多層感知機 (Multilayer perceptron, MLP)。MLP其實就是可以用多層和多個Perceptron來達到最後目的，有點像用很多個回歸方法/線性分類器一層一層疊加來達到目的。<br />
</p>

<p>
中間一堆的hidden layer其實就是在做資料的特徵擷取，可以降維，也可以增加維度，而這個過程不是經驗法則去設計，而是由資料去學習得來，最後的輸出才是做分類，所以最後一層也可以用SVM來分類。<br />
</p>

<p>
如果層數再多也可以稱為深度神經網路(deep neural network, DNN)，所以現在稱的DNN其實就是人工神經網路的MLP。有一說法是說因為MLP相關的神經網路在之前因為電腦限制所以performance一直都沒有很好的突破，所以相關研究沒有像SVM這麼的被接受，因此後來Deep learning的聲名大噪，MLP也換個較酷炫的名字(deep neural network)來反轉神經網路這個名稱的聲勢。<br />
</p>

<p>
多層感知機是一種前向傳遞類神經網路，至少包含三層結構(輸入層、隱藏層和輸出層)，並且利用到「倒傳遞」的技術達到學習(model learning)的監督式學習，以上是傳統的定義。現在深度學習的發展，其實MLP是深度神經網路(deep neural network, DNN)的一種special case，概念基本上一樣，DNN只是在學習過程中多了一些手法和層數會更多更深<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>。<br />
</p>

<p>
本章參考來源:<a href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-%E5%A4%9A%E5%B1%A4%E6%84%9F%E7%9F%A5%E6%A9%9F-multilayer-perceptron-mlp-%E9%81%8B%E4%BD%9C%E6%96%B9%E5%BC%8F-f0e108e8b9af">機器學習- 神經網路(多層感知機 Multilayer perceptron, MLP)運作方式</a><br />
</p>
</div>
</div>
</div>

<div id="outline-container-org9328760" class="outline-2">
<h2 id="org9328760"><span class="section-number-2">2.</span> 類神經網路</h2>
<div class="outline-text-2" id="text-2">
</div>
<div id="outline-container-org54242cd" class="outline-3">
<h3 id="org54242cd"><span class="section-number-3">2.1.</span> 概念</h3>
<div class="outline-text-3" id="text-2-1">
<p>
生物神經系統的結構，神經元(Neuron)之間 互相連結，由外部神經元接收信號，再層層 傳導至其他神經元，最後作出反應的過程<br />
</p>

<div id="orgd86b43b" class="figure">
<p><img src="images/Neuron.jpg" alt="Neuron.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 9: </span>神經元與神經網路</p>
</div>
<p width="400">
<img src="images/Perceptron.jpg" alt="Perceptron.jpg" width="400" /><br />
神經網絡是非常重要的一種機器學習機制，它是一種模仿人類大腦學習機制的一種方法。人類的大腦從外界接受刺激，並處理這些輸入（通過神經元處理），最終產生輸出<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>。<br />
當任務變得複雜的時候，大腦會使用多個神經元來形成一個複雜的網絡，並在神經元之間傳遞信息，人工神經網絡就是模仿這種處理機制的一種算法。如下圖的網絡是一種由多個互聯的神經元組成的網絡<sup><a id="fnr.2.100" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>。<br />
</p>


<div id="org6a7add5" class="figure">
<p><img src="images/2021-05-22_23-30-45.jpg" alt="2021-05-22_23-30-45.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 10: </span>DEEP NEURAL NETWORK</p>
</div>

<ul class="org-ul">
<li><p>
圖<a href="#org6a7add5">10</a>中的圓圈就是神經元(perceptron)。每一個perceptron都由weight、bias和activation function(如圖<a href="#orgff28a0c">11</a>)組成。數據由input layer輸入。然後perceptron基於weight和bias，通過線性變換將輸入數據進行轉換。activation function則是用來完成非線性的轉換。數據從輸入層輸入後，會來到隱藏層。隱藏層將會對數據進行處理並輸送到輸出層。這種數據處理方式就是著名的前向傳播。<br />
</p>

<div id="orgff28a0c" class="figure">
<p><img src="images/ActivationFunction.jpg" alt="ActivationFunction.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 11: </span>Activation function</p>
</div></li>

<li><p>
當輸出層的結果和我們期望的結果相差很大怎麼辦呢？在神經網絡中，我們可以基於這種錯誤來更新神經元的權重和偏移。這種處理方式被稱為後向傳播算法。一旦所有的數據經歷過這個處理，最終的權重和便宜就可以用來做預測了<sup><a id="fnr.2.100" class="footref" href="#fn.2" role="doc-backlink">2</a></sup><sup>, </sup><sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>。<br />
</p>

<div id="org5d64441" class="figure">
<p><img src="images/nn.gif" alt="nn.gif" width="500" /><br />
</p>
<p><span class="figure-number">Figure 12: </span>Neural Network</p>
</div></li>
</ul>
</div>
</div>

<div id="outline-container-org5aa3d07" class="outline-3">
<h3 id="org5aa3d07"><span class="section-number-3">2.2.</span> NN如何學習</h3>
<div class="outline-text-3" id="text-2-2">
</div>
<ol class="org-ol">
<li><a id="org9ecaabd"></a>神經網路學習(ANN)的基本步驟<br />
<div class="outline-text-4" id="text-2-2-1">
<p>
利用隨機梯度下降法(SGD)求梯度並更新 weight 和 bias 參數<br />
</p>
<ol class="org-ol">
<li>從訓練資料中隨機選擇一部分資料<br /></li>
<li>構建網路，利用前向傳播，求出輸出值。利用輸出值與目標值得到損失函數(loss Function)，利用損失函數，並用反向傳播方法，求各參數的梯度。<br /></li>
<li>將權重參數沿梯度方向進行微小更新<br /></li>
<li>重複以上步驟(epoch=100, 500, 1000, &#x2026;)直到誤差最小化<br />
(<a href="http://www.feiguyunai.com/index.php/2019/03/31/python-ml-24th-backp/">http://www.feiguyunai.com/index.php/2019/03/31/python-ml-24th-backp/</a>)<br /></li>
</ol>
</div>
</li>
<li><a id="org29281c6"></a>Error Function<br />
<div class="outline-text-4" id="text-2-2-2">
<p>
Error(錯誤) = 真正的值(t) + 預測值(y)<br />
\[ E=\frac{1}{2}\sum^{n}_{i=1}(y_i = t_i)^2\]<br />
\[ E=-\sum_{i=1}^{n}t_i*\log{y_i}\]<br />
</p>
</div>
</li>
<li><a id="orgf46a394"></a>Keras 在 losses.py 定義常用的損失函數<br />
<div class="outline-text-4" id="text-2-2-3">
<ul class="org-ul">
<li><a href="https://github.com/keras-team/keras/blob/master/keras/losses.py">https://github.com/keras-team/keras/blob/master/keras/losses.py</a><br /></li>
<li>MSE: mean_squared_error<br /></li>
<li>MAE: mean_absolute_error<br /></li>
<li>mape = MAPE = mean_absolute_percentage_error<br /></li>
<li>msle = MSLE = mean_squared_logarithmic_error<br /></li>
<li>kld = KLD = kullback_leibler_divergence<br /></li>
<li>cosine = cosine_proximity<br /></li>
</ul>
</div>
</li>
<li><a id="org5adeeb6"></a>誤差實際計算<br />
<div class="outline-text-4" id="text-2-2-4">

<div id="orgd9bc5cc" class="figure">
<p><img src="images/Error-Computation.jpg" alt="Error-Computation.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 13: </span>誤差計算範例</p>
</div>
</div>
</li>
<li><a id="org5fe892f"></a>學習步驟<br />
<div class="outline-text-4" id="text-2-2-5">

<div id="orgd72b59a" class="figure">
<p><img src="images/ANN-1.jpg" alt="ANN-1.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 14: </span>類神經網路</p>
</div>
<ol class="org-ol">
<li>將 weight 與 bias 隨機設定，例如，全設為 0<br /></li>
<li>計算出模型結果值<br /></li>
<li>計算出誤差：即計算結果與真實值之差異<br /></li>
</ol>

<div id="orgf9ec224" class="figure">
<p><img src="images/ANN-4.jpg" alt="ANN-4.jpg" /><br />
</p>
<p><span class="figure-number">Figure 15: </span>類神經網路</p>
</div>
<ol class="org-ol">
<li>定義 weight 與 bias 的更新策略(update the weights and bias)，其更新策略為從錯誤中學習。學習規則為<br />
\[\Delta w=\eta X^T\cdot (\hat{y} - y)\]: \(w\)(新的)=\(w\)(前一回)+\(\Delta w\)(誤差變動)<br />
\[\Delta b=\eta (\hat{y} - y)\]: \(b\)(新的)=\(b\)(前靣)+\(\Delta b\)(誤差變動)<br /></li>
</ol>

<div id="org6461040" class="figure">
<p><img src="images/ANN-5.jpg" alt="ANN-5.jpg" /><br />
</p>
<p><span class="figure-number">Figure 16: </span>類神經網路</p>
</div>
<ol class="org-ol">
<li>使用新的 weight 與 bias，計算直到誤差很小(定義何時結束)<br />
\[\Delta w=\eta X^T\cdot (\hat{y} - y)\]: \(w\)(新的)=\(w\)(前一回)+\(\Delta w\)(誤差變動)<br />
\[\Delta b=\eta (\hat{y} - y)\]: \(b\)(新的)=\(b\)(前靣)+\(\Delta b\)(誤差變動)<br />
\[\eta\] = 學習率<br /></li>
</ol>
</div>
</li>
</ol>
</div>

<div id="outline-container-org8fed99e" class="outline-3">
<h3 id="org8fed99e"><span class="section-number-3">2.3.</span> 為什麼沒有Activation Function的perceptron為linear model</h3>
<div class="outline-text-3" id="text-2-3">
<p>
思考下方堆疊兩層感知器的例子(這裡我們捨棄偏權值b，使問題單純化)：<br />
</p>

<div id="org6c01942" class="figure">
<p><img src="images/2perceptrons.png" alt="2perceptrons.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 17: </span>Stacked perceptrons</p>
</div>

<p>
其中<br />
\[
y_1 = w_1^1x_1 + w_2^1x_2 \\
y_2 = w_3^1x_1 + w_4^1x_2 \\
\hat{y} = w_1^2y_1 + w_2^2y_2
\]<br />
若將\(y_1\)與\(y_2\)代入\(\hat{y}\)，可以發現仍然是原始數據的線性組合，代表即使堆疊多層的感知器時，若沒有使用激活函數，本質上可以被單層感知器取代，如下<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>：<br />
\[\hat{y} = w_1^2y_1 + w_2^2y_2  = (w_1^1w_1^2 + w_2^1x_2^2)x_1 + (w_3^1w_1^2 + w_4^1x_2^2)x_2\]<br />
</p>
</div>
</div>

<div id="outline-container-org6eed8f5" class="outline-3">
<h3 id="org6eed8f5"><span class="section-number-3">2.4.</span> Backprogation: 如何快速計算 w 與 b 的更新</h3>
<div class="outline-text-3" id="text-2-4">
</div>
<ol class="org-ol">
<li><a id="org34ce8d0"></a>Godfather of Deep Learning<br />
<div class="outline-text-4" id="text-2-4-1">
<ul class="org-ul">
<li>1986: the backpropagation algorithm for training multi-layer neural networks<br /></li>
<li>Hinton, G. E., Osindero, S. and Teh, Y. (2006), A fast learning algorithm for deep belief nets. Neural Computation, 18, pp. 1527-1554<br /></li>
<li>2012: AlexNet (Alex Krizhevsky)啟動 AI/NN 電腦視覺熱潮<br /></li>
<li>2018: Turing Prize (Yoshua Bengio and Yann LeCun)<br /></li>
</ul>
<p>
(<a href="https://www.cs.toronto.edu/~hinton/coursera_slides.html">https://www.cs.toronto.edu/~hinton/coursera_slides.html</a>)<br />
</p>
</div>
</li>
<li><a id="org5f6eb6b"></a>倒遞神經網路(Back-Propagation) 數學推導<br />
<ol class="org-ol">
<li><a id="org898d672"></a><a href="https://honglung.pixnet.net/blog/post/202201656-back-propagation-%E6%95%B8%E5%AD%B8%E6%8E%A8%E5%B0%8E">數學推導</a><br />
<div class="outline-text-5" id="text-2-4-2-1">
<ol class="org-ol">
<li>背景: 以下 a,b,c 為輸入值.並且激勵函數(activity function) 採用 sigmoid 進行說明:<br /></li>
</ol>
<p width="400">
<img src="images/Backprogation-1.png" alt="Backprogation-1.png" width="400" /><br />
依上圖所述，數學式如下：<br />
\[ z = aw_a + bw_b + cw_c \]<br />
\[ \delta (z) = \frac{1}{1+e^{-x}} \]<br />
</p>
<ol class="org-ol">
<li>由於在訓練過程中目的是找到最適合的權重值(weight).可透過微分求解梯度後可以找到權重應該修正的數值.因為每個變數都需要各自進行微分,因此透過偏微分求解(在遇到非目標為分數值時,將其視為常數):<br />
\[ \frac{\vartheta z}{\vartheta w_a} = a ,  \frac{\vartheta z}{\vartheta w_b} = b, \frac{\vartheta z}{\vartheta w_c} = c\]<br />
\[ \delta '(z) = (1-\delta (z)) \cdot \delta (z) \]<br /></li>
<li>得知以上各個偏微分數值後,透過連鎖律進行神經元的偏微分,可得到以下式子:<br />
\[\frac{\vartheta f}{\vartheta w_a} = \frac {\vartheta f}{\vartheta z} \cdot \frac{\vartheta z}{\vartheta w_a} = [(1-\delta (z)) \cdot \delta (z)] \cdot a\]<br /></li>
</ol>
</div>
</li>
</ol>
</li>
</ol>
</div>

<div id="outline-container-org6e4459e" class="outline-3">
<h3 id="org6e4459e"><span class="section-number-3">2.5.</span> Activation Function</h3>
<div class="outline-text-3" id="text-2-5">
<p>
Activation function是神經網絡中極其重要的概念。它們決定了某個神經元是否被activate，這個神經元接受到的信息是否是有用的，是否該留下或者是該拋棄。<br />
Activation function的形式如下<sup><a id="fnr.2.100" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>：<br />
</p>
<ul class="org-ul">
<li>在神經元裡定義一些權重(weight)，經過計算之後，判斷結果是否超過一個閾值(threshold)，如果超過，神經元輸出為 1；反之，則輸出 0。<br /></li>
<li>如果不用Activation function，每一層output都是上層input的線性函數，無論神經網絡有多少層，輸出都是輸入的線性組合。<br /></li>
<li>加入非線性的function就能改變線性問題<br /></li>
</ul>
</div>

<ol class="org-ol">
<li><a id="org10a1966"></a>Step function<br />
<div class="outline-text-4" id="text-2-5-1">
<p>
Step function，若輸入超過 0，則輸出 1；若輪入小於等於 0，則輸出 0，如公式\eqref{orge5ccfc9}所示，其函數圖形則如圖<a href="#org43752c2">18</a>所示。<br />
</p>
\begin{equation}
\label{orge5ccfc9}
h(x) = \begin{cases}
1 & \text{if } x > 0 \\
0 & \text{if } x\leq 0
\end{cases}
\end{equation}
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">step_function</span>(x):
<span id="coderef-why" class="coderef-off"><span class="linenr"> 5: </span>    <span style="color: #51afef;">return</span> np.array(x&gt;<span style="color: #da8548; font-weight: bold;">0</span>, dtype=np.<span style="color: #c678dd;">int</span>)</span>
<span class="linenr"> 6: </span>
<span class="linenr"> 7: </span>x = np.arange(-<span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>)
<span class="linenr"> 8: </span>y = step_function(x)
<span class="linenr"> 9: </span>plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">10: </span>plt.plot(x, y)
<span class="linenr">11: </span>plt.ylim(-<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">1.1</span>)
<span class="linenr">12: </span>plt.savefig(<span style="color: #98be65;">"images/stepFuncPlot.png"</span>)
<span class="linenr">13: </span><span style="color: #51afef;">return</span> <span style="color: #98be65;">"images/stepFuncPlot.png"</span>
</pre>
</div>

<div id="org43752c2" class="figure">
<p><img src="images/stepFuncPlot.png" alt="stepFuncPlot.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 18: </span>階梯函數圖</p>
</div>
<ul class="org-ul">
<li>為什麼第<a href="#coderef-why" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-why');" onmouseout="CodeHighlightOff(this, 'coderef-why');">5</a>行(np.array(x&gt;0, dtype=np.int))這樣寫會變成0/1<br /></li>
<li>當調整參數時，節點輸出的值在 0 和 1 之間躍遷，對調整參數造成很大不便。<br /></li>
</ul>
</div>
</li>

<li><a id="org4c25c7e"></a>Sigmoid 函數<br />
<div class="outline-text-4" id="text-2-5-2">
<p>
公式\eqref{org8edd1e7}即為 sigmoid 函數(sigmoid function)，其中的\(exp(-x)\)代表\(e^{-x}\)，\(e\)為納皮爾常數(Napier&rsquo;s constant)2.71828&#x2026;的實數。<br />
</p>
\begin{equation}
\label{org8edd1e7}
h(x) = \frac{1}{1+exp(-x)} = \frac{1}{1+e^{-x}}
\end{equation}

<p>
sigmoid 函數的 python 實作如下所述，而其圖形結果為平滑曲線(圖<a href="#org391fe04">19</a>)，針對輸入產生連續性的輸出，但仍與階梯函數相同，以 0 為界線，這種平滑度對於神經網路有相當重要的意義。此外，step function只能回傳 0 或 1，而 sigmoid 函數可以回傳實數。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 4: </span>    <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 5: </span><span style="color: #dcaeea;">x</span> = np.arange(-<span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>)
<span class="linenr"> 6: </span><span style="color: #dcaeea;">y</span> = sigmoid(x)
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(y)
<span class="linenr"> 8: </span>plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 9: </span>plt.plot(x, y)
<span class="linenr">10: </span>plt.ylim(-<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">1.1</span>)
<span class="linenr">11: </span>plt.savefig(<span style="color: #98be65;">"images/sigmoidplot2.png"</span>)
<span class="linenr">12: </span><span style="color: #51afef;">return</span> <span style="color: #98be65;">"images/sigmoidplot2.png"</span>
</pre>
</div>

<div id="org391fe04" class="figure">
<p><img src="images/sigmoidplot2.png" alt="sigmoidplot2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 19: </span>sigmoid 函數圖</p>
</div>
<ul class="org-ul">
<li>非線性功能<br /></li>
<li>輸出：（0,1）<br /></li>
<li>處理極端值<br /></li>
<li>常用於隱藏層和輸出層<br /></li>
<li>常用於二元分類<br /></li>
<li>梯度消失問題<br /></li>
</ul>
</div>
</li>

<li><a id="org116aa5a"></a>ReLU 函數<br />
<div class="outline-text-4" id="text-2-5-3">
<p>
神經網路感知器使用的活化函數多為非線性函數，如階梯函數與 sigmoid 函數，其主要原因在於，若在神經網路中使用線性函數，則不論加深多少層，這些函數都能合併為一個單一函數。例如，一個以\(h(x)=cx\)為活化函數的三層網路，其執行結果便相當於\(y(x)=h(h(h(x)))\)，亦即，其執行網果就如同一個以\(y(h)=ax\)為活化函數的一層網路，其中\(a=c^{3}\)。<br />
</p>

<p>
雖然 sigmoid 函數很早就應用於神經網路中，但最近較常使用的為 ReLU (Rectified Linear Unit)函數，若輸入超過 0，則直接輸出；若輪入小於 0，則輸出 0，如公式\eqref{orgfcc3b72}所示，其函數圖形則如圖<a href="#org3ebb273">20</a>所示。<br />
</p>
\begin{equation}
\label{orgfcc3b72}
h(x) = \begin{cases} x & \text{if } x > 0
\\ 0 & \text{if } x\leq 0 \end{cases}
\end{equation}
<p>
or<br />
\[h(x) = max(0,x)\]<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">relu</span>(x):
<span class="linenr"> 4: </span>    <span style="color: #51afef;">return</span> np.maximum(<span style="color: #da8548; font-weight: bold;">0</span>, x)
<span class="linenr"> 5: </span><span style="color: #dcaeea;">x</span> = np.arange(-<span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">5.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>)
<span class="linenr"> 6: </span><span style="color: #dcaeea;">y</span> = relu(x)
<span class="linenr"> 7: </span><span style="color: #c678dd;">print</span>(y)
<span class="linenr"> 8: </span>plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr"> 9: </span>plt.plot(x, y)
<span class="linenr">10: </span>plt.ylim(-<span style="color: #da8548; font-weight: bold;">0.5</span>, <span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">11: </span>plt.savefig(<span style="color: #98be65;">"images/ReLUPlot.png"</span>)
<span class="linenr">12: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">return "ReLUPlot.png"</span>
</pre>
</div>

<div id="org3ebb273" class="figure">
<p><img src="images/ReLUPlot.png" alt="ReLUPlot.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 20: </span>ReLU 函數圖</p>
</div>
<ul class="org-ul">
<li>非線性功能<br /></li>
<li>輸出範圍：（0，\(\infty\)）<br /></li>
<li>捨棄一些信息<br /></li>
<li>常用於隱藏層<br /></li>
<li>梯度爆炸問題<br /></li>
</ul>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgeb83d0b" class="outline-2">
<h2 id="orgeb83d0b"><span class="section-number-2">3.</span> 感知器(Perception)</h2>
<div class="outline-text-2" id="text-3">
</div>
<div id="outline-container-org71c0a28" class="outline-3">
<h3 id="org71c0a28"><span class="section-number-3">3.1.</span> 何謂感知器</h3>
<div class="outline-text-3" id="text-3-1">
<p class="verse">
Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks.<br />
</p>
<p>
收到多個輸入訊號之後，再當作一個訊號輸出，如圖<a href="#orga817778">21</a>所示，\(x_1, x_2\)為輸入訊號，\(y\)為輸出訊號，\(w_1, w_2\)代表權重(weight)，圖中的圓圈稱為「神經元」或稱作「節點」。神經元\(x_1, x_2\)的訊號是否會觸發神經元\(y\)使其輸出訊號則取決於\(w_1x_1+w_2x_2\)是否會超過某個臨界值\(\theta\)。<br />
</p>

<div id="orga817778" class="figure">
<p><img src="images/sensor1.png" alt="sensor1.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 21: </span>收到兩組輸入訊號的感知器</p>
</div>

<p>
若以算式表示此一觸發條件則如公式\eqref{org26847da}所示。<br />
</p>
\begin{equation}
\label{org26847da}
y = \begin{cases} 0 & \text (w_1x_1+w_2x_2 \leq 0)
\\ 1 & \text (w_1x_1 + w_2x_2 > 0) \end{cases}
\end{equation}
</div>
</div>

<div id="outline-container-orgd7e8c0c" class="outline-3">
<h3 id="orgd7e8c0c"><span class="section-number-3">3.2.</span> 感知器工作原理</h3>
<div class="outline-text-3" id="text-3-2">
</div>
<ol class="org-ol">
<li><a id="org715146a"></a>Version #1: 使用weight<br />
<div class="outline-text-4" id="text-3-2-1">
<p>
感知器（perceptron）是 人造神經元（artificial neuron）的一種，也是最基本的一種。它接受一些輸入，產生一個輸出。<br />
</p>

<div id="orgbbd1831" class="figure">
<p><img src="./images/perceptron-1.png" alt="perceptron-1.png" width="400" /><br />
</p>
<p><span class="figure-number">Figure 22: </span>Perceptron version 1</p>
</div>
<ul class="org-ul">
<li>這種架構的輸入/輸出關係為線性<br /></li>
<li>神經網路中再多的線性perceptron叠加，仍為線性<br /></li>
<li>無法解決 <b>線性不可分</b> 的問題<br /></li>
</ul>
</div>
<ol class="org-ol">
<li><a id="org6d1de04"></a>線性可分 v.s. 線性不可分<br />
<div class="outline-text-5" id="text-3-2-1-1">
<p>
簡而言之，如果存在一個超平面完全分離H元素和M元素，那麼上面的表達式表示H和M是線性可分的<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>。<br />
</p>

<div id="orge1bc4c3" class="figure">
<p><img src="images/2021-05-23_14-10-30.jpg" alt="2021-05-23_14-10-30.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 23: </span>線性和非線性分類</p>
</div>

<p>
在圖<a href="#orge1bc4c3">23</a>中，A顯示了一個線性分類問題，B顯示了一個非線性的分類問題。在A中，我們的決策邊界是一個線性的，它將藍色的點和綠色的點完全分開。在這個場景中，可以實現幾個線性分類器。<br />
</p>
</div>
</li>
<li><a id="org4cc9496"></a>低維映射至高維<br />
<div class="outline-text-5" id="text-3-2-1-2">
<p>
在SVM的解決方案中，可以透過一個非線性的映射將低維空間線性不可分的樣本轉換至高維空間，使其成為線性可分   <sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup>，例如:<br />
</p>

<div id="orgc17456d" class="figure">
<p><img src="images/2021-05-23_14-14-31.jpg" alt="2021-05-23_14-14-31.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 24: </span>Kernal function mapping</p>
</div>
</div>
</li>
</ol>
</li>

<li><a id="orgbd81f05"></a>Version #2: 加入bias<br />
<div class="outline-text-4" id="text-3-2-2">

<div id="orgd777657" class="figure">
<p><img src="./images/perceptron-2.png" alt="perceptron-2.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 25: </span>Perceptron version 2</p>
</div>
<ul class="org-ul">
<li>不加 bias 你的分類線(面)就必須過原點，這顯然是不靈活的<br /></li>
<li>透過bias，可以將NN進行左右調整，以適應(fit)更多情況<br /></li>
<li>可以將bias視為一個activate perceptron的threshold<br /></li>
<li>bias也可以視為當輸入均為0時的輸出值<br /></li>
<li>從仿生學的角度，刺激生物神經元使它興奮需要刺激強度超過一定的閾值，同樣神經元模型也仿照這點設置了bias<br /></li>
</ul>
</div>
</li>

<li><a id="org9bb3a34"></a>Version #3: 加入activation function<br />
<div class="outline-text-4" id="text-3-2-3">
<p>
加入activation function<br />
</p>

<div id="org2344b8d" class="figure">
<p><img src="./images/perceptron-3.png" alt="perceptron-3.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 26: </span>Perceptron version 2</p>
</div>
</div>
</li>

<li><a id="orgcacfe86"></a>結論: 什麼是perceptron<br />
<div class="outline-text-4" id="text-3-2-4">
<p>
其實Perceptron就只是一個兩層的神經網路，輸入層和輸出層<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgb57da11" class="outline-3">
<h3 id="orgb57da11"><span class="section-number-3">3.3.</span> 執行感知器: 邏輯閘實作</h3>
<div class="outline-text-3" id="text-3-3">
</div>
<ol class="org-ol">
<li><a id="org3b7e156"></a>AND gate(版本#1：只設定權重)<br />
<div class="outline-text-4" id="text-3-3-1">
<p>
以上述感知器的運作方式來模擬 AND 邏輯閘的功能，可由以下程式碼實現出來。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">AND</span>(x1, x2):
<span class="linenr"> 2: </span>    <span style="color: #dcaeea;">w1</span>, <span style="color: #dcaeea;">w2</span>, <span style="color: #dcaeea;">theta</span> = <span style="color: #da8548; font-weight: bold;">0.5</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>, <span style="color: #da8548; font-weight: bold;">0.7</span>
<span class="linenr"> 3: </span>    <span style="color: #dcaeea;">tmp</span> = x1*w1 + x2*w2
<span class="linenr"> 4: </span>    <span style="color: #51afef;">if</span> tmp &lt;= theta:
<span class="linenr"> 5: </span>        <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 6: </span>    <span style="color: #51afef;">else</span>:
<span class="linenr"> 7: </span>        <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr"> 8: </span>
<span id="coderef-run-and" class="coderef-off"><span class="linenr"> 9: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"AND(1, 0): "</span>, AND(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">0</span>))</span>
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"AND(1, 0): "</span>, AND(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">1</span>))
</pre>
</div>

<pre class="example">
AND(1, 0):  0
AND(1, 0):  1
</pre>


<p>
如程式碼第<a href="#coderef-run-and" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-run-and');" onmouseout="CodeHighlightOff(this, 'coderef-run-and');">9</a>行所示，傳入\(1, 0\)為輸入訊號，而根據計算結果是否達到臨界值(\(\theta=0.7\))來決定最終的輸出訊號\(0\)。<br />
</p>
</div>
</li>

<li><a id="org37a7f2c"></a>AND gate(版本#2: 導入權重及偏權值)<br />
<div class="outline-text-4" id="text-3-3-2">
<p>
在版本#1 的實作中，我們只透過輸入訊號與權重的計算來實現感知器的運作，事實上，這個感知器也可以用公式\eqref{orgae0c0d9}來表示，這裡利用\(b\)這個被稱作「偏移值」或「偏權值」的參數來控制神經元的觸發難度，此時的感知器如圖<a href="#org437f647">27</a>所示。<br />
</p>
\begin{equation}
\label{orgae0c0d9}
y = \begin{cases} 0 & \text (b+w_1x_1+w_2x_2 \leq 0)
\\ 1 & \text (b+w_1x_1 + w_2x_2 > 0) \end{cases}
\end{equation}


<div id="org437f647" class="figure">
<p><img src="images/biassensor2.png" alt="biassensor2.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 27: </span>加入偏移值\(b\)的感知器</p>
</div>

<p>
在圖<a href="#org437f647">27</a>中，增加了一個權重為\(b\)、輸入值為 1 的訊號，偏權值的作用與權重\(w_1\)及\(w_2\)不同，\(w_1\)與\(w_2\)的功能是控制輸入訊號重要程度的參數，但是\(b\)的功能是調整輸出訊號 1 的參數，偏權值也包含了「偏移」的意思。這是指在沒有輸入時，輸出究竟會產生多少偏移量。此外，由於偏移值的輸入訊號固定為 1，以圖形表示時，會用灰色填滿神經元，藉此區隔其他神經元。<br />
</p>

<p>
以下為針對上述版本#1 的 AND 邏輯閘模擬程式碼進行的修正，將偏移值的機制加入感知器的運作流程中。<br />
</p>

<div id="org952626c" class="figure">
<p><img src="images/AndGate.jpg" alt="AndGate.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 28: </span>And Gate perceptron</p>
</div>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">AND</span>(x1, x2):
<span class="linenr"> 3: </span>      <span style="color: #dcaeea;">x</span> = np.array([x1, x2])
<span class="linenr"> 4: </span>      <span style="color: #dcaeea;">w</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.5</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>])
<span class="linenr"> 5: </span>      <span style="color: #dcaeea;">b</span> = -<span style="color: #da8548; font-weight: bold;">0.7</span>
<span class="linenr"> 6: </span>      <span style="color: #dcaeea;">theta</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 7: </span>      <span style="color: #dcaeea;">tmp</span> = np.<span style="color: #c678dd;">sum</span>(w*x) + b
<span class="linenr"> 8: </span>      <span style="color: #51afef;">if</span> tmp &lt;= theta:
<span class="linenr"> 9: </span>          <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">10: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">11: </span>          <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">12: </span>
<span class="linenr">13: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"1 AND 0 -&gt; "</span>,AND(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">0</span>))
<span class="linenr">14: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"1 AND 1 -&gt; "</span>,AND(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">15: </span>
<span class="linenr">16: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">draw corresponding dots</span>
<span class="linenr">17: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">18: </span>  <span style="color: #dcaeea;">xs</span> = np.array([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">19: </span>  <span style="color: #dcaeea;">ys</span> = np.array([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">20: </span>  <span style="color: #51afef;">for</span> xi, yi <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(xs, ys):
<span class="linenr">21: </span>      <span style="color: #c678dd;">print</span>(AND(x, y))
<span class="linenr">22: </span>
</pre>
</div>

<pre class="example">
1 AND 0 -&gt;  0
1 AND 1 -&gt;  1
</pre>
</div>
</li>

<li><a id="orgb106c5f"></a>OR, NAND gates<br />
<div class="outline-text-4" id="text-3-3-3">
<p>
同樣的感知器架構亦可實作出 OR 邏輯閘的功能：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">OR</span>(x1, x2):
<span class="linenr"> 3: </span>      <span style="color: #dcaeea;">x</span> = np.array([x1, x2])
<span class="linenr"> 4: </span>      <span style="color: #dcaeea;">w</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.5</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>])
<span class="linenr"> 5: </span>      <span style="color: #dcaeea;">b</span> = -<span style="color: #da8548; font-weight: bold;">0.2</span>
<span class="linenr"> 6: </span>      <span style="color: #dcaeea;">theta</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 7: </span>      <span style="color: #dcaeea;">tmp</span> = np.<span style="color: #c678dd;">sum</span>(w*x) + b
<span class="linenr"> 8: </span>      <span style="color: #51afef;">if</span> tmp &lt;= theta:
<span class="linenr"> 9: </span>          <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">10: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">11: </span>          <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">12: </span>  <span style="color: #c678dd;">print</span>(OR(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">0</span>))
</pre>
</div>

<p>
也可實作出 NAND 邏輯閘的原理。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">NAND</span>(x1, x2):
<span class="linenr"> 3: </span>      <span style="color: #dcaeea;">x</span> = np.array([x1, x2])
<span class="linenr"> 4: </span>      <span style="color: #dcaeea;">w</span> = np.array([-<span style="color: #da8548; font-weight: bold;">0.5</span>, -<span style="color: #da8548; font-weight: bold;">0.5</span>])
<span class="linenr"> 5: </span>      <span style="color: #dcaeea;">b</span> = <span style="color: #da8548; font-weight: bold;">0.7</span>
<span class="linenr"> 6: </span>      <span style="color: #dcaeea;">tmp</span> = np.<span style="color: #c678dd;">sum</span>(w*x) + b
<span class="linenr"> 7: </span>      <span style="color: #51afef;">if</span> tmp &lt;= <span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr"> 8: </span>          <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 9: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">10: </span>          <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">11: </span>  <span style="color: #c678dd;">print</span>(NAND(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">0</span>))
</pre>
</div>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-PerceptronDemo" class="outline-2">
<h2 id="PerceptronDemo"><span class="section-number-2">4.</span> 以感知器解決分類問題(監督式學習範例)</h2>
<div class="outline-text-2" id="text-PerceptronDemo">
<p>
從「鳶尾花資料集」取出兩類花(Setosa, Versicolor)，藉由不同的兩個屬性（花萼長、花瓣長）對其行分類，如圖<a href="#org8c0340e">29</a>。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  3: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">  4: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr">  5: </span>  <span style="color: #51afef;">from</span> matplotlib.colors <span style="color: #51afef;">import</span> ListedColormap
<span class="linenr">  6: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#24863;&#30693;&#22120;&#27169;&#22411;</span>
<span class="linenr">  7: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">Perceptron</span>(<span style="color: #c678dd;">object</span>):
<span class="linenr">  8: </span>      <span style="color: #83898d;">"""Perceptron classifier.</span>
<span class="linenr">  9: </span><span style="color: #83898d;">      &#21443;&#25976;&#65306;</span>
<span class="linenr"> 10: </span><span style="color: #83898d;">      ------------</span>
<span class="linenr"> 11: </span><span style="color: #83898d;">      eta : float: &#23416;&#32722;&#29575; (0.0 ~ 1.0)</span>
<span class="linenr"> 12: </span><span style="color: #83898d;">      n_iter : int: &#35347;&#32244;&#27425;&#25976;</span>
<span class="linenr"> 13: </span><span style="color: #83898d;">        Passes over the training dataset.</span>
<span class="linenr"> 14: </span><span style="color: #83898d;">      random_state : int</span>
<span class="linenr"> 15: </span><span style="color: #83898d;">      &#23660;&#24615;</span>
<span class="linenr"> 16: </span><span style="color: #83898d;">      -----------</span>
<span class="linenr"> 17: </span><span style="color: #83898d;">      w_ : 1d-array: &#35347;&#32244;&#24460;&#30340;&#27402;&#37325;</span>
<span class="linenr"> 18: </span><span style="color: #83898d;">        Weights after fitting.</span>
<span class="linenr"> 19: </span><span style="color: #83898d;">      errors_ : list: &#27599;&#27425;&#35347;&#32244;&#30340;&#37679;&#35492;&#27425;&#25976;</span>
<span class="linenr"> 20: </span>
<span class="linenr"> 21: </span><span style="color: #83898d;">      """</span>
<span class="linenr"> 22: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, eta=<span style="color: #da8548; font-weight: bold;">0.01</span>, n_iter=<span style="color: #da8548; font-weight: bold;">50</span>, random_state=<span style="color: #da8548; font-weight: bold;">1</span>):
<span class="linenr"> 23: </span>          <span style="color: #51afef;">self</span>.eta = eta
<span class="linenr"> 24: </span>          <span style="color: #51afef;">self</span>.n_iter = n_iter
<span class="linenr"> 25: </span>          <span style="color: #51afef;">self</span>.random_state = random_state
<span class="linenr"> 26: </span>
<span class="linenr"> 27: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">fit</span>(<span style="color: #51afef;">self</span>, X, y):
<span class="linenr"> 28: </span>          <span style="color: #83898d;">"""Fit training data.</span>
<span class="linenr"> 29: </span><span style="color: #83898d;">          Parameters</span>
<span class="linenr"> 30: </span><span style="color: #83898d;">          ----------</span>
<span class="linenr"> 31: </span><span style="color: #83898d;">          X : {array-like}, shape = [n_samples, n_features]</span>
<span class="linenr"> 32: </span><span style="color: #83898d;">            Training vectors, where n_samples is the number of samples and</span>
<span class="linenr"> 33: </span><span style="color: #83898d;">            n_features is the number of features.</span>
<span class="linenr"> 34: </span><span style="color: #83898d;">          y : array-like, shape = [n_samples]</span>
<span class="linenr"> 35: </span><span style="color: #83898d;">            Target values.</span>
<span class="linenr"> 36: </span>
<span class="linenr"> 37: </span><span style="color: #83898d;">          Returns</span>
<span class="linenr"> 38: </span><span style="color: #83898d;">          -------</span>
<span class="linenr"> 39: </span><span style="color: #83898d;">          self : object</span>
<span class="linenr"> 40: </span>
<span class="linenr"> 41: </span><span style="color: #83898d;">          """</span>
<span class="linenr"> 42: </span>          rgen = np.random.RandomState(<span style="color: #51afef;">self</span>.random_state)
<span class="linenr"> 43: </span>          <span style="color: #51afef;">self</span>.w_ = rgen.normal(loc=<span style="color: #da8548; font-weight: bold;">0.0</span>, scale=<span style="color: #da8548; font-weight: bold;">0.01</span>, size=<span style="color: #da8548; font-weight: bold;">1</span> + X.shape[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr"> 44: </span>          <span style="color: #51afef;">self</span>.errors_ = []
<span class="linenr"> 45: </span>
<span class="linenr"> 46: </span>          <span style="color: #51afef;">for</span> _ <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #51afef;">self</span>.n_iter):
<span class="linenr"> 47: </span>              errors = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 48: </span>              <span style="color: #51afef;">for</span> xi, target <span style="color: #51afef;">in</span> <span style="color: #c678dd;">zip</span>(X, y):
<span class="linenr"> 49: </span>                  update = <span style="color: #51afef;">self</span>.eta * (target - <span style="color: #51afef;">self</span>.predict(xi))
<span class="linenr"> 50: </span>                  <span style="color: #51afef;">self</span>.w_[<span style="color: #da8548; font-weight: bold;">1</span>:] += update * xi
<span class="linenr"> 51: </span>                  <span style="color: #51afef;">self</span>.w_[<span style="color: #da8548; font-weight: bold;">0</span>] += update
<span class="linenr"> 52: </span>                  errors += <span style="color: #c678dd;">int</span>(update != <span style="color: #da8548; font-weight: bold;">0.0</span>)
<span class="linenr"> 53: </span>              <span style="color: #51afef;">self</span>.errors_.append(errors)
<span class="linenr"> 54: </span>          <span style="color: #51afef;">return</span> <span style="color: #51afef;">self</span>
<span class="linenr"> 55: </span>
<span class="linenr"> 56: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">net_input</span>(<span style="color: #51afef;">self</span>, X):
<span class="linenr"> 57: </span>          <span style="color: #83898d;">"""Calculate net input"""</span>
<span class="linenr"> 58: </span>          <span style="color: #51afef;">return</span> np.dot(X, <span style="color: #51afef;">self</span>.w_[<span style="color: #da8548; font-weight: bold;">1</span>:]) + <span style="color: #51afef;">self</span>.w_[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr"> 59: </span>
<span class="linenr"> 60: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(<span style="color: #51afef;">self</span>, X):
<span class="linenr"> 61: </span>          <span style="color: #83898d;">"""Return class label after unit step"""</span>
<span class="linenr"> 62: </span>          <span style="color: #51afef;">return</span> np.where(<span style="color: #51afef;">self</span>.net_input(X) &gt;= <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, -<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 63: </span>
<span class="linenr"> 64: </span>  v1 = np.array([<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>])
<span class="linenr"> 65: </span>  v2 = <span style="color: #da8548; font-weight: bold;">0.5</span> * v1
<span class="linenr"> 66: </span>  np.arccos(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
<span class="linenr"> 67: </span>
<span class="linenr"> 68: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35712;&#20837;&#35347;&#32244;&#38598;&#36039;&#26009;</span>
<span class="linenr"> 69: </span>  df = pd.read_csv(<span style="color: #98be65;">'https://archive.ics.uci.edu/ml/'</span>
<span class="linenr"> 70: </span>          <span style="color: #98be65;">'machine-learning-databases/iris/iris.data'</span>, header=<span style="color: #a9a1e1;">None</span>)
<span class="linenr"> 71: </span>  <span style="color: #c678dd;">print</span>(df.tail())
<span class="linenr"> 72: </span>
<span class="linenr"> 73: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">select setosa and versicolor</span>
<span class="linenr"> 74: </span>  y = df.iloc[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">100</span>, <span style="color: #da8548; font-weight: bold;">4</span>].values
<span class="linenr"> 75: </span>  y = np.where(y == <span style="color: #98be65;">'Iris-setosa'</span>, -<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 76: </span>
<span class="linenr"> 77: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">extract sepal length and petal length</span>
<span class="linenr"> 78: </span>  X = df.iloc[<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #da8548; font-weight: bold;">100</span>, [<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">2</span>]].values
<span class="linenr"> 79: </span>
<span class="linenr"> 80: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot data</span>
<span class="linenr"> 81: </span>  plt.clf()
<span class="linenr"> 82: </span>  plt.scatter(X[:<span style="color: #da8548; font-weight: bold;">50</span>, <span style="color: #da8548; font-weight: bold;">0</span>], X[:<span style="color: #da8548; font-weight: bold;">50</span>, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 83: </span>              color=<span style="color: #98be65;">'red'</span>, marker=<span style="color: #98be65;">'o'</span>, label=<span style="color: #98be65;">'setosa'</span>)
<span class="linenr"> 84: </span>  plt.scatter(X[<span style="color: #da8548; font-weight: bold;">50</span>:<span style="color: #da8548; font-weight: bold;">100</span>, <span style="color: #da8548; font-weight: bold;">0</span>], X[<span style="color: #da8548; font-weight: bold;">50</span>:<span style="color: #da8548; font-weight: bold;">100</span>, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr"> 85: </span>              color=<span style="color: #98be65;">'blue'</span>, marker=<span style="color: #98be65;">'x'</span>, label=<span style="color: #98be65;">'versicolor'</span>)
<span class="linenr"> 86: </span>
<span class="linenr"> 87: </span>  plt.xlabel(<span style="color: #98be65;">'sepal length [cm]'</span>)
<span class="linenr"> 88: </span>  plt.ylabel(<span style="color: #98be65;">'petal length [cm]'</span>)
<span class="linenr"> 89: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr"> 90: </span>
<span class="linenr"> 91: </span>  plt.savefig(<span style="color: #98be65;">'02_06.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr"> 92: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr"> 93: </span>
<span class="linenr"> 94: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;&#24863;&#30693;&#22120;&#27169;&#22411;</span>
<span class="linenr"> 95: </span>  ppn = Perceptron(eta=<span style="color: #da8548; font-weight: bold;">0.1</span>, n_iter=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr"> 96: </span>  ppn.fit(X, y)
<span class="linenr"> 97: </span>  plt.clf()
<span class="linenr"> 98: </span>  plt.plot(<span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #c678dd;">len</span>(ppn.errors_) + <span style="color: #da8548; font-weight: bold;">1</span>), ppn.errors_, marker=<span style="color: #98be65;">'o'</span>)
<span class="linenr"> 99: </span>  plt.xlabel(<span style="color: #98be65;">'Epochs'</span>)
<span class="linenr">100: </span>  plt.ylabel(<span style="color: #98be65;">'Number of updates'</span>)
<span class="linenr">101: </span>
<span class="linenr">102: </span>  plt.savefig(<span style="color: #98be65;">'02_07.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">103: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">104: </span>
<span class="linenr">105: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">### A function for plotting decision regions</span>
<span class="linenr">106: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_decision_regions</span>(X, y, classifier, resolution=<span style="color: #da8548; font-weight: bold;">0.02</span>):
<span class="linenr">107: </span>
<span class="linenr">108: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">setup marker generator and color map</span>
<span class="linenr">109: </span>      markers = (<span style="color: #98be65;">'s'</span>, <span style="color: #98be65;">'x'</span>, <span style="color: #98be65;">'o'</span>, <span style="color: #98be65;">'^'</span>, <span style="color: #98be65;">'v'</span>)
<span class="linenr">110: </span>      colors = (<span style="color: #98be65;">'red'</span>, <span style="color: #98be65;">'blue'</span>, <span style="color: #98be65;">'lightgreen'</span>, <span style="color: #98be65;">'gray'</span>, <span style="color: #98be65;">'cyan'</span>)
<span class="linenr">111: </span>      cmap = ListedColormap(colors[:<span style="color: #c678dd;">len</span>(np.unique(y))])
<span class="linenr">112: </span>
<span class="linenr">113: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot the decision surface</span>
<span class="linenr">114: </span>      x1_min, x1_max = X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">0</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">115: </span>      x2_min, x2_max = X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">min</span>() - <span style="color: #da8548; font-weight: bold;">1</span>, X[:, <span style="color: #da8548; font-weight: bold;">1</span>].<span style="color: #c678dd;">max</span>() + <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">116: </span>      xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
<span class="linenr">117: </span>                             np.arange(x2_min, x2_max, resolution))
<span class="linenr">118: </span>      Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
<span class="linenr">119: </span>      Z = Z.reshape(xx1.shape)
<span class="linenr">120: </span>      plt.contourf(xx1, xx2, Z, alpha=<span style="color: #da8548; font-weight: bold;">0.3</span>, cmap=cmap)
<span class="linenr">121: </span>      plt.xlim(xx1.<span style="color: #c678dd;">min</span>(), xx1.<span style="color: #c678dd;">max</span>())
<span class="linenr">122: </span>      plt.ylim(xx2.<span style="color: #c678dd;">min</span>(), xx2.<span style="color: #c678dd;">max</span>())
<span class="linenr">123: </span>
<span class="linenr">124: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">plot class samples</span>
<span class="linenr">125: </span>      <span style="color: #51afef;">for</span> idx, cl <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(np.unique(y)):
<span class="linenr">126: </span>          plt.scatter(x=X[y == cl, <span style="color: #da8548; font-weight: bold;">0</span>],
<span class="linenr">127: </span>                      y=X[y == cl, <span style="color: #da8548; font-weight: bold;">1</span>],
<span class="linenr">128: </span>                      alpha=<span style="color: #da8548; font-weight: bold;">0.8</span>,
<span class="linenr">129: </span>                      c=colors[idx],
<span class="linenr">130: </span>                      marker=markers[idx],
<span class="linenr">131: </span>                      label=cl,
<span class="linenr">132: </span>                      edgecolor=<span style="color: #98be65;">'black'</span>)
<span class="linenr">133: </span>  plt.clf()
<span class="linenr">134: </span>  plot_decision_regions(X, y, classifier=ppn)
<span class="linenr">135: </span>  plt.xlabel(<span style="color: #98be65;">'sepal length [cm]'</span>)
<span class="linenr">136: </span>  plt.ylabel(<span style="color: #98be65;">'petal length [cm]'</span>)
<span class="linenr">137: </span>  plt.legend(loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">138: </span>
<span class="linenr">139: </span>
<span class="linenr">140: </span>  plt.savefig(<span style="color: #98be65;">'02_08.png'</span>, dpi=<span style="color: #da8548; font-weight: bold;">300</span>)
<span class="linenr">141: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">142: </span>
</pre>
</div>

<pre class="example">
       0    1    2    3               4
145  6.7  3.0  5.2  2.3  Iris-virginica
146  6.3  2.5  5.0  1.9  Iris-virginica
147  6.5  3.0  5.2  2.0  Iris-virginica
148  6.2  3.4  5.4  2.3  Iris-virginica
149  5.9  3.0  5.1  1.8  Iris-virginica
</pre>



<div id="org8c0340e" class="figure">
<p><img src="images/02_06.png" alt="02_06.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 29: </span>待分類的兩種花依其不同屬性之分佈狀況</p>
</div>


<div id="orgb6a5a72" class="figure">
<p><img src="images/02_07.png" alt="02_07.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 30: </span>感知器訓練過程</p>
</div>


<div id="org0e03f95" class="figure">
<p><img src="images/02_08.png" alt="02_08.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 31: </span>訓練後的分類結果</p>
</div>
</div>
</div>

<div id="outline-container-org25af096" class="outline-2">
<h2 id="org25af096"><span class="section-number-2">5.</span> 單層感知器的極限與多層感知器: XOR gate</h2>
<div class="outline-text-2" id="text-5">
<p>
到目前為止，透過權重及偏權值可以設計 AND、NAND、OR, 但無法完成 XOR。然而，其實我們可以透過 NAND、OR 及 AND 三個功能組合出 XOR 還輯閘，其組合方式如圖<a href="#org900936a">32</a>所示，顯然，若要再以感知器來模擬其運作原理，單層感知器已不敷使用。<br />
</p>

<div id="org900936a" class="figure">
<p><img src="images/xor.jpg" alt="xor.jpg" width="400" /><br />
</p>
<p><span class="figure-number">Figure 32: </span>XOR 還輯閘的組合</p>
</div>
</div>

<div id="outline-container-orgfd9a2b5" class="outline-3">
<h3 id="orgfd9a2b5"><span class="section-number-3">5.1.</span> 多層感知器(Multilayer perceptron, MLP): XOR gate 實作</h3>
<div class="outline-text-3" id="text-5-1">

<div id="orgd17cad7" class="figure">
<p><img src="images/XORMLP.jpg" alt="XORMLP.jpg" /><br />
</p>
<p><span class="figure-number">Figure 33: </span>MLP解決XOR Gate</p>
</div>


<div id="org4824b28" class="figure">
<p><img src="images/LinearlySeparable.jpg" alt="LinearlySeparable.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 34: </span>Linearly Separable</p>
</div>

<p>
參考圖<a href="#org900936a">32</a>的架構，我們可以藉由增加感知器的層數來實現 XOR 的功能，其結構如圖<a href="#org280e44c">35</a>所示。<br />
</p>

<div id="org280e44c" class="figure">
<p><img src="images/2LayerSensor.png" alt="2LayerSensor.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 35: </span>多層感知器模擬 XOR 還輯閘</p>
</div>

<p>
至於其實作程式碼則如下。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">python code for XOR gate simulation</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">AND</span>(x1, x2):
<span class="linenr"> 5: </span>    <span style="color: #dcaeea;">x</span> = np.array([x1, x2])
<span class="linenr"> 6: </span>    <span style="color: #dcaeea;">w</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.5</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>])
<span class="linenr"> 7: </span>    <span style="color: #dcaeea;">b</span> = -<span style="color: #da8548; font-weight: bold;">0.7</span>
<span class="linenr"> 8: </span>    <span style="color: #dcaeea;">theta</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr"> 9: </span>    <span style="color: #dcaeea;">tmp</span> = np.<span style="color: #c678dd;">sum</span>(w*x) + b
<span class="linenr">10: </span>    <span style="color: #51afef;">if</span> tmp &lt;= <span style="color: #dcaeea;">theta</span>:
<span class="linenr">11: </span>        <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">12: </span>    <span style="color: #51afef;">else</span>:
<span class="linenr">13: </span>        <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">14: </span>
<span class="linenr">15: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">OR</span>(x1, x2):
<span class="linenr">16: </span>    x = np.array([x1, x2])
<span class="linenr">17: </span>    <span style="color: #dcaeea;">w</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.5</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>])
<span class="linenr">18: </span>    <span style="color: #dcaeea;">b</span> = -<span style="color: #da8548; font-weight: bold;">0.2</span>
<span class="linenr">19: </span>    <span style="color: #dcaeea;">theta</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">20: </span>    <span style="color: #dcaeea;">tmp</span> = np.<span style="color: #c678dd;">sum</span>(w*x) + b
<span class="linenr">21: </span>    <span style="color: #51afef;">if</span> tmp &lt;= <span style="color: #dcaeea;">theta</span>:
<span class="linenr">22: </span>        <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">23: </span>    <span style="color: #51afef;">else</span>:
<span class="linenr">24: </span>        <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">25: </span>
<span class="linenr">26: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">NAND</span>(x1, x2):
<span class="linenr">27: </span>    x = np.array([x1, x2])
<span class="linenr">28: </span>    <span style="color: #dcaeea;">w</span> = np.array([-<span style="color: #da8548; font-weight: bold;">0.5</span>, -<span style="color: #da8548; font-weight: bold;">0.5</span>])
<span class="linenr">29: </span>    <span style="color: #dcaeea;">b</span> = <span style="color: #da8548; font-weight: bold;">0.7</span>
<span class="linenr">30: </span>    <span style="color: #dcaeea;">tmp</span> = np.<span style="color: #c678dd;">sum</span>(w*x) + b
<span class="linenr">31: </span>    <span style="color: #51afef;">if</span> tmp &lt;= <span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">32: </span>        <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">33: </span>    <span style="color: #51afef;">else</span>:
<span class="linenr">34: </span>        <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">35: </span>
<span class="linenr">36: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">XOR</span>(x1, x2):
<span class="linenr">37: </span>    s1 = NAND(x1, x2)
<span class="linenr">38: </span>    <span style="color: #dcaeea;">s2</span> = OR(x1, x2)
<span class="linenr">39: </span>    <span style="color: #dcaeea;">y</span> = AND(s1, s2)
<span class="linenr">40: </span>    <span style="color: #51afef;">return</span> y
<span class="linenr">41: </span>
<span class="linenr">42: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"XOR(0,0): "</span>, XOR(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">0</span>))
<span class="linenr">43: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"XOR(0,1): "</span>, XOR(<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">44: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"XOR(1,0): "</span>, XOR(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">0</span>))
<span class="linenr">45: </span><span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"XOR(1,1): "</span>, XOR(<span style="color: #da8548; font-weight: bold;">1</span>,<span style="color: #da8548; font-weight: bold;">1</span>))
</pre>
</div>

<pre class="example">
XOR(0,0):  0
XOR(0,1):  1
XOR(1,0):  1
XOR(1,1):  0
</pre>
</div>
</div>
</div>

<div id="outline-container-org40e669c" class="outline-2">
<h2 id="org40e669c"><span class="section-number-2">6.</span> 神經網路的學習</h2>
<div class="outline-text-2" id="text-6">
</div>
<div id="outline-container-orgf60a4ef" class="outline-3">
<h3 id="orgf60a4ef"><span class="section-number-3">6.1.</span> 從資料中學習</h3>
<div class="outline-text-3" id="text-6-1">
<p>
神經網路的特色是可以從資料中學習，此處的學習指「自動決定權重參數」，以人工來指定權重參數值這個工作太過鉅大以至於可行性不高，因為有些神經網路的權重參數可能高達數千億個。機器學習與神經網路最大的差異也在於此。以辨識手寫字（如圖<a href="#orga59f6a0">36</a>）為例，機器學習的作法是先以人工找出該字圖形的特徵量（可以透過 SIFT、SURF、HOG 等視覺領域方法），將該特徵量轉換為向量，再利用機器學習的辨識器（如 SVM、KNN）來學習這些轉換過的特徵向量；而神經網路的作法則是直接將原始圖形當成輸入，至於特徵量本身也是透過神經網路自行學習取得，再透過這些行習得的特徵量進行學習辨識。<br />
</p>

<p>
在機器學習的過程中，一般會把資料分成訓練資料與測試資料兩類，其中訓練資料用來進行學習、尋找最佳參數；而測試資料則是用來評估訓練後的模型成效。若只使用同一種資料集（如同一個人的字跡）來進行訓練與評估，則可能導致模型只對此人的字跡有辨識成效，這㮔問題稱為過度擬合（overfitting）。<br />
</p>

<div id="orga59f6a0" class="figure">
<p><img src="images/MNIST-Image.png" alt="MNIST-Image.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 36: </span>手寫字體辨識</p>
</div>
</div>
</div>

<div id="outline-container-orgda43946" class="outline-3">
<h3 id="orgda43946"><span class="section-number-3">6.2.</span> Overfitting v.s. underfitting</h3>
<div class="outline-text-3" id="text-6-2">
<p>
機器學習的本質即在於最佳化與普遍性之間的拉扯，最佳化(optimization)是指調整模型使得模型能在訓練資料上獲得最佳表現的過程；而普遍性(generalization)是指已訓練過的模型對從未見過的資料的預測能力。在訓練之初，最佳化和普遍性是高度相關的，訓練資料的損失越低、測試資料的損失也越低。此時，模型仍處於低度擬合狀態，仍有進步空間；在訓練資料經過一定回合(epoch)的訓練後，普遍程度的改善幅逐漸停止，驗證指標隨之停滯然後開始變差，於是模型開始 overfitting。也就是說，模型已經學習了一些訓練集特有的模式，但這些特有模式根本和新資料不相關、甚或會誤導對新資料的預測。<br />
</p>

<p>
為了防止模型在訓練資料中學到錯誤或不相關的模式，最好的解決方案是取得更多訓練資料，如果無法做到這一點，次佳的解決方案是調配模型儲存的訊訊量或者限制儲存資訊的類型或數值，如果一個神經網路只能記住少量的模式，那麼優化過程將迫使它專注於最顯著的那些模式，而不會去記住那些不相關的資訊，如此才能適應從未見過的新資料。<br />
</p>

<p>
這種週配或限制模型資訊(即參數)以對抗 overfitting 的方式稱為常規化(regularization)，幾個常見的常規化技術如下：<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orga458305"></a>縮減神經網路的大小<br />
<div class="outline-text-4" id="text-6-2-1">
<p>
即減少模型可用來學習的參數數量(包含網路的層數和每層的神經元個數)，在深度學習中，模型中可學習參數的數量(權重數量)通常被稱為容量(capacity)。直覺上，有更多參數的模型具有更多的記憶能力(memorization capacity)，因此可以很容易在訓練樣本與其目標之間做出完美的對應，但卻沒有任何適應學習的能力。例如，我們可以建立具有 500000 個參數的模型來輕鬆學習(記憶)MNIST 訓練集中的每個數字所屬類別，但這種模型對於預測新的數字圖案毫無用處。<br />
</p>

<p>
此外，如果神經網路的記憶資源有限，則無法輕易地直接在訓練樣本與目標之間做出對應，因此，要讓損失最小化，神經網路必須採用萃取過的資料表示法，以建立對目標的預測能力；另一方面，我們仍須讓神經網路擁有足夠的參數，以免模型 underfitting。也就是說，我們必須在容量過大(too much capacity)和容量不足(not enough capacity)之間取得平衡。<br />
</p>

<p>
然而，到目前為止還沒有有效的公式來確定網路最佳的層數和神經元數，我們必須實際在神經網路進行多次評估(在驗證集上，而非測試集)，以找到正確的模型大小。我們通常會從較少的層數和 units 數開始，再逐漸增加層的 units 數或增加新層數，直到驗證損失不再進步為止。<br />
</p>

<p>
以 imdb 的例子來看，原本的網路模型如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">2: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">3: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">4: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>

<p>
如果我們將每一層的神經元數量縮小：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">2: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">4</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">3: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">4</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">4: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>

<p>
則兩個版本的驗證損失比較如下圖，黑點為縮小網路 capacity 的驗證損失值，可以看出較原始版本的表現更佳，這個版本的神經網路在第 6 個 epoch 之後才開始 overfitting（原始的模型在第 4 個 epoch）就開始 overfitting。<br />
</p>


<div id="orgce27501" class="figure">
<p><img src="images/img-191120103824.jpg" alt="img-191120103824.jpg" /><br />
</p>
<p><span class="figure-number">Figure 37: </span>模型容量對驗證損失分數的影響-較小的模型</p>
</div>

<p>
相反的，如果我們刻意將神經網路的 capacity 擴大，如下，每一層的神經元數量由原來的 16 增加到 512，則結果如圖<a href="#org1195838">38</a>，擴大版的神經網路幾乎在訓練之初就開始 overfitting，而且越來越嚴重，其驗證損失的表現也較原始版本差。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">2: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">3: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">512</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">4: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>


<div id="org1195838" class="figure">
<p><img src="images/img-191120103825.jpg" alt="img-191120103825.jpg" /><br />
</p>
<p><span class="figure-number">Figure 38: </span>模型容量對驗證損失分數的影響-較大的容量</p>
</div>

<p>
較大模型的神經網路雖然導致驗證損失的效能下降，但在訓練損失上的表現則否，由圖<a href="#org85266c0">39</a>的黑點可明顯看出大網路模型在訓練期間的損失極低，然而這種效能在面對新資料（驗證資烞十）時確無法表現出來。<br />
</p>


<div id="org85266c0" class="figure">
<p><img src="images/img-191120103826.jpg" alt="img-191120103826.jpg" /><br />
</p>
<p><span class="figure-number">Figure 39: </span>模型容量對訓練損失分數的影響-較大的容量</p>
</div>
</div>
</li>

<li><a id="org3302676"></a>加入權重常規化 (weight regularization)<br />
<div class="outline-text-4" id="text-6-2-2">
<p>
在設計網路模型時，一個原則為：如果兩種模型有同樣的效能表現，則較簡單的模型通常是更好的設計，也更不容易導致 overfitting。這裡所謂的簡單指的是參數值分佈的熵比較小(entropy of distribution of parameter values las less entropy)的模型，或是使用較少參數的模型。因此，降低 overfitting 就是想辦法採用較小的權重值以限制神經網路的複雜性，這會讓權重的分佈更為常規化(regularized)。權重值常規化(weight regularization)在作法上就是對損失函數中較大的權重加上代價(cost)項目，通常有兩種方式：<br />
</p>

<ul class="org-ul">
<li>L1 regularization: 在損失函數多加上一項 cost，這一項和權重係數的絕對值成正比。<br /></li>
<li>L2 regularization: 在損失函數多加上一項 cost，這一項和權重係數的平方成正比。L2 也稱為 weight decay。<br /></li>
</ul>

<p>
在 Keras 中，常規化的實作只要利用 model.add()並指名參數把權重常規化物件傳入神經網路層即可。以下為 L2 regularization 的做法：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> regularizers
<span class="linenr">2: </span>
<span class="linenr">3: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr">4: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, kernel_regularizer=regularizers.l2(<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr">5: </span>                         activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr">6: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, kernel_regularizer=regularizers.l2(<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr">7: </span>                         activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">8: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>

<p>
上述程式中 l2(0.001)的意思是表示該層權重矩陣中的每個係數都會加上(0.001*權重係數值)到神經網路的總損失函數上，由圖<a href="#orgbfcf37c">40</a>的結果可看，加入 L2 regularization 的模組，其驗證損失的表現較佳。<br />
</p>


<div id="orgbfcf37c" class="figure">
<p><img src="images/img-19112010382A.jpg" alt="img-19112010382A.jpg" /><br />
</p>
<p><span class="figure-number">Figure 40: </span>權重正規化對訓練損失分數的影響</p>
</div>
</div>
</li>

<li><a id="org0511852"></a>丟棄法 (dropout)<br />
<div class="outline-text-4" id="text-6-2-3">
<p>
由 Ceoff Hinton 教授和他在多倫多大學的學生所開發出來的常規化技術之一，主要是在訓練期間隨機丟棄(dropping out，即把 feature 值歸零)layer 的一些輸出特徵，假設某層在訓練期間的某一狀態下其正常輸出向量為[0.2, 0.5, 1.3, 0.8, 1.1]，在 dropout 後，某幾個輸輸向量的值會被歸零，如[0, 0.5, 1.3, 0, 1.1]。丟棄率則是指被歸零的特徵值個數佔特徵值總數的比例，以此例而言丟棄率為 2/5=0.4。<br />
</p>

<p>
丟棄率通常介於 0.2 到 0.5 之間，而在測試時，其實沒有任何特徵質會被丟棄，取而代之的是層的輸出值將依照丟棄率的比例縮小，以平䚘訓練時的輸出被歸零的影響。隨機歸零的核心想法是在 layer 的輸出值中加入&ldquo;雜訊&rdquo;，這樣可以打破不重要的偶然模式，如果沒有雜訊，神經網路就會開始&ldquo;死記&rdquo;。在 Keras 中，我們可以很簡單的透過 add()來加入 Dropout：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> regularizers
<span class="linenr"> 2: </span>
<span class="linenr"> 3: </span>  <span style="color: #dcaeea;">model</span> = models.Sequential()
<span class="linenr"> 4: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, kernel_regularizer=regularizers.l2(<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr"> 5: </span>                         activation=<span style="color: #98be65;">'relu'</span>, input_shape=(<span style="color: #da8548; font-weight: bold;">10000</span>,)))
<span class="linenr"> 6: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr"> 7: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">16</span>, kernel_regularizer=regularizers.l2(<span style="color: #da8548; font-weight: bold;">0.001</span>),
<span class="linenr"> 8: </span>                         activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr"> 9: </span>  model.add(layers.Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">10: </span>  model.add(layers.Dense(<span style="color: #da8548; font-weight: bold;">1</span>, activation=<span style="color: #98be65;">'sigmoid'</span>))
</pre>
</div>

<p>
在加入兩個 Dropout 層後對於降低 overfitting 的效果如圖<a href="#org660767f">41</a>所示。<br />
</p>


<div id="org660767f" class="figure">
<p><img src="images/img-19112010382B.jpg" alt="img-19112010382B.jpg" /><br />
</p>
<p><span class="figure-number">Figure 41: </span>權重正規化對訓練損失分數的影響</p>
</div>
</div>
</li>
</ol>
</div>

<div id="outline-container-org75933f7" class="outline-3">
<h3 id="org75933f7"><span class="section-number-3">6.3.</span> 三層神經網路</h3>
<div class="outline-text-3" id="text-6-3">
<p>
圖<a href="#org16262bf">42</a>為典型的三層神經網路，輸入層(layer 0)有 2 個神經元，兩個隱藏層(layer 1, 2)各有 3 個及 2 個神經元，輸出層(layer 3)則有兩個神經元。神經網路中各層的神經元的訊息傳遞則是透過矩陣相乘來進行。<br />
</p>


<div id="org16262bf" class="figure">
<p><img src="images/3LayerNetwork.png" alt="3LayerNetwork.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 42: </span>三層神經網路</p>
</div>

<p>
神網網路間的訊息傳遞(propagation)可分為以下兩類:<br />
</p>
<ul class="org-ul">
<li>前向傳遞(Forward propagation): 較簡單 (只有線性合成，和非線性轉換)<br /></li>
<li>反向傳遞 (Backward propagation): 較複雜 (因為多微分方程)<br /></li>
</ul>
</div>
</div>

<div id="outline-container-orgf76312e" class="outline-3">
<h3 id="orgf76312e"><span class="section-number-3">6.4.</span> 損失函數</h3>
<div class="outline-text-3" id="text-6-4">
<p>
至於神經網路模型的成效則可由損失函數（loss function）來判斷，常見的損失函數有均方誤差與交叉熵誤差兩種。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org00451c0"></a>均方誤差<br />
<div class="outline-text-4" id="text-6-4-1">
<p>
均方誤差（mean squared error）可以公式\eqref{org021e737}表示：<br />
</p>
\begin{equation}
\label{org021e737}
E = \frac{1}{2} \sum_{k}(y_k-t_k)^2
\end{equation}
<p>
公式\eqref{org021e737}中的\(y_k\)為神經網路的輸出，\(t_k\)為訓練資料，\(k\)為資料的維度，其實際運算的資料內容與結果可由如下程式碼，當 y 的預測機率越接近正確答案時，均方誤差的值越小。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">mean_squared_error</span>(y, t):
<span class="linenr"> 3: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0.5</span> * np.<span style="color: #c678dd;">sum</span>((y-t)**<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">case 1: &#27491;&#30906;&#31572;&#26696;&#28858;2, y[2]&#30340;&#27231;&#29575;&#26368;&#39640;&#26178;</span>
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">y</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>])
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">t</span> = np.array([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 7: </span>  <span style="color: #c678dd;">print</span>(mean_squared_error(y,t))
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">case 2: &#27491;&#30906;&#31572;&#26696;&#28858;2, y[7]&#30340;&#27231;&#29575;&#26368;&#39640;&#26178;</span>
<span class="linenr">10: </span>  <span style="color: #dcaeea;">y</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>])
<span class="linenr">11: </span>  <span style="color: #c678dd;">print</span>(mean_squared_error(y,t))
</pre>
</div>

<pre class="example">
0.09750000000000003
0.5975
</pre>
</div>
</li>

<li><a id="org469435d"></a>交叉熵誤差<br />
<div class="outline-text-4" id="text-6-4-2">
<p>
交叉熵誤差（cross entropy error）可以公式\eqref{orgc08bf53}表示：<br />
</p>
\begin{equation}
\label{orgc08bf53}
E = -\sum_{k}t_k\log y_k
\end{equation}
<p>
公式\eqref{orgc08bf53}中的\(y_k\)為神經網路的輸出，\(t_k\)為訓練資料（即正確答案標籤，為 one-hot 形式，只有正確案為 1，其餘為 0），\(log\)為以\(e\)為底的自然對數，其輸出圖形如圖<a href="#orgd740ca0">43</a>所示。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">2: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr">3: </span>  <span style="color: #dcaeea;">x</span> = np.arange(-<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">0.0001</span>)
<span class="linenr">4: </span>  <span style="color: #dcaeea;">y</span> = np.log(x)
<span class="linenr">5: </span>  plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">4</span>,<span style="color: #da8548; font-weight: bold;">3</span>))
<span class="linenr">6: </span>  plt.plot(x, y)
<span class="linenr">7: </span>  plt.ylim(-<span style="color: #da8548; font-weight: bold;">5.1</span>, <span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">8: </span>  plt.savefig(<span style="color: #98be65;">"log.png"</span>)
<span class="linenr">9: </span>  <span style="color: #51afef;">return</span> <span style="color: #98be65;">"log.png"</span>
</pre>
</div>

<div id="orgd740ca0" class="figure">
<p><img src="images/log.png" alt="log.png" /><br />
</p>
<p><span class="figure-number">Figure 43: </span>自然對數\(y=\log x\)曲線圖</p>
</div>

<p>
以前述範例進行交叉熵的計算，其運算結果如下所示。進行\(log\)運算前先加上一極小值 delta 的原因在於避免因 log(0)產生無限大的-inf。執行結果與前例大致相同，越接近正確答案，誤差值越小。<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">cross_entropy_error</span>(y, t):
<span class="linenr"> 3: </span>      <span style="color: #dcaeea;">delta</span> = 1e-<span style="color: #da8548; font-weight: bold;">7</span>
<span class="linenr"> 4: </span>      <span style="color: #51afef;">return</span> -np.<span style="color: #c678dd;">sum</span>(t * np.log(y + delta))
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">case 1: &#27491;&#30906;&#31572;&#26696;&#28858;2, y[2]&#30340;&#27231;&#29575;&#26368;&#39640;&#26178;</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">y</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>])
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">t</span> = np.array([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 8: </span>  <span style="color: #c678dd;">print</span>(cross_entropy_error(y,t))
<span class="linenr"> 9: </span>
<span class="linenr">10: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">case 2: &#27491;&#30906;&#31572;&#26696;&#28858;2, y[7]&#30340;&#27231;&#29575;&#26368;&#39640;&#26178;</span>
<span class="linenr">11: </span>  <span style="color: #dcaeea;">y</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.05</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>])
<span class="linenr">12: </span>  <span style="color: #c678dd;">print</span>(cross_entropy_error(y,t))
</pre>
</div>

<pre class="example">
0.510825457099338
2.302584092994546
</pre>
</div>
</li>

<li><a id="org320b7e2"></a>批次學習<br />
<div class="outline-text-4" id="text-6-4-3">
<p>
機器學習的目的在於使用訓練資料找能能儘量縮小損失函數的參數，因此，計算損失函數自然不是逐一計算，而是以縮小所有訓練資料的損失函數總和為最終目標。<br />
因此，前節中的交叉熵誤差（公式\eqref{orgc08bf53}）就要修改為以下算式（\eqref{org546a0c1}）：<br />
</p>
\begin{equation}
\label{org546a0c1}
E = -\frac{1}{N}\sum_{n}\sum_{k}t_{nk}\log y_{nk}
\end{equation}
<p>
公式\eqref{org546a0c1}的 N 為資料筆數，\(y_{nk}\)代表神經網路第 n 筆資料的第 k 個輸出、\(t){nk}\)則為第 n 筆訓練資料的第 k 個值，最後除以 N、進行正規化，所得即為這 N 筆資料的「平均損失函數」。然而，實際執行時並無法取出所有訓練資料來運算，以 MNIST 資料集為例，訓練資料就有 60000 筆，若是其他大數劇則資料筆數可能達數千萬，較務實的作法，是隨機自資料集中抽取 N 筆資料（稱之為 mini batch）來計算平均損失函數，以代表整體。<br />
那麼，這個隨機抽取的資料筆數要設定為多少才適合？可以從以下兩個觀點來看：<br />
</p>
<ul class="org-ul">
<li>在合理範圍內，增大 Batch_Size 有何好處？<br />
<ul class="org-ul">
<li>內存利用率提高了，大矩陣乘法的並行化效率提高。<br /></li>
<li>跑完一次 epoch（全數據集）所需的迭代次數減少，對於相同數據量的處理速度進一步加快。<br /></li>
<li>在一定範圍內，一般來說 Batch_Size 越大，其確定的下降方向越准，引起訓練震蕩越小。<br /></li>
</ul></li>
<li>盲目增大 Batch_Size 有何壞處？<br />
<ul class="org-ul">
<li>內存利用率提高了，但是內存容量可能撐不住了。<br /></li>
<li>跑完一次 epoch（全數據集）所需的迭代次數減少，要想達到相同的精度，其所花費的時間大大增加了，從而對參數的修正也就顯得更加緩慢。<br /></li>
<li>Batch_Size 增大到一定程度，其確定的下降方向已經基本不再變化。<br /></li>
</ul></li>
</ul>
</div>
</li>

<li><a id="org0d2066c"></a>設定損失函數做為效能指標的原因<br />
<div class="outline-text-4" id="text-6-4-4">
<p>
進行神經網路的學習，目的在於找出辨識準確度高的參數，那麼，為什麼不以「辨識準確率」做為指標，而是要導入損失函數？<br />
神經網路的學習過程在於尋找最佳參數（即偏移值與權重），藉以找出能盡量縮小損失函數的參數，為達此目的，我們要對某特定參數所得的損失函數進行微分（即找出其斜度），亦即，找出「如果稍微改變這些參數，損失函數將會有何種變化？」的答案，假如微分後損失函數的值變為負值，則我們就將參數往正向變化以減少損失函數；假如微分後損失函數的值變為正值，我們就讓參數往負值變化，以減少損失函數；直到損失函數微分後的值變為 0 為止。<br />
無法將辨識準確率作為指標的原因即在於，不論我們如何調整參數，辨識準確率的微分值都為 0。<br />
例如，有 100 張訓練資料，如果能成功辨識出 32 張影像，其準確率為 32%，如果調整參數後只得到極小的變化，辨識準確度也只能在 33%、34%這些不連續的變化，而不會有 32.0124%的連續性變化。然而，如果以損失函數為指標，則能顯示出 0.92345&#x2026;.的結果；而略為改變參數值，即能得到 0.24884 的連續變化。<br />
辨識準確率幾乎不會反應改變參數所帶來的微小變化，即使有反應也是不連續的變化，這個道理就好像把階梯函數拿來當成活化函數，對階梯函數的任何點（除 0 以外）做切線，其斜率均為 0。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org0da7f73" class="outline-3">
<h3 id="org0da7f73"><span class="section-number-3">6.5.</span> 數值微分</h3>
<div class="outline-text-3" id="text-6-5">
<p>
梯度法使用梯度的資料來決定學習（或找出最佳參數）的方向，此處會應用到數學的微分。微分指的是某個瞬間的變化量（如行進中的車輛瞬間速度的變化量），可以公式\eqref{org57aa022}來定義：<br />
</p>
\begin{equation}
\label{org57aa022}
\frac{df(x)}{dx}=\lim_{h\to0}\frac{f(x+h)-f(x)}{h}
\end{equation}
<p>
上述公式中，等號左側\(\frac{df(x)}{dx}\)代表\(f(x)\)中對\(x\)微分，亦即，找出相對 x 之 f(x)的變化，也就是希望能找出：「隨著\(x\)的細微改變，函數\(f(x)\)會出現何種變化？」的答案，此處的\(h\)趨近於 0。<br />
以公式\eqref{org57aa022}實際進行 python 運算會因為 h 值太小而導致四捨五入無法計算出真實結果，如下列程式碼，若\(x=10e-50\)，其計算結果為 0。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_diff</span>(f, x):
<span class="linenr">2: </span>      <span style="color: #dcaeea;">h</span> = 10e - <span style="color: #da8548; font-weight: bold;">50</span>
<span class="linenr">3: </span>      <span style="color: #51afef;">return</span> (f(x+h) - f(x)) /h
</pre>
</div>
<p>
原因是因為\(h\)值過小而被四捨五入為 0，改善方式為將\(h\)設定為\(10^-4\)。<br />
另一個要處理的問題是：我們想求得的是函數\(f(x)\)在點\(x\)的斜率，然而上述程式碼所計算出的是函數\(f(x)\)在\(x\)與\(x+h\)區間的斜率，改善方式是改為計算\(f(x+h)\)與\(f(x-h)\)區間的斜率，即進行數值微分（數值梯度），其程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_diff</span>(f, x):
<span class="linenr">2: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr">3: </span>      <span style="color: #51afef;">return</span> (f(x+h) - f(x-h)) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
</pre>
</div>
<p>
若以上述方式，分別在\(x=5\)及\(x=10\)時對計算算式\(y=0.01x^2+0.1x\)的微分，其結果如下，橘色與綠色直線分別為\(x=5\)與\(x=10\)的切線。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_diff</span>(f, x):
<span class="linenr"> 4: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 5: </span>      <span style="color: #51afef;">return</span> (f(x+h) - f(x-h)) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr"> 6: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_1</span>(x):
<span class="linenr"> 7: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">0.01</span>*x**<span style="color: #da8548; font-weight: bold;">2</span> + <span style="color: #da8548; font-weight: bold;">0.1</span>*x
<span class="linenr"> 8: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">tangent_line</span>(f, x):
<span class="linenr"> 9: </span>      <span style="color: #dcaeea;">d</span> = numerical_diff(f, x)
<span class="linenr">10: </span>      <span style="color: #dcaeea;">y</span> = f(x) - d*x
<span class="linenr">11: </span>      <span style="color: #51afef;">return</span> <span style="color: #51afef;">lambda</span> t: d*t + y
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x</span> = np.arange(<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">20.0</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>)
<span class="linenr">13: </span>  <span style="color: #dcaeea;">y</span> = function_1(x)
<span class="linenr">14: </span>  <span style="color: #dcaeea;">tf</span> = tangent_line(function_1, <span style="color: #da8548; font-weight: bold;">5</span>)
<span class="linenr">15: </span>  <span style="color: #dcaeea;">y2</span> = tf(x)
<span class="linenr">16: </span>  <span style="color: #dcaeea;">tf2</span> = tangent_line(function_1, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">17: </span>  <span style="color: #dcaeea;">y3</span> = tf2(x)
<span class="linenr">18: </span>  plt.xlabel(<span style="color: #98be65;">"x"</span>)
<span class="linenr">19: </span>  plt.ylabel(<span style="color: #98be65;">"f(x)"</span>)
<span class="linenr">20: </span>  plt.plot(x, y)
<span class="linenr">21: </span>  plt.plot(x, y2)
<span class="linenr">22: </span>  plt.plot(x, y3)
<span class="linenr">23: </span>  plt.savefig(<span style="color: #98be65;">"function_1.png"</span>)
<span class="linenr">24: </span>  <span style="color: #c678dd;">print</span>(numerical_diff(function_1,<span style="color: #da8548; font-weight: bold;">5</span>))
<span class="linenr">25: </span>  <span style="color: #c678dd;">print</span>(numerical_diff(function_1,<span style="color: #da8548; font-weight: bold;">10</span>))
<span class="linenr">26: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">return "function_1.png"</span>
</pre>
</div>

<pre class="example">
0.1999999999990898
0.2999999999986347
</pre>



<div id="orgafd3dbb" class="figure">
<p><img src="images/function_1.png" alt="function_1.png" /><br />
</p>
<p><span class="figure-number">Figure 44: </span>\(f(x)=0.01x^2+0.1x\)圖表</p>
</div>

<p>
此處計算的結果與數學解相比：\(\frac{df(x)}{dx}=0.02x+0.1\)，\(x=5\)、\(x=10\)的實際微分值應為 0.2、0.3，嚴格來說並不一致，但因誤差太小，可以視為相同。<br />
</p>

<p>
若再進一步考慮兩個變數時的微分，如算式\eqref{org2002462}，這種由多個變數形成的微分，稱作偏微分，，該公式對應的 python 程式碼如下：<br />
</p>
\begin{equation}
\label{org2002462}
\(f(x_0,x_1)=x_0^2+x_1^2\)
\end{equation}

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_2</span>(x):
<span class="linenr">2: </span>      <span style="color: #51afef;">return</span> x[<span style="color: #da8548; font-weight: bold;">0</span>]**<span style="color: #da8548; font-weight: bold;">2</span> + x[<span style="color: #da8548; font-weight: bold;">1</span>]**<span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr">3: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25110; return np.sum(x**2)</span>
</pre>
</div>
<p>
公式\(fx_0,x_1)=x_0^2+x_1^2\)的圖表如圖<a href="#org45dc1ce">45</a>所示。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_2</span>(x):
<span class="linenr"> 2: </span>    <span style="color: #51afef;">return</span> x[<span style="color: #da8548; font-weight: bold;">0</span>]**<span style="color: #da8548; font-weight: bold;">2</span> + x[<span style="color: #da8548; font-weight: bold;">1</span>]**<span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">3D&#12464;&#12521;&#12501;&#12434;&#25551;&#30011;</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">cat multivariate_func_save.py</span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 8: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 9: </span>  plt.switch_backend(<span style="color: #98be65;">'agg'</span>)
<span class="linenr">10: </span>  <span style="color: #51afef;">from</span> mpl_toolkits.mplot3d <span style="color: #51afef;">import</span> Axes3D
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x</span> = np.meshgrid(np.arange(-<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>), np.arange(-<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">0.1</span>))
<span class="linenr">13: </span>  <span style="color: #dcaeea;">z</span> = x[<span style="color: #da8548; font-weight: bold;">0</span>]**<span style="color: #da8548; font-weight: bold;">2</span> + x[<span style="color: #da8548; font-weight: bold;">1</span>]**<span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #dcaeea;">fig</span> = plt.figure()
<span class="linenr">16: </span>  <span style="color: #dcaeea;">ax</span> = Axes3D(fig)
<span class="linenr">17: </span>  ax.plot_wireframe(x[<span style="color: #da8548; font-weight: bold;">0</span>], x[<span style="color: #da8548; font-weight: bold;">1</span>], z)
<span class="linenr">18: </span>
<span class="linenr">19: </span>  plt.xlim(-<span style="color: #da8548; font-weight: bold;">3.5</span>, <span style="color: #da8548; font-weight: bold;">3.5</span>)
<span class="linenr">20: </span>  plt.ylim(-<span style="color: #da8548; font-weight: bold;">4.5</span>, <span style="color: #da8548; font-weight: bold;">4.5</span>)
<span class="linenr">21: </span>  plt.xlabel(<span style="color: #98be65;">"x0"</span>)
<span class="linenr">22: </span>  plt.ylabel(<span style="color: #98be65;">"x1"</span>)
<span class="linenr">23: </span>  plt.savefig(<span style="color: #98be65;">'multivariate_func.png'</span>)
</pre>
</div>


<div id="org45dc1ce" class="figure">
<p><img src="images/multivariate_func.png" alt="multivariate_func.png" /><br />
</p>
<p><span class="figure-number">Figure 45: </span>\(f(x_0,x_1)=x_0^2+x_1^2\)圖表</p>
</div>


<pre class="example">
6.00000000000378
7.999999999999119
</pre>


<p>
上述程式碼中，function_mult1 是先將計算「將\(x_1\)固定為常數 4.0 的新函數」，對\(x_0\)進行微分；function_multi2 則是先將\(x_0\)固定為常數 3，再對\(x_1\)求出微分。<br />
</p>
</div>
</div>

<div id="outline-container-orgee634ad" class="outline-3">
<h3 id="orgee634ad"><span class="section-number-3">6.6.</span> Forward propagation</h3>
<div class="outline-text-3" id="text-6-6">
</div>
<ol class="org-ol">
<li><a id="orge5f49e6"></a>各層間的訊息傳遞<br />
<div class="outline-text-4" id="text-6-6-1">
<p>
以輸入層傳遞訊息至 layer 1 的神經元 a1 為例，a1 的訊息可由公式\eqref{orgd048660}計算出來。<br />
</p>
\begin{equation}
\label{orgd048660}
a_1^{(1)}=w_{11}^{(1)}x_1+w_{12}^{(1)}x_2+b_1^{(1)}
\end{equation}
<p>
其中<br />
</p>
<ul class="org-ul">
<li>\(w_{ab}^{(c)}\)中的\(a\)指的是下一層的第\(a\)個神經元<br /></li>
<li>\(w_{ab}^{(c)}\)中\(b\)指的是上一層的第\(b\)個神經<br /></li>
<li>\(w_{ab}^{(c)}\)中\(c\)指的則是第\(c\)層的權重。<br /></li>
</ul>

<p>
而透過矩陣乘積，可由公式\eqref{org0d573db}顯示 layer 1 的「加權總和」。<br />
</p>
\begin{equation}
\label{org0d573db}
A_1^{(1)}=XW^{(1)}+B^{(1)}
\end{equation}

<p>
其中，<br />
\[
X = \begin{pmatrix} x_1 & x_2 \end{pmatrix},
A^{(1)} = \begin{pmatrix}
          a_1^{(1)} & a_2^{(1)} & a_3^{(1)}
          \end{pmatrix},
B^{(1)} = \begin{pmatrix}
          b_1^{(1)} & b_2^{(1)} & b_3^{(1)}
          \end{pmatrix}, \\
W^{(1)} = \begin{pmatrix}
  w_{11}^{(1)} & w_{21}^{(1)} & w_{31}^{(1)} \\
  w_{12}^{(1)} & w_{22}^{(1)} & w_{32}^{(1)}
\end{pmatrix}
\]<br />
</p>

<p>
圖<a href="#org686494b">46</a>以\(a_i\)代表隱藏層 layer 1 中各節點的加權總和，以\(z\)代表以活化函數轉換後的訊號，至於\(h()\)則表示活化函數 sigmoid。<br />
</p>

<div id="org686494b" class="figure">
<p><img src="images/3LayerNetworkv1.png" alt="3LayerNetworkv1.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 46: </span>三層神經網路中 layer 0 至 layer 1 間的訊息傳遞</p>
</div>


<p>
以下列 python 執行此次轉換的結果如下。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span><span style="color: #5B6268;">#</span><span style="color: #5B6268;">python code for message from layer 0 to node a1</span>
<span class="linenr"> 2: </span><span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span><span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 4: </span>    <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 5: </span><span style="color: #dcaeea;">X</span> = np.array([<span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>])
<span class="linenr"> 6: </span><span style="color: #dcaeea;">W1</span> = np.array([[<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>], [<span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>]])
<span class="linenr"> 7: </span><span style="color: #dcaeea;">B1</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>])
<span class="linenr"> 8: </span><span style="color: #dcaeea;">A1</span> = np.dot(X, W1) + B1
<span class="linenr"> 9: </span><span style="color: #dcaeea;">Z1</span> = sigmoid(A1)
<span class="linenr">10: </span><span style="color: #c678dd;">print</span>(X)
<span class="linenr">11: </span><span style="color: #c678dd;">print</span>(Z1)
</pre>
</div>

<pre class="example">
[1.  0.5]
[0.57444252 0.66818777 0.75026011]
</pre>


<p>
各層間的訊息傳遞大致如上述模式，唯一稍有不同的地方在於當隱藏層 layer 2 將訊習傳遞給輸出層 layer 4 時，所應用的活化函數為 identitiy function，其實作方式如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">python code for identity function</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">identity_fun</span>(x):
<span class="linenr"> 4: </span>      <span style="color: #51afef;">return</span> x
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">W3</span> = np.array([[<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>], [<span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>]])
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">B3</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>])
<span class="linenr"> 7: </span>  <span style="color: #dcaeea;">Z2</span> = [<span style="color: #da8548; font-weight: bold;">0.62624937</span>, <span style="color: #da8548; font-weight: bold;">0.7710107</span>]
<span class="linenr"> 8: </span>  <span style="color: #dcaeea;">A3</span> = np.dot(Z2, W3) + B3
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">Y</span> = identity_fun(A3)
<span class="linenr">10: </span>  <span style="color: #c678dd;">print</span>(Y)
</pre>
</div>

<pre class="example">
[0.31682708 0.69627909]
</pre>
</div>
</li>
<li><a id="org3c65c0f"></a>Python 實作: 多層神經網路的訊息傳遞<br />
<div class="outline-text-4" id="text-6-6-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">python code for complete multi-layer network message trasmission</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">identity_fun</span>(x):
<span class="linenr"> 4: </span>      <span style="color: #51afef;">return</span> x
<span class="linenr"> 5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 6: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">init_network</span>():
<span class="linenr"> 8: </span>      <span style="color: #dcaeea;">network</span> = {}
<span class="linenr"> 9: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'W1'</span>] = np.array([[<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>], [<span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>]])
<span class="linenr">10: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'b1'</span>] = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>])
<span class="linenr">11: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'W2'</span>] = np.array([[<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>], [<span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>], [<span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #da8548; font-weight: bold;">0.6</span>]])
<span class="linenr">12: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'b2'</span>] = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>])
<span class="linenr">13: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'W3'</span>] = np.array([[<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.3</span>], [<span style="color: #da8548; font-weight: bold;">0.2</span>, <span style="color: #da8548; font-weight: bold;">0.4</span>]])
<span class="linenr">14: </span>      <span style="color: #dcaeea;">network</span>[<span style="color: #98be65;">'b3'</span>] = np.array([<span style="color: #da8548; font-weight: bold;">0.1</span>, <span style="color: #da8548; font-weight: bold;">0.2</span>])
<span class="linenr">15: </span>      <span style="color: #51afef;">return</span> network
<span class="linenr">16: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">forword</span>(network, x):
<span class="linenr">17: </span>      <span style="color: #dcaeea;">W1</span>, <span style="color: #dcaeea;">W2</span>, <span style="color: #dcaeea;">W3</span> = network[<span style="color: #98be65;">'W1'</span>], network[<span style="color: #98be65;">'W2'</span>], network[<span style="color: #98be65;">'W3'</span>]
<span class="linenr">18: </span>      <span style="color: #dcaeea;">b1</span>, <span style="color: #dcaeea;">b2</span>, <span style="color: #dcaeea;">b3</span> = network[<span style="color: #98be65;">'b1'</span>], network[<span style="color: #98be65;">'b2'</span>], network[<span style="color: #98be65;">'b3'</span>]
<span class="linenr">19: </span>      <span style="color: #dcaeea;">a1</span> = np.dot(x, W1) + b1
<span class="linenr">20: </span>      <span style="color: #dcaeea;">z1</span> = sigmoid(a1)
<span class="linenr">21: </span>      <span style="color: #dcaeea;">a2</span> = np.dot(z1, W2) + b2
<span class="linenr">22: </span>      <span style="color: #dcaeea;">z2</span> = sigmoid(a2)
<span class="linenr">23: </span>      <span style="color: #dcaeea;">a3</span> = np.dot(z2, W3) + b3
<span class="linenr">24: </span>      <span style="color: #dcaeea;">z3</span> = sigmoid(a3)
<span class="linenr">25: </span>      <span style="color: #dcaeea;">y</span> = identity_fun(z3)
<span class="linenr">26: </span>      <span style="color: #51afef;">return</span> y
<span class="linenr">27: </span>  <span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">28: </span>  <span style="color: #dcaeea;">x</span> = np.array([<span style="color: #da8548; font-weight: bold;">1.0</span>, <span style="color: #da8548; font-weight: bold;">0.5</span>])
<span class="linenr">29: </span>  <span style="color: #dcaeea;">y</span> = forword(network, x)
<span class="linenr">30: </span>
<span class="linenr">31: </span>  <span style="color: #c678dd;">print</span>(y)
</pre>
</div>

<pre class="example">
[0.57855079 0.66736228]
</pre>
</div>
</li>
</ol>
</div>

<div id="outline-container-org4d8681f" class="outline-3">
<h3 id="org4d8681f"><span class="section-number-3">6.7.</span> Backward propagation</h3>
<div class="outline-text-3" id="text-6-7">
<p>
反向傳遞的目的就是利用最後的目標函數(loss/cost function)來進行參數的更新，一般來說都是用誤差均方和(mean square error)當作目標函數。如果誤差值越大，代表參數學得不好，所以需要繼續學習，直到參數或是誤差值收斂<sup><a id="fnr.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup>。<br />
回到圖<a href="#org16262bf">42</a><br />
\(x^{(i)}\)為第i筆輸入，其輸出值為<br />
\[ y^{(i)}=\begin{bmatrix}
y_1^{(1)} \\
y_2^{(1)} \\
\end{bmatrix}\]<br />
而輸出值與目標的誤差為<br />
\[
E^{(i)}=\frac{1}{2}\sum_{j=0}^m(\hat{y}_j^{(i)}-y_j^{(i)})^2
\]<br />
目標函數為所有樣本的誤差和<br />
\[
E=\sum_{i=0}^nE^{(i)}
\]<br />
最佳化的目的就是讓「所有樣本的誤差均方和」越小越好，所以目標是將最小化(微分方程式等於0找解)。<br />
</p>
</div>
</div>

<div id="outline-container-org2bb8588" class="outline-3">
<h3 id="org2bb8588"><span class="section-number-3">6.8.</span> 輸出層的設計：恆等函數與 softmax 函數</h3>
<div class="outline-text-3" id="text-6-8">
<p>
神經網路可以用來解決分類問題與迴歸問題，端視輸出層所使用的活化函數，解決迴歸問題時使用恆等函數，而分類問題則使用 <i>softmax 函數</i> 。恆等函數對於輸入的內容完全不做任何處理，直接輸出，其神經網路的結構如圖<a href="#org582f0eb">47</a>所示。<br />
</p>

<div id="org582f0eb" class="figure">
<p><img src="images/identity-network.png" alt="identity-network.png" width="300" /><br />
</p>
<p><span class="figure-number">Figure 47: </span>恆等函數神經網路圖</p>
</div>

<p>
而分類問題使用的 softmax 函數則如公式\eqref{orgb25a332}所示，\(exp(x)\)為代表\(e^x\)的指標函數，輸出層有 n 個節點，而每個節點收到的訊息\(y_k\)來自前一層以箭頭連接的所有訊號輸入，由公式\eqref{orgb25a332}的分母也可以看出，輸出的各個神經元會依以「依各節點訊號量比例」的模式影響下一層的輸入。<br />
</p>

\begin{equation}
\label{orgb25a332}
y_k = \frac{exp(a_k)}{\sum_{i=1}^{n}exp(a_i)}
\end{equation}


<div id="org38dc6f7" class="figure">
<p><img src="images/softmax-network.png" alt="softmax-network.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 48: </span>softmax 函數神經網路圖</p>
</div>

<p>
至於 softmax 的 python 實作則如下程式碼所示，為了避免因矩陣 a 的值過大而導至指數函數運算出現溢位，程式碼第<a href="#coderef-softmax-overflow1" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-softmax-overflow1');" onmouseout="CodeHighlightOff(this, 'coderef-softmax-overflow1');">4</a>行的內容也可以改由第<a href="#coderef-softmax-overflow2" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-softmax-overflow2');" onmouseout="CodeHighlightOff(this, 'coderef-softmax-overflow2');">5</a>行替代。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">python code for softmax funtion</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(a):
<span id="coderef-softmax-overflow1" class="coderef-off"><span class="linenr"> 4: </span>      <span style="color: #dcaeea;">exp_a</span> = np.exp(a)</span>
<span id="coderef-softmax-overflow2" class="coderef-off"><span class="linenr"> 5: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">exp_a = np.exp(a - np.max(a))</span></span>
<span class="linenr"> 6: </span>      <span style="color: #dcaeea;">sum_exp_a</span> = np.<span style="color: #c678dd;">sum</span>(exp_a)
<span class="linenr"> 7: </span>      <span style="color: #dcaeea;">y</span> = exp_a / sum_exp_a
<span class="linenr"> 8: </span>      <span style="color: #51afef;">return</span>(y)
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">a</span> = np.array([<span style="color: #da8548; font-weight: bold;">0.3</span>, <span style="color: #da8548; font-weight: bold;">2.9</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])
<span class="linenr">10: </span>  <span style="color: #c678dd;">print</span>(softmax(a))
</pre>
</div>
<pre class="example">
[0.01821127 0.24519181 0.73659691]
</pre>
</div>
</div>

<div id="outline-container-org34585b6" class="outline-3">
<h3 id="org34585b6"><span class="section-number-3">6.9.</span> softmax 函數的特色</h3>
<div class="outline-text-3" id="text-6-9">
<p>
softmaxe 的輸出為介於 0 到 1 間的實數，此外，其輸出總和為 1，這個性質使的 softmax 函數的輸出也可解釋為「機率」。例如，前節程式碼的輸出結果為[0.01821127 0.24519181 0.73659691]，從以機率的角度我們可以說：分類的結果有 1.8%的機率為第 0 類；有 24.52%的機率為第 1 類；有 73.66%的機率為第 2 類。換言之，使用 softmax 函數可以針對問題提出對應的機率。<br />
softmax 函數的另一個特色是其輸出結果仍保持與輸入訊息一致的大小關係，這是因為惷數函數\(y=exp(x)\)為單週函數。一般而言，神經網路會把輸出最大神經元的類別當作辨識結果，然而，因為 softmax 不影響大小順序，所以一般會省略 softmax 函數。<br />
</p>

<p>
輸出層的節點數量取決於要解決的問題，例如，如果要解決的問題為「判斷一個手寫數字的結果」，則輸出層會有 10 個節點(分別代表 0~9)，而輸出訊息最大的結點則為最有可能的答案類別。<br />
</p>
</div>
</div>

<div id="outline-container-org1fcc3f7" class="outline-3">
<h3 id="org1fcc3f7"><span class="section-number-3">6.10.</span> 梯度</h3>
<div class="outline-text-3" id="text-6-10">
<p>
若要一次計算出\(x_0\)與\(x_1\)的偏微分，可以計算\((\frac{\partial{f}}{\partial{x_0}},\frac{\partial{f}}{\partial{x_1}})\)，這種方式也叫梯度（gradient），對應之計算程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x0, x1) = x0**2 + x1**2&#12398;&#21246;&#37197;&#22259;&#12434;&#25551;&#12367;&#12469;&#12531;&#12503;&#12523;&#12467;&#12540;&#12489;&#23455;&#34892;</span>
<span class="linenr"> 2: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">cat gradient_2d_save.py</span>
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">cf.http://d.hatena.ne.jp/white_wheels/20100327/p3</span>
<span class="linenr"> 6: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 7: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 8: </span>  plt.switch_backend(<span style="color: #98be65;">'agg'</span>)
<span class="linenr"> 9: </span>  <span style="color: #51afef;">from</span> mpl_toolkits.mplot3d <span style="color: #51afef;">import</span> Axes3D
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">_numerical_gradient_no_batch</span>(f, x):
<span class="linenr">12: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">0.0001</span>
<span class="linenr">13: </span>      <span style="color: #dcaeea;">grad</span> = np.zeros_like(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;&#21644;x&#30456;&#21516;&#24418;&#29376;&#12289;&#20839;&#23481;&#22343;&#28858;0&#30340;&#38499;&#21015;</span>
<span class="linenr">14: </span>
<span class="linenr">15: </span>      <span style="color: #51afef;">for</span> idx <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(x.size):
<span class="linenr">16: </span>          <span style="color: #dcaeea;">tmp_val</span> = x[idx]
<span class="linenr">17: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;f(x+h)</span>
<span class="linenr">18: </span>          <span style="color: #dcaeea;">x</span>[<span style="color: #dcaeea;">idx</span>] = <span style="color: #c678dd;">float</span>(tmp_val) + h
<span class="linenr">19: </span>          <span style="color: #dcaeea;">fxh1</span> = f(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x+h)</span>
<span class="linenr">20: </span>          <span style="color: #dcaeea;">x</span>[<span style="color: #dcaeea;">idx</span>] = tmp_val - h
<span class="linenr">21: </span>          <span style="color: #dcaeea;">fxh2</span> = f(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x-h)</span>
<span class="linenr">22: </span>          <span style="color: #dcaeea;">grad</span>[<span style="color: #dcaeea;">idx</span>] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr">23: </span>
<span class="linenr">24: </span>          <span style="color: #dcaeea;">x</span>[<span style="color: #dcaeea;">idx</span>] = tmp_val <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24674;&#24489;&#21407;&#20540;</span>
<span class="linenr">25: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr">26: </span>
<span id="coderef-NumerGrad" class="coderef-off"><span class="linenr">27: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, X):</span>
<span class="linenr">28: </span>      <span style="color: #51afef;">if</span> X.ndim == <span style="color: #da8548; font-weight: bold;">1</span>:
<span class="linenr">29: </span>          <span style="color: #51afef;">return</span> _numerical_gradient_no_batch(f, X)
<span class="linenr">30: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">31: </span>          grad = np.zeros_like(X)
<span class="linenr">32: </span>          <span style="color: #51afef;">for</span> idx, x <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(X):
<span class="linenr">33: </span>              grad[<span style="color: #dcaeea;">idx</span>] = _numerical_gradient_no_batch(f, x)
<span class="linenr">34: </span>          <span style="color: #51afef;">return</span> grad
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_2</span>(x):
<span class="linenr">37: </span>      <span style="color: #51afef;">if</span> x.ndim == <span style="color: #da8548; font-weight: bold;">1</span>:
<span class="linenr">38: </span>          <span style="color: #51afef;">return</span> np.<span style="color: #c678dd;">sum</span>(x**<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">39: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">40: </span>          <span style="color: #51afef;">return</span> np.<span style="color: #c678dd;">sum</span>(x**<span style="color: #da8548; font-weight: bold;">2</span>, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">41: </span>
<span class="linenr">42: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">tangent_line</span>(f, x):
<span class="linenr">43: </span>      d = numerical_gradient(f, x)
<span class="linenr">44: </span>      <span style="color: #c678dd;">print</span>(d)
<span class="linenr">45: </span>      y = f(x) - d*x
<span class="linenr">46: </span>      <span style="color: #51afef;">return</span> <span style="color: #51afef;">lambda</span> t: d*t + y
<span class="linenr">47: </span>
<span class="linenr">48: </span>  x0 = np.arange(-<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2.5</span>, <span style="color: #da8548; font-weight: bold;">0.25</span>)
<span class="linenr">49: </span>  x1 = np.arange(-<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2.5</span>, <span style="color: #da8548; font-weight: bold;">0.25</span>)
<span class="linenr">50: </span>  <span style="color: #dcaeea;">X</span>, <span style="color: #dcaeea;">Y</span> = np.meshgrid(x0, x1)
<span class="linenr">51: </span>  X = X.flatten()
<span class="linenr">52: </span>  Y = Y.flatten()
<span class="linenr">53: </span>  grad = numerical_gradient(function_2, np.array([X, Y]) )
<span class="linenr">54: </span>  plt.figure()
<span class="linenr">55: </span>  plt.quiver(X, Y, -grad[<span style="color: #da8548; font-weight: bold;">0</span>], -grad[<span style="color: #da8548; font-weight: bold;">1</span>],  angles=<span style="color: #98be65;">"xy"</span>,color=<span style="color: #98be65;">"#666666"</span>)
<span class="linenr">56: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">,headwidth=10,scale=40,color="#444444")</span>
<span class="linenr">57: </span>  plt.xlim([-<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>])
<span class="linenr">58: </span>  plt.ylim([-<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>])
<span class="linenr">59: </span>  plt.xlabel(<span style="color: #98be65;">'x0'</span>)
<span class="linenr">60: </span>  plt.ylabel(<span style="color: #98be65;">'x1'</span>)
<span class="linenr">61: </span>  plt.grid()
<span class="linenr">62: </span>  plt.legend()
<span class="linenr">63: </span>  plt.draw()
<span class="linenr">64: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">65: </span>  plt.savefig(<span style="color: #98be65;">'gradient_2d.png'</span>)
<span class="linenr">66: </span>  <span style="color: #c678dd;">print</span>(numerical_gradient(function_2, np.array([<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])))
<span class="linenr">67: </span>  <span style="color: #c678dd;">print</span>(numerical_gradient(function_2, np.array([<span style="color: #da8548; font-weight: bold;">0.0</span>, <span style="color: #da8548; font-weight: bold;">2.0</span>])))
<span class="linenr">68: </span>  <span style="color: #c678dd;">print</span>(numerical_gradient(function_2, np.array([<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">0.0</span>])))
</pre>
</div>

<p>
上述程式碼中，函數 numerical\textunderscore{}gradient 以函數 function\textunderscore{}2 以及陣列 x 為參數（程式碼第<a href="#coderef-NumerGrad" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-NumerGrad');" onmouseout="CodeHighlightOff(this, 'coderef-NumerGrad');">27</a>行），針對陣列 x 的各元素計算數值微分，計算(3, 4)、(0, 2)、(3, 0)各點的梯度結果如下：<br />
</p>
<pre class="example">
[6. 8.]
[0. 4.]
[6. 0.]
</pre>

<p>
如圖<a href="#org1d415c4">49</a>所示，所謂梯度，指的是函數\(f(x0,x1)\)的「最低位置（最小值）」，這裡的箭頭就如同羅盤，離「最低位置」越遠，箭頭越大。雖然圖<a href="#org1d415c4">49</a>中箭頭所指為最低位置，但實際上不一定如此，梯度所指其實為各點之最低方向，其數學意義為：函數值減少最多的方向。<br />
</p>

<div id="org1d415c4" class="figure">
<p><img src="images/gradient_2d.png" alt="gradient_2d.png" /><br />
</p>
<p><span class="figure-number">Figure 49: </span>\(f(x_0,x_1)=x_0^2+x_1^2\)之梯度圖形</p>
</div>
</div>

<ol class="org-ol">
<li><a id="org3ef9a1e"></a>梯度法<br />
<div class="outline-text-4" id="text-6-10-1">
<p>
神經網路的學習過程在於尋找最佳參數，這裡的意義也可說成：尋找可以讓損失函數為最小值的參數，以梯度來達成此目的的方法即為梯度法（gradient method），而最小值稱為鞍點（saddle point），鞍點是相對意義的存在，即，由某些方向看來是極大，但由其他方向看來為極小值，梯度法目的在找出梯度為 0 的位置，但不見的是極小值，當函數形成複雜扭曲形狀時，可能進入幾乎平坦而無法繼繼續學習的狀態，稱為「停滯期」。<br />
由此觀之，梯度法的精神即在於「朝著正確的方向前進一段距離、重新計算出正確方向、再前進一段距離&#x2026;.」。同樣的方法可以用來找出極小值（此時為梯度下降法，gradient descent method)，也能用來找出極大值（梯度上升法，gradient ascent method）。<br />
若以公式來顯示梯度法，其結果如公式\eqref{org5024ba1}所示：<br />
</p>
\begin{equation}
\label{org5024ba1}
\begin{split}
&x_0=x_0-\eta\frac{\partial{f}}{\partial{x_0}}\\
&x_1=x_1-\eta\frac{\partial{f}}{\partial{x_1}}
\end{split}
\end{equation}
<p>
公式\eqref{org5024ba1}中的\(\eta\)代表更新的量，在神經網路中稱為學習率（learning rate），即，在一次學習中，要學習多少，要更新多少參數。公式\eqref{org5024ba1}為一次更新的內容，或者，我們也可以看成「每次朝著正確方向前進的距離」。以 Python 實作的梯度下降法如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, x): <span style="color: #5B6268;">#</span><span style="color: #5B6268;">f &#28858;&#35201;&#36914;&#34892;&#26368;&#20339;&#21270;&#30340;&#30446;&#27161;&#20989;&#25976;</span>
<span class="linenr"> 3: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">0.0001</span>
<span class="linenr"> 4: </span>      <span style="color: #dcaeea;">grad</span> = np.zeros_like(x)
<span class="linenr"> 5: </span>      <span style="color: #51afef;">for</span> idx <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(x.size):
<span class="linenr"> 6: </span>          <span style="color: #dcaeea;">tmp_val</span> = x[idx]
<span class="linenr"> 7: </span>          <span style="color: #dcaeea;">x</span>[<span style="color: #dcaeea;">idx</span>] = tmp_val + h <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">&#35336;&#31639;f(x+h)</span>
<span class="linenr"> 8: </span>          <span style="color: #dcaeea;">fxh1</span> = f(x)
<span class="linenr"> 9: </span>          <span style="color: #dcaeea;">x</span>[<span style="color: #dcaeea;">idx</span>] = tmp_val - h <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;(x-h)</span>
<span class="linenr">10: </span>          <span style="color: #dcaeea;">fxh2</span> = f(x)
<span class="linenr">11: </span>          <span style="color: #dcaeea;">grad</span>[<span style="color: #dcaeea;">idx</span>] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr">12: </span>          <span style="color: #dcaeea;">x</span>[<span style="color: #dcaeea;">idx</span>] = tmp_val <span style="color: #5B6268;"># </span><span style="color: #5B6268;">restore original value</span>
<span class="linenr">13: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr">14: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">gradient_descent</span>(f, init_x, lr=<span style="color: #da8548; font-weight: bold;">0.01</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>):
<span class="linenr">15: </span>      x = init_x
<span class="linenr">16: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(step_num):
<span class="linenr">17: </span>          grad = numerical_gradient(f, x)
<span class="linenr">18: </span>          x -= lr * grad
<span class="linenr">19: </span>      <span style="color: #51afef;">return</span> x
<span class="linenr">20: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_2</span>(x):
<span class="linenr">21: </span>      <span style="color: #51afef;">return</span> x[<span style="color: #da8548; font-weight: bold;">0</span>]**<span style="color: #da8548; font-weight: bold;">2</span> + x[<span style="color: #da8548; font-weight: bold;">1</span>]**<span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr">22: </span>  init_x = np.array([-<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])
<span class="linenr">23: </span>  <span style="color: #c678dd;">print</span>(gradient_descent(function_2, init_x=init_x, lr=<span style="color: #da8548; font-weight: bold;">0.1</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>))
<span class="linenr">24: </span>  init_x = np.array([<span style="color: #da8548; font-weight: bold;">20.0</span>, -<span style="color: #da8548; font-weight: bold;">30.0</span>])
<span class="linenr">25: </span>  <span style="color: #c678dd;">print</span>(gradient_descent(function_2, init_x=init_x, lr=<span style="color: #da8548; font-weight: bold;">0.1</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>))
<span class="linenr">26: </span>  init_x = np.array([<span style="color: #da8548; font-weight: bold;">220.0</span>, -<span style="color: #da8548; font-weight: bold;">330.0</span>])
<span class="linenr">27: </span>  <span style="color: #c678dd;">print</span>(gradient_descent(function_2, init_x=init_x, lr=<span style="color: #da8548; font-weight: bold;">0.1</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>))
<span class="linenr">28: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#23416;&#32722;&#29575;&#35373;&#28858;10.0"</span>)
<span class="linenr">29: </span>  init_x = np.array([-<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])
<span class="linenr">30: </span>  <span style="color: #c678dd;">print</span>(gradient_descent(function_2, init_x=init_x, lr=<span style="color: #da8548; font-weight: bold;">10.0</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>))
<span class="linenr">31: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#23416;&#32722;&#29575;&#35373;&#28858;1e-10"</span>)
<span class="linenr">32: </span>  init_x = np.array([-<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])
<span class="linenr">33: </span>  <span style="color: #c678dd;">print</span>(gradient_descent(function_2, init_x=init_x, lr=1e-<span style="color: #da8548; font-weight: bold;">10</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>))
</pre>
</div>

<pre class="example">
[-6.11110793e-10  8.14814391e-10]
[ 4.07407195e-09 -6.11110793e-09]
[ 4.48147915e-08 -6.72221872e-08]
學習率設為 10.0
[-2.58983747e+13 -1.29524862e+12]
學習率設為 1e-10
[-2.99999994  3.99999992]
</pre>


<p>
上述程式中，init_x 為預設值、lr 代表 learning rate、step\textunderscore{}num 為重複次數，梯度由 numerical\textunderscore{}gradient(f,x)計算，程式執行結果為函數\(f(x_0,x_1)=x_0^2+x_1^2\)的最小值，預設值為\(x_0=-3,x_1=4\)，以梯度法求最小值，最後結果為(-6.1e-10,-8.1-10)，趨近於真實答案(0,0)，即使以其他預設值做為起點，所找到的最小值仍趨近正確答案, 而學習率太大或太小均無法得到良好的結果（學習率太大則會往大數值擴散；學習率太小則幾乎不更新就結束）。至於學習率這種依靠人工進行設定的值稱為超參數（hyperparameter），其最佳設定有賴以各種測試取得。至於梯度法的學習流程則如圖<a href="#org5a78636">50</a>所示。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">cat gradient_method_save.py</span>
<span class="linenr"> 2: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr"> 5: </span>  plt.switch_backend(<span style="color: #98be65;">'agg'</span>)
<span class="linenr"> 6: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, x): <span style="color: #5B6268;">#</span><span style="color: #5B6268;">f &#28858;&#35201;&#36914;&#34892;&#26368;&#20339;&#21270;&#30340;&#30446;&#27161;&#20989;&#25976;</span>
<span class="linenr"> 7: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">0.0001</span>
<span class="linenr"> 8: </span>      <span style="color: #dcaeea;">grad</span> = np.zeros_like(x)
<span class="linenr"> 9: </span>      <span style="color: #51afef;">for</span> idx <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(x.size):
<span class="linenr">10: </span>          <span style="color: #dcaeea;">tmp_val</span> = x[idx]
<span class="linenr">11: </span>          <span style="color: #dcaeea;">x</span>[<span style="color: #dcaeea;">idx</span>] = tmp_val + h <span style="color: #5B6268;">#  </span><span style="color: #5B6268;">&#35336;&#31639;f(x+h)</span>
<span class="linenr">12: </span>          <span style="color: #dcaeea;">fxh1</span> = f(x)
<span class="linenr">13: </span>          <span style="color: #dcaeea;">x</span>[<span style="color: #dcaeea;">idx</span>] = tmp_val - h <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;(x-h)</span>
<span class="linenr">14: </span>          <span style="color: #dcaeea;">fxh2</span> = f(x)
<span class="linenr">15: </span>          <span style="color: #dcaeea;">grad</span>[<span style="color: #dcaeea;">idx</span>] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr">16: </span>          <span style="color: #dcaeea;">x</span>[<span style="color: #dcaeea;">idx</span>] = tmp_val <span style="color: #5B6268;"># </span><span style="color: #5B6268;">restore original value</span>
<span class="linenr">17: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr">18: </span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">gradient_descent</span>(f, init_x, lr=<span style="color: #da8548; font-weight: bold;">0.01</span>, step_num=<span style="color: #da8548; font-weight: bold;">100</span>):
<span class="linenr">21: </span>      x = init_x
<span class="linenr">22: </span>      x_history = []
<span class="linenr">23: </span>
<span class="linenr">24: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(step_num):
<span class="linenr">25: </span>          x_history.append( x.copy() )
<span class="linenr">26: </span>
<span class="linenr">27: </span>          grad = numerical_gradient(f, x)
<span class="linenr">28: </span>          x -= lr * grad
<span class="linenr">29: </span>
<span class="linenr">30: </span>      <span style="color: #51afef;">return</span> x, np.array(x_history)
<span class="linenr">31: </span>
<span class="linenr">32: </span>
<span class="linenr">33: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">function_2</span>(x):
<span class="linenr">34: </span>      <span style="color: #51afef;">return</span> x[<span style="color: #da8548; font-weight: bold;">0</span>]**<span style="color: #da8548; font-weight: bold;">2</span> + x[<span style="color: #da8548; font-weight: bold;">1</span>]**<span style="color: #da8548; font-weight: bold;">2</span>
<span class="linenr">35: </span>
<span class="linenr">36: </span>  init_x = np.array([-<span style="color: #da8548; font-weight: bold;">3.0</span>, <span style="color: #da8548; font-weight: bold;">4.0</span>])
<span class="linenr">37: </span>
<span class="linenr">38: </span>  lr = <span style="color: #da8548; font-weight: bold;">0.1</span>
<span class="linenr">39: </span>  step_num = <span style="color: #da8548; font-weight: bold;">20</span>
<span class="linenr">40: </span>  x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)
<span class="linenr">41: </span>
<span class="linenr">42: </span>  plt.plot( [-<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">5</span>], [<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #98be65;">'--b'</span>)
<span class="linenr">43: </span>  plt.plot( [<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">0</span>], [-<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">5</span>], <span style="color: #98be65;">'--b'</span>)
<span class="linenr">44: </span>  plt.plot(x_history[:,<span style="color: #da8548; font-weight: bold;">0</span>], x_history[:,<span style="color: #da8548; font-weight: bold;">1</span>], <span style="color: #98be65;">'o'</span>)
<span class="linenr">45: </span>
<span class="linenr">46: </span>  plt.xlim(-<span style="color: #da8548; font-weight: bold;">3.5</span>, <span style="color: #da8548; font-weight: bold;">3.5</span>)
<span class="linenr">47: </span>  plt.ylim(-<span style="color: #da8548; font-weight: bold;">4.5</span>, <span style="color: #da8548; font-weight: bold;">4.5</span>)
<span class="linenr">48: </span>  plt.xlabel(<span style="color: #98be65;">"X0"</span>)
<span class="linenr">49: </span>  plt.ylabel(<span style="color: #98be65;">"X1"</span>)
<span class="linenr">50: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">51: </span>  plt.savefig(<span style="color: #98be65;">'gradient_process_method.png'</span>)
<span class="linenr">52: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">return "gradient_process_method.png"</span>
</pre>
</div>

<div id="org5a78636" class="figure">
<p><img src="images/gradient_process_method.png" alt="gradient_process_method.png" /><br />
</p>
<p><span class="figure-number">Figure 50: </span>\(f(x_0,x_1)=x_0^2+x_1^2\)的梯度法更新過程</p>
</div>
</div>
</li>

<li><a id="orgca60f4f"></a>神經網路的梯度<br />
<div class="outline-text-4" id="text-6-10-2">
<p>
神經網路的梯度指與權重參數有關的損失函數梯度，若有個形狀為\(2*3\)、權重為\(W\)的神經網路，以\(L\)代表損失函數，則可以用\(\frac{\partial{L}}{\partial{W}}\)來表示梯度，其公式如下：<br />
</p>
\begin{equation}
\label{org004c09b}
\begin{split}
W=
 \begin{pmatrix}
  w_{11} & w_{12} & w_{13} \\
  w_{21} & w_{22} & w_{23}
 \end{pmatrix} \\
\frac{\partial{L}}{\partial{W}}=
 \begin{pmatrix}
  \frac{\partial{L}}{\partial{w_{11}}} & \frac{\partial{L}}{\partial{w_{12}}} & \frac{\partial{L}}{\partial{w_{13}}} \\
  \frac{\partial{L}}{\partial{w_{21}}} & \frac{\partial{L}}{\partial{w_{22}}} & \frac{\partial{L}}{\partial{w_{23}}}
 \end{pmatrix} \\
\end{splix}
\end{equation}
<p>
\(\frac{\partial{L}}{\partial{W}}\)的各元素係由該元素的偏微分構成，如\(\frac{\partial{L}}{\partial{w_{11}}}\)即為在略改變\(w_{11}\)後損失函數所造成的變化，神經網路計算梯度的方式如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 2: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(x):
<span class="linenr"> 3: </span>      <span style="color: #51afef;">if</span> x.ndim == <span style="color: #da8548; font-weight: bold;">2</span>:
<span class="linenr"> 4: </span>          x = x.T
<span class="linenr"> 5: </span>          x = x - np.<span style="color: #c678dd;">max</span>(x, axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 6: </span>          y = np.exp(x) / np.<span style="color: #c678dd;">sum</span>(np.exp(x), axis=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 7: </span>          <span style="color: #51afef;">return</span> y.T
<span class="linenr"> 8: </span>
<span class="linenr"> 9: </span>      x = x - np.<span style="color: #c678dd;">max</span>(x)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#28322;&#20986;&#23545;&#31574;</span>
<span class="linenr">10: </span>      <span style="color: #51afef;">return</span> np.exp(x) / np.<span style="color: #c678dd;">sum</span>(np.exp(x))
<span class="linenr">11: </span>
<span class="linenr">12: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">cross_entropy_error</span>(y, t):
<span class="linenr">13: </span>      <span style="color: #51afef;">if</span> y.ndim == <span style="color: #da8548; font-weight: bold;">1</span>:
<span class="linenr">14: </span>          t = t.reshape(<span style="color: #da8548; font-weight: bold;">1</span>, t.size)
<span class="linenr">15: </span>          y = y.reshape(<span style="color: #da8548; font-weight: bold;">1</span>, y.size)
<span class="linenr">16: </span>
<span class="linenr">17: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30417;&#30563;&#25968;&#25454;&#26159;one-hot-vector&#30340;&#24773;&#20917;&#19979;&#65292;&#36716;&#25442;&#20026;&#27491;&#30830;&#35299;&#26631;&#31614;&#30340;&#32034;&#24341;</span>
<span class="linenr">18: </span>      <span style="color: #51afef;">if</span> t.size == y.size:
<span class="linenr">19: </span>          t = t.argmax(axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">20: </span>
<span class="linenr">21: </span>      batch_size = y.shape[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">22: </span>      <span style="color: #51afef;">return</span> -np.<span style="color: #c678dd;">sum</span>(np.log(y[np.arange(batch_size), t] + 1e-<span style="color: #da8548; font-weight: bold;">7</span>)) / batch_size
<span class="linenr">23: </span>
<span class="linenr">24: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">_numerical_gradient_no_batch</span>(f, x):
<span class="linenr">25: </span>      h = 1e-<span style="color: #da8548; font-weight: bold;">4</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">0.0001</span>
<span class="linenr">26: </span>      grad = np.zeros_like(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;&#21644;x&#30456;&#21516;&#24418;&#29376;&#12289;&#20839;&#23481;&#22343;&#28858;0&#30340;&#38499;&#21015;</span>
<span class="linenr">27: </span>
<span class="linenr">28: </span>      <span style="color: #51afef;">for</span> idx <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(x.size):
<span class="linenr">29: </span>          tmp_val = x[idx]
<span class="linenr">30: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;f(x+h)</span>
<span class="linenr">31: </span>          x[<span style="color: #dcaeea;">idx</span>] = <span style="color: #c678dd;">float</span>(tmp_val) + h
<span class="linenr">32: </span>          fxh1 = f(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x+h)</span>
<span class="linenr">33: </span>          x[<span style="color: #dcaeea;">idx</span>] = tmp_val - h
<span class="linenr">34: </span>          fxh2 = f(x) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x-h)</span>
<span class="linenr">35: </span>          grad[<span style="color: #dcaeea;">idx</span>] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr">36: </span>
<span class="linenr">37: </span>          x[<span style="color: #dcaeea;">idx</span>] = tmp_val <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24674;&#24489;&#21407;&#20540;</span>
<span class="linenr">38: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr">39: </span>
<span id="coderef-NumerGrad" class="coderef-off"><span class="linenr">40: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, X):</span>
<span class="linenr">41: </span>      <span style="color: #51afef;">if</span> X.ndim == <span style="color: #da8548; font-weight: bold;">1</span>:
<span class="linenr">42: </span>          <span style="color: #51afef;">return</span> _numerical_gradient_no_batch(f, X)
<span class="linenr">43: </span>      <span style="color: #51afef;">else</span>:
<span class="linenr">44: </span>          grad = np.zeros_like(X)
<span class="linenr">45: </span>          <span style="color: #51afef;">for</span> idx, x <span style="color: #51afef;">in</span> <span style="color: #c678dd;">enumerate</span>(X):
<span class="linenr">46: </span>              grad[<span style="color: #dcaeea;">idx</span>] = _numerical_gradient_no_batch(f, x)
<span class="linenr">47: </span>          <span style="color: #51afef;">return</span> grad
<span class="linenr">48: </span>
<span class="linenr">49: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">simpleNet</span>:
<span class="linenr">50: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>):
<span class="linenr">51: </span>          <span style="color: #51afef;">self</span>.W = np.random.randn(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">3</span>)  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#20197;&#24120;&#24907;&#20998;&#20296;&#21021;&#22987;&#21270;&#27402;&#37325;&#20998;&#37197;</span>
<span class="linenr">52: </span>
<span class="linenr">53: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(<span style="color: #51afef;">self</span>, x):
<span class="linenr">54: </span>          <span style="color: #51afef;">return</span> np.dot(x, <span style="color: #51afef;">self</span>.W)
<span class="linenr">55: </span>
<span class="linenr">56: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">loss</span>(<span style="color: #51afef;">self</span>, x, t):
<span class="linenr">57: </span>          z = <span style="color: #51afef;">self</span>.predict(x)
<span class="linenr">58: </span>          y = softmax(z)
<span class="linenr">59: </span>          loss = cross_entropy_error(y, t)
<span class="linenr">60: </span>          <span style="color: #51afef;">return</span> loss
<span class="linenr">61: </span>
<span class="linenr">62: </span>  net = simpleNet()
<span class="linenr">63: </span>  <span style="color: #c678dd;">print</span>(net.W)
<span class="linenr">64: </span>  x = np.array([<span style="color: #da8548; font-weight: bold;">0.6</span>, <span style="color: #da8548; font-weight: bold;">0.9</span>])
<span class="linenr">65: </span>  p = net.predict(x)
<span class="linenr">66: </span>  <span style="color: #c678dd;">print</span>(p)
<span class="linenr">67: </span>  t = np.array([<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">68: </span>  <span style="color: #c678dd;">print</span>(net.loss(x, t))
<span class="linenr">69: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"========================="</span>)
<span class="linenr">70: </span>  f = <span style="color: #51afef;">lambda</span> w: net.loss(x,t)
<span class="linenr">71: </span>  dW = numerical_gradient(f, net.W)
<span class="linenr">72: </span>  <span style="color: #c678dd;">print</span>(dW)
</pre>
</div>

<pre class="example">
[[-0.17058278  0.11372612 -1.53368818]
 [ 0.19931583 -0.60202593  0.07473724]]
[ 0.07703458 -0.47358766 -0.85294939]
1.6086010849717751
=========================
[[ 0.30439054  0.17550882 -0.47989936]
 [ 0.45658581  0.26326323 -0.71984904]]
</pre>


<p>
simpleNet 以一個\(2*3\)的陣列做為權重參數 W，x 為輸入資料，t為正確答案標籤，此處我們希望針對 loss 函數進行梯度法取得最小值，所以定義一個函數 f，代入 x 及 t 傳回損失函數，然後將函數 f 做為參數傳給 numerical\textunderscore{}gradient 進行梯度下降運算，由運算結果 dW 的陣列內容可以看出，每增加一個\(h\)，\(w_{11}\)就增加 0.45、而\(w_{23}\)則下降 0.72。由此看來，為了要減少損失函數，應該持續增加\(w_{11}\)、減少\(w{23}\)，而且，\(w{23}\)的貢獻最大。<br />
</p>
</div>
</li>

<li><a id="org667b06b"></a>進階閱讀<br />
<div class="outline-text-4" id="text-6-10-3">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=reOgXrNxwV8">https://www.youtube.com/watch?v=reOgXrNxwV8</a><br /></li>
<li><a href="https://www.youtube.com/watch?v=FPD2fsryGYk">https://www.youtube.com/watch?v=FPD2fsryGYk</a><br /></li>
<li><a href="https://github.com/Qinbf">https://github.com/Qinbf</a><br /></li>
<li><a href="https://www.youtube.com/channel/UCHbGAYq21PNz9REWNwe8Z-Q/videos">https://www.youtube.com/channel/UCHbGAYq21PNz9REWNwe8Z-Q/videos</a><br /></li>
</ul>
</div>
</li>
</ol>
</div>

<div id="outline-container-orgb592c2c" class="outline-3">
<h3 id="orgb592c2c"><span class="section-number-3">6.11.</span> 梯度下降法</h3>
<div class="outline-text-3" id="text-6-11">
<p>
<a href="https://www.youtube.com/watch?v=reOgXrNxwV8">https://www.youtube.com/watch?v=reOgXrNxwV8</a><br />
<a href="https://www.youtube.com/watch?v=FPD2fsryGYk">https://www.youtube.com/watch?v=FPD2fsryGYk</a><br />
<a href="https://github.com/Qinbf">https://github.com/Qinbf</a><br />
<a href="https://www.youtube.com/channel/UCHbGAYq21PNz9REWNwe8Z-Q/videos">https://www.youtube.com/channel/UCHbGAYq21PNz9REWNwe8Z-Q/videos</a><br />
</p>
</div>
</div>

<div id="outline-container-orgdc64721" class="outline-3">
<h3 id="orgdc64721"><span class="section-number-3">6.12.</span> 學習演算法</h3>
<div class="outline-text-3" id="text-6-12">
<p>
至此，神經網路的學習步驟大致如下：<br />
</p>
<ol class="org-ol">
<li>小批次: 即從訓練資料中所隨機挑選部份數據<br /></li>
<li>計算梯度: 然後朝著「減少小批次損失函數」為目標前進<br /></li>
<li>更新參數: 每前進一步，就更新參數<br /></li>
<li>回到步驟 1<br /></li>
</ol>

<p>
上述作法稱之為「準確率梯度下降法(<i>stochastic gradient descent</i>)」，意味著「針對所選出的資料進行梯度下降」，一般以 SGD 為名。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="orgf0a228e"></a>Gradient Descent<br />
<ol class="org-ol">
<li><a id="orge81ce48"></a>Watch: <a href="https://www.youtube.com/watch?v=yKKNr-QKz2Q">https://www.youtube.com/watch?v=yKKNr-QKz2Q</a><br /></li>
<li><a id="orgfca35c2"></a>Gradient Descent (GD)<br /></li>
<li><a id="org92363e2"></a>Batch Gradient Descent (BGD)<br /></li>
<li><a id="orgbb8df47"></a>Mini-Batch Gradient Descent (MBGD)<br /></li>
<li><a id="org65fa10a"></a>Stochastic Gradient Descent (SGD): 亂數選一個<br />
<div class="outline-text-5" id="text-6-12-1-5">
<ul class="org-ul">
<li>\( L=\sum_n{(\hat y ^n - (b+\sum {w_ix_i^n)})}^2\)<br /></li>
<li>Gradient Descent: \( \theta ^i = \theta ^{i-1} - \eta \triangledown L(\theta {i-1}) \)<br /></li>
<li>Stochastic Gradient Descent: \( L^n = (\hat y ^n - (b+\sum {w_ix_i^n)})}^2, \theta ^i=\theta ^{i-1} - \eta \triangledown L(\theta {i-1}) \)<br /></li>
</ul>
</div>
</li>
</ol>
</li>

<li><a id="org6d865f2"></a>優化器演算法 Optimizer<br />
<ol class="org-ol">
<li><a id="orgff4f637"></a>Momentum<br />
<div class="outline-text-5" id="text-6-12-2-1">
<p>
主要是用在計算參數更新方向前會考慮前一次參數更新的方向(<br />
</p>
</div>
</li>

<li><a id="orgfce318e"></a>Adaptive Learning Rates<br />
<ol class="org-ol">
<li><a id="orgaa2c095"></a>Reduce the learning rate by some factor every few epochs.<br />
<div class="outline-text-6" id="text-6-12-2-2-1">
<ul class="org-ul">
<li>At the beginning, we are far from the destination, so we use larger learning rate<br /></li>
<li>After several epochs, we are close to the destination, so we reduce the learning rate<br /></li>
<li>e.g. \(\frac{1}{t}\)decay: \(\eta^t=\frac{\eta}{\sqrt{t+1}} \)<br /></li>
</ul>
</div>
</li>
<li><a id="orgb011c4a"></a>Learning cannot be on-size-fits-all<br />
<div class="outline-text-6" id="text-6-12-2-2-2">
<ul class="org-ul">
<li>Given different parameters different learning rate<br /></li>
</ul>
</div>
</li>
</ol>
</li>
<li><a id="org41fa56c"></a>Adaptive Learning Rates 範例<br />
<ol class="org-ol">
<li><a id="orgc964a08"></a>Adagrad (John Duchi, JMLR&rsquo;11)<br />
<div class="outline-text-6" id="text-6-12-2-3-1">
<ul class="org-ul">
<li>Original: \( w \leftarrow w - \frac{\eta\partial L}{\partial w} \)<br /></li>
<li>Adagrad: \( w \leftarrow - \frac{\eta_w\partial L}{\partial w}, where \partial_w = \frac{\eta}{\sqrt{\sum^t_i=0 (g^i)^2}} \), \(g^i\)is \(\frac{\partial L}{\partial w}\)obtained at the i-th update, and \(\partial_w\)means parameter dependent learning rate.<br /></li>
</ul>
</div>
</li>
<li><a id="org075c525"></a>RMSprop<br />
<div class="outline-text-6" id="text-6-12-2-3-2">
<ul class="org-ul">
<li><a href="https://www.youtube.com/watch?v=O3sxAc4hxZU">https://www.youtube.com/watch?v=O3sxAc4hxZU</a><br /></li>
</ul>
</div>
</li>
<li><a id="org23367cf"></a>Adadelta (Metthew D. Zeiler, arXiv&rsquo;12)<br /></li>
<li><a id="org7439b1b"></a>&ldquo;No more pesky learning rates&rdquo; (Tom Schaul, arXiv&rsquo;12)<br /></li>
<li><a id="org3ac1716"></a>AdaSecant (Caglar Gulcehre, arXiv&rsquo;14)<br /></li>
<li><a id="orgd30d91d"></a>Adam (Diederik P. Kingma, ICLR&rsquo;15)<br />
<div class="outline-text-6" id="text-6-12-2-3-6">
<ul class="org-ul">
<li>Momentum: 計算參數更新方向前會考慮前一次參數更新的方向<br /></li>
<li>RMSprop: 在學習率上依據梯度的大小對學習率進行加強或是衰減<br /></li>
<li>Adam: 合併上述，各自做偏差的修正<br /></li>
<li>Video: <a href="https://www.youtube.com/watch?v=_JB0AO7QxSA">Standfard: Lecture 7 | Training Neural Networks II</a><br /></li>
</ul>
</div>
</li>
<li><a id="org48e790b"></a>Nadam<br />
<div class="outline-text-6" id="text-6-12-2-3-7">
<ul class="org-ul">
<li><a href="http://cs229.standford.edu/proj2015/054_report.pdf">http://cs229.standford.edu/proj2015/054_report.pdf</a><br /></li>
</ul>
</div>
</li>
</ol>
</li>
</ol>
</li>

<li><a id="org42532ab"></a>雙層神經網路的類別<br />
<div class="outline-text-4" id="text-6-12-3">
<p>
以下以一個雙層神經網路的實作來進行說明：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">coding: utf-8</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> sys, os
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span>  sys.path.append(os.pardir)
<span class="linenr"> 5: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">from common.functions import *</span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">from common.gradient import numerical_gradient</span>
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 9: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(a):
<span id="coderef-softmax-overflow" class="coderef-off"><span class="linenr">12: </span>      <span style="color: #dcaeea;">exp_a</span> = np.exp(a)</span>
<span id="coderef-softmax-overflow2" class="coderef-off"><span class="linenr">13: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">exp_a = np.exp(a - np.max(a))</span></span>
<span class="linenr">14: </span>      <span style="color: #dcaeea;">sum_exp_a</span> = np.<span style="color: #c678dd;">sum</span>(exp_a)
<span class="linenr">15: </span>      <span style="color: #dcaeea;">y</span> = exp_a / sum_exp_a
<span class="linenr">16: </span>      <span style="color: #51afef;">return</span>(y)
<span class="linenr">17: </span>
<span class="linenr">18: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">cross_entropy_error</span>(y, t):
<span class="linenr">19: </span>      <span style="color: #dcaeea;">delta</span> = 1e-<span style="color: #da8548; font-weight: bold;">7</span>
<span class="linenr">20: </span>      <span style="color: #51afef;">return</span> -np.<span style="color: #c678dd;">sum</span>(t * np.log(y + delta))
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, x):
<span class="linenr">23: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr">24: </span>      <span style="color: #dcaeea;">grad</span> = np.zeros_like(x)
<span class="linenr">25: </span>      <span style="color: #dcaeea;">it</span> = np.nditer(x, flags=[<span style="color: #98be65;">'multi_index'</span>], op_flags=[<span style="color: #98be65;">'readwrite'</span>])
<span class="linenr">26: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25976;&#20540;&#24494;&#20998;&#35336;&#31639;&#21443;&#25976;&#26799;&#24230;</span>
<span class="linenr">27: </span>      <span style="color: #51afef;">while</span> <span style="color: #51afef;">not</span> it.finished:
<span class="linenr">28: </span>          idx = it.multi_index
<span class="linenr">29: </span>          tmp_val = x[idx]
<span class="linenr">30: </span>          x[<span style="color: #dcaeea;">idx</span>] = <span style="color: #c678dd;">float</span>(tmp_val) + h
<span class="linenr">31: </span>          fxh1 = f(x)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x+h)</span>
<span class="linenr">32: </span>
<span class="linenr">33: </span>          x[<span style="color: #dcaeea;">idx</span>] = tmp_val - h
<span class="linenr">34: </span>          fxh2 = f(x)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x-h)</span>
<span class="linenr">35: </span>          grad[<span style="color: #dcaeea;">idx</span>] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr">36: </span>
<span class="linenr">37: </span>          x[<span style="color: #dcaeea;">idx</span>] = tmp_val  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20516;&#12434;&#20803;&#12395;&#25147;&#12377;</span>
<span class="linenr">38: </span>          it.iternext()
<span class="linenr">39: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr">40: </span>
<span class="linenr">41: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">TwoLayerNet</span>:
<span id="coderef-TLN-init" class="coderef-off"><span class="linenr">42: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, input_size, hidden_size, output_size, weight_init_std=<span style="color: #da8548; font-weight: bold;">0.01</span>):</span>
<span class="linenr">43: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38568;&#27231;&#20998;&#37197;weight, bias&#35373;&#28858;0</span>
<span id="coderef-TLN-params" class="coderef-off"><span class="linenr">44: </span>          <span style="color: #51afef;">self</span>.params = {}</span>
<span class="linenr">45: </span>          <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W1'</span>] = weight_init_std * np.random.randn(input_size, hidden_size)
<span class="linenr">46: </span>          <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b1'</span>] = np.zeros(hidden_size)
<span class="linenr">47: </span>          <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W2'</span>] = weight_init_std * np.random.randn(hidden_size, output_size)
<span class="linenr">48: </span>          <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b2'</span>] = np.zeros(output_size)
<span class="linenr">49: </span>
<span id="coderef-TLN-pred" class="coderef-off"><span class="linenr">50: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(<span style="color: #51afef;">self</span>, x):</span>
<span class="linenr">51: </span>          W1, W2 = <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W1'</span>], <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W2'</span>]
<span class="linenr">52: </span>          b1, b2 = <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b1'</span>], <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b2'</span>]
<span class="linenr">53: </span>
<span class="linenr">54: </span>          a1 = np.dot(x, W1) + b1
<span class="linenr">55: </span>          z1 = sigmoid(a1)
<span class="linenr">56: </span>          a2 = np.dot(z1, W2) + b2
<span class="linenr">57: </span>          y = softmax(a2)
<span class="linenr">58: </span>          <span style="color: #51afef;">return</span> y
<span class="linenr">59: </span>
<span class="linenr">60: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">x : &#36664;&#20837;&#36039;&#26009;, t : &#35347;&#32244;&#36039;&#26009;</span>
<span id="coderef-TLN-loss" class="coderef-off"><span class="linenr">61: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">loss</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr">62: </span>          y = <span style="color: #51afef;">self</span>.predict(x)
<span class="linenr">63: </span>          <span style="color: #51afef;">return</span> cross_entropy_error(y, t)
<span class="linenr">64: </span>
<span id="coderef-TLN-accu" class="coderef-off"><span class="linenr">65: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">accuracy</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr">66: </span>          y = <span style="color: #51afef;">self</span>.predict(x)
<span class="linenr">67: </span>          y = np.argmax(y, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">68: </span>          t = np.argmax(t, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr">69: </span>          accuracy = np.<span style="color: #c678dd;">sum</span>(y == t) / <span style="color: #c678dd;">float</span>(x.shape[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">70: </span>          <span style="color: #51afef;">return</span> accuracy
<span class="linenr">71: </span>
<span class="linenr">72: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35336;&#31639;&#21508;&#21443;&#25976;&#30340;&#26799;&#24230;</span>
<span id="coderef-TLN-numgrad" class="coderef-off"><span class="linenr">73: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr">74: </span>          loss_W = <span style="color: #51afef;">lambda</span> W: <span style="color: #51afef;">self</span>.loss(x, t)
<span id="coderef-TLN-grads" class="coderef-off"><span class="linenr">75: </span>          grads = {}</span>
<span class="linenr">76: </span>          grads[<span style="color: #98be65;">'W1'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W1'</span>])
<span class="linenr">77: </span>          grads[<span style="color: #98be65;">'b1'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b1'</span>])
<span class="linenr">78: </span>          grads[<span style="color: #98be65;">'W2'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W2'</span>])
<span class="linenr">79: </span>          grads[<span style="color: #98be65;">'b2'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b2'</span>])
<span id="coderef-TLN-grad" class="coderef-off"><span class="linenr">80: </span>          <span style="color: #51afef;">return</span> grads</span>
<span class="linenr">81: </span>
<span class="linenr">82: </span>  <span style="color: #51afef;">if</span> <span style="color: #c678dd;">__name__</span> == <span style="color: #98be65;">"__main__"</span>:
<span class="linenr">83: </span>      net = TwoLayerNet(input_size=<span style="color: #da8548; font-weight: bold;">784</span>, hidden_size=<span style="color: #da8548; font-weight: bold;">100</span>, output_size=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">84: </span>      x = np.random.rand(<span style="color: #da8548; font-weight: bold;">100</span>, <span style="color: #da8548; font-weight: bold;">784</span>)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;100&#24373;&#34395;&#25836;&#36039;&#26009;,&#36664;&#20837;&#24433;&#20687;&#23610;&#21515;&#28858;28*28</span>
<span class="linenr">85: </span>      y = net.predict(x)
<span class="linenr">86: </span>      t = np.random.rand(<span style="color: #da8548; font-weight: bold;">100</span>, <span style="color: #da8548; font-weight: bold;">10</span>)   <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#29986;&#29983;100&#20491;&#34395;&#25836;&#30340;&#27491;&#30906;&#31572;&#26696;&#27161;&#31844;,&#27599;&#20491;&#27161;&#31844;&#26377;10&#20491;&#39006;&#21029;</span>
<span class="linenr">87: </span>      rands = net.numerical_gradient(x, t) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;&#26799;&#24230;</span>
<span class="linenr">88: </span>      <span style="color: #c678dd;">print</span>(rands[<span style="color: #98be65;">'W1'</span>])
<span class="linenr">89: </span>      <span style="color: #c678dd;">print</span>(rands[<span style="color: #98be65;">'b1'</span>])
<span class="linenr">90: </span>      <span style="color: #c678dd;">print</span>(rands[<span style="color: #98be65;">'W2'</span>].shape)
<span class="linenr">91: </span>      <span style="color: #c678dd;">print</span>(rands[<span style="color: #98be65;">'b2'</span>].shape)
</pre>
</div>

<pre class="example" id="orgeaee81d">
[[ 0.0226828   0.02099976  0.0072832  ... -0.01241413  0.01743683
   0.00993156]
 [ 0.01682113  0.02138894  0.00436742 ... -0.01206379  0.01297583
   0.01304035]
 [ 0.01799636  0.01291382  0.01139191 ... -0.01196413  0.00845844
   0.00695222]
 ...
 [ 0.01469396  0.01507025  0.01288363 ... -0.01490424  0.0104561
   0.01669903]
 [ 0.01329594  0.01683885  0.0005939  ... -0.01176064  0.01377726
   0.01745393]
 [ 0.00360546  0.02353488  0.00761169 ... -0.01557991  0.00776723
   0.01624418]]
[ 0.03290075  0.03490604  0.01284017 -0.01780122 -0.00668078  0.00027693
  0.07401821  0.02162429  0.03047587  0.02048195  0.01182109 -0.0355056
  0.02144739 -0.01236585  0.03770265 -0.01983078  0.03263528 -0.02679656
  0.01665097  0.03983289  0.01158896  0.00058352  0.01279443  0.01702231
  0.00547242  0.04439308 -0.06806267  0.02272559 -0.01069871 -0.01259218
  0.00342859 -0.03044122  0.02350026 -0.02573413 -0.00946125  0.00146879
 -0.03312301  0.01705135  0.01477106  0.00567291  0.04036002  0.00070219
 -0.02369712  0.06778704 -0.02958217  0.02881278  0.03482847  0.0426526
 -0.00032182 -0.00342597 -0.03101994  0.006919    0.00050134 -0.0290734
  0.02713591 -0.0429896  -0.03356116  0.04456035  0.03420876 -0.04981342
 -0.00948666  0.03227471  0.00046413 -0.00104241 -0.03213071 -0.07219453
  0.01028855 -0.00079787  0.00348281 -0.02041278  0.00601914 -0.00067298
 -0.01878876  0.02557294 -0.02868863 -0.03383165  0.03133762  0.01079506
 -0.02313358  0.02697757  0.01809054  0.03784744 -0.00824714  0.05378442
  0.01241693  0.03035761  0.00919517 -0.03753735 -0.06883637  0.00995454
  0.01601707 -0.02033777 -0.02233426  0.00668454  0.0055338  -0.00663979
  0.00126532 -0.02274279  0.02845813  0.02777448]
(100, 10)
(10,)
</pre>

<p>
上述程式變數補充說明如表<a href="#orgcddeaf6">1</a>，<br />
</p>
<table id="orgcddeaf6" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> TwoLayerNet 變數說明</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">變數</th>
<th scope="col" class="org-left">行數</th>
<th scope="col" class="org-left">說明</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">params</td>
<td class="org-left"><a href="#coderef-TLN-params" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-params');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-params');">44</a></td>
<td class="org-left">神經網路參數，為字典型態變數</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">params[&rsquo;W1&rsquo;]為第 1 層權重、params[&rsquo;b1&rsquo;]為第 1 層偏權值</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">grads</td>
<td class="org-left"><a href="#coderef-TLN-grads" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-grads');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-grads');">75</a></td>
<td class="org-left">numerical_\textunderscore{}gradient 函數的傳回值，為字典型態變數</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">grads[&rsquo;W1&rsquo;]為第 1 層權重的梯度、grads[&rsquo;b1&rsquo;&rsquo;為第 1 層偏權值的梯度</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">input\textunderscore{}size</td>
<td class="org-left"><a href="#coderef-TLN-init" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-init');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-init');">42</a></td>
<td class="org-left">輸入層神經元數量</td>
</tr>

<tr>
<td class="org-left">hidden\textunderscore{}size</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">隱藏層神經元數量</td>
</tr>

<tr>
<td class="org-left">output\textunderscore{}size</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">輸出層神經元數量</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">predict(self, x)</td>
<td class="org-left"><a href="#coderef-TLN-pred" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-pred');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-pred');">50</a></td>
<td class="org-left">傳入輸入資料(x)為參數，進行辨識預測</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">loss(self, x, t)</td>
<td class="org-left"><a href="#coderef-TLN-loss" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-loss');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-loss');">61</a></td>
<td class="org-left">傳入影像資料(x)、正確答案標籤(t)為參數，計算損失函數自然不是逐一計算</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">accuracy(self, x, t)</td>
<td class="org-left"><a href="#coderef-TLN-accu" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-accu');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-accu');">65</a></td>
<td class="org-left">計算準確度</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">numericalk\textunderscore{}gradient</td>
<td class="org-left"><a href="#coderef-TLN-numgrad" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-numgrad');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-numgrad');">73</a></td>
<td class="org-left">計算權重值及偏權值梯度</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">gradient(self, x, t)</td>
<td class="org-left"><a href="#coderef-TLN-grad" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-TLN-grad');" onmouseout="CodeHighlightOff(this, 'coderef-TLN-grad');">80</a></td>
<td class="org-left">同上，速度較快</td>
</tr>
</tbody>
</table>
<p>
雙層神經網路則如圖<a href="#org0a74aea">51</a>所示<br />
</p>

<div id="org0a74aea" class="figure">
<p><img src="images/tln.png" alt="tln.png" /><br />
</p>
<p><span class="figure-number">Figure 51: </span>雙層神經網路</p>
</div>
</div>
</li>

<li><a id="org26bc12b"></a>小批次學習<br />
<div class="outline-text-4" id="text-6-12-4">
<p>
以下以前節之 TwoLayerNet 類別、以小批次學習來解決 MNIST 手寫字母辨識問題。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  2: </span>  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr">  3: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr">  4: </span>
<span class="linenr">  5: </span>  <span style="color: #dcaeea;">mnist</span> = tf.keras.datasets.mnist
<span class="linenr">  6: </span>  (x_train, t_train), (<span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">t_test</span>) = mnist.load_data()
<span class="linenr">  7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#22294;&#29255;&#36681;&#25563;&#28858;&#19968;&#20491;60000*784&#30340;&#21521;&#37327;&#65292;&#20006;&#19988;&#27161;&#28310;&#21270;</span>
<span class="linenr">  8: </span>  <span style="color: #dcaeea;">x_train</span> = x_train.reshape(x_train.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">  9: </span>  <span style="color: #dcaeea;">x_test</span> = x_test.reshape(x_test.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr"> 10: </span>  <span style="color: #dcaeea;">x_train</span> = x_train.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 11: </span>  <span style="color: #dcaeea;">x_test</span> = x_test.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr"> 12: </span>  <span style="color: #dcaeea;">x_train</span> = x_train/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr"> 13: </span>  <span style="color: #dcaeea;">x_test</span> = x_test/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr"> 14: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;y&#36681;&#25563;&#25104;one-hot encoding</span>
<span class="linenr"> 15: </span>  <span style="color: #dcaeea;">t_train</span> = np_utils.to_categorical(t_train, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr"> 16: </span>  <span style="color: #dcaeea;">t_test</span> = np_utils.to_categorical(t_test, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr"> 17: </span>
<span class="linenr"> 18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">========================================================</span>
<span class="linenr"> 19: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 20: </span>      <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 21: </span>
<span class="linenr"> 22: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(a):
<span id="coderef-softmax-overflow" class="coderef-off"><span class="linenr"> 23: </span>      <span style="color: #dcaeea;">exp_a</span> = np.exp(a)</span>
<span id="coderef-softmax-overflow2" class="coderef-off"><span class="linenr"> 24: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">exp_a = np.exp(a - np.max(a))</span></span>
<span class="linenr"> 25: </span>      <span style="color: #dcaeea;">sum_exp_a</span> = np.<span style="color: #c678dd;">sum</span>(exp_a)
<span class="linenr"> 26: </span>      <span style="color: #dcaeea;">y</span> = exp_a / sum_exp_a
<span class="linenr"> 27: </span>      <span style="color: #51afef;">return</span>(y)
<span class="linenr"> 28: </span>
<span class="linenr"> 29: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">cross_entropy_error</span>(y, t):
<span class="linenr"> 30: </span>      <span style="color: #dcaeea;">delta</span> = 1e-<span style="color: #da8548; font-weight: bold;">7</span>
<span class="linenr"> 31: </span>      <span style="color: #51afef;">return</span> -np.<span style="color: #c678dd;">sum</span>(t * np.log(y + delta))
<span class="linenr"> 32: </span>
<span class="linenr"> 33: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(f, x):
<span class="linenr"> 34: </span>      <span style="color: #dcaeea;">h</span> = 1e-<span style="color: #da8548; font-weight: bold;">4</span>
<span class="linenr"> 35: </span>      <span style="color: #dcaeea;">grad</span> = np.zeros_like(x)
<span class="linenr"> 36: </span>      <span style="color: #dcaeea;">it</span> = np.nditer(x, flags=[<span style="color: #98be65;">'multi_index'</span>], op_flags=[<span style="color: #98be65;">'readwrite'</span>])
<span class="linenr"> 37: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25976;&#20540;&#24494;&#20998;&#35336;&#31639;&#21443;&#25976;&#26799;&#24230;</span>
<span class="linenr"> 38: </span>      <span style="color: #51afef;">while</span> <span style="color: #51afef;">not</span> it.finished:
<span class="linenr"> 39: </span>          idx = it.multi_index
<span class="linenr"> 40: </span>          tmp_val = x[idx]
<span class="linenr"> 41: </span>          x[<span style="color: #dcaeea;">idx</span>] = <span style="color: #c678dd;">float</span>(tmp_val) + h
<span class="linenr"> 42: </span>          fxh1 = f(x)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x+h)</span>
<span class="linenr"> 43: </span>
<span class="linenr"> 44: </span>          x[<span style="color: #dcaeea;">idx</span>] = tmp_val - h
<span class="linenr"> 45: </span>          fxh2 = f(x)  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">f(x-h)</span>
<span class="linenr"> 46: </span>          grad[<span style="color: #dcaeea;">idx</span>] = (fxh1 - fxh2) / (<span style="color: #da8548; font-weight: bold;">2</span>*h)
<span class="linenr"> 47: </span>
<span class="linenr"> 48: </span>          x[<span style="color: #dcaeea;">idx</span>] = tmp_val  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20516;&#12434;&#20803;&#12395;&#25147;&#12377;</span>
<span class="linenr"> 49: </span>          it.iternext()
<span class="linenr"> 50: </span>      <span style="color: #51afef;">return</span> grad
<span class="linenr"> 51: </span>
<span class="linenr"> 52: </span>  <span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">TwoLayerNet</span>:
<span id="coderef-TLN-init" class="coderef-off"><span class="linenr"> 53: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">__init__</span>(<span style="color: #51afef;">self</span>, input_size, hidden_size, output_size, weight_init_std=<span style="color: #da8548; font-weight: bold;">0.01</span>):</span>
<span class="linenr"> 54: </span>          <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38568;&#27231;&#20998;&#37197;weight, bias&#35373;&#28858;0</span>
<span id="coderef-TLN-params" class="coderef-off"><span class="linenr"> 55: </span>          <span style="color: #51afef;">self</span>.params = {}</span>
<span class="linenr"> 56: </span>          <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W1'</span>] = weight_init_std * np.random.randn(input_size, hidden_size)
<span class="linenr"> 57: </span>          <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b1'</span>] = np.zeros(hidden_size)
<span class="linenr"> 58: </span>          <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W2'</span>] = weight_init_std * np.random.randn(hidden_size, output_size)
<span class="linenr"> 59: </span>          <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b2'</span>] = np.zeros(output_size)
<span class="linenr"> 60: </span>
<span id="coderef-TLN-pred" class="coderef-off"><span class="linenr"> 61: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(<span style="color: #51afef;">self</span>, x):</span>
<span class="linenr"> 62: </span>          W1, W2 = <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W1'</span>], <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W2'</span>]
<span class="linenr"> 63: </span>          b1, b2 = <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b1'</span>], <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b2'</span>]
<span class="linenr"> 64: </span>
<span class="linenr"> 65: </span>          a1 = np.dot(x, W1) + b1
<span class="linenr"> 66: </span>          z1 = sigmoid(a1)
<span class="linenr"> 67: </span>          a2 = np.dot(z1, W2) + b2
<span class="linenr"> 68: </span>          y = softmax(a2)
<span class="linenr"> 69: </span>          <span style="color: #51afef;">return</span> y
<span class="linenr"> 70: </span>
<span class="linenr"> 71: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">x : &#36664;&#20837;&#36039;&#26009;, t : &#35347;&#32244;&#36039;&#26009;</span>
<span id="coderef-TLN-loss" class="coderef-off"><span class="linenr"> 72: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">loss</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr"> 73: </span>          y = <span style="color: #51afef;">self</span>.predict(x)
<span class="linenr"> 74: </span>          <span style="color: #51afef;">return</span> cross_entropy_error(y, t)
<span class="linenr"> 75: </span>
<span id="coderef-TLN-accu" class="coderef-off"><span class="linenr"> 76: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">accuracy</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr"> 77: </span>          y = <span style="color: #51afef;">self</span>.predict(x)
<span class="linenr"> 78: </span>          y = np.argmax(y, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 79: </span>          t = np.argmax(t, axis=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 80: </span>          accuracy = np.<span style="color: #c678dd;">sum</span>(y == t) / <span style="color: #c678dd;">float</span>(x.shape[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr"> 81: </span>          <span style="color: #51afef;">return</span> accuracy
<span class="linenr"> 82: </span>
<span class="linenr"> 83: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35336;&#31639;&#21508;&#21443;&#25976;&#30340;&#26799;&#24230;</span>
<span id="coderef-TLN-numgrad" class="coderef-off"><span class="linenr"> 84: </span>      <span style="color: #51afef;">def</span> <span style="color: #c678dd;">numerical_gradient</span>(<span style="color: #51afef;">self</span>, x, t):</span>
<span class="linenr"> 85: </span>          loss_W = <span style="color: #51afef;">lambda</span> W: <span style="color: #51afef;">self</span>.loss(x, t)
<span id="coderef-TLN-grads" class="coderef-off"><span class="linenr"> 86: </span>          grads = {}</span>
<span class="linenr"> 87: </span>          grads[<span style="color: #98be65;">'W1'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W1'</span>])
<span class="linenr"> 88: </span>          grads[<span style="color: #98be65;">'b1'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b1'</span>])
<span class="linenr"> 89: </span>          grads[<span style="color: #98be65;">'W2'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'W2'</span>])
<span class="linenr"> 90: </span>          grads[<span style="color: #98be65;">'b2'</span>] = numerical_gradient(loss_W, <span style="color: #51afef;">self</span>.params[<span style="color: #98be65;">'b2'</span>])
<span class="linenr"> 91: </span>          <span style="color: #51afef;">return</span> grads
<span class="linenr"> 92: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">=====================================================================</span>
<span class="linenr"> 93: </span>  train_loss_list = []
<span class="linenr"> 94: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36229;&#21443;&#25976;&#35373;&#23450;</span>
<span class="linenr"> 95: </span>  iters_num = <span style="color: #da8548; font-weight: bold;">10000</span>
<span class="linenr"> 96: </span>  train_size = x_train.shape[<span style="color: #da8548; font-weight: bold;">0</span>]
<span id="coderef-batchSize" class="coderef-off"><span class="linenr"> 97: </span>  batch_size = <span style="color: #da8548; font-weight: bold;">100</span></span>
<span class="linenr"> 98: </span>  learning_rate = <span style="color: #da8548; font-weight: bold;">0.1</span>
<span class="linenr"> 99: </span>
<span class="linenr">100: </span>  network = TwoLayerNet(input_size=<span style="color: #da8548; font-weight: bold;">784</span>, hidden_size=<span style="color: #da8548; font-weight: bold;">50</span>, output_size=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">101: </span>
<span class="linenr">102: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">for i in range(iters_num):</span>
<span id="coderef-forIters" class="coderef-off"><span class="linenr">103: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">3</span>):</span>
<span class="linenr">104: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21462;&#24471;&#23567;&#25209;&#27425;</span>
<span id="coderef-batchMask" class="coderef-off"><span class="linenr">105: </span>      batch_mask = np.random.choice(train_size, batch_size)</span>
<span class="linenr">106: </span>      x_batch = x_train[batch_mask]
<span class="linenr">107: </span>      t_batch = t_train[batch_mask]
<span class="linenr">108: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35336;&#31639;&#26799;&#24230;</span>
<span id="coderef-batchGrad" class="coderef-off"><span class="linenr">109: </span>      grad = network.numerical_gradient(x_batch, t_batch)</span>
<span class="linenr">110: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#26356;&#26032;&#21443;&#25976;</span>
<span id="coderef-updateParams" class="coderef-off"><span class="linenr">111: </span>      <span style="color: #51afef;">for</span> key <span style="color: #51afef;">in</span> (<span style="color: #98be65;">'W1'</span>, <span style="color: #98be65;">'b1'</span>, <span style="color: #98be65;">'W2'</span>, <span style="color: #98be65;">'b2'</span>):</span>
<span class="linenr">112: </span>          network.params[<span style="color: #dcaeea;">key</span>] -= learning_rate + grad[key]
<span class="linenr">113: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35352;&#37636;&#23416;&#32722;&#36942;&#31243;</span>
<span class="linenr">114: </span>      loss = network.loss(x_batch, t_batch)
<span class="linenr">115: </span>      train_loss_list.append(loss)
<span class="linenr">116: </span>      <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36664;&#20986;&#32080;&#26524;</span>
<span class="linenr">117: </span>      <span style="color: #c678dd;">print</span>(train_loss_list)
</pre>
</div>

<pre class="example">
[990.0180885577449]
[990.0180885577449, 995.0232568391127]
[990.0180885577449, 995.0232568391127, 1150.311320292553]
</pre>


<p>
上述程式碼中，小批次大小為 100（第<a href="#coderef-batchSize" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-batchSize');" onmouseout="CodeHighlightOff(this, 'coderef-batchSize');">97</a>行），每次從 60000 個訓練資料中隨機取出 100 筆資料（第<a href="#coderef-batchMask" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-batchMask');" onmouseout="CodeHighlightOff(this, 'coderef-batchMask');">105</a>行，包含影像資料與正確答案標籤資料），接下來以此 100 筆資料計算梯度（第<a href="#coderef-batchGrad" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-batchGrad');" onmouseout="CodeHighlightOff(this, 'coderef-batchGrad');">109</a>行），然後利用準確率梯度下降法（SGD）更新參數（第<a href="#coderef-updateParams" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-updateParams');" onmouseout="CodeHighlightOff(this, 'coderef-updateParams');">111</a>行）,然後記錄損失函數，如此整個步驟重複執行 10000 次（第<a href="#coderef-forIters" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-forIters');" onmouseout="CodeHighlightOff(this, 'coderef-forIters');">103</a>行）。<br />
</p>
</div>
</li>

<li><a id="org3ea34d8"></a>模型成效評估<br />
<div class="outline-text-4" id="text-6-12-5">
<p>
如何評估<br />
</p>
</div>
</li>
</ol>
</div>
</div>

<div id="outline-container-orgb3422ea" class="outline-2">
<h2 id="orgb3422ea"><span class="section-number-2">7.</span> 深度神經網路 DNN (Deep Neural Network)</h2>
<div class="outline-text-2" id="text-7">
</div>
<div id="outline-container-org5d9e180" class="outline-3">
<h3 id="org5d9e180"><span class="section-number-3">7.1.</span> 學習與參數:以迴歸問題為例</h3>
<div class="outline-text-3" id="text-7-1">
<p>
想像一個國中的數學問題：在平面上畫出\(y=2x+3\)的直線，如圖<a href="#org322de72">52</a>左的直線(\(y=ax+b\))，決定這條直線的因素有二：斜率\(a\)與截距\(b\)，這兩項因素即可視為該直線的參數，像這種由已知參數去畫出對映直線的問題稱之為順向問題；反之，如果目前只知道平面上有幾個點，希望能畫出最符合的這些點的線，這種問題就稱為逆向問題。像圖<a href="#org322de72">52</a>右圖的紅線明顯就不是一條最符合的線，而解決這個問題就變成透過「尋找最佳參數」來畫出最理想的迴歸線，神經網路便是希望能藉由網路模型的不斷學習來找出最佳參數。<br />
</p>

<div id="org322de72" class="figure">
<p><img src="images/simplefx-1.png" alt="simplefx-1.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 52: </span>由已知函數畫出直線與由已知點找未知函數</p>
</div>

<p>
同理，如果我們將解題目標改變為「預測學生學測總級分」，那麼，我們得先了解有那些因素會影響學生的學測成績，初步估計也許包括以下因素：<br />
</p>
<ol class="org-ol">
<li>上課狀況<br /></li>
<li>是否認真寫作業<br /></li>
<li>歷次段考成績<br /></li>
<li>校內模考成績<br /></li>
<li>回家後是否努力讀書<br /></li>
<li>是否沉迷網路遊戲或手機遊戲<br /></li>
<li>是否有男/女朋友<br /></li>
</ol>

<p>
此時，我們的預測模型就如圖<a href="#orgb4d8654">53</a>所示<br />
</p>

<div id="orgb4d8654" class="figure">
<p><img src="images/exam-Network1.png" alt="exam-Network1.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 53: </span>學測成績預測模型#1</p>
</div>

<p>
然而，上述因素只是一般性的文字描述，畢竟過於模糊而無法對之進行精確計算，所以，我們有必要再對其進行更精確的描述，此處的參數（即影響因素及相對權重）又稱為特徵值。此外，每個因素影響學測結果的程度理應會有所差異，因此也有必要對各因素賦予「加權」（也稱為權重），詳細考慮後的因素及加權列表如下。<br />
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> 學測預測模型因素列表</caption>

<colgroup>
<col  class="org-right" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-right">no</th>
<th scope="col" class="org-left">因素編號</th>
<th scope="col" class="org-left">模糊描述</th>
<th scope="col" class="org-left">精確描述</th>
<th scope="col" class="org-left">權重</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-right">1</td>
<td class="org-left">\(x_1\)</td>
<td class="org-left">上課狀況</td>
<td class="org-left">平均每次上課時認真聽講的時間百分比</td>
<td class="org-left">\(w_1\)</td>
</tr>

<tr>
<td class="org-right">2</td>
<td class="org-left">\(x_2\)</td>
<td class="org-left">是否認真寫作業</td>
<td class="org-left">作業平均成績</td>
<td class="org-left">\(w_2\)</td>
</tr>

<tr>
<td class="org-right">3</td>
<td class="org-left">\(x_3\)</td>
<td class="org-left">歷次段考成績</td>
<td class="org-left">各科段考平均成績</td>
<td class="org-left">\(w_3\)</td>
</tr>

<tr>
<td class="org-right">4</td>
<td class="org-left">\(x_4\)</td>
<td class="org-left">校內模考成績</td>
<td class="org-left">歷次模考平均成績</td>
<td class="org-left">\(w_4\)</td>
</tr>

<tr>
<td class="org-right">5</td>
<td class="org-left">\(x_5\)</td>
<td class="org-left">放學後是否努力讀書</td>
<td class="org-left">放學後花在課業上的時間</td>
<td class="org-left">\(w_5\)</td>
</tr>

<tr>
<td class="org-right">6</td>
<td class="org-left">\(x_6\)</td>
<td class="org-left">是否沉迷網路遊戲或手機遊戲</td>
<td class="org-left">每天平均花在遊戲的時間</td>
<td class="org-left">\(w_6\)</td>
</tr>

<tr>
<td class="org-right">7</td>
<td class="org-left">\(x_7\)</td>
<td class="org-left">是否花太多時間交異性朋友</td>
<td class="org-left">有/無男女朋友</td>
<td class="org-left">\(w_7\)</td>
</tr>
</tbody>
</table>

<p>
此時，我們的預測模型就如圖<a href="#orgad08783">54</a>所示，換言之，是在解一個\(f(x)=x_1*w_1+x_2*w_2+x_3*w_3+...+x_7*w_7\)的函式問題。我們可以先針對這些特徵值對學生進行問卷調查，並追踪學生的學測成績，最後將取得的大量的特徵值輸入到到我們的函數模型（圖<a href="#orgad08783">54</a>）中，觀察計算結果與實際資料的吻合程度，藉由不斷的調整參數（權重）來控制函數，讓輸出的計算結果與實際答案完全吻合，以便求得最準確的函數。<br />
</p>

<div id="orgad08783" class="figure">
<p><img src="images/exam-network2.png" alt="exam-network2.png" width="600" /><br />
</p>
<p><span class="figure-number">Figure 54: </span>學測成績預測模型#2</p>
</div>

<p>
然而，無論我們事前研究分析調查的再如何嚴謹，實際的計算結果與真實分數總會存在誤差，如表<a href="#orgaf8d1a0">3</a>，分別觀察這些誤差值並不容易看出吻合程度，但如果將個別的誤差平方後加總，則可以得到一個明確的誤差函數=\(3^2+(-3)^2+(-2)^2+(-2)^2...\)，至此，解題的任務便轉為：找出能讓誤差函數最小化的一組參數。<br />
</p>
<table id="orgaf8d1a0" border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 3:</span> 誤差的計算</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-right">學生 A</th>
<th scope="col" class="org-right">學生 B</th>
<th scope="col" class="org-right">學生 C</th>
<th scope="col" class="org-right">學生 D</th>
<th scope="col" class="org-left">&#x2026;</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">資料</td>
<td class="org-right">70</td>
<td class="org-right">65</td>
<td class="org-right">68</td>
<td class="org-right">50</td>
<td class="org-left">&#x2026;</td>
</tr>

<tr>
<td class="org-left">模型</td>
<td class="org-right">67</td>
<td class="org-right">68</td>
<td class="org-right">70</td>
<td class="org-right">52</td>
<td class="org-left">&#x2026;</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">誤差</td>
<td class="org-right">3</td>
<td class="org-right">-3</td>
<td class="org-right">-2</td>
<td class="org-right">-2</td>
<td class="org-left">&#x2026;</td>
</tr>
</tbody>
</table>

<p>
像這種透過比對現有資料不斷調整參數以便將誤差函數減至最小的學習過程稱為「監督式學習」，而至於「非監督式學習」則是藉由找出不同學生在這些特徵值上的相似程度，將學生分群，而同一群組的學生共通之處或許便會直接與學測成績相關，學童在成長過程中的學習就是非監督式學習，透過觀週遭的一切事物，也許有些事情仍不了解，但多少都會從中整理、歸納出一些重要的知識；待到後開始上學，則較接近監督式學習。<br />
</p>
</div>
</div>

<div id="outline-container-org3534063" class="outline-3">
<h3 id="org3534063"><span class="section-number-3">7.2.</span> 如何調整參數</h3>
<div class="outline-text-3" id="text-7-2">
<p>
前節提及，為了能找到最理想的預測函數，我們可以不斷調整權重來把誤差函數降到最低。實務上，我們可以每次以最細微的調幅逐一調整（增加或減少）權重值來試圖減小損失函數，直到其無法再減少為止，此種方式稱為「坡度法」，而這種每次稍微調整一點點再觀察結果變化的手段稱為微分；若是同時微幅調整所有權重以將損失函數降到最低，這種方式則稱為「梯度下降法」。然而類似梯度法並不保證能找到將損失函數降到最小的權重組合，如圖<a href="#orgd30de31">55</a>所示，以梯度法可能只會找到 A 點這個局部最小值，然而全體的最小值其實發生在 B 點。<br />
</p>

<div id="orgd30de31" class="figure">
<p><img src="images/dotAB.png" alt="dotAB.png" width="500" /><br />
</p>
<p><span class="figure-number">Figure 55: </span>極小值與最小值的差異</p>
</div>
</div>
</div>

<div id="outline-container-org6dbb808" class="outline-3">
<h3 id="org6dbb808"><span class="section-number-3">7.3.</span> 模型的極限</h3>
<div class="outline-text-3" id="text-7-3">
<p>
在我們透過問卷取得大量的數據後，想像一下我們以這些數據來做為調整模型參數的依據，最後，我們如何評估這個模型的效能呢？一般來說，我們會把數據分成兩部份：一部份做為調整或學習參數的依據，稱做訓練資料；另一部份用來測試或檢驗模型的效能。之所以用不同的數據進行訓練與測試，是為了避免「過擬合」的狀況，即，因為測試資料與訓練資料一致，導致測試結果十分完美，然而，一旦把模型拿來應用到新的數據上（或是實際應用模型到真實世界中）時，反而效果會不如預期。<br />
</p>

<p>
過擬合就好比學生在學習時只死記課本的習題，對於其他題型完全不予理會，如果考試也考課本的習題，考試成績自然優異，然而如果考試時題型略做變化，則考試結果就可能十分悲慘。<br />
</p>

<p>
實際進行測試時，可以將資料分成數組，將其中一組當成測試資料。例如，分為 A、B、C、D4 組，然後輪流拿這 4 組資料中的一組做為測試資料進行 4 次相同的測試，目的在於提升模型的「泛化能力」，也就是減少其過擬合的可能性。<br />
</p>
</div>
</div>

<div id="outline-container-org1d5b4e1" class="outline-3">
<h3 id="org1d5b4e1"><span class="section-number-3">7.4.</span> 神經網路為什麼要有那麼多層</h3>
<div class="outline-text-3" id="text-7-4">
<p>
前節提及，我們的預測模型就如圖<a href="#orgad08783">54</a>所示，換言之，是在解一個\(f(x)=x_1*w_1+x_2*w_2+x_3*w_3+...+x_7*w_7\)的函式問題，那麼，為了能找到最理想的預測函數，可否把函數變的更加複雜，例如，將函數變為二次函數或更複雜的函數以提升預測的精準度？實則，這種社會科學的問題並不如自然科學的物理現象可以用明確的公式來解決，神經網路採用的是以組合的方式來將函數複雜化，例如，把圖<a href="#orgad08783">54</a>變為下圖的樣式，如此藉由改變各因素以及權值的組合，等於建立了許多新的特徵值，也增加了模型的複雜度。<br />
</p>

<div class="org-src-container">
<pre class="src src-dot">digraph exam3 {
    rankdir=LR;
    nodesep=.05;
    graph[ranksep="3"]
    x4[label = "..."];
    x1[label = "&#19978;&#35506;&#35469;&#30495;&#32893;&#35611;&#26178;&#38291;&#30334;&#20998;&#27604;"];
    x2[label = "&#20316;&#26989;&#24179;&#22343;&#25104;&#32318;"];
    x3[label = "&#21508;&#31185;&#27573;&#32771;&#24179;&#22343;&#25104;&#32318;"];
    o1[label = "&#26032;&#30340;&#29305;&#24501;&#20540;&#20043;2"];
    o2[label = "&#26032;&#30340;&#29305;&#24501;&#20540;&#20043;3"];
    o4[label = "....."];
    o3[label = "&#26032;&#30340;&#29305;&#24501;&#20540;&#20043;1"];
    x1 -&gt; o3[label = w11]; x1-&gt; o1[label = w12]; x1 -&gt; o2[label = w13]; x1 -&gt; o4[label = w14];
    x2 -&gt; o3[label = w21]; x2-&gt; o1[label = w22]; x2 -&gt; o2[label = w23]; x2 -&gt; o4[label = w24];
    x3 -&gt; o3[label = w31]; x3-&gt; o1[label = w32]; x3 -&gt; o2[label = w33]; x3 -&gt; o4[label = w34];
    x4 -&gt; o3[label = w41]; x4-&gt; o1[label = w42]; x4 -&gt; o2[label = w43]; x4 -&gt; o4[label = w44];
}
</pre>
</div>
<p>
然而，如果只是以這種「新增特徵值組合與權重」進而產生新特徵值的方式來改變模型，那麼，再多的層數也能合併為一層，因為這些運算方式均屬於線性轉換，為了有效讓模型更加複雜，此處可以在模型中加入非線性轉換，如圖<a href="#org743cabf">56</a>中的 ReLU 函數。在將類似圖<a href="#org3ebb273">20</a>這類非線性轉換函數加入模型後，其結果如圖<a href="#orgd2ec150">57</a>所示。<br />
</p>


<div id="org743cabf" class="figure">
<p><img src="images/MNIST-CNN.png" alt="MNIST-CNN.png" /><br />
</p>
<p><span class="figure-number">Figure 56: </span>MNIST-NeuralNet</p>
</div>


<div id="orgd2ec150" class="figure">
<p><img src="./images/exam-Network4.png" alt="exam-Network4.png" width="700" /><br />
</p>
<p><span class="figure-number">Figure 57: </span>學測成績預測模型#4</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgc4682b3" class="outline-2">
<h2 id="orgc4682b3"><span class="section-number-2">8.</span> CNN 卷積神經網路:&#xa0;&#xa0;&#xa0;<span class="tag"><span class="CNN">CNN</span>&#xa0;<span class=""></span>&#xa0;<span class="Keeras">Keeras</span></span></h2>
<div class="outline-text-2" id="text-8">
<p>
卷積神經網路(CNN)是由一位計算機科學家楊・勒丘恩(Yann LeCun)所提出，此人在機器學習、計算機視覺、計算機神經科學等領域都有很多貢獻。<br />
</p>

<p>
CNN 也是模仿人類大腦的認知方式，譬如我們辨識一個圖像，會先注意到顏色鮮明的點、線、面，之後將它們構成一個個不同的形狀(眼睛、鼻子、嘴巴&#x2026;)，這種抽象化的過程就是 CNN 演算法建立模型的方式。卷積層(Convolution Layer) 就是由點的比對轉成局部的比對，透過一塊塊的特徵研判，逐步堆疊綜合比對結果，就可以得到比較好的辨識結果，過程如下圖。<sup><a id="fnr.8" class="footref" href="#fn.8" role="doc-backlink">8</a></sup><br />
</p>

<p>
<img src="images/CNN-1.png" alt="CNN-1.png" /><br />
資料來源：<sup><a id="fnr.9" class="footref" href="#fn.9" role="doc-backlink">9</a></sup><br />
</p>
</div>

<div id="outline-container-orge74389f" class="outline-3">
<h3 id="orge74389f"><span class="section-number-3">8.1.</span> CNN v.s. MLP</h3>
<div class="outline-text-3" id="text-8-1">
<p>
CNN 與前章所述之 MLP 的主要差異可由下圖看出，即，CNN 在輪入層中多了卷積層與池化層，MLP 有兩個缺點<sup><a id="fnr.10" class="footref" href="#fn.10" role="doc-backlink">10</a></sup>：<br />
</p>
<ol class="org-ol">
<li>需要大量記憶體:在處理 256x256 大小的彩色圖片時，會需要用到 256 * 256 * 3 =196,608 個 Input Neuron，如果中間的隱藏層有 1000 個 Neuron，每個神經元需要一個浮點數的權重值 (8bytes)，那麼總共需要 196,608 * 1000 * 8 = 1.4648GBytes 的記憶體才夠。更何況這還只是個簡單的模型。<br /></li>
<li>多層感知器只針對圖片中每個單一像素去作判斷，完全捨棄重要的影像特徵。人類在判斷所看到的物體時，會從不同部位的特徵先作個別判斷，例如當你看到一架飛機，會先從機翼、機鼻、機艙體等這些特徵，再跟記憶中的印象來判斷是否為一架飛機，甚至再進一步判斷為客機還是戰鬥機。但是多層感知器沒有利用這些特徵，所以在影像的判讀上準確率就沒有接下來要討論的 CNN 來得好。<br /></li>
</ol>

<p>
與 MLP 相比，CNN 多了卷積層與池化層，卷積層用來強調圖案(資料)特徵，池化層則可以縮減取樣，減少 overfitting 問題。二者架構差異如圖<a href="#org59c348c">58</a>所示。<br />
</p>

<div id="org59c348c" class="figure">
<p><img src="images/CNN-MLP.jpg" alt="CNN-MLP.jpg" /><br />
</p>
<p><span class="figure-number">Figure 58: </span>CNN 與 MLP 之差異</p>
</div>
</div>
</div>

<div id="outline-container-org0b55554" class="outline-3">
<h3 id="org0b55554"><span class="section-number-3">8.2.</span> CNN 模型架構</h3>
<div class="outline-text-3" id="text-8-2">
<p>
典型的 CNN 模型架構如圖<a href="#org7c83c27">59</a>所示，主要可分為兩大部份：<br />
</p>

<div id="org7c83c27" class="figure">
<p><img src="images/CNN-arch-1.jpg" alt="CNN-arch-1.jpg" /><br />
</p>
<p><span class="figure-number">Figure 59: </span>CNN 架構圖</p>
</div>
</div>

<ol class="org-ol">
<li><a id="orgcbf6004"></a>影像特徵擷取<br />
<div class="outline-text-4" id="text-8-2-1">
<p>
透過卷積層及池化層擷取影像特徵，每個 Conv2D 和 MAsxPooling2D 層的輸出都是 shape 為(height, width, channels)的 3D 張量，隨著神經網路進入更深的層、寬度和高度也隨之縮小。<br />
</p>
</div>
</li>

<li><a id="org9409c2b"></a>完全連結神經網路<br />
<div class="outline-text-4" id="text-8-2-2">
<p>
包含平坦層、隠藏層、輸出層所組成的神經網路，由於最後一個 Conv2D 層輸出的 36 個 7*7 必須送到接續的分類器神經網路(也就是 Dense 層)，分類器神經網路能處理的是 1D 向量，但前一層的輸出是 3D 張量，所以要把 3D 張量展平為 1D。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org13aea44" class="outline-3">
<h3 id="org13aea44"><span class="section-number-3">8.3.</span> 卷積運算</h3>
<div class="outline-text-3" id="text-8-3">
<p>
以全連接層（Affine 層）來處理神經網路的問題在於：全連接層會忽略資料的「形狀」。例如，假設輸入資料為影像，則通常會包含水平、垂直、色版方向的三維形狀，然而當這些資料輸入全連接層時，三維資料就必須變為平面（一維資料），如前述的 MNist 資料集，輸入為(1, 28, 28)，即，一種顏色、28*28 像素，輸入全連接層後會變成一行的 784 個資料。<br />
</p>

<p>
三維形狀的影像包含了許多重要的空間資料，例如，類似的空間有著相似的象素值、RGB 各色版間有緊密連接的關連性，距離較遠的像素彼此沒有關連&#x2026;等特質，這些特質會在全連接層中被忽略掉。卷積層（Convolution layer）則能維持這些形狀，我們把 CNN 的卷積層輸出入資料稱作特徵圖（input / output feature map），而在卷積層中執行的處理則稱為卷積運算，若以影像處理來比喻卷積運算，則相當於「濾鏡效果」。卷積層的意義即是將原本一個影像經由卷積運算產生多個影像，每個影像均代表不同特徵。卷積運算方式如下：<br />
</p>

<p>
典型的卷積運算如圖<a href="#org44eeb9a">60</a>，此處特徵圖為(4,4)，濾鏡為(3,3)，輸出為(2,2)，這裡的濾鏡又稱為「核(kernel)」，其初始值由隨機方式產生。此外，在全連接網路層中，除了權重參數（核）之外，還有偏權值，其結果如圖<a href="#org19a828e">61</a>。<br />
</p>

<div id="org44eeb9a" class="figure">
<p><img src="images/CNN-struc1.png" alt="CNN-struc1.png" /><br />
</p>
<p><span class="figure-number">Figure 60: </span>卷積運算</p>
</div>


<div id="org19a828e" class="figure">
<p><img src="images/CNN-struc2.png" alt="CNN-struc2.png" /><br />
</p>
<p><span class="figure-number">Figure 61: </span>卷積運算 2</p>
</div>

<p>
由圖<a href="#org44eeb9a">60</a>、<a href="#org19a828e">61</a>的卷積運算可以看出，輸入特徵值在經過運算後，其大小會縮小，可以預見的結果是，在經過多層神經網路反複進行卷積運算後，輸出特徵值大小很快就會縮小為 1，而無法再進行卷積運算。為了避免這種情況發生，在進行卷積運算前，可以針對輸入資料周圍補上一圈固定的資料（例如 0），這個動作稱為填補(padding)，如果我們對在卷積運算前對於圖<a href="#org44eeb9a">60</a>中的輸入特徵值進行寬度 1 的填補，則其結果如圖<a href="#orge32d9ed">62</a>所示。<br />
</p>


<div id="orge32d9ed" class="figure">
<p><img src="images/CNN-struc3.png" alt="CNN-struc3.png" /><br />
</p>
<p><span class="figure-number">Figure 62: </span>卷積運算 3</p>
</div>

<p>
進行上述卷積運算時，每次自左而右、自上而下，自輸入特徵值取出一個與核大小相同的子矩陣與核進行運算，而每個取出的子矩陣的間隔稱為步幅(stride)，上述範例中的 stide 均為 1，若 stride 設為 2，則一個(7,7)的輸入特徵值與一個(3,3)的核進行運算後，其輸出特徵值大小只剩(3,3)。<br />
</p>

<p>
上述範例中的特徵值均為二維矩陣，亦即，僅能表示水平與垂直方向的單色點陣圖，若輸入中包含色彩資料(RGB)，則輸入特徵圖將升及為三維矩陣（三層二維矩陣，每層表示一種 RGB 值），核的結構也是三維矩陣，但輸出特徵值則為二維矩陣。 以 MNist 資料集為例，將數字 7 的 28*28 影像以隨機產生的 5*5 濾鏡(或稱 filter weight、kernel)對其進行卷積，其結果如圖<a href="#org84a249b">63</a>所示，這種效果有助於提取輸入影像的不同特徵，例如邊緣、線條&#x2026;等。<br />
</p>

<div id="org84a249b" class="figure">
<p><img src="images/CNN-FW-1.jpg" alt="CNN-FW-1.jpg" /><br />
</p>
<p><span class="figure-number">Figure 63: </span>卷積運算 4</p>
</div>

<p>
實際建構模型時，不會只進行單一 kenrel 的卷積，圖<a href="#org2807e2a">64</a>即是隨機產生 16 個 kernel 對輸入影像提取不同特徵。<br />
</p>

<div id="org2807e2a" class="figure">
<p><img src="images/CNN-FW-2.jpg" alt="CNN-FW-2.jpg" /><br />
</p>
<p><span class="figure-number">Figure 64: </span>卷積運算 5</p>
</div>

<p>
密集層和卷積層之間的根本區別在於：密集層會由輸入的特徵空間中學習全域的 pattern，而卷積層則是學習局部的 pattern，以影像辨識為例，convent 會把輸入分解成小小的 2D 窗格，然後從中找出 pattern。這個關鍵特性為 CNN 提供了兩個有趣的特質：<br />
</p>

<ul class="org-ul">
<li>學習到的 pattern 具平移不變性(translation invariant)：在影像的右下角學習到的某個 pattern，CNN 可以在任何位置識別這樣的 pattern(如左上角)，相對的，當密集層連接神經網路看到 pattern 出現在新位置時，就必須重新學習。這使的 CNN 可以更有效率地處理影像資料。<br /></li>
<li>學習到 pattern 的空間層次結構(spatial hierarchies of patterns)：第一層卷積層會學習到諸如邊邊角角的小局部圖案，第二層卷積層則會基於第一層學到的特徵(小圖案)來學習較大的圖案，這使得 CNN 能夠有效地學習越來越複雜和抽象的視覺概念。<br /></li>
</ul>

<p>
CNN 所運算的 3D 張量稱為特徵映射圖(feature maps，簡稱特徵圖)，特徵圖有 2 個空間軸(height, width)以及 1 個色深度軸(depth / channel)，對於 RGB 影像來說，depth 值為 3。卷積運算會從輸入特徵中萃取出各種小區塊 pattern，當它對整張影像都做完萃取之後，就會產生輸出特徵映射圖(output feature map)。輸出特徵映射圖仍是 3D 張量，具有寬度和高度，但此時其深度軸已不再代表 RGB 顏色值，此時它代表過濾器(filter)，每一種 filter 會對輸入資料進行特定面向的編碼、萃取出結果，例如，filter 可以萃取到「在輸入資料中出現一張臉」這種高階抽象的概念，將不是臉的都過濾掉。<br />
</p>
</div>
</div>

<div id="outline-container-org986dc0b" class="outline-3">
<h3 id="org986dc0b"><span class="section-number-3">8.4.</span> 池化層(Pooling Layer)</h3>
<div class="outline-text-3" id="text-8-4">
<p>
卷積層之間通常會加一個池化層(Pooling Layer)，它是一個壓縮圖片並保留重要資訊的方法，取樣的方法一樣是採滑動視窗，但是通常取最大值(Max-Pooling)，而非加權總和，若滑動視窗大小設為 2，『滑動步長』(Stride) 也為 2，則資料量就降為原本的四分之一，但因為取最大值，它還是保留局部範圍比對的最大可能性。也就是說，池化後的資訊更專注於圖片中是否存在相符的特徵，而非圖片中『哪裡』存在這些特徵，幫助 CNN 判斷圖片中是否包含某項特徵，而不必關心特徵所在的位置，這樣圖像偏移，一樣可以辨識出來。其架構如圖<a href="#org77f9646">65</a>所示。<sup><a id="fnr.9.100" class="footref" href="#fn.9" role="doc-backlink">9</a></sup><br />
</p>

<div id="org77f9646" class="figure">
<p><img src="images/CNN-Pooling.png" alt="CNN-Pooling.png" /><br />
</p>
<p><span class="figure-number">Figure 65: </span>CNN 之池化層</p>
</div>

<p>
池化層以縮減取樣(downsampling)的方式縮小影像可以帶來以下優點：<br />
</p>

<ol class="org-ol">
<li>減少需要處理的資料點：減少後續運算所需時間。<br /></li>
<li>讓影像位置差異變小：影像要辨識的目標（如 MNist 資料集的數字）可能因為在影像中的位置不同而影響辦識，減小影像可以讓數字的位置差異變小。<br /></li>
<li>參數的數量程計算量下降：同時也能控制 overfitting。<br /></li>
</ol>

<p>
圖<a href="#orge650226">66</a>即為使用 Max-Pool 對 16 個卷積影像進行縮減取樣(downsampling)的效果，將 16 個 28*28 個影像縮小為 16 個 14*14 的影像，但仍保持其特徵。MaxPooling 主要是由輸入特徵圖中做採樣並輸出樣本的最大值，它在概念上類似於卷積層操作，但並不是用卷積核(convolution kernel)張量積的方式來轉換局部區塊，而是經由手動編碼的 max 張量操作進行轉換。與卷積層操作的很大區別是 MaxPooling 通常用 2&times;2 窗格和步長(strides)2 來完成，以便將特徵圖每一軸的採樣減少到原來的 1/2，而卷積層操作通常使用 3&times;3 窗格且不指定步長(即使用預設步長 1)。<br />
</p>


<div id="orge650226" class="figure">
<p><img src="images/CNN-PL-1.jpg" alt="CNN-PL-1.jpg" /><br />
</p>
<p><span class="figure-number">Figure 66: </span>池化效果</p>
</div>

<p>
為什麼要採用這種方式來縮小採樣特徵圖而不用原來的特徵圖尺寸一路執行下去？假設一個有不加入 MaxPooling 而是以全卷積層的模型設計如下：<br />
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">model_no_max_pool</span> = models.Sequential()
<span class="linenr">2: </span>  model_no_max_pool.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">32</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>),
<span class="linenr">3: </span>                        input_shape=(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">1</span>))
<span class="linenr">4: </span>  model_no_max_pool.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">64</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>),)
<span class="linenr">5: </span>  model_no_max_pool.add(layers.Conv2D(<span style="color: #da8548; font-weight: bold;">64</span>, (<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), activation=<span style="color: #98be65;">'relu'</span>),)
</pre>
</div>

<p>
上述設計的問題可以從以下兩個面向來看：<sup><a id="fnr.11" class="footref" href="#fn.11" role="doc-backlink">11</a></sup><br />
</p>

<ul class="org-ul">
<li>不利於學習特徵的空間層次結構。當 strides=1，第二層的 3&times;3 窗格要走過 5&times;5 的區域才能包含第一層的 7&times;7 區城，進而生成下一層的完整 3&times;3 區域，依此類推，第三層的 3&times;3 窗格也要走過 5&times;5 的區域才能包含第二層的 5&times;5 區域。也就是說，第三層的 3&times;3 窗格實際上僅包含來自原始輸入的 7&times;7 區城的資訊。相對於初始輸入，由卷積神經網路學習的高階 pattern 仍然進展很小，不足以學習分類數字。我們希望的是：最後一個卷積層的輸出特徵已經能提供有關輸入資料的總體訊息。<br /></li>
<li>最終特徵圖的每個樣本總係數為 22&times;22&times;64=3-976，這是相當巨大的 model，如果要將其展平以在頂部連接大小為 512 的 Dense 層，則該層將會有 1580 萬個參數，這對小 model 而言實在是太大，而且會導致 overfitting。<br /></li>
</ul>

<p>
雖然 MaxPooling 並不是縮小探樣特徵圖的唯一方法（也可以透過卷積層的 strides 來調整，或是使用平均池化而非 MaxPooling）,不過就經驗來看，MaxPooling 往往比這些方案好。主要原因是：特徵通常是源自於空間某些有特色的 pattern，因此要取其最大值才更具訊息性，若採用平均值，則特色就被掩蓋了。<br />
</p>
</div>
</div>

<div id="outline-container-org3dc9344" class="outline-3">
<h3 id="org3dc9344"><span class="section-number-3">8.5.</span> 以 CNN 實作 MNIST</h3>
<div class="outline-text-3" id="text-8-5">
</div>
<ol class="org-ol">
<li><a id="org1a554bf"></a>資料預處理(preprocess)<br />
<div class="outline-text-4" id="text-8-5-1">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">1. &#21295;&#20837;&#25152;&#38656;&#27169;&#32068;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35201;&#23559;table&#36681;&#28858;one-hot encoding</span>
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 4: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">9527</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr"> 5: </span>
<span class="linenr"> 6: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">2. &#21295;&#20837;Keras&#27169;&#32068;&#20197;&#19979;&#36617;MNist&#36039;&#26009;&#38598;'''</span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 8: </span>  <span style="color: #5B6268;">###  </span><span style="color: #5B6268;">&#35712;&#21462;MNist&#36039;&#26009;&#38598;</span>
<span class="linenr"> 9: </span>  (x_Train, y_Train), (<span style="color: #dcaeea;">x_Test</span>, <span style="color: #dcaeea;">y_Test</span>) = mnist.load_data()
<span class="linenr">10: </span>
<span class="linenr">11: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">3. &#23559;features&#36681;&#28858;4&#30697;&#38499;(6000*28*28*1)</span>
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x_Train4D</span> = x_Train.reshape(x_Train.shape[<span style="color: #da8548; font-weight: bold;">0</span>],<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">1</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">13: </span>  <span style="color: #dcaeea;">x_Test4D</span> = x_Test.reshape(x_Test.shape[<span style="color: #da8548; font-weight: bold;">0</span>],<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">1</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">14: </span>
<span class="linenr">15: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">3. &#23559;features&#27161;&#28310;&#21270;</span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">x_Train4D_normalize</span> = x_Train4D / <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">17: </span>  <span style="color: #dcaeea;">x_Test4D_normalize</span> = x_Test4D / <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">18: </span>
<span class="linenr">19: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">4. Onehot encoding</span>
<span class="linenr">20: </span>  <span style="color: #dcaeea;">y_TrainOneHot</span> = np_utils.to_categorical(y_Train)
<span class="linenr">21: </span>  <span style="color: #dcaeea;">y_TestOneHot</span> = np_utils.to_categorical(y_Test)
</pre>
</div>
</div>
</li>

<li><a id="orgb1098d3"></a>建立模型<br />
<div class="outline-text-4" id="text-8-5-2">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">1. &#21295;&#20837;&#25152;&#38656;&#27169;&#32068;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">2. &#24314;&#31435;keras&#30340;Sequential&#27169;&#22411;</span>
<span class="linenr"> 6: </span>  <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr"> 7: </span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">3. &#21367;&#31309;&#23652;1&#33287;&#27744;&#21270;&#23652;1 (&#19968;&#20491;&#23436;&#25972;&#30340;&#21367;&#31309;&#36939;&#31639;&#21253;&#21547;&#19968;&#20491;&#21367;&#31309;&#23652;&#33287;&#27744;&#21270;&#23652;)&#65292;&#28670;&#37857;&#22823;&#23567;&#28858;5*5</span>
<span class="linenr"> 9: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#36664;&#20837;&#30340;&#24433;&#20687;&#28858;28*28&#65292;&#21367;&#31309;&#36939;&#31639;&#24460;&#29986;&#29983;16&#20491;&#24433;&#20687;&#65292;&#22823;&#23567;&#19981;&#35722;(padding=same)</span>
<span class="linenr">10: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#22240;&#28858;&#24433;&#20687;&#28858;&#21934;&#33394;&#65292;&#25925;&#24433;&#20687;&#22823;&#23567;&#28858; 28*28*1</span>
<span class="linenr">11: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#38928;&#35373;input_shape: image_height, image:width, image_channels</span>
<span class="linenr">12: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">16</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">5</span>), padding=<span style="color: #98be65;">'same'</span>,
<span class="linenr">13: </span>                   input_shape(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">1</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">14: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#23559;16&#20491;28*28&#24433;&#20687;&#36681;&#28858;16&#20491;14*14&#24433;&#20687;</span>
<span class="linenr">15: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">16: </span>
<span class="linenr">17: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">4. &#21367;&#31309;&#23652;2&#33287;&#27744;&#21270;&#23652;2 (&#19968;&#20491;&#23436;&#25972;&#30340;&#21367;&#31309;&#36939;&#31639;&#21253;&#21547;&#19968;&#20491;&#21367;&#31309;&#23652;&#33287;&#27744;&#21270;&#23652;)&#65292;&#28670;&#37857;&#22823;&#23567;&#28858;5*5</span>
<span class="linenr">18: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#23559;&#21407;&#26412;16&#20491;&#24433;&#20687;&#36681;&#25563;&#28858;36&#20491;&#24433;&#20687;</span>
<span class="linenr">19: </span>
<span class="linenr">20: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">36</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">5</span>), padding=<span style="color: #98be65;">'same'</span>,
<span class="linenr">21: </span>                   input_shape(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">1</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">22: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#23559;36&#20491;14*14&#24433;&#20687;&#36681;&#28858;36&#20491;7*7&#30340;&#24433;&#20687;</span>
<span class="linenr">23: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">24: </span>
<span class="linenr">25: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">5. &#21152;&#20837;Dropout&#36991;&#20813;overfitting&#65292;&#38568;&#27231;&#25918;&#26820;&#31070;&#32147;&#32178;&#36335;&#20013;25%&#30340;&#31070;&#32147;&#20803;</span>
<span class="linenr">26: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr">27: </span>
<span class="linenr">28: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">6. &#24314;&#31435;&#31070;&#32147;&#32178;&#36335;(&#24179;&#22374;&#23652;&#12289;&#38560;&#34255;&#23652;&#12289;&#36664;&#20986;&#23652;)</span>
<span class="linenr">29: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#38560;&#34255;&#23652;&#26377;128&#20491;&#31070;&#32147;&#20803;&#65292;&#20006;&#38568;&#27231;&#25918;&#26820;50%&#30340;&#31070;&#32147;&#20803;</span>
<span class="linenr">30: </span>  model.add(Flatten())
<span class="linenr">31: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">128</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">32: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">33: </span>  <span style="color: #5B6268;">###   </span><span style="color: #5B6268;">&#36664;&#20986;&#23652;&#26377;0~9&#20849;10&#20491;&#31070;&#32147;&#20803;&#65292;&#20197;softmax&#36664;&#20986;&#23565;&#26144;&#27231;&#29575;</span>
<span class="linenr">34: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">7. &#26597;&#30475;&#27169;&#22411;&#25688;&#35201;</span>
<span class="linenr">37: </span>  <span style="color: #c678dd;">print</span>(model.summary())
</pre>
</div>
</div>
</li>

<li><a id="org50c132b"></a>訓練模型<br />
<div class="outline-text-4" id="text-8-5-3">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">1. &#23450;&#32681;&#35347;&#32244;&#26041;&#24335;</span>
<span class="linenr"> 2: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#22312;&#28145;&#24230;&#23416;&#32722;&#20013;&#20351;&#29992;cross_entropy&#20132;&#21449;&#29109;&#35347;&#32244;&#25928;&#26524;&#36611;&#22909;</span>
<span class="linenr"> 3: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#20351;&#29992;adam&#20570;&#26368;&#20339;&#21270;&#21487;&#20197;&#35731;&#35347;&#32244;&#26356;&#24555;&#25910;&#27483;&#65292;&#25552;&#39640;&#28310;&#30906;&#29575;</span>
<span class="linenr"> 4: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#26368;&#24460;&#20197;accuracy&#20358;&#35413;&#20272;&#27169;&#22411;&#28310;&#30906;&#29575;</span>
<span class="linenr"> 5: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr"> 6: </span>                optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr"> 7: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#22519;&#34892;10&#27425;&#35347;&#32244;&#36913;&#26399;&#65292;&#27599;&#19968;&#25209;&#27425;&#31639;300&#31558;&#36039;&#26009;</span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;">###    </span><span style="color: #5B6268;">&#22240;&#28858;&#20849;&#26377;48000&#31558;&#35347;&#32244;&#36039;&#26009;&#65292;&#27599;&#25209;&#27425;300&#31558;&#65292;&#32004;&#20998;160&#25209;&#27425;&#35347;&#32244;</span>
<span class="linenr"> 9: </span>  train_history=model.fit(x=x_Train4D_normalize,
<span class="linenr">10: </span>                          y=y_TrainOneHot, validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">11: </span>                          epochs=<span style="color: #da8548; font-weight: bold;">10</span>, batch_size=<span style="color: #da8548; font-weight: bold;">300</span>, verbose=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">12: </span>
<span class="linenr">13: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">2. &#30059;&#20986;&#25928;&#26524;&#22294;&#21450;&#35492;&#24046;&#22294;</span>
<span class="linenr">14: </span>  show_train_history(<span style="color: #98be65;">'acc'</span>, <span style="color: #98be65;">'val_acc'</span>)
<span class="linenr">15: </span>  show_train_history(<span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
</pre>
</div>
</div>
</li>

<li><a id="orgb507e57"></a>評估模型準確率<br />
<div class="outline-text-4" id="text-8-5-4">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">scores</span> = model.evaluate(x_Test4D_normalize, y_TestOneHot)
<span class="linenr">2: </span>  <span style="color: #c678dd;">print</span>(scores[<span style="color: #da8548; font-weight: bold;">1</span>])
</pre>
</div>
</div>
</li>

<li><a id="orgbae9b8e"></a>進行預測<br />
<div class="outline-text-4" id="text-8-5-5">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #dcaeea;">prediction</span> = model.predict_classes(x_Test4D_normalize)
<span class="linenr">2: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#39023;&#31034;&#21069;10&#31558;&#38928;&#28204;&#32080;&#26524;</span>
<span class="linenr">3: </span>  <span style="color: #c678dd;">print</span>(prediction[:<span style="color: #da8548; font-weight: bold;">10</span>]
<span class="linenr">4: </span>  plot_images_labels_prediction(x_Test, y_Test, prediction, idx=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">5: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#39023;&#31034;&#28151;&#28102;&#30697;&#38499;</span>
<span class="linenr">6: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">7: </span>  pd.crosstab(y_Test, prediction, rownames=[<span style="color: #98be65;">'label'</span>], colnames=[<span style="color: #98be65;">'predict'</span>])
</pre>
</div>
</div>
</li>

<li><a id="org1b43bc1"></a>執行結果<br />
<div class="outline-text-4" id="text-8-5-6">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">define function</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> os
<span class="linenr"> 3: </span>  os.<span style="color: #dcaeea;">environ</span>[<span style="color: #98be65;">'TF_CPP_MIN_LOG_LEVEL'</span>]=<span style="color: #98be65;">'2'</span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_images_labels_prediction</span>(imgname, images, labels, prediction, idx, num = <span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr"> 6: </span>      fig = plt.gcf()
<span class="linenr"> 7: </span>      fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">8</span>,<span style="color: #da8548; font-weight: bold;">4</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35373;&#23450;&#39023;&#31034;&#22294;&#24418;&#22823;&#23567;</span>
<span class="linenr"> 8: </span>      <span style="color: #51afef;">if</span> num&gt;<span style="color: #da8548; font-weight: bold;">25</span>: num=<span style="color: #da8548; font-weight: bold;">25</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#36039;&#26009;&#31558;&#25976;&#19978;&#38480;</span>
<span class="linenr"> 9: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, num):
<span class="linenr">10: </span>          ax=plt.subplot(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">1</span>+i) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27492;&#34389;&#39023;&#31034;10&#31558;&#22294;&#24418;&#65292;2*5&#20491;</span>
<span class="linenr">11: </span>          ax.imshow(images[idx], cmap=<span style="color: #98be65;">'binary'</span>) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#30059;&#22294;</span>
<span class="linenr">12: </span>          title= <span style="color: #98be65;">"label="</span> +<span style="color: #c678dd;">str</span>(labels[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21152;&#20837;&#23376;&#22294;&#24418;title</span>
<span class="linenr">13: </span>          <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(prediction)&gt;<span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">14: </span>              title+=<span style="color: #98be65;">",predict="</span>+<span style="color: #c678dd;">str</span>(prediction[idx]) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27161;&#38988;title&#21152;&#20837;&#38928;&#28204;&#32080;&#26524;</span>
<span class="linenr">15: </span>          ax.set_title(title,fontsize=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">16: </span>          ax.set_xticks([]);ax.set_yticks([]) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#19981;&#39023;&#31034;&#21051;&#24230;</span>
<span class="linenr">17: </span>          idx+=<span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">18: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.show()</span>
<span class="linenr">19: </span>      <span style="color: #5B6268;">#</span><span style="color: #5B6268;">plt.plot()</span>
<span class="linenr">20: </span>      plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">21: </span>
<span class="linenr">22: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(imgname, train_acc, test_acc):
<span class="linenr">23: </span>      plt.plot(train_history.history[train_acc])
<span class="linenr">24: </span>      plt.plot(train_history.history[test_acc])
<span class="linenr">25: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr">26: </span>      plt.ylabel(<span style="color: #98be65;">'Accuracy'</span>)
<span class="linenr">27: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr">28: </span>      plt.legend([<span style="color: #98be65;">'train'</span>,<span style="color: #98be65;">'test'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr">29: </span>      plt.savefig(imgname+<span style="color: #98be65;">".png"</span>)
<span class="linenr">30: </span>      plt.clf()
<span class="linenr">31: </span>
<span class="linenr">32: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">1. &#36039;&#26009;&#38928;&#34389;&#29702;(preprocess)</span>
<span class="linenr">33: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr">34: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">35: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">9527</span>) <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#35731;&#27599;&#27425;&#29986;&#29983;&#30340;&#20098;&#25976;&#19968;&#33268;</span>
<span class="linenr">36: </span>
<span class="linenr">37: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr">38: </span>  <span style="color: #51afef;">from</span> keras <span style="color: #51afef;">import</span> layers
<span class="linenr">39: </span>  (x_Train, y_Train), (x_Test, y_Test) = mnist.load_data()
<span class="linenr">40: </span>  x_Train4D = x_Train.reshape(x_Train.shape[<span style="color: #da8548; font-weight: bold;">0</span>],<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">1</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">41: </span>  x_Test4D = x_Test.reshape(x_Test.shape[<span style="color: #da8548; font-weight: bold;">0</span>],<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">28</span>,<span style="color: #da8548; font-weight: bold;">1</span>).astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">42: </span>  x_Train4D_normalize = x_Train4D / <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">43: </span>  x_Test4D_normalize = x_Test4D / <span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">44: </span>  y_TrainOneHot = np_utils.to_categorical(y_Train)
<span class="linenr">45: </span>  y_TestOneHot = np_utils.to_categorical(y_Test)
<span class="linenr">46: </span>
<span class="linenr">47: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">2. &#24314;&#31435;&#27169;&#22411;</span>
<span class="linenr">48: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">49: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D
<span class="linenr">50: </span>
<span class="linenr">51: </span>  model = Sequential()
<span class="linenr">52: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Conv2D(16, (5,5), activation='relu', input_shape=(28,28,1)))</span>
<span class="linenr">53: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">16</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">5</span>), padding=<span style="color: #98be65;">'same'</span>,
<span id="coderef-MNIST-CNN-1" class="coderef-off"><span class="linenr">54: </span>                  input_shape=(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">1</span>), activation=<span style="color: #98be65;">'relu'</span>))</span>
<span class="linenr">55: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.MaxPooling2D((2,2)))</span>
<span class="linenr">56: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">57: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Conv2D(36, (5,5), activation='relu', input_shape=(28,28,1)))</span>
<span class="linenr">58: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">36</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">5</span>), padding=<span style="color: #98be65;">'same'</span>,
<span class="linenr">59: </span>                  input_shape=(<span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">28</span>, <span style="color: #da8548; font-weight: bold;">1</span>), activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">60: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.MaxPooling2D((2,2)))</span>
<span class="linenr">61: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr">62: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Dropout(0.25))</span>
<span class="linenr">63: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr">64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Flatten())</span>
<span class="linenr">65: </span>  model.add(Flatten())
<span class="linenr">66: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Dense(128, activation='relu'))</span>
<span class="linenr">67: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">128</span>, activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">68: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Dropout(0.5))</span>
<span class="linenr">69: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.5</span>))
<span class="linenr">70: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">model.add(layers.Dense(10, activation='softmax'))</span>
<span class="linenr">71: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">72: </span>  <span style="color: #c678dd;">print</span>(model.summary())
<span class="linenr">73: </span>
<span class="linenr">74: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">3. &#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr">75: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr">76: </span>                optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">77: </span>  train_history=model.fit(x=x_Train4D_normalize,
<span class="linenr">78: </span>                          y=y_TrainOneHot, validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr">79: </span>                          epochs=<span style="color: #da8548; font-weight: bold;">10</span>, batch_size=<span style="color: #da8548; font-weight: bold;">300</span>, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">80: </span>  <span style="color: #c678dd;">print</span>(train_history)
<span class="linenr">81: </span>  show_train_history(<span style="color: #98be65;">"CNN-MNist-acc"</span>, <span style="color: #98be65;">'accuracy'</span>, <span style="color: #98be65;">'val_accuracy'</span>)
<span class="linenr">82: </span>  show_train_history(<span style="color: #98be65;">"CNN-MNist-loss"</span>, <span style="color: #98be65;">'loss'</span>, <span style="color: #98be65;">'val_loss'</span>)
<span class="linenr">83: </span>
<span class="linenr">84: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">4. &#35413;&#20272;&#27169;&#22411;&#28310;&#30906;&#29575;</span>
<span class="linenr">85: </span>  scores = model.evaluate(x_Test4D_normalize, y_TestOneHot)
<span class="linenr">86: </span>  <span style="color: #c678dd;">print</span>(scores[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">87: </span>
<span class="linenr">88: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">5. &#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr">89: </span>  prediction = model.predict_classes(x_Test4D_normalize)
<span class="linenr">90: </span>  <span style="color: #c678dd;">print</span>(prediction[:<span style="color: #da8548; font-weight: bold;">10</span>])
<span class="linenr">91: </span>  plot_images_labels_prediction(<span style="color: #98be65;">"CNN_MNist"</span>, x_Test, y_Test, prediction, idx=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">92: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">93: </span>  p = pd.crosstab(y_Test, prediction, rownames=[<span style="color: #98be65;">'label'</span>], colnames=[<span style="color: #98be65;">'predict'</span>])
<span class="linenr">94: </span>  <span style="color: #c678dd;">print</span>(p)
</pre>
</div>

<pre class="example" id="org3f84c2d">
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 28, 28, 16)        416
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 14, 14, 36)        14436
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 7, 7, 36)          0
_________________________________________________________________
dropout_1 (Dropout)          (None, 7, 7, 36)          0
_________________________________________________________________
flatten_1 (Flatten)          (None, 1764)              0
_________________________________________________________________
dense_1 (Dense)              (None, 128)               225920
_________________________________________________________________
dropout_2 (Dropout)          (None, 128)               0
_________________________________________________________________
dense_2 (Dense)              (None, 10)                1290
=================================================================
Total params: 242,062
Trainable params: 242,062
Non-trainable params: 0
_________________________________________________________________
None

0.9930999875068665
[7 2 1 0 4 1 4 9 5 9]
predict    0     1     2     3    4    5    6     7    8    9
label
0        977     0     0     0    0    0    1     1    1    0
1          0  1133     2     0    0    0    0     0    0    0
2          0     0  1031     0    0    0    0     1    0    0
3          0     0     1  1008    0    0    0     0    1    0
4          0     0     0     0  974    0    1     0    1    6
5          2     0     0     9    0  879    1     0    0    1
6          3     2     0     0    1    4  947     0    1    0
7          0     1     2     2    0    0    0  1022    1    0
8          1     0     1     2    0    1    0     2  965    2
9          2     1     0     3    3    1    0     2    2  995
</pre>


<div id="org41fd219" class="figure">
<p><img src="images/CNN-MNist-acc.png" alt="CNN-MNist-acc.png" /><br />
</p>
<p><span class="figure-number">Figure 67: </span>訓練精確度</p>
</div>


<div id="orgc09e29d" class="figure">
<p><img src="images/CNN-MNist-loss.png" alt="CNN-MNist-loss.png" /><br />
</p>
<p><span class="figure-number">Figure 68: </span>訓練 loss 誤差</p>
</div>


<div id="org1da3128" class="figure">
<p><img src="images/CNN_MNist.png" alt="CNN_MNist.png" /><br />
</p>
<p><span class="figure-number">Figure 69: </span>前 10 筆預測結果</p>
</div>

<p>
在上例中，第一個卷積層輸入了大小為(28, 28, 1)(第<a href="#coderef-MNIST-CNN-1" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-MNIST-CNN-1');" onmouseout="CodeHighlightOff(this, 'coderef-MNIST-CNN-1');">54</a>行)的特徵圖，輸出了大小為(5, 5, 16)的特徵圖，此處卷積層對輸入資料做了 16 種 filter 運算，在這 16 個輸出 channel 中，每個 channel 都包含了一個 5*5 的網格值，這是每個 filter 對於輸入資料的響應圖(response map)，表示該過濾器在輸入資料(影像)中各個位置(滑動)取得的回應值(經由卷積運算)所組成的影像，這就是特徵圖(feature map)這個術語的含義：深度軸中的每個維度都是一個特徵(filter)，而 2D 輸出張量[:,:,n]是第 n 個 filter 對輸入資料回應的 2D 空間圖(response map)。<br />
</p>

<p>
卷積層由兩個關鍵參數定義：<br />
</p>

<ul class="org-ul">
<li>由輸入採樣的區塊大小(size of the patches extracted from the input): 也就是掃描窗格的大小(windows_height, window_width，通常是 3*3 或 5*5)，也就是 filter 大小。<br /></li>
<li>輸出特徵圖的深度(depth of the output feature map)：也就是卷積層過濾器數量，在本例中以 16 開頭，以 36 結束。Keras 的 Conv2D 層傳的參數為：<br /></li>
</ul>

<div class="org-src-container">
<pre class="src src-python">Conv2D(output_depth, (window_height, window_width))
</pre>
</div>

<p>
卷積層的主要工作就是透過在 3D 輸入特徵圖上滑動(sliding)3&times;3 或 5&times;5 的小窗格，在該位置上取得特徵(input__{}depth 最開始是有 RGB 顏色的，但接下來就變成 filter)；其次，將每個 3D 區塊轉換(即卷積運算)成 shape=(output_depth,)的 1D 向量，然後將所有這些向量依照空間上的位置排列重新組裝成 shape=(height, width, output_output)的 3D 輸出特徵圖。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org1b7480b" class="outline-3">
<h3 id="org1b7480b"><span class="section-number-3">8.6.</span> 以 Keras 實作 Cifar-10</h3>
<div class="outline-text-3" id="text-8-6">
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">  1: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">1.  Import Library</span>
<span class="linenr">  2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> cifar10
<span class="linenr">  3: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">  4: </span>  np.random.seed(<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">  5: </span>
<span class="linenr">  6: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">2. &#36039;&#26009;&#28310;&#20633;</span>
<span class="linenr">  7: </span>  (x_img_train,y_label_train),(<span style="color: #dcaeea;">x_img_test</span>,<span style="color: #dcaeea;">y_label_test</span>) = cifar10.load_data()
<span class="linenr">  8: </span>  <span style="color: #dcaeea;">x_img_train_normalize</span> = x_img_train.astype(<span style="color: #98be65;">'float32'</span>) / <span style="color: #da8548; font-weight: bold;">255.0</span>
<span class="linenr">  9: </span>  <span style="color: #dcaeea;">x_img_test_normalize</span> = x_img_test.astype(<span style="color: #98be65;">'float32'</span>) / <span style="color: #da8548; font-weight: bold;">255.0</span>
<span class="linenr"> 10: </span>  <span style="color: #83898d;">'''&#27491;&#35215;&#21270;'''</span>
<span class="linenr"> 11: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 12: </span>  <span style="color: #dcaeea;">y_label_train_OneHot</span> = np_utils.to_categorical(y_label_train)
<span class="linenr"> 13: </span>  <span style="color: #dcaeea;">y_label_test_OneHot</span> = np_utils.to_categorical(y_label_test)
<span class="linenr"> 14: </span>
<span class="linenr"> 15: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">3. &#24314;&#31435;&#27169;&#22411;</span>
<span class="linenr"> 16: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr"> 17: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Dense, Dropout, Activation, Flatten
<span class="linenr"> 18: </span>  <span style="color: #51afef;">from</span> keras.layers <span style="color: #51afef;">import</span> Conv2D, MaxPooling2D, ZeroPadding2D
<span class="linenr"> 19: </span>  <span style="color: #98be65;">'''&#21367;&#31309;&#23652;1&#33287;&#27744;&#21270;&#23652;1'''</span>
<span class="linenr"> 20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38568;&#27231;&#29986;&#29983;32&#20491;3*3&#30340;&#28670;&#37857;&#65292;&#36664;&#20837;&#30340;&#24433;&#20687;&#28858;32*32*3(RGB)</span>
<span class="linenr"> 21: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">32</span>,kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>,<span style="color: #da8548; font-weight: bold;">3</span>), input_shape=(<span style="color: #da8548; font-weight: bold;">32</span>, <span style="color: #da8548; font-weight: bold;">32</span>, <span style="color: #da8548; font-weight: bold;">3</span>),
<span class="linenr"> 22: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>))
<span class="linenr"> 23: </span>  model.add(Dropout(rate=<span style="color: #da8548; font-weight: bold;">0.25</span>)) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27599;&#27425;&#35347;&#32244;&#36845;&#20195;&#26178;&#26371;&#38568;&#27231;&#25918;&#26820;25%&#30340;&#31070;&#32147;&#20803;</span>
<span class="linenr"> 24: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">32</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>),
<span class="linenr"> 25: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>))
<span class="linenr"> 26: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36914;&#34892;&#31532;&#19968;&#27425;&#32302;&#28187;&#21462;&#27171;&#65292;&#23559;&#24433;&#20687;&#32302;&#28858;16*16</span>
<span class="linenr"> 27: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 28: </span>  <span style="color: #98be65;">'''&#21367;&#31309;&#23652;2&#33287;&#27744;&#21270;&#23652;2'''</span>
<span class="linenr"> 29: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#21069;&#19968;&#23652;&#20659;&#36914;&#30340;32&#20491;16*16&#24433;&#20687;&#36681;&#28858;64&#20491;16*16&#24433;&#20687;</span>
<span class="linenr"> 30: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">64</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>), <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#29986;&#29983;64&#20491;&#24433;&#20687;</span>
<span class="linenr"> 31: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>))
<span class="linenr"> 32: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.25</span>))
<span class="linenr"> 33: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">64</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>),
<span class="linenr"> 34: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>))
<span class="linenr"> 35: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20877;&#23559;64&#20491;16*16&#24433;&#20687;&#32302;&#28187;&#21462;&#27171;&#28858;8*8</span>
<span class="linenr"> 36: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 37: </span>  <span style="color: #98be65;">'''&#21367;&#31309;&#23652;3&#33287;&#27744;&#21270;&#23652;3'''</span>
<span class="linenr"> 38: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;64&#20491;8*8&#20491;&#24433;&#20687;&#36681;&#28858;128&#20491;</span>
<span class="linenr"> 39: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">128</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>),
<span class="linenr"> 40: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>))
<span class="linenr"> 41: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.3</span>))
<span class="linenr"> 42: </span>  model.add(Conv2D(filters=<span style="color: #da8548; font-weight: bold;">128</span>, kernel_size=(<span style="color: #da8548; font-weight: bold;">3</span>, <span style="color: #da8548; font-weight: bold;">3</span>),
<span class="linenr"> 43: </span>                   activation=<span style="color: #98be65;">'relu'</span>, padding=<span style="color: #98be65;">'same'</span>))
<span class="linenr"> 44: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20877;&#23559;128&#20491;8*8&#24433;&#20687;&#32302;&#28187;&#21462;&#27171;&#28858;4*4</span>
<span class="linenr"> 45: </span>  model.add(MaxPooling2D(pool_size=(<span style="color: #da8548; font-weight: bold;">2</span>, <span style="color: #da8548; font-weight: bold;">2</span>)))
<span class="linenr"> 46: </span>  <span style="color: #98be65;">'''&#24314;&#31435;&#31070;&#32147;&#32178;&#36335;(&#24179;&#22374;&#23652;&#12289;&#38577;&#34255;&#23652;&#12289;&#36664;&#20986;&#23652;)'''</span>
<span class="linenr"> 47: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;128*4*4&#30340;3&#32173;&#30697;&#38499;&#36681;&#28858;1&#32173;(2048&#20491;float&#25976;&#23383;&#65292;&#23565;&#25033;&#21040;2048&#20491;&#31070;&#32147;&#20803;)</span>
<span class="linenr"> 48: </span>  model.add(Flatten())
<span class="linenr"> 49: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.3</span>))
<span class="linenr"> 50: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">2500</span>, activation=<span style="color: #98be65;">'relu'</span>)) <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#38560;&#34255;&#23652;1&#26377;2500&#20491;&#31070;&#32147;&#20803;</span>
<span class="linenr"> 51: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.3</span>))
<span class="linenr"> 52: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">1500</span>, activation=<span style="color: #98be65;">'relu'</span>)) <span style="color: #5B6268;">### </span><span style="color: #5B6268;">&#38560;&#34255;&#23652;2&#26377;1500&#31070;&#32147;&#20803;</span>
<span class="linenr"> 53: </span>  model.add(Dropout(<span style="color: #da8548; font-weight: bold;">0.3</span>))
<span class="linenr"> 54: </span>  model.add(Dense(<span style="color: #da8548; font-weight: bold;">10</span>, activation=<span style="color: #98be65;">'softmax'</span>)) <span style="color: #5B6268;">### </span><span style="color: #5B6268;">10&#20491;label</span>
<span class="linenr"> 55: </span>  <span style="color: #c678dd;">print</span>(model.summary())
<span class="linenr"> 56: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">4. &#36617;&#20837;&#20043;&#21069;&#35347;&#32244;&#30340;&#27169;&#22411;</span>
<span class="linenr"> 57: </span>  <span style="color: #51afef;">try</span>:
<span class="linenr"> 58: </span>      model.load_weights(<span style="color: #98be65;">"SaveModel/cifarCnnModelnew1.h5"</span>)
<span class="linenr"> 59: </span>      <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#36617;&#20837;&#27169;&#22411;&#25104;&#21151;!&#32380;&#32396;&#35347;&#32244;&#27169;&#22411;"</span>)
<span class="linenr"> 60: </span>  <span style="color: #51afef;">except</span>:
<span class="linenr"> 61: </span>      <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"&#36617;&#20837;&#27169;&#22411;&#22833;&#25943;!&#38283;&#22987;&#35347;&#32244;&#19968;&#20491;&#26032;&#27169;&#22411;"</span>)
<span class="linenr"> 62: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">5. &#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr"> 63: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#35347;&#32244;&#21069;&#20808;&#20197;compile&#35373;&#23450;&#27169;&#22411;, &#35373;&#23450;&#20839;&#23481;&#21253;&#21547;</span>
<span class="linenr"> 64: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">1. &#25613;&#22833;&#20989;&#25976;, 2. &#26368;&#20339;&#21270;&#26041;&#27861;, 3. &#35413;&#20272;&#27169;&#22411;&#30340;&#26041;&#27861;</span>
<span class="linenr"> 65: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,
<span class="linenr"> 66: </span>                optimizer=<span style="color: #98be65;">'adam'</span>, metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr"> 67: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38283;&#22987;&#35347;&#32244;, &#22519;&#34892;50&#27425;&#35347;&#32244;&#36913;&#26399;&#12289;&#27599;&#19968;&#25209;&#27425;500&#31558;&#36039;&#26009;</span>
<span class="linenr"> 68: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">40000&#31558;&#36039;&#26009;&#65292;&#27599;&#19968;&#25209;&#27425;500&#31558;&#36039;&#26009;, &#20998;&#28858;80&#25209;&#27425;&#36914;&#34892;&#35347;&#32244;</span>
<span class="linenr"> 69: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#27599;&#20491;epoch&#35347;&#32244;&#23436;&#24460;&#26371;&#23384;&#19968;&#31558;accuracy&#21644;loss&#35352;&#37636;&#21040;train_history</span>
<span class="linenr"> 70: </span>  train_history=model.fit(x_img_train_normalize, y_label_train_OneHot,
<span class="linenr"> 71: </span>                          validation_split=<span style="color: #da8548; font-weight: bold;">0.2</span>,
<span class="linenr"> 72: </span>                          epochs=<span style="color: #da8548; font-weight: bold;">50</span>, batch_size=<span style="color: #da8548; font-weight: bold;">500</span>, verbose=<span style="color: #da8548; font-weight: bold;">1</span>)
<span class="linenr"> 73: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 74: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_train_history</span>(train_acc,test_acc):
<span class="linenr"> 75: </span>      plt.plot(train_history.history[train_acc])
<span class="linenr"> 76: </span>      plt.plot(train_history.history[test_acc])
<span class="linenr"> 77: </span>      plt.title(<span style="color: #98be65;">'Train History'</span>)
<span class="linenr"> 78: </span>      plt.ylabel(<span style="color: #98be65;">'Accuracy'</span>)
<span class="linenr"> 79: </span>      plt.xlabel(<span style="color: #98be65;">'Epoch'</span>)
<span class="linenr"> 80: </span>      plt.legend([<span style="color: #98be65;">'train'</span>, <span style="color: #98be65;">'test'</span>], loc=<span style="color: #98be65;">'upper left'</span>)
<span class="linenr"> 81: </span>      plt.show()
<span class="linenr"> 82: </span>  show_train_history(<span style="color: #98be65;">'acc'</span>,<span style="color: #98be65;">'val_acc'</span>)
<span class="linenr"> 83: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">6. &#35413;&#20272;&#27169;&#22411;&#28310;&#30906;&#29575;</span>
<span class="linenr"> 84: </span>  scores = model.evaluate(x_img_test_normalize,
<span class="linenr"> 85: </span>                          y_label_test_OneHot, verbose=<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr"> 86: </span>  scores[<span style="color: #da8548; font-weight: bold;">1</span>]
<span class="linenr"> 87: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">7. &#36914;&#34892;&#38928;&#28204;</span>
<span class="linenr"> 88: </span>  prediction=model.predict_classes(x_img_test_normalize)
<span class="linenr"> 89: </span>  <span style="color: #c678dd;">print</span>(prediction[:<span style="color: #da8548; font-weight: bold;">10</span>])
<span class="linenr"> 90: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">8. &#26597;&#30475;&#38928;&#28204;&#32080;&#26524;</span>
<span class="linenr"> 91: </span>  label_dict={<span style="color: #da8548; font-weight: bold;">0</span>:<span style="color: #98be65;">"airplane"</span>,<span style="color: #da8548; font-weight: bold;">1</span>:<span style="color: #98be65;">"automobile"</span>,<span style="color: #da8548; font-weight: bold;">2</span>:<span style="color: #98be65;">"bird"</span>,<span style="color: #da8548; font-weight: bold;">3</span>:<span style="color: #98be65;">"cat"</span>,<span style="color: #da8548; font-weight: bold;">4</span>:<span style="color: #98be65;">"deer"</span>,
<span class="linenr"> 92: </span>              <span style="color: #da8548; font-weight: bold;">5</span>:<span style="color: #98be65;">"dog"</span>,<span style="color: #da8548; font-weight: bold;">6</span>:<span style="color: #98be65;">"frog"</span>,<span style="color: #da8548; font-weight: bold;">7</span>:<span style="color: #98be65;">"horse"</span>,<span style="color: #da8548; font-weight: bold;">8</span>:<span style="color: #98be65;">"ship"</span>,<span style="color: #da8548; font-weight: bold;">9</span>:<span style="color: #98be65;">"truck"</span>}
<span class="linenr"> 93: </span>  <span style="color: #51afef;">import</span> matplotlib.pyplot <span style="color: #51afef;">as</span> plt
<span class="linenr"> 94: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">plot_images_labels_prediction</span>(images,labels,prediction,
<span class="linenr"> 95: </span>                                    idx,num=<span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr"> 96: </span>      fig = plt.gcf()
<span class="linenr"> 97: </span>      fig.set_size_inches(<span style="color: #da8548; font-weight: bold;">12</span>, <span style="color: #da8548; font-weight: bold;">14</span>)
<span class="linenr"> 98: </span>      <span style="color: #51afef;">if</span> num&gt;<span style="color: #da8548; font-weight: bold;">25</span>: num=<span style="color: #da8548; font-weight: bold;">25</span>
<span class="linenr"> 99: </span>      <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, num):
<span class="linenr">100: </span>          ax=plt.subplot(<span style="color: #da8548; font-weight: bold;">5</span>,<span style="color: #da8548; font-weight: bold;">5</span>, <span style="color: #da8548; font-weight: bold;">1</span>+i)
<span class="linenr">101: </span>          ax.imshow(images[idx],cmap=<span style="color: #98be65;">'binary'</span>)
<span class="linenr">102: </span>
<span class="linenr">103: </span>          title=<span style="color: #c678dd;">str</span>(i)+<span style="color: #98be65;">','</span>+label_dict[labels[i][<span style="color: #da8548; font-weight: bold;">0</span>]]
<span class="linenr">104: </span>          <span style="color: #51afef;">if</span> <span style="color: #c678dd;">len</span>(prediction)&gt;<span style="color: #da8548; font-weight: bold;">0</span>:
<span class="linenr">105: </span>              title+=<span style="color: #98be65;">'=&gt;'</span>+label_dict[prediction[i]]
<span class="linenr">106: </span>
<span class="linenr">107: </span>          ax.set_title(title,fontsize=<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">108: </span>          ax.set_xticks([]);ax.set_yticks([])
<span class="linenr">109: </span>          idx+=<span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">110: </span>      plt.show()
<span class="linenr">111: </span>
<span class="linenr">112: </span>  plot_images_labels_prediction(x_img_test,y_label_test,
<span class="linenr">113: </span>                                prediction,<span style="color: #da8548; font-weight: bold;">0</span>,<span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">114: </span>
<span class="linenr">115: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">9. &#26597;&#30475;&#38928;&#28204;&#27231;&#29575;</span>
<span class="linenr">116: </span>  Predicted_Probability=model.predict(x_img_test_normalize)
<span class="linenr">117: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">show_Predicted_Probability</span>(y,prediction,
<span class="linenr">118: </span>                                 x_img,Predicted_Probability,i):
<span class="linenr">119: </span>      <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'label:'</span>,label_dict[y[i][<span style="color: #da8548; font-weight: bold;">0</span>]],
<span class="linenr">120: </span>            <span style="color: #98be65;">'predict:'</span>,label_dict[prediction[i]])
<span class="linenr">121: </span>      plt.figure(figsize=(<span style="color: #da8548; font-weight: bold;">2</span>,<span style="color: #da8548; font-weight: bold;">2</span>))
<span class="linenr">122: </span>      plt.imshow(np.reshape(x_img_test[i],(<span style="color: #da8548; font-weight: bold;">32</span>, <span style="color: #da8548; font-weight: bold;">32</span>,<span style="color: #da8548; font-weight: bold;">3</span>)))
<span class="linenr">123: </span>      plt.show()
<span class="linenr">124: </span>      <span style="color: #51afef;">for</span> j <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">10</span>):
<span class="linenr">125: </span>          <span style="color: #c678dd;">print</span>(label_dict[j]+
<span class="linenr">126: </span>                <span style="color: #98be65;">' Probability:%1.9f'</span>%(Predicted_Probability[i][j]))
<span class="linenr">127: </span>
<span class="linenr">128: </span>  show_Predicted_Probability(y_label_test,prediction,
<span class="linenr">129: </span>                             x_img_test,Predicted_Probability,<span style="color: #da8548; font-weight: bold;">0</span>)
<span class="linenr">130: </span>  show_Predicted_Probability(y_label_test,prediction,
<span class="linenr">131: </span>                             x_img_test,Predicted_Probability,<span style="color: #da8548; font-weight: bold;">3</span>)
<span class="linenr">132: </span>
<span class="linenr">133: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">10. confusion matrix</span>
<span class="linenr">134: </span>  <span style="color: #51afef;">import</span> pandas <span style="color: #51afef;">as</span> pd
<span class="linenr">135: </span>  <span style="color: #c678dd;">print</span>(label_dict)
<span class="linenr">136: </span>  pd.crosstab(y_label_test.reshape(-<span style="color: #da8548; font-weight: bold;">1</span>),prediction,
<span class="linenr">137: </span>              rownames=[<span style="color: #98be65;">'label'</span>],colnames=[<span style="color: #98be65;">'predict'</span>])
<span class="linenr">138: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">11. Save model to JSON</span>
<span class="linenr">139: </span>  model_json = model.to_json()
<span class="linenr">140: </span>  <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(<span style="color: #98be65;">"SaveModel/cifarCnnModelnew.json"</span>, <span style="color: #98be65;">"w"</span>) <span style="color: #51afef;">as</span> json_file:
<span class="linenr">141: </span>      json_file.write(model_json)
<span class="linenr">142: </span>
<span class="linenr">143: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">12. Save Model to YAML</span>
<span class="linenr">144: </span>  model_yaml = model.to_yaml()
<span class="linenr">145: </span>  <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(<span style="color: #98be65;">"SaveModel/cifarCnnModelnew.yaml"</span>, <span style="color: #98be65;">"w"</span>) <span style="color: #51afef;">as</span> yaml_file:
<span class="linenr">146: </span>      yaml_file.write(model_yaml)
<span class="linenr">147: </span>
<span class="linenr">148: </span>  <span style="color: #5B6268;">### </span><span style="color: #5B6268;">13. Save Weight to h5</span>
<span class="linenr">149: </span>  model.save_weights(<span style="color: #98be65;">"SaveModel/cifarCnnModelnew.h5"</span>)
<span class="linenr">150: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">"Saved model to disk"</span>)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org68dd221" class="outline-2">
<h2 id="org68dd221"><span class="section-number-2">9.</span> 辨識手寫數字: MNIST 資料集</h2>
<div class="outline-text-2" id="text-9">
<p>
使用神經網路解決問題分為兩個步驟：「學習」與「推論」，學習指使用訓練資料進行權重參數的學習；而推論指使用學習過的參數進行資料分類。<br />
</p>
</div>

<div id="outline-container-org75a6347" class="outline-3">
<h3 id="org75a6347"><span class="section-number-3">9.1.</span> MNIST 資料集</h3>
<div class="outline-text-3" id="text-9-1">
<p>
MNIST 是機器學習領域中相當著名的資料集，號稱機器學習領域的「Hello world.」，其重要性不言可喻。MNIST 資料集由 0~9 的數字影像構成(如圖<a href="#org59c4f98">70</a>)，共計 60000 張訓練影像、10000 張測試影像。一般的 MMIST 資料集的用法為：使用訓練影像進行學習，再利用學習後的模型預測能否正確分類測試影像。<br />
</p>

<div id="org59c4f98" class="figure">
<p><img src="images/MNIST.jpg" alt="MNIST.jpg" /><br />
</p>
<p><span class="figure-number">Figure 70: </span>MNIST 資料集內容範例</p>
</div>

<p>
準備資料是訓練模型的第一步，基礎資料可以是網上公開的資料集，也可以是自己的資料集。視覺、語音、語言等各種型別的資料在網上都能找到相應的資料集。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org89bd355"></a>準備 MNIST 資料<br />
<div class="outline-text-4" id="text-9-1-1">
<p>
MNIST 數據集來自美國國家標準與技術研究所, National Institute of Standards and Technology (NIST). 訓練集 (training set) 由來自 250 個不同人手寫的數字構成, 其中 50% 是高中學生, 50% 來自人口普查局 (the Census Bureau) 的工作人員. 測試集(test set) 也是同樣比例的手寫數字數據。MNIST 數據集可在 <a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a> 獲取, 它包含了四個部分:<br />
</p>
<ol class="org-ol">
<li>Training set images: train-images-idx3-ubyte.gz (9.9 MB, 解壓後 47 MB, 包含 60,000 個樣本)<br /></li>
<li>Training set labels: train-labels-idx1-ubyte.gz (29 KB, 解壓後 60 KB, 包含 60,000 個標籤)<br /></li>
<li>Test set images: t10k-images-idx3-ubyte.gz (1.6 MB, 解壓後 7.8 MB, 包含 10,000 個樣本)<br /></li>
<li>Test set labels: t10k-labels-idx1-ubyte.gz (5KB, 解壓後 10 KB, 包含 10,000 個標籤)<br /></li>
</ol>

<p>
MNIST 資料集是一個適合拿來當作 TensotFlow 的練習素材，在 Tensorflow 的現有套件中，也已經有內建好的 MNIST 資料集，我們只要在安裝好 TensorFlow 的 Python 環境中執行以下程式碼，即可將 MNIST 資料成功讀取進來。.<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr">1: </span>  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr">2: </span>  <span style="color: #dcaeea;">mnist</span> = tf.keras.datasets.mnist
<span id="coderef-get-keras-mnist" class="coderef-off"><span class="linenr">3: </span>  (x_train, y_train), (<span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_test</span>) = mnist.load_data()</span>
</pre>
</div>
<p>
在訓練模型之前，需要將樣本資料劃分為訓練集、測試集，有些情況下還會劃分為訓練集、測試集、驗證集。由上述程式第<a href="#coderef-get-keras-mnist" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-get-keras-mnist');" onmouseout="CodeHighlightOff(this, 'coderef-get-keras-mnist');">3</a>行可知，下載後的 MNIST 資料分成訓練資料(training data)與測試資料(testing data)，其中 x 為圖片、y為所對應數字。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 2: </span>  <span style="color: #dcaeea;">mnist</span> = tf.keras.datasets.mnist
<span class="linenr"> 3: </span>  (x_train, y_train), (<span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_test</span>) = mnist.load_data()
<span class="linenr"> 4: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=====================================</span>
<span class="linenr"> 5: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#21028;&#26039;&#36039;&#26009;&#24418;&#29376;</span>
<span class="linenr"> 6: </span>  <span style="color: #c678dd;">print</span>(x_train.shape)
<span class="linenr"> 7: </span>  <span style="color: #c678dd;">print</span>(x_test.shape)
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#31532;&#19968;&#20491;label&#30340;&#20839;&#23481;</span>
<span class="linenr"> 9: </span>  <span style="color: #c678dd;">print</span>(y_train[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">10: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#39023;&#31034;&#24433;&#20687;&#20839;&#23481;</span>
<span class="linenr">11: </span>  <span style="color: #51afef;">import</span> matplotlib.pylab <span style="color: #51afef;">as</span> plt
<span class="linenr">12: </span>  <span style="color: #dcaeea;">img</span> = x_train[<span style="color: #da8548; font-weight: bold;">0</span>]
<span class="linenr">13: </span>  plt.imshow(img)
<span class="linenr">14: </span>  plt.savefig(<span style="color: #98be65;">"MNIST-Image.png"</span>)
</pre>
</div>
<pre class="example">
(60000, 28, 28)
(10000, 28, 28)
5
</pre>


<p>
由上述程式輸出結果可以看到載入的 x 為大小為 28*28 的圖片共 60000 張，每一筆 MNIST 資料的照片(x)由 784 個 pixels 組成（28*28），照片內容如圖<a href="#orga59f6a0">36</a>，訓練集的標籤(y)則為其對應的數字(0～9)，此例為 5。<br />
</p>

<div id="org8d1e965" class="figure">
<p><img src="images/MNIST-Image.png" alt="MNIST-Image.png" /><br />
</p>
<p><span class="figure-number">Figure 71: </span>MNIST 影像示例</p>
</div>

<p>
x 的影像資料為灰階影像，每個像素的數值介於 0~255 之間，矩陣裡每一項的資料則是代表每個 pixel 顏色深淺的數值，如下圖<a href="#orgb5a9b4c">72</a>所示：<br />
</p>

<div id="orgb5a9b4c" class="figure">
<p><img src="images/MNIST-Matrix.png" alt="MNIST-Matrix.png" /><br />
</p>
<p><span class="figure-number">Figure 72: </span>MNIST 資料矩陣</p>
</div>

<p>
載入的 y 為所對應的數字 0~9，在這我們要運用 keras 中的 np_utils.to_categorical 將 y 轉成 one-hot 的形式，將他轉為一個 10 維的 vector，例如：我們所拿到的資料為 y=3，經過 np_utils.to_categorical，會轉換為 y=[0,0,0,1,0,0,0,0,0,0]。這部份的轉換程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 3: </span>
<span class="linenr"> 4: </span>  <span style="color: #51afef;">import</span> tensorflow <span style="color: #51afef;">as</span> tf
<span class="linenr"> 5: </span>  <span style="color: #dcaeea;">mnist</span> = tf.keras.datasets.mnist
<span class="linenr"> 6: </span>  (x_train, y_train), (<span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_test</span>) = mnist.load_data()
<span class="linenr"> 7: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">=====================================</span>
<span class="linenr"> 8: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#22294;&#29255;&#36681;&#25563;&#28858;&#19968;&#20491;60000*784&#30340;&#21521;&#37327;&#65292;&#20006;&#19988;&#27161;&#28310;&#21270;</span>
<span class="linenr"> 9: </span>  <span style="color: #dcaeea;">x_train</span> = x_train.reshape(x_train.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">10: </span>  <span style="color: #dcaeea;">x_test</span> = x_test.reshape(x_test.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">11: </span>  <span style="color: #dcaeea;">x_train</span> = x_train.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">12: </span>  <span style="color: #dcaeea;">x_test</span> = x_test.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">13: </span>  <span style="color: #dcaeea;">x_train</span> = x_train/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">14: </span>  <span style="color: #dcaeea;">x_test</span> = x_test/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">15: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;y&#36681;&#25563;&#25104;one-hot encoding</span>
<span class="linenr">16: </span>  <span style="color: #dcaeea;">y_train</span> = np_utils.to_categorical(y_train, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">17: </span>  <span style="color: #dcaeea;">y_test</span> = np_utils.to_categorical(y_test, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">18: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22238;&#20659;&#34389;&#29702;&#23436;&#30340;&#36039;&#26009;</span>
<span class="linenr">19: </span>  <span style="color: #c678dd;">print</span>(y_train[<span style="color: #da8548; font-weight: bold;">0</span>])
<span class="linenr">20: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">21: </span>  np.set_printoptions(precision=<span style="color: #da8548; font-weight: bold;">2</span>)
<span class="linenr">22: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">print(x_train[0])</span>
</pre>
</div>

<pre class="example">
[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]
</pre>
</div>
</li>

<li><a id="orgbf93479"></a>MNIST 的推論處理<br />
<div class="outline-text-4" id="text-9-1-2">
<p>
如圖<a href="#org743cabf">56</a>所示，MNIST 的推論神經網路最前端的輸入層有 784 (\(28*28=784\))個神經元，最後的輸出端有 10 個神經元(\(0~9\)個數字)，至於中間的隠藏層有兩個，第 1 個隱藏層有 50 個神經元，第 2 層有 100 個。此處的 50、100 可以設定為任意數（如，也可以是 128、64）。<br />
</p>

<div id="org93f7df2" class="figure">
<p><img src="images/MNIST-CNN.png" alt="MNIST-CNN.png" /><br />
</p>
<p><span class="figure-number">Figure 73: </span>MNIST-NeuralNet</p>
</div>

<p>
為了完成上述推論，此處定義三個函數：get_data()、init_network()、predict()，其中 init_work()直接讀入作者已經訓練好的網絡權重。在以下這段程式碼中，權重與偏權值的參數會儲存成字典型態的變數。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets.mnist <span style="color: #51afef;">import</span> load_data
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> pickle
<span class="linenr"> 4: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 5: </span>    <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 6: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38450;&#27490;&#28322;&#20986;&#22411;</span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(x):
<span class="linenr"> 8: </span>    <span style="color: #dcaeea;">c</span> = np.<span style="color: #c678dd;">max</span>(x)
<span class="linenr"> 9: </span>    <span style="color: #dcaeea;">exp_x</span> = np.exp(x - c)
<span class="linenr">10: </span>    <span style="color: #dcaeea;">sum_exp_x</span> = np.<span style="color: #c678dd;">sum</span>(exp_x)
<span class="linenr">11: </span>    <span style="color: #51afef;">return</span> exp_x / sum_exp_x
<span class="linenr">12: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_data</span>():
<span class="linenr">13: </span>    (X_train, y_train), (<span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_test</span>) = load_data()
<span class="linenr">14: </span>    <span style="color: #51afef;">return</span> X_test.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>), y_test
<span class="linenr">15: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">init_network</span>():
<span class="linenr">16: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">https://github.com/Bingyy/deep-learning-from-scratch/blob/master/ch03/sample_weight.pkl</span>
<span class="linenr">17: </span>    <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(<span style="color: #98be65;">'/Volumes/Vanessa/MNIST/data/mnist/sample_weight.pkl'</span>, <span style="color: #98be65;">'rb'</span>) <span style="color: #51afef;">as</span> <span style="color: #dcaeea;">f</span>:
<span class="linenr">18: </span>      network = pickle.load(f)
<span class="linenr">19: </span>      <span style="color: #51afef;">return</span> network
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23384;&#20786;&#30340;&#26159;&#32178;&#32097;&#21443;&#25976;&#23383;&#20856;</span>
<span class="linenr">21: </span>  <span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">22: </span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32068;&#21512;&#32178;&#32097;&#27969;&#31243;&#65292;&#29992;&#26044;&#38928;&#28204;</span>
<span id="coderef-MNIST-predict" class="coderef-off"><span class="linenr">24: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(network, x):</span>
<span class="linenr">25: </span>    <span style="color: #dcaeea;">W1</span>, <span style="color: #dcaeea;">W2</span>, <span style="color: #dcaeea;">W3</span> = network[<span style="color: #98be65;">'W1'</span>], network[<span style="color: #98be65;">'W2'</span>], network[<span style="color: #98be65;">'W3'</span>]
<span class="linenr">26: </span>    <span style="color: #dcaeea;">b1</span>, <span style="color: #dcaeea;">b2</span>, <span style="color: #dcaeea;">b3</span> = network[<span style="color: #98be65;">'b1'</span>], network[<span style="color: #98be65;">'b2'</span>], network[<span style="color: #98be65;">'b3'</span>]
<span class="linenr">27: </span>    <span style="color: #dcaeea;">a1</span> = np.dot(x,W1) + b1
<span class="linenr">28: </span>    <span style="color: #dcaeea;">z1</span> = sigmoid(a1)
<span class="linenr">29: </span>    <span style="color: #dcaeea;">a2</span> = np.dot(z1, W2) + b2
<span class="linenr">30: </span>    <span style="color: #dcaeea;">z2</span> = sigmoid(a2)
<span class="linenr">31: </span>    <span style="color: #dcaeea;">a3</span> = np.dot(z2, W3) + b3
<span class="linenr">32: </span>    <span style="color: #dcaeea;">y</span> = softmax(a3) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#39006;&#29992;&#30340;&#26368;&#24460;&#36664;&#20986;&#23652;&#30340;&#28608;&#27963;&#20989;&#25976;</span>
<span class="linenr">33: </span>    <span style="color: #51afef;">return</span> y
<span class="linenr">34: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#32178;&#32097;&#38928;&#28204;</span>
<span class="linenr">35: </span>  <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_test</span> = get_data() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24471;&#21040;&#28204;&#35430;&#25976;&#25818;</span>
<span class="linenr">36: </span>  <span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">37: </span>
<span class="linenr">38: </span>  <span style="color: #dcaeea;">accuracy_cnt</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">39: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #c678dd;">len</span>(X_test)):
<span id="coderef-y-predict" class="coderef-off"><span class="linenr">40: </span>    <span style="color: #dcaeea;">y</span> = predict(network, X_test[i])</span>
<span id="coderef-np-argmax" class="coderef-off"><span class="linenr">41: </span>    <span style="color: #dcaeea;">p</span> = np.argmax(y)</span>
<span class="linenr">42: </span>    np.set_printoptions(precision=<span style="color: #da8548; font-weight: bold;">4</span>, suppress=<span style="color: #a9a1e1;">True</span>)
<span class="linenr">43: </span>    <span style="color: #51afef;">if</span> p == y_test[i]:
<span class="linenr">44: </span>      accuracy_cnt += <span style="color: #da8548; font-weight: bold;">1</span>
<span class="linenr">45: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#28310;&#30906;&#29575;&#65306;'</span>, <span style="color: #c678dd;">str</span>(<span style="color: #c678dd;">float</span>(accuracy_cnt) / <span style="color: #c678dd;">len</span>(X_test)))
</pre>
</div>

<pre class="example">
準確率： 0.0002
</pre>


<p>
上述程式中，predict 程序(第<a href="#coderef-MNIST-predict" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-MNIST-predict');" onmouseout="CodeHighlightOff(this, 'coderef-MNIST-predict');">24</a>)透過矩陣相乘運算完成神經網路的參數傳遞，最後必須進行準確率的評估，程式碼第<a href="#coderef-y-predict" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-y-predict');" onmouseout="CodeHighlightOff(this, 'coderef-y-predict');">40</a>行為神經網路針對輸入圖片的預測結果，所傳回的值為各猜測值的機率陣列，如：[0.0004 0.0011 0.9859 0.0065 0.     0.0007 0.0051 0.     0.0003 0.    ]；而程式碼第<a href="#coderef-np-argmax" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-np-argmax');" onmouseout="CodeHighlightOff(this, 'coderef-np-argmax');">41</a>則是該圖片的應對標籤，np.argmax(y)會傳回 y 的最大值所在順序，若 y=[0,0,0,1,0,0,0,0,0,0]，則傳回 3，藉此計算預測正確的百分比。<br />
</p>
</div>
</li>

<li><a id="org05f7716"></a>Python 與神經網路運算的批次處理<br />
<div class="outline-text-4" id="text-9-1-3">
<p>
前節程式碼中最後以 for 迴圈來逐一處理預測結果與比較，輸入(X)為單一圖片，其處理程序如圖<a href="#orgedcccf5">74</a>所示：<br />
</p>

<div id="orgedcccf5" class="figure">
<p><img src="images/MNIST-single.png" alt="MNIST-single.png" /><br />
</p>
<p><span class="figure-number">Figure 74: </span>MNIST-單一處理架構</p>
</div>

<p>
事實上，在使用批次處理（如一次處理 100 張圖）反而能大幅縮短每張圖片的處理時間，因為多數處理數值運算的函式庫都會針對大型陣列運算進行最佳化，尤其是透過 GPU 來處理時更是如此，這時，傳送單張圖片反而成為效能瓶頸，以批次處理則可減輕匯流排頻寛負擔。若以每次處理 100 張為例，其處理程序則如圖<a href="#org911371e">75</a>所示。<br />
</p>

<div id="org911371e" class="figure">
<p><img src="images/MNIST-batch.png" alt="MNIST-batch.png" /><br />
</p>
<p><span class="figure-number">Figure 75: </span>MNIST-批次處理架構</p>
</div>

<p>
至於批次運算的程式碼如下。<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #51afef;">from</span> keras.datasets.mnist <span style="color: #51afef;">import</span> load_data
<span class="linenr"> 2: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr"> 3: </span>  <span style="color: #51afef;">import</span> pickle
<span class="linenr"> 4: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">sigmoid</span>(x):
<span class="linenr"> 5: </span>    <span style="color: #51afef;">return</span> <span style="color: #da8548; font-weight: bold;">1</span> / (<span style="color: #da8548; font-weight: bold;">1</span> + np.exp(-x))
<span class="linenr"> 6: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38450;&#27490;&#28322;&#20986;&#22411;</span>
<span class="linenr"> 7: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">softmax</span>(x):
<span class="linenr"> 8: </span>    <span style="color: #dcaeea;">c</span> = np.<span style="color: #c678dd;">max</span>(x)
<span class="linenr"> 9: </span>    <span style="color: #dcaeea;">exp_x</span> = np.exp(x - c)
<span class="linenr">10: </span>    <span style="color: #dcaeea;">sum_exp_x</span> = np.<span style="color: #c678dd;">sum</span>(exp_x)
<span class="linenr">11: </span>    <span style="color: #51afef;">return</span> exp_x / sum_exp_x
<span class="linenr">12: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">get_data</span>():
<span class="linenr">13: </span>    (X_train, y_train), (<span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_test</span>) = load_data()
<span class="linenr">14: </span>    <span style="color: #51afef;">return</span> X_test.reshape(<span style="color: #da8548; font-weight: bold;">10000</span>, <span style="color: #da8548; font-weight: bold;">784</span>), y_test
<span class="linenr">15: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">init_network</span>():
<span class="linenr">16: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">https://github.com/Bingyy/deep-learning-from-scratch/blob/master/ch03/sample_weight.pkl</span>
<span class="linenr">17: </span>    <span style="color: #51afef;">with</span> <span style="color: #c678dd;">open</span>(<span style="color: #98be65;">'/Volumes/Vanessa/MNIST/sample_weight.pkl'</span>, <span style="color: #98be65;">'rb'</span>) <span style="color: #51afef;">as</span> <span style="color: #dcaeea;">f</span>:
<span class="linenr">18: </span>      network = pickle.load(f)
<span class="linenr">19: </span>      <span style="color: #51afef;">return</span> network
<span class="linenr">20: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23384;&#20786;&#30340;&#26159;&#32178;&#32097;&#21443;&#25976;&#23383;&#20856;</span>
<span class="linenr">21: </span>  <span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">22: </span>
<span class="linenr">23: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#32068;&#21512;&#32178;&#32097;&#27969;&#31243;&#65292;&#29992;&#26044;&#38928;&#28204;</span>
<span class="linenr">24: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">predict</span>(network, x):
<span class="linenr">25: </span>    <span style="color: #dcaeea;">W1</span>, <span style="color: #dcaeea;">W2</span>, <span style="color: #dcaeea;">W3</span> = network[<span style="color: #98be65;">'W1'</span>], network[<span style="color: #98be65;">'W2'</span>], network[<span style="color: #98be65;">'W3'</span>]
<span class="linenr">26: </span>    <span style="color: #dcaeea;">b1</span>, <span style="color: #dcaeea;">b2</span>, <span style="color: #dcaeea;">b3</span> = network[<span style="color: #98be65;">'b1'</span>], network[<span style="color: #98be65;">'b2'</span>], network[<span style="color: #98be65;">'b3'</span>]
<span class="linenr">27: </span>    <span style="color: #dcaeea;">a1</span> = np.dot(x,W1) + b1
<span class="linenr">28: </span>    <span style="color: #dcaeea;">z1</span> = sigmoid(a1)
<span class="linenr">29: </span>    <span style="color: #dcaeea;">a2</span> = np.dot(z1, W2) + b2
<span class="linenr">30: </span>    <span style="color: #dcaeea;">z2</span> = sigmoid(a2)
<span class="linenr">31: </span>    <span style="color: #dcaeea;">a3</span> = np.dot(z2, W3) + b3
<span class="linenr">32: </span>    <span style="color: #dcaeea;">y</span> = softmax(a3) <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20998;&#39006;&#29992;&#30340;&#26368;&#24460;&#36664;&#20986;&#23652;&#30340;&#28608;&#27963;&#20989;&#25976;</span>
<span class="linenr">33: </span>    <span style="color: #51afef;">return</span> y
<span class="linenr">34: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#20351;&#29992;&#32178;&#32097;&#38928;&#28204;</span>
<span class="linenr">35: </span>  <span style="color: #dcaeea;">X_test</span>, <span style="color: #dcaeea;">y_test</span> = get_data() <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#24471;&#21040;&#28204;&#35430;&#25976;&#25818;</span>
<span class="linenr">36: </span>  <span style="color: #dcaeea;">network</span> = init_network()
<span class="linenr">37: </span>
<span class="linenr">38: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#25209;&#27425;&#34389;&#29702;&#26550;&#27083;</span>
<span class="linenr">39: </span>  <span style="color: #dcaeea;">batch_size</span> = <span style="color: #da8548; font-weight: bold;">100</span>
<span class="linenr">40: </span>  <span style="color: #dcaeea;">accuracy_cnt</span> = <span style="color: #da8548; font-weight: bold;">0</span>
<span class="linenr">41: </span>  <span style="color: #51afef;">for</span> i <span style="color: #51afef;">in</span> <span style="color: #c678dd;">range</span>(<span style="color: #da8548; font-weight: bold;">0</span>, <span style="color: #c678dd;">len</span>(X_test), batch_size):
<span id="coderef-b-mnist-x" class="coderef-off"><span class="linenr">42: </span>    <span style="color: #dcaeea;">x_batch</span> = X_test[i:i+batch_size]</span>
<span class="linenr">43: </span>    <span style="color: #dcaeea;">y_batch</span> = predict(network, x_batch)
<span id="coderef-b-mnist-p" class="coderef-off"><span class="linenr">44: </span>    <span style="color: #dcaeea;">p</span> = np.argmax(y_batch, axis=<span style="color: #da8548; font-weight: bold;">1</span>)</span>
<span class="linenr">45: </span>    accuracy_cnt += np.<span style="color: #c678dd;">sum</span>(p == y_test[i:i+batch_size])
<span class="linenr">46: </span>  <span style="color: #c678dd;">print</span>(<span style="color: #98be65;">'&#28310;&#30906;&#29575;&#65306;'</span>, <span style="color: #c678dd;">str</span>(<span style="color: #c678dd;">float</span>(accuracy_cnt) / <span style="color: #c678dd;">len</span>(X_test)))
</pre>
</div>

<pre class="example">
準確率： 0.9207
</pre>


<p>
上述程式中，第<a href="#coderef-b-mnist-x" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-b-mnist-x');" onmouseout="CodeHighlightOff(this, 'coderef-b-mnist-x');">42</a>行每次取出 100 張圖形檔(X 陣列),第<a href="#coderef-b-mnist-p" class="coderef" onmouseover="CodeHighlightOn(this, 'coderef-b-mnist-p');" onmouseout="CodeHighlightOff(this, 'coderef-b-mnist-p');">44</a>行則取得這 100 筆資料中各筆資料最大值索引值，若以每次 4 筆資料為例，所得的估計值 p 可能為[7 2 1 0]，相對應的正確標籤值則儲存於 y_test[0:4]中，以此進行準確率的計算。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org9795c6c" class="outline-3">
<h3 id="org9795c6c"><span class="section-number-3">9.2.</span> MNIST 資料集:以 DNN Sequential 模型為例</h3>
<div class="outline-text-3" id="text-9-2">
<p>
此處以最簡單的 DNN (deep neural network) 作為範例。以 Keras 的核心為模型，應用最常使用 Sequential 模型。藉由.add()我們可以一層一層的將神經網路疊起。在每一層之中我們只需要簡單的設定每層的大小(units)與激活函數(activation function)。需要特別記得的是：第一層要記得寫輸入的向量大小、最後一層的 units 要等於輸出的向量大小。在這邊我們最後一層使用的激活函數(activation function)為 softmax。<br />
相對應程式碼如下：<br />
</p>
<div class="org-src-container">
<pre class="src src-python"><span class="linenr"> 1: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36617;&#20837;&#36039;&#26009;</span>
<span class="linenr"> 2: </span>  <span style="color: #51afef;">from</span> keras.datasets <span style="color: #51afef;">import</span> mnist
<span class="linenr"> 3: </span>  <span style="color: #51afef;">from</span> keras.utils <span style="color: #51afef;">import</span> np_utils
<span class="linenr"> 4: </span>
<span class="linenr"> 5: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">load_data</span>():
<span class="linenr"> 6: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#36617;&#20837;minst&#30340;&#36039;&#26009;</span>
<span class="linenr"> 7: </span>    (x_train, y_train), (<span style="color: #dcaeea;">x_test</span>, <span style="color: #dcaeea;">y_test</span>) = mnist.load_data()
<span class="linenr"> 8: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;&#22294;&#29255;&#36681;&#25563;&#28858;&#19968;&#20491;60000*784&#30340;&#21521;&#37327;&#65292;&#20006;&#19988;&#27161;&#28310;&#21270;</span>
<span class="linenr"> 9: </span>    <span style="color: #dcaeea;">x_train</span> = x_train.reshape(x_train.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">10: </span>    <span style="color: #dcaeea;">x_test</span> = x_test.reshape(x_test.shape[<span style="color: #da8548; font-weight: bold;">0</span>], <span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>)
<span class="linenr">11: </span>    <span style="color: #dcaeea;">x_train</span> = x_train.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">12: </span>    <span style="color: #dcaeea;">x_test</span> = x_test.astype(<span style="color: #98be65;">'float32'</span>)
<span class="linenr">13: </span>    <span style="color: #dcaeea;">x_train</span> = x_train/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">14: </span>    <span style="color: #dcaeea;">x_test</span> = x_test/<span style="color: #da8548; font-weight: bold;">255</span>
<span class="linenr">15: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#23559;y&#36681;&#25563;&#25104;one-hot encoding</span>
<span class="linenr">16: </span>    <span style="color: #dcaeea;">y_train</span> = np_utils.to_categorical(y_train, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">17: </span>    <span style="color: #dcaeea;">y_test</span> = np_utils.to_categorical(y_test, <span style="color: #da8548; font-weight: bold;">10</span>)
<span class="linenr">18: </span>    <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#22238;&#20659;&#34389;&#29702;&#23436;&#30340;&#36039;&#26009;</span>
<span class="linenr">19: </span>    <span style="color: #51afef;">return</span> (x_train, y_train), (x_test, y_test)
<span class="linenr">20: </span>
<span class="linenr">21: </span>  <span style="color: #51afef;">import</span> numpy <span style="color: #51afef;">as</span> np
<span class="linenr">22: </span>  <span style="color: #51afef;">from</span> keras.models <span style="color: #51afef;">import</span> Sequential
<span class="linenr">23: </span>  <span style="color: #51afef;">from</span> keras.layers.core <span style="color: #51afef;">import</span> Dense,Activation
<span class="linenr">24: </span>  <span style="color: #51afef;">from</span> keras.optimizers <span style="color: #51afef;">import</span>  Adam
<span class="linenr">25: </span>
<span class="linenr">26: </span>  <span style="color: #51afef;">def</span> <span style="color: #c678dd;">build_model</span>():<span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#24314;&#31435;&#27169;&#22411;</span>
<span class="linenr">27: </span>    <span style="color: #dcaeea;">model</span> = Sequential()
<span class="linenr">28: </span>    <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#23559;&#27169;&#22411;&#30090;&#36215;</span>
<span class="linenr">29: </span>    model.add(Dense(input_dim=<span style="color: #da8548; font-weight: bold;">28</span>*<span style="color: #da8548; font-weight: bold;">28</span>,units=<span style="color: #da8548; font-weight: bold;">500</span>,activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">30: </span>    model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">500</span>,activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">31: </span>    model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">500</span>,activation=<span style="color: #98be65;">'relu'</span>))
<span class="linenr">32: </span>    model.add(Dense(units=<span style="color: #da8548; font-weight: bold;">10</span>,activation=<span style="color: #98be65;">'softmax'</span>))
<span class="linenr">33: </span>    model.summary()
<span class="linenr">34: </span>    <span style="color: #51afef;">return</span> model
<span class="linenr">35: </span>
<span class="linenr">36: </span>  <span style="color: #5B6268;"># </span><span style="color: #5B6268;">&#38283;&#22987;&#35347;&#32244;&#27169;&#22411;&#65292;&#27492;&#34389;&#20351;&#29992;&#20102;Adam&#20570;&#28858;&#25105;&#20497;&#30340;&#20778;&#21270;&#22120;&#65292;loss function&#36984;&#29992;&#20102;categorical_crossentropy&#12290;</span>
<span class="linenr">37: </span>  (x_train,y_train),(<span style="color: #dcaeea;">x_test</span>,<span style="color: #dcaeea;">y_test</span>)=load_data()
<span class="linenr">38: </span>  model = build_model()
<span class="linenr">39: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#38283;&#22987;&#35347;&#32244;&#27169;&#22411;</span>
<span class="linenr">40: </span>  model.<span style="color: #c678dd;">compile</span>(loss=<span style="color: #98be65;">'categorical_crossentropy'</span>,optimizer=<span style="color: #98be65;">"adam"</span>,metrics=[<span style="color: #98be65;">'accuracy'</span>])
<span class="linenr">41: </span>  model.fit(x_train,y_train,batch_size=<span style="color: #da8548; font-weight: bold;">100</span>,epochs=<span style="color: #da8548; font-weight: bold;">20</span>)
<span class="linenr">42: </span>  <span style="color: #5B6268;">#</span><span style="color: #5B6268;">&#39023;&#31034;&#35347;&#32244;&#32080;&#26524;</span>
<span class="linenr">43: </span>  score = model.evaluate(x_train,y_train)
<span class="linenr">44: </span>  <span style="color: #c678dd;">print</span> (<span style="color: #98be65;">'\nTrain Acc:'</span>, score[<span style="color: #da8548; font-weight: bold;">1</span>])
<span class="linenr">45: </span>  score = model.evaluate(x_test,y_test)
<span class="linenr">46: </span>  <span style="color: #c678dd;">print</span> (<span style="color: #98be65;">'\nTest Acc:'</span>, score[<span style="color: #da8548; font-weight: bold;">1</span>])
</pre>
</div>

<pre class="example" id="org8b5b3f0">
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_1 (Dense)              (None, 500)               392500
_________________________________________________________________
dense_2 (Dense)              (None, 500)               250500
_________________________________________________________________
dense_3 (Dense)              (None, 500)               250500
_________________________________________________________________
dense_4 (Dense)              (None, 10)                5010
=================================================================
Total params: 898,510
Trainable params: 898,510
Non-trainable params: 0
_________________________________________________________________
Epoch 1/20

  100/60000 [..............................] - ETA: 2:55 - loss: 2.2917 - acc: 0.1300
  800/60000 [..............................] - ETA: 25s - loss: 1.6424 - ACM: 0.5362
.......
16300/60000 [=======&gt;......................] - ETA: 4s - loss: 0.3752 - acc: 0.8898
17000/60000 [=======&gt;......................] - ETA: 4s - loss: 0.3681 - acc: 0.8916
.......
50600/60000 [========================&gt;.....] - ETA: 0s - loss: 0.2232 - acc: 0.9335
51300/60000 [========================&gt;.....] - ETA: 0s - loss: 0.2220 - acc: 0.9338
.......
59700/60000 [============================&gt;.] - ETA: 0s - loss: 0.2078 - acc: 0.9377
60000/60000 [==============================] - 5s 81us/step - loss: 0.2074 - acc: 0.9379
Epoch 2/20

  100/60000 [..............................] - ETA: 5s - loss: 0.0702 - acc: 0.9800
......
60000/60000 [==============================] - 5s 77us/step - loss: 0.0832 - acc: 0.9740
Epoch 3/20
......
Epoch 29/20

   32/60000 [..............................] - ETA: 1:10
 1440/60000 [..............................] - ETA: 3s
......
58496/60000 [============================&gt;.] - ETA: 0s
60000/60000 [==============================] - 2s 34us/step

Train Acc: 0.9981666666666666

   32/10000 [..............................] - ETA: 0s
 1568/10000 [===&gt;..........................] - ETA: 0s
 3104/10000 [========&gt;.....................] - ETA: 0s
 4640/10000 [============&gt;.................] - ETA: 0s
 6176/10000 [=================&gt;............] - ETA: 0s
 7680/10000 [======================&gt;.......] - ETA: 0s
 9184/10000 [==========================&gt;...] - ETA: 0s
10000/10000 [==============================] - 0s 33us/step

Test Acc: 0.9823
</pre>
</div>
</div>
</div>

<div id="outline-container-orga084c0a" class="outline-2">
<h2 id="orga084c0a"><span class="section-number-2">10.</span> 名詞解釋</h2>
<div class="outline-text-2" id="text-10">
<p>
本節資料來源: <a href="https://kknews.cc/zh-tw/tech/b4zkbom.html">主流的深度學習模型有哪些？</a><br />
</p>
<ul class="org-ul">
<li>perceptron: 單一神經元，可視為一層的neural network<br /></li>
<li>MPL: Multi-layer perceptron: 多層的perceptron，也可視為neural network<br /></li>
<li>Shallow neural netwokrs: 一般來說有1-2個隱藏層的neural network就可稱為多層，準確來說叫淺層<br /></li>
<li>Deep learning: 隨著隱藏層增加，更深的神經網路(一般來說超過5層)就都叫深度學習<br /></li>
<li>「深度」只是一個商業概念，很多時候工業界把3層隱藏層也叫做「深度學習」<br /></li>
<li>在機器學習領域的約定俗成是，名字中有深度(Deep)的網絡僅代表其有超過5-7層的隱藏層。<br /></li>
</ul>
</div>

<div id="outline-container-orgb2db0c6" class="outline-3">
<h3 id="orgb2db0c6"><span class="section-number-3">10.1.</span> Supervised neural networks</h3>
<div class="outline-text-3" id="text-10-1">
</div>
<ol class="org-ol">
<li><a id="org5167499"></a>ANN (Artificial Neural Network) / DNN (Deep Neural Networks)<br />
<div class="outline-text-4" id="text-10-1-1">
<p>
神經網絡的基礎模型是感知機(Perceptron)，因此神經網絡也可以叫做多層感知機(Multi-layer Perceptron)，簡稱MLP。單層感知機叫做感知機，多層感知機(MLP)≈人工神經網絡(ANN)。<br />
</p>

<p>
那麼多層到底是幾層？一般來說有1-2個隱藏層的神經網絡就可以叫做多層，準確的說是(淺層)神經網絡(Shallow Neural Networks)。隨著隱藏層的增多，更深的神經網絡(一般來說超過5層)就都叫做深度學習(DNN)。<br />
</p>

<p>
然而，「深度」只是一個商業概念，很多時候工業界把3層隱藏層也叫做「深度學習」，所以不要在層數上太較真。在機器學習領域的約定俗成是，名字中有深度(Deep)的網絡僅代表其有超過5-7層的隱藏層。<br />
</p>
</div>
</li>

<li><a id="org1671e70"></a>RNN: Recurrent Neural Network，Recursive neural networks。<br />
<div class="outline-text-4" id="text-10-1-2">
<p>
雖然很多時候我們把這兩種網絡都叫做RNN，但事實上這兩種網路的結構事實上是不同的。而我們常常把兩個網絡放在一起的原因是：它們都可以處理有序列的問題，比如時間序列等。<br />
  舉個最簡單的例子，我們預測股票走勢用RNN就比普通的DNN效果要好，原因是股票走勢和時間相關，今天的價格和昨天、上周、上個月都有關係。而RNN有「記憶」能力，可以「模擬」數據間的依賴關係(Dependency)。<br />
</p>
</div>

<ol class="org-ol">
<li><a id="org8dc6ce5"></a>LSTM:<br />
<div class="outline-text-5" id="text-10-1-2-1">
<p>
為了加強這種RNN的「記憶能力」，人們開發各種各樣的變形體，如非常著名的Long Short-term Memory(LSTM)，用於解決「長期及遠距離的依賴關係」。如下圖所示，左邊的小圖是最簡單版本的循環網絡，而右邊是人們為了增強記憶能力而開發的LSTM。<br />
</p>

<div id="org8fae5c9" class="figure">
<p><img src="images/3r5o0000r126proo7o8q.jpg" alt="3r5o0000r126proo7o8q.jpg" width="500" /><br />
</p>
<p><span class="figure-number">Figure 76: </span>LSTM</p>
</div>
</div>
</li>
<li><a id="org6402dd2"></a>Bi-directional RNN:<br />
<div class="outline-text-5" id="text-10-1-2-2">
<p>
另一個循環網絡的變種 - 雙向循環網絡(Bi-directional RNN)也是現階段自然語言處理和語音分析中的重要模型。開發雙向循環網絡的原因是語言/語音的構成取決於上下文，即「現在」依託於「過去」和「未來」。單向的循環網絡僅著重於從「過去」推出「現在」，而無法對「未來」的依賴性有效的建模。<br />
</p>
</div>
</li>
</ol>
</li>

<li><a id="org3a9e236"></a>卷積網絡(Convolutional Neural Networks)<br />
<div class="outline-text-4" id="text-10-1-3">
<p>
卷積運算是一種數學計算，和矩陣相乘不同，卷積運算可以實現稀疏相乘和參數共享，可以壓縮輸入端的維度。和普通DNN不同，CNN並不需要為每一個神經元所對應的每一個輸入數據提供單獨的權重。<br />
</p>
<p width="500">
<img src="images/CNN-1.png" alt="CNN-1.png" width="500" /><br />
以上圖為例，卷積、池化的過程將一張圖片的維度進行了壓縮。從圖示上我們不難看出卷積網絡的精髓就是適合處理結構化數據，而該數據在跨區域上依然有關聯。<br />
</p>

<p>
應用場景：雖然我們一般都把CNN和圖片聯繫在一起，但事實上CNN可以處理大部分格狀結構化數據(Grid-like Data)。舉個例子，圖片的像素是二維的格狀數據，時間序列在等時間上抽取相當於一維的的格狀數據，而視頻數據可以理解為對應視頻幀寬度、高度、時間的三維數據。<br />
</p>
</div>
</li>
</ol>
</div>

<div id="outline-container-org80a8ecb" class="outline-3">
<h3 id="org80a8ecb"><span class="section-number-3">10.2.</span> Unsupervised Pre-trained Neural Networks</h3>
<div class="outline-text-3" id="text-10-2">
</div>
<ol class="org-ol">
<li><a id="org5325c72"></a>Deep Generative Models<br />
<ol class="org-ol">
<li><a id="org24b550a"></a>Boltzmann Machines, Restricted Boltzmann Machines<br /></li>
<li><a id="orgfaf48ee"></a>Deep Belief Neural Networks<br /></li>
<li><a id="org9cbb083"></a>Generative Adversarial Networks<br />
<div class="outline-text-5" id="text-10-2-1-3">
<p>
簡單的說，GAN訓練兩個網絡：1. 生成網絡用於生成圖片使其與訓練數據相似 2. 判別式網絡用於判斷生成網絡中得到的圖片是否是真的是訓練數據還是偽裝的數據。生成網絡一般有逆卷積層(deconvolutional layer)而判別網絡一般就是上文介紹的CNN。下圖左邊是生成網絡，右邊是判別網絡，相愛相殺。<br />
</p>
</div>
</li>
</ol>
</li>
</ol>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-%E5%A4%9A%E5%B1%A4%E6%84%9F%E7%9F%A5%E6%A9%9F-multilayer-perceptron-mlp-%E5%90%AB%E8%A9%B3%E7%B4%B0%E6%8E%A8%E5%B0%8E-ee4f3d5d1b41">機器學習- 神經網路(多層感知機 Multilayer perceptron, MLP) 含倒傳遞( Backward propagation)詳細推導</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://kknews.cc/code/o8m4gpq.html">五分鐘理解深度學習中激活函數以及不同激活函數的使用場景</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53">What the Hell is Perceptron?</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-%E5%A4%9A%E5%B1%A4%E6%84%9F%E7%9F%A5%E6%A9%9F-multilayer-perceptron-mlp-%E9%81%8B%E4%BD%9C%E6%96%B9%E5%BC%8F-f0e108e8b9af">機器學習- 神經網路(多層感知機 Multilayer perceptron, MLP)運作方式</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://kknews.cc/code/o8qaazo.html">機器學習：Python測試線性可分性的方法</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://s-top.github.io/blog/2018-04-19-svm">Support Vector Machine（全面推导）</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7" role="doc-backlink">7</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://chih-sheng-huang821.medium.com/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-%E5%A4%9A%E5%B1%A4%E6%84%9F%E7%9F%A5%E6%A9%9F-multilayer-perceptron-mlp-%E5%90%AB%E8%A9%B3%E7%B4%B0%E6%8E%A8%E5%B0%8E-ee4f3d5d1b41">機器學習- 神經網路(多層感知機 Multilayer perceptron, MLP) 含倒傳遞( Backward propagation)詳細推導</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.8" class="footnum" href="#fnr.8" role="doc-backlink">8</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://ithelp.ithome.com.tw/articles/10191820">Day 06：處理影像的利器 &#x2013; 卷積神經網路(Convolutional Neural Network) </a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.9" class="footnum" href="#fnr.9" role="doc-backlink">9</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">An Intuitive Explanation of Convolutional Neural Networks</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.10" class="footnum" href="#fnr.10" role="doc-backlink">10</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://medium.com/@syshen/%E5%85%A5%E9%96%80%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-2-d694cad7d1e5">入門深度學習 — 2</a><br />
</p></div></div>

<div class="footdef"><sup><a id="fn.11" class="footnum" href="#fnr.11" role="doc-backlink">11</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
<a href="https://www.books.com.tw/products/0010822932">Deep learning 深度學習必讀：Keras 大神帶你用 Python 實作</a><br />
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: Yung-Chin Yen</p>
<p class="date">Created: 2022-07-03 Sun 20:38</p>
</div>
</body>
</html>
